{"patent_number": "US-6356990-B1", "publication_id": 72940080, "family_id": 23972785, "publication_date": "2002-03-12", "titles": [{"lang": "EN", "text": "Set-associative cache memory having a built-in set prediction array"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50283136\"><p>A set-associative cache memory having a built-in set prediction array is disclosed. The cache memory can be accessed via an effective address having a tag field, a line index field, and a byte field. The cache memory includes a directory, a memory array, a translation lookaside buffer, and a set prediction array. The memory array is associated with the directory such that each tag entry within the directory corresponds to a cache line within the memory array. In response to a cache access by an effective address, the translation lookaside buffer determines whether or not the data associated with the effective address is stored within the memory array. The set prediction array is built-in within the memory array such that an access to a line entry within the set prediction array can be performed in a same access cycle as an access to a cache line within the memory array.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6356990-B1-CLM-00001\" num=\"1\"><claim-text>1. A cache memory that can be accessed via an effective address having a tag field, a line index field, and a byte field, said cache memory comprising:</claim-text><claim-text>a directory; </claim-text><claim-text>a memory array associated with said directory, wherein said memory array includes a plurality of congruence classes and each of said congruence classes includes a plurality of sets, wherein said memory array includes a set prediction array integrated within said memory array via an association of a line entry within said set prediction array to a congruence class within said memory array such that said line entry within said set prediction array and said congruence class are accessed in a same cycle, wherein said line entry within said set prediction array includes a plurality of set prediction resolution slots, the number of said plurality of set prediction resolution slots does not correspond to the number of sets within each of said congruence classes, and one of said plurality of prediction resolution slots is selected during said cache access by a subset of bits from said tag field; and </claim-text><claim-text>a translation lookaside buffer for determining whether or not said cache memory stores data associated with an effective address, in response to said cache access by said effective address. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6356990-B1-CLM-00002\" num=\"2\"><claim-text>2. The cache memory according to <claim-ref idref=\"US-6356990-B1-CLM-00001\">claim 1</claim-ref>, wherein the number of said prediction resolution slots is greater than the number of sets within each of said congruence classes.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6356990-B1-CLM-00003\" num=\"3\"><claim-text>3. The cache memory according to <claim-ref idref=\"US-6356990-B1-CLM-00001\">claim 1</claim-ref>, wherein bits within said set prediction slots are assigned according to the most-recently used set of a previous cycle.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6356990-B1-CLM-00004\" num=\"4\"><claim-text>4. The cache memory according to <claim-ref idref=\"US-6356990-B1-CLM-00001\">claim 1</claim-ref>, wherein each of said plurality of set prediction resolution slots includes a six-transistor storage cell.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6356990-B1-CLM-00005\" num=\"5\"><claim-text>5. The cache memory according to <claim-ref idref=\"US-6356990-B1-CLM-00001\">claim 1</claim-ref>, wherein said subset of bits from said tag field are the least significant bits from said tag field.</claim-text></claim>"}, {"num": 6, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6356990-B1-CLM-00006\" num=\"6\"><claim-text>6. A processor having a cache memory that can be accessed by utilizing an effective address, wherein said effective address includes a byte field, a line field, and an effective page number field, said processor comprising:</claim-text><claim-text>a plurality of execution units; and </claim-text><claim-text>a cache memory coupled to said plurality of execution units, wherein said cache memory includes </claim-text><claim-text>a directory; </claim-text><claim-text>a memory array associated with said directory, wherein said memory array includes a plurality of congruence classes and each of said congruence classes includes a plurality of sets, wherein said memory array includes a set prediction array integrated within said memory array via an association of a line entry within said set prediction array to a congruence class within said memory array such that said line entry within said set prediction array and said congruence class are accessed in a same cycle, wherein said line entry within said set prediction array includes a plurality of set prediction resolution slots, the number of said plurality of set prediction resolution slots does not correspond to the number of sets within each of said congruence classes, and one of said plurality of prediction resolution slots is selected during said cache access by a subset of bits from said tag field; and </claim-text><claim-text>a translation lookaside buffer for determining whether or not said cache memory stores data associated with an effective address, in response to said cache access by said effective address. </claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6356990-B1-CLM-00007\" num=\"7\"><claim-text>7. The processor according to <claim-ref idref=\"US-6356990-B1-CLM-00006\">claim 6</claim-ref>, wherein the number of said prediction resolution slots is greater than the number of sets within each of said congruence classes.</claim-text></claim>"}, {"num": 8, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6356990-B1-CLM-00008\" num=\"8\"><claim-text>8. The processor according to <claim-ref idref=\"US-6356990-B1-CLM-00006\">claim 6</claim-ref>, wherein bits within said set prediction slots are assigned according to the most-recently used set of a previous cycle.</claim-text></claim>"}, {"num": 9, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6356990-B1-CLM-00009\" num=\"9\"><claim-text>9. The processor according to <claim-ref idref=\"US-6356990-B1-CLM-00006\">claim 6</claim-ref>, wherein each of said set prediction resolution slots includes a six-transistor storage cell.</claim-text></claim>"}, {"num": 10, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6356990-B1-CLM-00010\" num=\"10\"><claim-text>10. The processor according to <claim-ref idref=\"US-6356990-B1-CLM-00006\">claim 6</claim-ref>, wherein said subset of bits from said tag field are the least significant bits from said tag field.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53527006\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Technical Field</p><p>The present invention relates to cache memories in general, and in particular to set-associative cache memories. Still more particularly, the present invention relates to a set-associative cache memory having a built-in set prediction array.</p><p>2. Description of the Prior Art</p><p>In order to increase the speed of access to data stored within a system memory, modern data processing systems generally maintain the most recently used data in a high-speed memory known as a cache memory. This cache memory has multiple cache lines, with several bytes per cache line for storing information in contiguous addresses within the system memory. In addition, each cache line has an associated tag that typically identifies a partial address of a corresponding page of the system memory. Because the information within each cache line may come from different pages of the system memory, the tag provides a convenient way to identify to which page of the system memory a cache line belongs.</p><p>In order to improve cache hit ratio, set-associative cache memories are commonly utilized in most data processing systems. Generally speaking, for a set-associative cache memory, a higher number of sets typically yields a higher hit ratio. However, most set-associative cache memories employ a so-called \u201clate select\u201d scheme that requires all sets within a set-associative cache memory to be activated simultaneously, and a set-select multiplexor to select one of the sets in which the \u201chit\u201d cache line resided. Thus, more power will be consumed as the number of sets increases.</p><p>One solution to the above-mentioned problem is to use a set prediction scheme. By allowing only one wordline to be activated based on a prediction method to select only one of the many sets, the set prediction scheme saves power and also improves access time. A bit called the most-recently used (MRU) bit is usually used to predict one of the sets. The MRU bit typically requires to access a translation lookaside buffer (TLB) before the MRU bit can be sent from the TLB to the memory array of the cache memory. The access path to the TLB is known to be one of critical paths for cache accesses such that additional cycles are commonly required. However, because of its relatively large size, the TLB usually cannot be placed at close proximity to the memory array. As a result, the total cache access time of a set-associative cache memory increases with the sizes of its TLB and memory arrays. Consequently, it would be desirable to provide an improved set-associative cache memory with fast access time and yet low power consumption.</p><h4>SUMMARY OF THE INVENTION</h4><p>In accordance with a preferred embodiment of the present invention, a cache memory can be accessed via an effective address having a tag field, a line index field, and a byte field. The cache memory includes a directory, a memory array, a translation lookaside buffer, and a set prediction array. The memory array is associated with the directory such that each tag entry within the directory corresponds to a cache line within the memory array. In response to a cache access by an effective address, the translation lookaside buffer determines whether or not the data associated with the effective address is stored within the memory array. The set prediction array is built-in within the memory array such that an access to a line entry within the set prediction array can be performed in a same access cycle as an access to a cache line within the memory array.</p><p>All objects, features, and advantages of the present invention will become apparent in the following detailed written description.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The invention itself, as well as a preferred mode of use, further objects, and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein:</p><p>FIG. 1 is a block diagram of a processor in which a preferred embodiment of the present invention may be incorporated;</p><p>FIG. 2 is a block diagram of a set-associative cache memory according to the prior art;</p><p>FIG. 3 is a block diagram of a set-associative cache memory having a built-in set prediction array, in accordance with a preferred embodiment of the present invention; and</p><p>FIG. 4 is a circuit diagram of the set prediction array from FIG. 3, in accordance with a preferred embodiment of the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF A PREFERRED EMBODIMENT</h4><p>The present invention may be executed in a variety of processors having a cache memory. The cache memory may be, for example, a primary cache, a secondary cache, or a tertiary cache.</p><p>Referring now to the drawings and in particular to FIG. 1, there is depicted a block diagram of a processor in which a preferred embodiment of the present invention may be incorporated. Within a processor <b>10</b>, a bus interface unit <b>12</b> is coupled to a data cache <b>13</b> and an instruction cache <b>14</b>. Both data cache <b>13</b> and instruction cache <b>14</b> are high speed set-associative caches which enable processor <b>10</b> to achieve a relatively fast access time to a subset of data or instructions previously transferred from a main memory (not shown). Instruction cache <b>14</b> is further coupled to an instruction unit <b>11</b> which fetches instructions from instruction cache <b>14</b> during each execution cycle.</p><p>Processor <b>10</b> also includes three execution units, namely, an integer unit <b>15</b>, a load/store unit <b>16</b>, and a floating-point unit <b>17</b>. Each of execution units <b>15</b>-<b>17</b> can execute one or more classes of instructions, and all execution units <b>15</b>-<b>17</b> can operate concurrently during each processor cycle. After execution has terminated, execution units <b>15</b>-<b>17</b> store data results to a respective rename buffer, depending upon the instruction type. Then, any one of execution units <b>15</b>-<b>17</b> signals a completion unit <b>20</b> that the instruction unit has been finished. Finally, instructions are completed in program order by transferring result data from the respective rename buffer to a general purpose register <b>18</b> or a floating-point register <b>19</b>.</p><p>With reference now to FIG. 2, there is illustrated a block diagram of a set-associative cache memory according to the prior art. The set-associative cache memory may be a data cache or an instruction cache. As shown, the <b>5</b> set-associative cache memory includes a memory array <b>21</b> along with a directory <b>22</b>, both of which are divided into two ways, namely, way 0 and way 1. Each cache line in memory array <b>21</b> has a corresponding row in directory <b>22</b>. The data or instructions portion of a cache line is maintained in memory array <b>21</b> while the tag portion of the same cache line is maintained in directory <b>22</b>.</p><p>The information stored in memory array <b>21</b> may be accessed by an effective address <b>20</b>. Effective address <b>20</b> includes a tag field, a line index field, and a byte field. The tag field of effective address <b>20</b> is utilized to provide cache \u201chit\u201d information as will be described infra. The line index field of effective address <b>20</b> is utilized to select a specific cache line within memory array <b>21</b>, and the byte field of effective address <b>20</b> is utilized to index a specific byte within the selected cache line.</p><p>Also shown in FIG. 2 is a translation lookaside buffer (TLB) <b>23</b> for translating an effective address to a corresponding real address. Specifically, TLB <b>23</b> translates the page number portion of an effective address to a corresponding real page number. For example, the tag field of effective address <b>20</b> (which is part of the page number of effective address <b>20</b>) is sent to TLB <b>23</b> to be translated to a corresponding real page number. This real page number is utilized for comparison with a tag of the selected cache line from directory <b>22</b> in order to determine whether there is a cache \u201chit\u201d or \u201cmiss.\u201d Incidentally, a match between a tag from one of two ways in directory <b>22</b> and the real page number implies a cache \u201chit.\u201d The cache \u201chit\u201d signal (i.e., Sel<sub>\u2014</sub>0 or Sel<sub>\u2014</sub>1) is also sent to a set-select multiplexor <b>25</b> to select an output from one of the two ways of memory array <b>21</b>.</p><p>Referring now to FIG. 3, there is illustrated a block diagram of a set-associative cache memory having a built-in set prediction array, in accordance with a preferred embodiment of the present invention. Similar to the set-associative cache memory from FIG. 2, set-associative cache memory in FIG. 3 includes a memory array <b>31</b> and a directory <b>32</b>, both of which are divided into two ways-way 0 and way 1, both of which have multiple cache lines. The data or instructions portion of a cache line is maintained in memory array <b>31</b>, while the tag portion of the same cache line is maintained in directory <b>32</b>. The set-associative cache memory may be a data cache, an instruction cache, or an unified cache storing both data and instructions.</p><p>The information stored in memory array <b>31</b> may be accessed by an effective address <b>30</b>. Effective address <b>30</b> includes a tag field for providing cache \u201chit\u201d information, a line index field for selecting a specific cache line, and a byte field for indexing a specific byte within the selected cache line. For a four-Kbyte page having 32 cache lines, the line index field can be, for example, five bits wide and the byte field can be, for example, seven bits wide.</p><p>In accordance with a preferred embodiment of the present invention, a set prediction array <b>36</b> is incorporated (or built-in) within memory array <b>31</b>. There are two inputs to set prediction array <b>36</b>, namely, a line index input <b>38</b> and a set prediction input <b>37</b>. Line index input <b>38</b> can be obtained by decoding the bits in line index field of effective address <b>30</b> via a decoder <b>35</b>. Set prediction input <b>37</b> can be obtained by decoding the last several bits, known as set prediction bits (indicated by the shaded area), of the tag field of effective address <b>30</b> via decoder <b>35</b>.</p><p>There are many line entries within set prediction array <b>36</b>. Each line entry within set prediction array <b>36</b> has a corresponding cache line in way 0 as well as way 1 of memory array <b>31</b>. In addition, each line entry within set prediction array <b>36</b> includes multiple set prediction slots. The selection of a line entry within set prediction array <b>36</b> is determined by line index input <b>38</b>, and the selection of a set prediction slot within a selected line entry is determined by set prediction input <b>37</b>. The number of set prediction bits defines the number of set prediction slots in each line entry, which also defines the set prediction resolution for set prediction array <b>36</b>. For example, if two set prediction bits are utilized, then the number of set prediction slots (or the set prediction resolution) is four, and if three set prediction bits are utilized, then the number of set prediction slots (or the set prediction resolution) is eight. Furthermore, each set prediction slot has, in this two-way cache memory implementation, one bit to indicate a predicted set. For example, a \u201c0\u201d bit in a set prediction slot indicates way 0 and a \u201c1\u201d bit indicates way 1. Additional bits will be required for a higher set associativity.</p><p>In addition, a set prediction line <b>39</b> that indicates the bit stored in the selected set prediction slot is utilized to generate a miss predict signal. The miss predict signal along with the \u201chit\u201d or \u201cmiss\u201d signal are then sent to some control circuitry (not shown), as it is known by those skilled in the relevant art.</p><p>With set prediction array <b>36</b> being incorporated within memory array <b>31</b>, any access to set prediction array <b>36</b> can be performed at the same cycle (concurrently) as sending an address, such as a line index, to memory array <b>31</b>. In other words, set prediction array <b>36</b> does not have to be accessed before sending the line index to memory array <b>31</b>. Thus, when compared to the prior art cache memory shown in FIG. 2, the cache memory shown in FIG. 3 has less total cache access latency.</p><p>With reference now to FIG. 4, there is illustrated a circuit diagram of set prediction array <b>36</b>, in accordance with a preferred embodiment of the present invention. As shown, a line entry within set prediction array <b>36</b> is selected according to a decoded line index from line index input <b>38</b> (from FIG. <b>3</b>). This line entry corresponds to a cache line from way 0 and a cache line from way 1, both having the same line index as the line entry. At the same time, X0-Xn signals are decoded signals from set prediction input <b>37</b> (from FIG. 3) to select a set prediction slot of the selected line entry. Specifically, only one of the X0-Xn signals is activated to select a corresponding set prediction slot, and the bit stored within the selected set prediction slot determines whether the corresponding selected cache line from way 0 or the corresponding selected cache line from way 1 should be activated. Each of the set prediction slots, such as set prediction slots <b>41</b><i>a</i>-<b>41</b><i>n</i>, can be implemented by a standard six-transistor storage cell having a true (T) output and a complement (C) output, as it is well-known in the art. The bits within set prediction slots <b>41</b><i>a</i>-<b>41</b><i>n </i>can be assigned in accordance with the most-recently used (MRU) set of the previous cycle.</p><p>Using the two cache lines shown in FIG. 4 as an example, if X0 signal line is activated, then only one of the two cache lines will be activated, depending on the bit stored within set prediction slot <b>41</b><i>a. </i></p><p>As has been described, the present invention provides a set-associative cache memory having a built-in set prediction array. Since the built-in set prediction array is used, the set-associative cache memory of the present to invention acts like a direct-mapped cache memory that is known to be relatively faster than a typical set-associative cache memory. In addition, the set-associative cache memory of the present invention provides a faster cache access with lower power and low \u201cmiss\u201d rate. Although a two-way set-associative cache memory is utilized to illustrate the present invention, it is understood by those skilled in the relevant art that the principle of the present invention can also be applicable to cache memories with higher set-associativities.</p><p>While the invention has been particularly shown and described with reference to a preferred embodiment, it will be understood by those skilled in the art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Naoaki", "last_name": "Aoki", "name": ""}, {"first_name": "Sang Hoo", "last_name": "Dhong", "name": ""}, {"first_name": "Nobuo", "last_name": "Kojima", "name": ""}, {"first_name": "Joel Abraham", "last_name": "Silberman", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTERNATIONAL BUSINESS MACHINES CORPORATION"}, {"first_name": "", "last_name": "TWITTER, INC.", "name": ""}, {"first_name": "", "last_name": "INTERNATIONAL BUSINESS MACHINES CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/08"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMUS"}, {"label": "G06F  12/10        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711205"}, {"primary": false, "label": "711128"}, {"primary": false, "label": "711E12063"}, {"primary": false, "label": "711E12018"}], "ecla_classes": [{"label": "S06F212:6082"}, {"label": "G06F  12/10L4P"}, {"label": "G06F  12/08B10"}], "cpc_classes": [{"label": "G06F2212/6082"}, {"label": "Y02D  10/00"}, {"label": "G06F  12/1054"}, {"label": "G06F  12/0864"}, {"label": "Y02D  10/00"}, {"label": "G06F  12/0864"}, {"label": "G06F  12/1054"}, {"label": "G06F2212/6082"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "2000-02-02", "application_date": "2000-02-02", "family_members": [{"ucid": "US-6356990-B1", "titles": [{"lang": "EN", "text": "Set-associative cache memory having a built-in set prediction array"}]}]}