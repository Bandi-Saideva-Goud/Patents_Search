{"patent_number": "US-6304954-B1", "publication_id": 72817113, "family_id": 26742710, "publication_date": "2001-10-16", "titles": [{"lang": "EN", "text": "Executing multiple instructions in multi-pipelined processor by dynamically switching memory ports of fewer number than the pipeline"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72627213\"><p>Three parallel instruction processing pipelines of a microprocessor share two data memory ports for obtaining operands and writing back results. Since a significant proportion of the instructions of a typical computer program do not require reading operands from the memory, the probability is high that at least one of any three program instructions to be executed at the same time need not fetch an operand from memory. The two memory ports are thus connected at any given time with the two of the three pipelines which are processing instructions that require memory access, the pipeline without access to the memory processing an instruction that does not need it. To do so, the added third pipeline need not have all the same resources as the other two pipelines, so its stages are made to have a reduced capability in order to save space and reduce power consumption. The stages of the three pipelines are also dynamically interchanged in response to the specific combination of three instructions being processed at the same time, in order to increase the rate of processing a large number of instructions.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00001\" num=\"1\"><claim-text>1. A microprocessor, comprising:</claim-text><claim-text>an instruction decoding stage configured to provide N sequences of decoded instructions as a plurality of N-instruction sets, each instruction of each N-instruction set corresponding to a respective one of the N sequences of decoded instructions; </claim-text><claim-text>a data memory having M ports, M being less than N; </claim-text><claim-text>N multi-staged pipelines, each configured to receive from the instruction decoding stage and process in parallel a corresponding instruction of each N-instruction set; and </claim-text><claim-text>a control circuit, configured to dynamically connect the M ports to M pipelines for each N-instruction set, such that each pipeline receiving an instruction requiring access to the data memory is connected to a port, and each pipeline receiving an instruction not requiring access to the data memory is not connected to a port. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00002\" num=\"2\"><claim-text>2. The microprocessor of claim <b>1</b>, wherein at least one of the pipelines is configured to have no access to the data memory.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00003\" num=\"3\"><claim-text>3. The microprocessor of claim <b>1</b>, wherein:</claim-text><claim-text>N is equal to three, and </claim-text><claim-text>M is equal to two. </claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00004\" num=\"4\"><claim-text>4. The microprocessor of claim <b>1</b>, wherein an instruction not requiring access to the data memory comprises a jump instruction.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00005\" num=\"5\"><claim-text>5. The microprocessor of claim <b>1</b>, wherein an instruction not requiring access to the data memory comprises an instruction to move data between two of a plurality of registers.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00006\" num=\"6\"><claim-text>6. The microprocessor of claim <b>1</b>, wherein an instruction not requiring access to the data memory comprises an instruction to perform at least one of an arithmetic operation and a logic operations on data in two of a plurality of registers.</claim-text></claim>"}, {"num": 7, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00007\" num=\"7\"><claim-text>7. The microprocessor of claim <b>1</b>, wherein:</claim-text><claim-text>M of the N multi-staged pipelines comprise a first address generation stage and a first instruction execution stage; and </claim-text><claim-text>N-M of the N multi-stage pipelines comprise a second address generation stage and a second instruction execution stage, the second address generation stage and the second instruction execution stage being less capable, smaller, and consuming less power than the first address generation stage and the first instruction execution stage. </claim-text></claim>"}, {"num": 8, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00008\" num=\"8\"><claim-text>8. The microprocessor of claim <b>1</b>, further comprising:</claim-text><claim-text>a set of registers, wherein </claim-text><claim-text>each of the N multi-stage pipelines being configured to read from and write to the set of registers. </claim-text></claim>"}, {"num": 9, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00009\" num=\"9\"><claim-text>9. A microprocessor, comprising:</claim-text><claim-text>N multi-staged pipelines configured to operate in parallel, each having a series of stages such that instructions are executed in steps, each step corresponding to a stage, N being at least 2; </claim-text><claim-text>M data memory access ports, M being at least one less than N; </claim-text><claim-text>a switching circuit configured to individually connect and disconnect the M data memory access ports and a selected stage of a corresponding one of the multi-staged pipelines based on a need for data memory access by an instruction being executed by the corresponding one of the multi-staged pipelines; wherein </claim-text><claim-text>at least one of the multi-staged pipelines not connected to a data memory access port is configured to execute instructions that do not require data memory access in parallel with instructions being executed by other multi-staged pipelines that are connected to data memory access ports. </claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00010\" num=\"10\"><claim-text>10. The microprocessor of claim <b>9</b>, further comprising:</claim-text><claim-text>P arithmetic logic units, P being at least one less than N, wherein; </claim-text><claim-text>the switching circuit is further configured to individually connect and disconnect the P arithmetic logic units and a selected stage of a corresponding one of the multi-staged pipelines based on a need for an arithmetic logic unit by an instruction being executed by the corresponding one of the multi-staged pipelines; wherein </claim-text><claim-text>at least one of the multi-staged pipelines not connected to an arithmetic logic unit is configured to execute instructions that do not require an arithmetic logic unit in parallel with instructions being executed by other multi-staged pipelines that are connected to arithmetic logic units. </claim-text></claim>"}, {"num": 11, "parent": 10, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00011\" num=\"11\"><claim-text>11. The microprocessor of claim <b>10</b>, further comprising:</claim-text><claim-text>a move unit, wherein, </claim-text><claim-text>the switching circuit is further configured to connect the move unit to one of the at least one of the multi-staged pipelines not connected to an arithmetic logic unit based on a need to execute an instruction that moves data between a register and one of another register and a memory. </claim-text></claim>"}, {"num": 12, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00012\" num=\"12\"><claim-text>12. A method of processing N parallel sequences of computer instructions through N pipelines having M data memory access ports, M being at least one less than N, comprising:</claim-text><claim-text>categorizing each instruction of an N-instruction set of instructions, as a first instruction that requires access to a memory through a data memory access port or a second instruction that does not require access to a memory through a data memory access port; </claim-text><claim-text>identifying at least one pipeline for each N-instruction set of instructions for executing instructions categorized as second instructions by the categorizing step; </claim-text><claim-text>coupling the M data memory access ports to all pipelines not identified by the identifying step; </claim-text><claim-text>calculating a memory address for each instruction categorized as a first instruction by the categorizing step; </claim-text><claim-text>reading data from the memory address calculated in the calculating step for each instruction categorized as a first instruction by the categorizing step; </claim-text><claim-text>executing each first instruction with the data read by the reading step in a pipeline provided with access to a data memory access point in the coupling step; and </claim-text><claim-text>executing each second instruction in the at least one pipeline identified by the identifying step. </claim-text></claim>"}, {"num": 13, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00013\" num=\"13\"><claim-text>13. The method of claim <b>12</b>, further comprising:</claim-text><claim-text>reserving at least one of the pipelines for executing instructions categorized as second instructions in the categorizing step. </claim-text></claim>"}, {"num": 14, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00014\" num=\"14\"><claim-text>14. The method of claim <b>12</b>, wherein the executing each first instruction step and the executing each second instruction step are performed concurrently.</claim-text></claim>"}, {"num": 15, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00015\" num=\"15\"><claim-text>15. The method of claim <b>12</b>, wherein the executing each second instruction step comprises executing each second instruction concurrently with the calculating step to calculate an address of another instruction.</claim-text></claim>"}, {"num": 16, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00016\" num=\"16\"><claim-text>16. The method of claim <b>12</b>, wherein N is equal to 3 and M is equal to 2.</claim-text></claim>"}, {"num": 17, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00017\" num=\"17\"><claim-text>17. The method of claim <b>12</b>, wherein at least one second instruction is a jump instruction that when executed calculates an address that is used to designate another N-instruction set of instructions for subsequent processing.</claim-text></claim>"}, {"num": 18, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304954-B1-CLM-00018\" num=\"18\"><claim-text>18. The method of claim <b>12</b>, wherein at least one second instruction is one of an arithmetic operation and a logic operation that when executed operates on data in two registers.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54740153\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>CROSS-REFERENCE TO RELATED APPLICATION</h4><p>This is a continuation-in-part of patent application Ser. No. 09/062,804, filed Apr. 20, 1998, now abandoned which application is expressly incorporated herein in its entirety by this reference.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>This invention relates generally to the architecture of microprocessors, and, more specifically, to the structure and use of parallel instruction processing pipelines.</p><p>A multi-staged pipeline is commonly used in a single integrated circuit chip microprocessor. A different step of the processing of an instruction is accomplished at each stage of the pipeline. For example, one important stage generates from the instruction and other data to which the instruction points, such as data stored in registers on the same chip, an address of the location in memory where an operand is stored that needs to be retrieved for processing. A next stage of the pipeline typically reads the memory at that address in order to fetch the operand and make it available for use within the pipeline. A subsequent stage typically executes the instruction with the operand and any other data pointed to by the instruction. The execution stage includes an arithmetic logic unit (ALU) that uses the operand and other data to perform either a calculation, such as addition, subtraction, multiplication, or division, or a logical combination according to what is specified by the instruction. The result is then, in a further stage, written back into either the memory or into one of the registers. As one instruction is moved along the pipeline, another is right behind it so that, in effect, a number of instructions equal to the number of stages in the pipeline are being simultaneously processed.</p><p>Two parallel multi-stage pipelines are also commonly used. Two instructions may potentially be processed in parallel as they move along the two pipelines. When some interdependency exists between two successive instructions, however, they often cannot be started along the pipeline at the same time. One such interdependency is where the second instruction requires for its execution the result of the execution of the first instruction. Each of the two pipelines has independent access to a data memory through one of two ports for reading operands from it and writing results of the instruction execution back into it. The memory accessed by the pipelines is generally on the integrated circuit chip as cache memory, which, in turn, accesses other semiconductor memory, a magnetic disk drive or other mass storage that is outside of the single microprocessor integrated circuit chip.</p><p>It continues to be a goal of processor design to increase the rate at which program instructions are processed. Therefore, it is the primary object of the present invention to provide an architecture for a pipelined microprocessor that makes possible an increased instruction processing throughput.</p><p>It is another object of the present invention to provide such a pipelined microprocessor that minimizes the additional amount of power consumed and integrated circuit space required to obtain a given increase the rate of processing program instructions.</p><h4>SUMMARY OF THE INVENTION</h4><p>These and additional objects are accomplished by the various aspects of the present invention, wherein, briefly and generally, according to one such aspect, three or more parallel pipelines are provided without having to use more than two data memory ports to retrieve operands or store the results of the instruction processing. It is undesirable to use a memory with more than two ports, or to use two or more separate data memories, since the complexity, power consumed and space taken by such many ported memories is highly undesirable. It has been recognized, as part of the present invention, that since a significant proportion of the individual instructions of most programs do not need access to data memory in order to be executed, an extra pipeline without such access still results in a significant increase in processing speed without a disproportionate increase in the amount of circuitry or power consumption. In a specific implementation of this aspect of the invention, three instructions are processed in parallel in three pipelines at one time so long as one of those instructions does not need access to the data memory. The two ports of the data memory are made available to the two pipelines processing instructions that need access to the data memory, while the third pipeline processes an instruction that does not require such access.</p><p>A three pipeline architecture is preferred. If all three instructions queued for entry into the three pipelines at one time all need access to the data memory, then one of the instructions is held. In this case, the third pipeline is not fully utilized for at least one cycle, but this does not occur excessively because of the high proportion of instructions in most operating systems and programs that do not need access to the data memory. A fourth pipeline may further be added for use with a two port data memory if that proportion of instructions not needing data memory access is high enough to justify the added integrated circuit space and power consumed by the additional pipeline circuitry.</p><p>According to another aspect of the present invention, the third pipeline is made simpler than the other two, since there is also a high enough proportion of instructions that do not need the complex, high performance pipeline stages normally supplied for processing the most complex instructions. A preferred form of the present invention includes two pipelines with stages having the normal full capability while at least some of the stages of the third pipeline are significantly simplified. In a specific implementation of this aspect of the present invention, the address generation stage of the third pipeline is made simpler than the address generation stage of the other two pipelines. The third address generation stage may, for example, be especially adapted to only calculate instruction addresses in response to jump instructions. The ALU of the execution stage of the third pipeline is also, in a specific implementation, made to be much simpler than the ALUs of the other two pipelines. The third ALU, for example, may be dedicated to executing move instructions. The simpler third pipeline stages minimize the extra integrated circuit space and power required of the third pipeline. Yet, a significant increase in through put of processing instructions is achieved.</p><p>According to a further aspect of the present invention, individual ones of the multiple stages of each of the pipelines are interconnectable with each other between the pipelines in order to take advantage of a multiple pipelined architecture where the capability and functions performed by a given stage of one pipeline is different than that of the same stage of another pipeline. This allows the pipelines to be dynamically configured according to the need of each instruction. Stages capable of processing a given instruction are connected together without having to use stages with excessive capability in most cases. One instruction, for example, may require a full capability address generator but then only needs the simplest ALU, so the instruction is routed through these two stages. For another instruction, as another example, no address generator may be necessary but a full capability ALU may be required.</p><p>The ideal operation which is sought to be achieved is to have three pipelines operating on three instructions all the time with no more circuitry (and thus no more space or power consumption) than is absolutely necessary to process each instruction. Each of the various aspects of the present invention contributes to moving closer to that ideal, the most improvement being obtained when all of these aspects of the present invention are implemented together.</p><p>Additional objects, advantages, and features of the present invention will become apparent from the following description of its preferred embodiments, which description should be take in conjunction with the accompanying drawings.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>FIG. 1 is a block diagram of a prior art two pipeline microprocessor architecture;</p><p>FIG. 2 illustrates, in a simplified form, a three pipeline microprocessor architecture utilizing the various aspects of the present invention;</p><p>FIG. 3 illustrates the major stages of a detailed example of a three pipeline microprocessor utilizing the various aspects of the present invention;</p><p>FIG. 4 is a block diagram showing additional details of the ID and IS stages of the microprocessor of FIG. 3;</p><p>FIGS. 5A and 5B illustrate the structure of the queue register and form of data stored in it, respectively, if the ID stage shown in FIG. 4;</p><p>FIG. 6 is a block diagram illustrating the AG and OF stages of the microprocessor of FIG. 3;</p><p>FIG. 7 is a block diagram of the EX and WB stages of the pipeline of FIG. 3;</p><p>FIG. 8 is a flowchart illustrating a preferred operation of the multiple pipeline microprocessor shown in FIGS. 3-7;</p><p>FIG. 9 is a flowchart showing the operation of the block <b>411</b> of the flowchart of FIG. 8; and</p><p>FIG. 10 is a flowchart showing the operation of the block <b>413</b> of the flowchart of FIG. <b>8</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>As background, a prior all architecture of a single chip microprocessor with two pipelines, each having multiple stages, is described with respect to FIG. <b>1</b>. What is shown in FIG. 1, is provided on a single integrated circuit chip. That includes some on-board memory, usually cache memory, such as an instruction cache <b>11</b> and a data cache <b>13</b>. The instruction cache <b>11</b> stores instructions that are frequently being executed, and the data cache <b>13</b> stores data that is frequently being accessed to execute the instructions. The instruction and data cache memories <b>11</b> and <b>13</b> are sometimes combined into one memory but more often are provided as physically separate memories. Access to dynamic random access memory (DRAM), magnetic disk drives, and other forms of mass storage, currently all off the microprocessor chip, is not shown in the figures of the present application but is operated with the on-board cache memories in a standard manner.</p><p>Addresses of instructions and memory are generated in a circuit <b>15</b> by an instruction fetch block <b>17</b>. A main component of the instruction fetch block <b>17</b> is a program counter that increments from a starting address within the cache memory <b>11</b> through successive addresses in order to serially read out in a circuit <b>19</b> successive instructions stored at those addresses. The instruction fetch block <b>17</b> is also responsive to an address in a circuit <b>21</b> to jump out of order to a specified beginning address from which the program counter then counts until another jump address is received.</p><p>The instructions read one at a time out of the cache memory <b>11</b> are stored in a buffer <b>23</b> that decodes them sufficiently so that one instruction is passed through circuits <b>25</b> and another instruction is passed through circuits <b>27</b> at the same time. The circuits <b>25</b> and <b>27</b> are the beginnings of the parallel pipeline stages, with the instruction buffer <b>23</b> providing an initial stage to each of these pipelines. Latches <b>29</b> and <b>31</b> are included in the paths of each of the two pipelines for temporarily storing the instructions that are being separately processed by the two pipelines.</p><p>Each of these instructions is also connected with a control unit <b>33</b> having outputs that are connected (not shown for simplicity) to most of the other blocks of the pipeline in order to control their operation. The control unit <b>33</b> decodes each of the instructions presented in the circuits <b>25</b> and <b>27</b> in order to specify how each of the stages of the two pipelines is to operate to execute that instruction. For example, a signal from the control unit <b>33</b> normally latches the instructions in the circuits <b>25</b> and <b>27</b> in the respective latches <b>29</b> and <b>31</b>. However, there are circumstances where both instructions are not allowed to proceed down the pipelines at the same time. For example, the instruction in the circuit <b>27</b> may require a result of the execution of the instruction and circuit <b>25</b>. Thus, the instruction in the circuit <b>27</b> is not stored in the latch <b>31</b> at the same time as the instruction is stored in the latch <b>29</b>. Rather, the instruction in the circuit <b>27</b> is entered into a pipeline in a subsequent cycle, so the result of the execution of the first instruction is available to it when required.</p><p>Each of the pipelines includes an address generation stage, their primary components being adders <b>35</b> and <b>37</b>. The purpose of each of these adders is to calculate an address in the data cache memory <b>13</b> where an operand is to be found that is necessary to execute the instruction. The address is calculated by each adder from information provided in the instruction itself or data read from one of several registers <b>39</b> that are also provided as part of the microprocessor integrated circuit. According to one architectural standard, eight such registers r<b>1</b> through r<b>8</b> are included, while more registers are used in other architectural standards. An instruction often requires data to be read from at least one of the registers in the course of calculating the address.</p><p>The calculated memory addresses of the two instructions being processed in parallel are then stored in latches <b>41</b> and <b>43</b>. These addresses are then applied to the data cache memory <b>13</b> through interfaces <b>45</b> and <b>47</b> to retrieve operands from the address locations in circuits <b>49</b> and <b>51</b>. These operands are then temporarily stored in latches <b>53</b> and <b>55</b> at the beginning of the next stage of the pipelines.</p><p>This next stage is the execution stage that includes two ALUs <b>57</b> and <b>59</b>. The operands read from the data cache memory <b>13</b>, other data stored in the registers <b>39</b>, and data provided in the instruction itself are all used by the ALUs <b>57</b> and <b>59</b> in the manner specified by the individual instructions being executed in the respective pipelines. The results of the arithmetic operations performed in the execution stage are then temporarily stored in latches <b>61</b> and <b>63</b>, at the beginning of the next stage of each pipeline.</p><p>That final stage includes blocks <b>65</b> and <b>67</b> for writing back the result of the execution into either the cache memory <b>13</b> or one of the registers <b>39</b>. The pipeline utilizing the block <b>65</b> writes to the cache memory <b>13</b> through its port A, and the second pipeline, through the block <b>67</b>, writes to the cache memory <b>13</b> through its port B.</p><p>It will be recognized that the prior art two pipeline architecture, as illustrated in FIG. 1, includes the maximum capability in each stage that may be required to process each instruction. As a result, many instructions do not use that capability. For example, any instruction that does not need to fetch an operand from the data cache <b>13</b> will skip over the address generation and operand fetch stages of adders <b>35</b> or <b>37</b> and memory interfaces <b>45</b> or <b>47</b>. Other instructions need very little arithmetic operation so that the capability of the ALUs <b>57</b> or <b>59</b> are only partially utilized to execute those types of instructions.</p><p>As part of the present invention, these characteristics of the operation of a two pipelined microprocessor have been recognized to allow the addition of a third pipeline without having to provide access to the data cache memory <b>13</b> by that third pipeline. The addition of another port to the data cache <b>13</b> requires a different memory that, when implemented, takes much more space and power than is practical. Thus, according to the present invention, a third pipeline without data memory access is utilized to process in parallel with the two main pipelines those instructions that do not need such access. And since all the instructions do not need the full power of a typical high-performance address generation stage adder or execution stage ALU, the third pipeline also implements these stages with a less complex, lower performance adder and ALU that are sufficient for a large proportion of instructions being processed. These instructions are then implemented in much less space and with the use of much less power than the full performance stages provided in the other two pipelines.</p><p>In addition, the present invention provides for switching stages between pipelines so that a given instruction has just enough resources that it needs for its processing but without the need to consume additional unnecessary resources.</p><p>An implementation of these various aspects of the present invention are conceptually illustrated in the three pipeline microprocessor of FIG. 2, wherein blocks performing functions substantially as in the prior art system of FIG. 1 are given the same reference numbers. A first stage of the pipelines, common to all three, is an instruction decoding (ID) stage including an instruction queue <b>71</b>. In this stage, the serial stream of instructions being read out of the instruction cache <b>11</b> are separated into their individual instructions, which are usually of variable length. Processing and predicting of target addresses of branch instructions as part of the instruction fetch <b>17</b> are given in copending patent application entitled \u201cImproved Branch Prediction Mechanism,\u201d of Sean P. Cummings et al., filed Sep. 4, 1998, which application is incorporated herein in its entirety by this reference.</p><p>A next stage, also common to each of the three pipelines, is an instruction issue (IS) stage including a circuit block <b>73</b> that receives the instructions from the queue <b>71</b> and outputs three at a time on circuits <b>75</b>, <b>77</b> and <b>79</b>. These instructions are individually applied to respective latches <b>81</b>, <b>83</b> and <b>85</b> at the beginning of the next stage of the processing, the address generation (AG) stage. These instructions are also received by a control unit <b>87</b> that decodes them and provides control signals to other stages and blocks of the microprocessor in order to configure them appropriately to provide the proper resources and operation to process each set of instructions.</p><p>The address generation stage of each of the three pipelines includes respective adders <b>89</b>, <b>91</b> and <b>93</b>. The adders <b>89</b> and <b>91</b> are full performance adders that are capable of generating an address for any of the known set of instructions, while the adder <b>93</b> is made to have less capability but remaining capable of performing the adder function with some subset of the fill set of instructions that are frequently encountered. This allows the third adder <b>93</b> to be efficiently utilized with the other two. In a specific implementation, the third adder <b>93</b> is especially designed to respond to jump instructions for calculating an address to which the instruction fetch unit <b>17</b> should jump. The jump address calculated by the third adder <b>93</b>, after being delayed for two operational cycles by being moved through latches <b>95</b> and <b>97</b> in sequence, is sent through circuits <b>99</b> as an address to the instruction fetch block <b>17</b>.</p><p>In the implementations of the various aspects of the present invention being described with respect to the drawings, instructions are issued by the block <b>73</b> so that three successive instructions are stored in order by the latches <b>81</b>, <b>83</b> and <b>85</b>. The adder <b>89</b> is provided with an input switch <b>101</b> that allows it to be connected to receive an instruction from either of the registers <b>81</b> or <b>83</b>. Similarly, the adder <b>91</b> has an input connected by a switch <b>103</b> to the instructions in either of the latches <b>83</b> or <b>85</b>. The third, less complex, adder <b>93</b> has its input connectable through a switch <b>105</b> to the instructions in any of the three latches <b>81</b>, <b>83</b> or <b>85</b>. Thus, it can be seen that two of the three instructions stored in the latches <b>81</b>, <b>83</b> and <b>85</b> requiring a full capability adder may be connected to the adders <b>89</b> and <b>91</b> while the remaining instruction, if it can be processed by the third adder <b>93</b>, is connectable to the adder <b>93</b> from any of the latches <b>81</b>. <b>83</b> or <b>85</b>.</p><p>The outputs of the full adders <b>89</b> and <b>91</b> are addresses that are stored in latches <b>107</b> and <b>109</b> of the next stage, the operand fetch (OF) stage. These addresses are applied through respective interface circuits <b>111</b> and <b>113</b> to the ports A and B of the data cache memory <b>13</b>. The resulting operands read from the memory <b>13</b> are stored in respective latches <b>115</b> and <b>117</b> in the next stage of each of the two primary pipelines. In the third pipeline, the outputs of the latches <b>81</b>, <b>83</b> and <b>85</b> are moved through the operand fetch and into the execution stages through latches <b>119</b> and <b>121</b>.</p><p>The execution units of the two primary pipelines include full capability ALUs <b>123</b> and <b>125</b>. The third pipeline includes a logic unit <b>127</b> having lesser capability, in this example, being dedicated to moving data from one location to another. Each of the ALUs <b>123</b> and <b>125</b> and the move unit <b>127</b> have accompanying input switches <b>129</b>, <b>131</b> and <b>133</b>, respectively. Each of the switches <b>129</b>, <b>131</b> and <b>133</b> of the execution stage, as well as the adder switches <b>101</b>, <b>103</b> and <b>105</b> of the address generation stages, are set by signals from the control unit <b>87</b> that result from decoding the instructions being executed.</p><p>The input of the move unit <b>127</b> is connectable through its switch <b>133</b> to either of the two operands read from the memory <b>13</b> and stored in the latches <b>115</b> and <b>117</b>, or to any of the three instructions being processed in parallel and stored in the latches <b>121</b>. The switch <b>131</b> connects the input to the full capability ALU <b>125</b> to any one of four of those same inputs, connection to the instruction which has come through the register <b>81</b> being omitted. Similarly, the ALU <b>123</b> is connectable through its input switch <b>129</b> to four of the same five inputs, the instruction coming through the register <b>85</b> being omitted. The switches <b>129</b>, <b>131</b> and <b>133</b> allow an ALU of an appropriate capability to be matched for executing an instruction that needs that capability and, with high probability, matched within an instruction that does not waste that capability.</p><p>Outputs of the ALUs <b>123</b> and <b>125</b> and the move unit <b>127</b>, are connected with respective multiplexers <b>135</b>, <b>137</b> and <b>139</b> for connecting their respective outputs to different selected ones of the eight registers <b>39</b>. These multiplexers are set by controls (indicated by \u00a9) from the control unit <b>87</b> consistent with the instructions that have been executed. Similarly, these two outputs of the ALUs <b>123</b> and <b>125</b>, and the output of the move unit <b>127</b>, are submitted to respective latches <b>141</b>, <b>143</b> and <b>145</b> for potential writing back into the data cache memory <b>13</b> through a write back circuit <b>147</b> for port A of the memory and <b>149</b> for its port B. Switches <b>151</b> and <b>153</b> are operated to connect data from two of the three latches <b>141</b>, <b>143</b> and <b>145</b> for writing in one cycle back into the data cache <b>13</b>. It can be seen that only two of the three pipelines may access the data memory <b>13</b> at one time. But since a large proportion of instructions of a usual program do not require data memory access, this limitation does not prevent execution of three instructions at the same time in most instances.</p><p>It will be recognized that, as with all pipelines, instructions are executed in sequence as they move through the pipelines from left to right of the block diagram of FIG. <b>2</b>. One set of instructions stored in the registers <b>81</b>, <b>83</b> and <b>85</b> are processed by respective ones of the adders <b>89</b>, <b>91</b> and <b>93</b> in one operating cycle, with the results stored in the latches <b>107</b>, <b>109</b> and <b>95</b>, respectively. At the same time these three instructions are moved to the latches <b>119</b>, a second set of instructions is then loaded into the latches <b>81</b>, <b>83</b> and <b>85</b> for processing in the AG stages during the next operating cycle at the same time that the first set of instructions is being processed as the OF stage. In a next operating cycle, the first set of instructions, and the results of partially processing them, is moved to the EX stage while a third set of instructions is loaded into the registers <b>81</b>, <b>83</b> and <b>85</b>. Lastly, the first set of instructions, after execution, is either written into the register <b>39</b> or moved to the output latches <b>141</b>, <b>143</b> and <b>145</b> for writing back into the data memory <b>13</b> in a fourth operating cycle, during which a fourth set of instructions is loaded into the registers <b>81</b>, <b>83</b> and <b>85</b> for processing in the AG stage.</p><p>Although the architecture conceptually illustrated in FIG. 2 has been described as three distinct pipelines it will be recognized that, because of the three sets of switches <b>101</b>/<b>103</b>/<b>105</b>, <b>129</b>/<b>131</b>/<b>133</b> and <b>151</b>/<b>153</b>, that a given instruction can travel through one stage in one pipeline, and through a subsequent stage in a different pipeline. This, in effect, dynamically creates, in response to the control unit <b>87</b> decoding the instructions and knowing the resources that each instruction needs, a separate pipeline for that instruction made up of one of the three possibilities for each stage that is consistent with the requirements of the instruction.</p><p>Some examples of the configuration of the various stages of FIG. 2 to process various types of instructions will now be described in general. An adder of the AG stage, and thus also the path taken in the OF stage, are selected for a given instruction independently of selecting the ALU in the EX stage. For example, if an instruction requires an arithmetic operation, one of the full capability ALUs <b>123</b> or <b>125</b> is selected for use in processing that instruction. Whether one of the full capability adders <b>89</b> or <b>91</b>, and their respective access to the ports of the data cache memory <b>13</b>, are required, depends on whether an operand to be used by a selected ALU is to come from the memory <b>13</b>. In many cases, however, the operands used by the selected ALU will come from the instruction itself, and/or the resisters <b>39</b>. In this latter case, the instruction reaches the ALU through the latches <b>119</b> and <b>121</b> without using either of the adders <b>89</b> or <b>91</b>.</p><p>Another example is an instruction for a move of data, in which case the move unit <b>127</b> is selected in the EX stage, if available, thereby leaving the full capability ALUs <b>123</b> and <b>125</b> for execution of other instructions at the same time. If the instruction calls for a move to be made between two of the registers <b>39</b>, then the control unit <b>87</b> causes the instruction to be sent directly to the move unit <b>127</b> through the registers <b>119</b> and <b>121</b>. However, if the instruction requires that data be moved out of the data memory <b>13</b>, then one of the adders <b>89</b> or <b>91</b>, with its access to the memory interfaces <b>111</b> and <b>13</b>, respectively, is used in order to provide that read data to the input of the move unit <b>127</b> through the switch <b>133</b>. In this case, the instruction flows through one of the two major pipelines until data is read from the cache memory <b>13</b>, at which time that data is then given to the move unit <b>127</b> of the third, reduced capability pipeline.</p><p>Similarly, if data is to be written into the cache memory <b>13</b> as part of a move instruction, one of the two write back units <b>147</b> and <b>149</b> is utilized. The particular configuration is set by the control unit <b>87</b> decoding the individual instructions and setting the switches appropriately. Yet another example is the processing of a jump instruction, which is processed almost entirely by the lesser capability adder <b>93</b>.</p><p>It will be noted, as mentioned earlier, that the instructions are loaded into the latches <b>81</b>, <b>83</b> and <b>85</b> in the order in which they are to be executed. These instructions are then individually routed through the various stages by the control unit <b>87</b> setting the various switches, as described. Alternatively, the control unit <b>87</b> could cause these instructions to be loaded into the latches <b>81</b>, <b>83</b> and <b>85</b> in a different order consistent with their resource requirements, and eliminate at least the switches <b>101</b>, <b>103</b> and <b>105</b>, and probably simplifying others. However, this makes it very hard to keep track of the order of the instructions being executed. The architecture described with respect to FIG. 2 provides the maximum flexibility in customizing the individual pipeline resources to the requirements of the instructions.</p><p>The embodiment of a three pipeline microprocessor conceptually described in FIG. 2 is given in more detail with respect to FIGS. 3-7. An overview of that implementation is given in FIG. <b>3</b>. The stages of the pipeline include initial instruction decode (ID) and instruction issue (IS) stages that are common to each of the three parallel pipelines. A set of three instructions is provided through circuits <b>151</b>, <b>153</b> and <b>155</b> to an address generation (AG) stage. The AG stage also receives data read from one or more of the registers <b>39</b> if so designated by an instruction being processed. Outputs <b>157</b>-<b>164</b> of the AG stage are applied to the operand fetch (OF) stage which in turn provides any read operands, instructions and other data to an execution stage (EX) through circuits <b>167</b>-<b>174</b>. The execution stage also receives data from one or more of the registers <b>39</b> if designated by an instruction being processed. The results of the processing of each set of three instructions is provided at circuits <b>177</b>, <b>179</b> and <b>181</b> to the write back (WB) stages. The EX stage also has an output which is a jump instruction that is applied back to the instruction fetch block <b>17</b>. The WB stages cause the results of the instruction processing to either be written back to the cache memory <b>13</b> through circuits <b>187</b> or <b>189</b>, or sent as a jump instruction through circuit <b>185</b> back to the instruction fetch block <b>17</b>, or some combination of these possibilities among the three instructions that have been processed. The results of the instruction processing of the EX stage could be written back to one or more of the registers <b>39</b> in the WB stage but the implementation being described writes to the registers <b>39</b> in the EX stage.</p><p>Further details of the structure and operation of the cache memories <b>11</b> and <b>13</b> in the processor of FIG. 3 are given in copending patent applications Ser. Nos. 09/100,551 and 09/100,846, both filed Jun. 19, 1998, which applications are expressly incorporated herein in their entirety by this reference.</p><p>Referring to FIGS. 4, <b>5</b>A and <b>5</b>B, the instruction decode (ID) stage of the FIG. 3 microprocessor is given in more detail. Instructions are serially read from the instruction cache <b>11</b> and into a queue register <b>201</b>. The system being described provides for the instructions having a variable number of bytes, depending primarily upon whether and individual instructions includes one or more bytes of address and/or one or more bytes of operand It is therefore necessary to separate the steady stream of bytes into individual instructions. This is accomplished by tagging the bytes within the queue register <b>201</b> and then decoding the stream of bytes by decoding circuitry <b>201</b> in order to group the bytes of each instruction together as a unit. An output <b>205</b> of the decoding circuitry <b>203</b> carries the bytes of individually identified instructions to the next pipeline stage.</p><p>FIGS. 5A and 5B illustrate how this level of decoding is accomplished. One or more bytes of instruction <b>207</b> is inputted at a time into one end of a logically defined shift register <b>201</b> from the instruction cache memory <b>11</b>. The instruction bytes are read out of the shift register <b>201</b>, one or more bytes <b>209</b> at a time. As instruction bytes are read out of the register <b>201</b>, other bytes in it are shifted up through the register and new ones added to the bottom from the instruction cache <b>11</b>. The register <b>201</b> in FIG. 5A is shown to have a width sufficient to contain a word illustrated in FIG. 5B that includes a byte <b>211</b> of instructions, a validity bit <b>213</b> and several control bits <b>215</b>. The control bits <b>215</b> identify the first byte of each instruction and designate the number of bytes in the instruction. As these bytes are individual read out of the register <b>201</b>, the decoder <b>203</b> identifies the beginning and ending byte of each instruction.</p><p>Various specific alternative structures of the queue register <b>201</b>, and their operation, are given in copending patent application entitled \u201cImproved Instruction Buffering Mechanism,\u201d of Kenneth K. Munson et al., filed Sep. 4, 1998, which application is incorporated herein in its entirety by this reference.</p><p>These instructions are then arranged by the instruction issue (IS) stage in their order of execution. Shown in the IS stage of FIG. 4 are six latches <b>217</b>-<b>222</b>, each of which is capable of storing the maximum number of bytes forming any instruction that is expected to be received by the stage. The three latches <b>217</b>-<b>219</b> present one set of three decoded instructions at a time to respective circuits <b>151</b>, <b>153</b> and <b>155</b>. Rather than loading the three latches <b>217</b>-<b>219</b> directly from the instruction decoder <b>203</b>, instructions are first loaded into the latches <b>220</b>-<b>222</b> and then individually moved tip into the latches <b>217</b>-<b>219</b> as instructions are sent from the latches <b>217</b>-<b>219</b> out along the remaining stages of the pipeline. This shifting of instructions upward among the latches <b>217</b>-<b>222</b> as instructions are moved out of the latches <b>217</b>-<b>219</b> is accomplished by a set of multiplexers <b>225</b>-<b>229</b>.</p><p>Although it is a goal to send a set of three instructions each cycle from all of the latches <b>217</b>-<b>219</b> along the pipeline, there will be situations where one or two instructions of a set may be held and sent down the pipeline in the next cycle. Thus, for example, if only one instruction in the latch <b>217</b> is sent down the pipeline in one cycle, the instructions in each of the remaining <b>218</b>-<b>222</b> are moved upward as part of that same cycle in order to reside in the latches <b>217</b>-<b>221</b>, respectively. A new set of three instructions is then readied for entry into the next stage of the pipelines. Another instruction is then loaded into the now empty latch <b>222</b> through the circuit <b>205</b>. In a case where all three instructions in the latches <b>217</b>-<b>219</b> are sent down the pipeline in a single cycle, the instructions residing in the remaining latches <b>220</b>-<b>222</b> are then moved up into the respective latches <b>217</b>-<b>219</b> in position to be sent down the pipeline during the next cycle.</p><p>Each set of three instructions that is poised in the latches <b>217</b>-<b>219</b> for being sent down the pipeline are also inputted to the control unit <b>87</b>. The control unit decodes the instructions in order to ascertain how many of the three instructions may be sent down the pipeline at the same time and to determine the resources that must be allocated in the subsequent stages down stream of the IS stage for processing each instruction. This is possible since there is a known set of instructions although the number of instructions is rather large. In determining the resources required to process each instruction, and thus routing them individually through the subsequent stages, the control unit also notes and takes into account whether the instruction includes any address and/or operand bytes.</p><p>The set of three instructions in the latches <b>217</b>-<b>219</b> is made available to respective latches <b>231</b>, <b>233</b> and <b>235</b> of the next stage, the address generation (AG) stage illustrated in FIG. <b>6</b>. The control unit <b>87</b> causes those individual instructions to be latched, and thus stored, within the individual latches <b>231</b>-<b>235</b> that are to be sent down the pipeline together during that cycle. Any remaining instructions not latched into the latches <b>231</b>, <b>233</b> and <b>235</b> are retained in the IS stage and moved up in the set of latches <b>217</b>-<b>222</b>, as previously described.</p><p>The primary components of the AG stage are three adders, a four input port adder <b>237</b>, another four input port <b>239</b> and a much simpler, two input port adder <b>241</b>. The results of the address calculations of each of these adders occurs in respective outputs <b>158</b>, <b>161</b> and <b>164</b>. The inputs to each of these adders <b>237</b>, <b>239</b> and <b>241</b> are controlled by respective multiplexers <b>243</b>, <b>245</b> and <b>247</b>. The multiplexer <b>243</b> selects, in response to a control signal from the control unit <b>87</b>, the instruction in either of the latches <b>231</b> or <b>233</b> that designates the inputs to the adder <b>237</b>. The multiplexer <b>245</b> serves a similar function with respect to the adder <b>239</b>, selecting the instruction in either of the latches <b>233</b> or <b>235</b>. Similarly, the multiplexer <b>245</b> selects from any three of the instructions stored in the latches <b>231</b>, <b>233</b> or <b>235</b>, to form one input <b>249</b> to the adder <b>241</b>.</p><p>Each of the adders <b>237</b> and <b>239</b> operate similarly to those of current two pipeline microprocessors. One component <b>253</b> of a selected instructions operates a multiplexer <b>255</b> to present at one of the input ports <b>257</b> to the adder <b>237</b> the contents of one of many registers <b>251</b> that are part of a standard microprocessor. Each of these registers contains a base address for a segment of memory in which certain types of data are stored. For example, a \u201cCS\u201d register contains the base address for a block of memory containing code, a \u201cDS\u201d register designating a base address of a block of memory for data, a register \u201cSS\u201d containing a base address for a block of memory used for a stack, and so forth.</p><p>A second input port <b>259</b> to the adder <b>237</b> receives a displacement component of the instruction, if there is such an address component to the instruction being processed during a given cycle. A third input port <b>261</b> receives the content of one of the eight register <b>39</b> as selected by a multiplexer <b>263</b> in response to a base offset portion <b>265</b> of the instruction. Similarly, a fourth input port <b>267</b> to the adder <b>237</b> is connnectable to another one of the registers <b>39</b> through a multiplexer <b>269</b> in response to an index pointer <b>271</b> component to the instruction.</p><p>The result at the output <b>158</b> of the adder <b>237</b> is an address within the cache <b>13</b> where an operand is to be found that is required to execute the instruction. This address is stored in a latch <b>273</b> within the next stage, the operand fetch (OF) stage. The adder <b>239</b> receives the same four inputs, although for a different one of the set of three instructions that are in the AG stage at the time, and similarly calculates another address in an output <b>161</b> that is stored in a latch <b>275</b>.</p><p>Another adder (not shown) can optionally be included within the AG stage as an auxiliary address generator to assist the adders <b>237</b> and <b>239</b> calculate addresses for string and jump instructions. This is described in copending patent application Ser. No. 09/088,233, filed Jun. 1, 1998, which application is expressly incorporated herein in its entirely by this reference.</p><p>The third adder <b>241</b> shown in FIG. 6 is, in this specific example, dedicated to calculating an address within the instruction cache memory <b>11</b> from a jump instruction. Thus, one of its input ports <b>277</b> receives the contents of the CS register within the group of registers <b>251</b> while a second input <b>249</b> receives a relative offset component of an address within the code segment of memory. A jump address calculated by the adder <b>241</b> appears that at its output <b>164</b> which is then stored in a latch <b>279</b> at the beginning of the next OF stage.</p><p>In addition, the AG stage selects by a multiplexer <b>281</b> the data from one of the instructions stored in the latches <b>231</b> or <b>233</b> for storage during the next operational cycle and a latch <b>283</b> at the beginning of the OF stage. Similarly, a multiplexer <b>285</b> selects data within either of the instructions stored in the latches <b>233</b> or <b>235</b> for storage during the next cycle in a latch <b>287</b>. Further latches <b>289</b>, <b>291</b> and <b>293</b> of the OF stage store addresses from the instructions stored respectively in latches <b>231</b>, <b>233</b> and <b>235</b> of the resisters <b>39</b>.</p><p>The primary operation occurring in the OF stage is to read up to two operands from the data memory <b>13</b> located at the addresses stored in the latches <b>273</b> and <b>275</b>. Memory interface circuits <b>295</b> and <b>297</b> provide such access respectively to the A and B ports of the data cache <b>13</b>. A result in the circuits <b>168</b> and <b>171</b> is two operands read from the data memory <b>13</b>, if indeed a given set of instructions present in the OF stage calls for two such operands. There may be cases where only one operand is fetched, or more unusually, when no operand is fetched by these stages.</p><p>It will be noted that the address outputs of the principal adder <b>237</b> and <b>239</b> are connected to access only the respective ports A and B of the data cache memory <b>13</b>. No multiplexing is provided to alter this connection since that element of flexibility is not required. The entire data cache memory <b>13</b> may be accessed through either of its ports A or B. The third adder <b>241</b>, of course, does not form an address for the memory <b>13</b>.</p><p>The next processing stage, the execution (EX) stage, has eight input latches <b>301</b>-<b>308</b> that store, in the next operational cycle, the contents of the circuits <b>167</b>-<b>174</b>. This stored information is available for use by full capability ALUs <b>311</b> and <b>313</b>, and by a specialized unit <b>315</b> to move data between the registers <b>39</b> and the data cache <b>13</b>, or between individual ones of the registers <b>39</b>. The move unit <b>315</b>, in effect, is a single input port, limited capability ALU. The ALU <b>311</b> has two input ports <b>317</b> and <b>319</b> that receive signals selected by respective multiplexers <b>321</b> and <b>323</b>. Similarly, the ALU <b>313</b> has corresponding two input ports <b>325</b> and <b>327</b> that receive inputs selected by respective multiplexers <b>329</b> and <b>331</b>. In addition, the ALU <b>313</b> is provided, in this particular sample, with a third input port <b>333</b> that is also connected to the output of the multiplexer <b>323</b>, for reasons described below. The data move unit <b>315</b> has a single input port <b>335</b> from an output of the multiplexer <b>337</b>.</p><p>The inputs to each of the multiplexers <b>321</b> and <b>323</b> for the ALU <b>311</b> are the same. Multiplexers <b>339</b> and <b>341</b> select the contents of one of the registers <b>39</b> as one of the respective inputs to each of the multiplexers <b>321</b> and <b>323</b>, in response to a register address from one of two instructions as selected by a multiplexer <b>343</b>. The remaining four inputs to each of the multiplexers <b>321</b> and <b>323</b> are the contents of the latches <b>302</b>, <b>303</b>, <b>305</b> and <b>306</b>.</p><p>Each of the multiplexers <b>329</b> and <b>331</b> supplying two of the input ports of the ALU <b>313</b> are similarly connected in order to provide that ALU with a similar range of potential inputs. Multiplexers <b>345</b> and <b>347</b> provide one of the respective inputs to each of the multiplexers <b>329</b> and <b>331</b>, which is one of the registers <b>39</b> that is selected by one of the instructions within the latches <b>304</b> and <b>307</b>, as selected by a multiplexer <b>349</b>. The remaining four inputs of each of the multiplexers <b>329</b> and <b>331</b> are connected with the contents of the latches <b>302</b>, <b>303</b>, <b>305</b> and <b>306</b>, respectively.</p><p>The multiplexer <b>337</b>, which selects an input <b>335</b> to the move unit <b>315</b>, similarly has an input connected to a multiplexer <b>351</b> that selects data from one of the registers <b>39</b> as one of its inputs, in response to the contents of any one of the three instructions stored in the latches <b>301</b>, <b>304</b> or <b>307</b>, as selected by a multiplexer <b>353</b>. The remaining four inputs to the multiplexer <b>337</b> are the same as the other multiplexers described above, namely, the contents of the latches <b>302</b>, <b>303</b>, <b>305</b> and <b>306</b>.</p><p>The data outputs of each of the ALUs <b>311</b> and <b>313</b>, and the move unit <b>315</b>, are stored in a next cycle in individual ones of latches <b>361</b>, <b>363</b> and <b>365</b> at the input to the next processing stage, a write back (WB) stage. The data outputs of units <b>311</b>, <b>313</b> and <b>315</b> are directed to the latches <b>361</b>, <b>363</b> and <b>365</b> by respective multiplexers <b>367</b>, <b>369</b> and <b>371</b>. The latch <b>361</b> may receive the data output of either the ALU <b>31</b> or the move unit <b>315</b>. The latch <b>363</b> may receive the output from any three of the units <b>311</b>, <b>313</b> or <b>315</b>, depending upon the control signal to the multiplexer <b>363</b>. The latch <b>365</b> receives the data output of either of the ALU <b>313</b> or the move unit <b>315</b>.</p><p>Since the outputs of the ALUs and move unit can be directed to any of the latches <b>361</b>, <b>363</b> or <b>365</b>, an order of the set of instructions being executed is reestablished to be the same as originally presented in latches <b>217</b>-<b>219</b> of the IS stage. For example, if the instruction of one set stored in the latch <b>217</b> (FIG. 4) can be executed with the move unit <b>315</b>, it can be routed to the move unit <b>315</b> without tying up a more complex ALU <b>311</b> or <b>313</b>. Once that instructions is executed by the EX stage of FIG. 7, the result is then stored in the latch <b>361</b> to take its place in the same order as when launched by the IS stage.</p><p>In the last WB state of the pipeline, one of the two executed results stored in the latches <b>361</b> or <b>363</b> is selected by a multiplexer <b>373</b> for writing back into data cache memory <b>13</b> through its port A. Similarly, a multiplexer <b>375</b> can connect either of the executed results within either of the registers <b>363</b> or <b>365</b> to the cache memory <b>313</b> port B. Of course, the executed data results are sent to the memory <b>13</b> only when the are to be stored in it.</p><p>If any of the data results are to be stored in the registers <b>39</b>, this occurs within the EX stage. The resultant data selected by each of the multiplexers <b>367</b>, <b>369</b> and <b>371</b> are respectively connectable to any one of the eight registers <b>39</b> through respective multiplexers <b>377</b>, <b>379</b> and <b>381</b>. Indeed, execution of an instruction that does not require writing a result back to the memory may be completed, and others then advanced along the pipeline behind it, without having allocate a processing cycle for the WB stage. This is further described in copending patent application Ser. No. 09/116,023, filed Jul. 15, 1998, which application is expressly incorporated herein in its entirety by this reference.</p><p>As previously noted, the ALU <b>313</b> is unusual in that it has a third input port <b>333</b> rather than the more conventional two input port ALU <b>311</b>. This added input port allows successive instructions to be processed together in parallel through two different pipelines when the second instruction requires data for its execution that is the result of executing the first instruction.</p><p>For example, consider a first instruction that calls for adding the value of a number in register r<b>1</b> to the value of a number at a given location in the data memory <b>13</b> and then write the result back into the register r<b>1</b>, and a second instruction that requires reading that new result from the register r<b>1</b> and then subtracting it from the value stored in register r<b>4</b>. Since the second instruction is dependent upon the first, the second instruction is typically held at the beginning of the pipeline for one operational cycle while the first instruction is processed. Enough time must elapse to allow the first instruction to write the new value in the register r<b>1</b> before the second instruction causes it to be read.</p><p>However, by providing the third port <b>333</b> to the ALU <b>313</b> and by allowing it to be connected to a data source through the multiplexer <b>323</b> that is different than its other two input ports <b>325</b> and <b>327</b>, both of these dependent instructions can be executed at the same time. Rather than the first instruction writing its resulting data back into the register r<b>1</b>, both instructions are executed together through two of the pipelines by inputting to the ALU <b>313</b> the two operands that are specified by to used by the first instruction. That is, rather than the ALU receiving an input that is the result of execution of the first instruction, it receives in two inputs the operands which were used to generate that result. In the example given above, two of the inputs of the ALU <b>313</b> are given the original data in r<b>1</b> plus that in memory which are called for by the first instruction, plus the data in the register r<b>4</b>. Both instructions are then executed at the same time by the ALU <b>313</b>. This technique of using a three input port ALU provides these advantages with a microprocessor having only two pipelines as well as in the improved three pipeline architecture being described. This feature is described in more detail in copending patent application Ser. No. 09/128,164, filed Aug. 3, 1998, which application is expressly incorporated herein in its entirety by this reference.</p><p>As can be seen from the foregoing description of a multi-pipeline microprocessor architecture, there is an extreme amount of flexibility available to the control unit <b>87</b> for routing instructions in order to maximize the throughput of the microprocessor. With reference to the flow chart of FIG. 8, a preferred operation of the microprocessor embodiment of FIGS. 3-7 is given. In a first step <b>401</b>, the latches <b>217</b>-<b>219</b> of the IS stage (FIG. 4) are loaded with a set of three instructions that are candidates for being executed in parallel through three different pipelines of the microprocessor. The control unit <b>87</b> examines each of the three instructions, in a step <b>403</b>, to determine whether any of the three instructions depend upon the results of any of the other three instructions in a manner that would prevent all three instructions from being executed in parallel. This is commonly done now with two pipeline microprocessors, so the same techniques are extended to examining three instructions at one time instead of just two. If there is any such dependency, the control unit <b>87</b> flags any such dependent instruction so that it will not be loaded into the respective one of latches <b>231</b>, <b>233</b> or <b>235</b> at the input to the AG stage (FIG. <b>6</b>). This is indicated in a step <b>405</b> of FIG. <b>8</b>. Of course, there will be fewer dependencies that can hold back parallel execution of instructions with the use of the three input port ALU <b>313</b> (FIG. 7) of one aspect of the present invention. If there are no unresolvable dependencies among the three instructions loaded in the latches <b>217</b>-<b>219</b>, the step <b>405</b> is omitted.</p><p>Regardless of resolution of dependencies, there will at least be an instruction in the latch <b>217</b> that can be executed. A next step <b>407</b> designates that first instruction for examination, and a step <b>409</b> causes the control unit <b>87</b> to decode the instruction so that it may be determined what pipeline resources are necessary to execute it.</p><p>A step <b>411</b> determines whether the instruction requires access to read an operand from the cache memory <b>13</b> and, if so, directs it to a full adder. If not, the reduced capability adder <b>241</b> may be used with the instruction. Details of this are shown in the flow diagram of FIG. 9, as described below.</p><p>Another step <b>413</b> looks at the type of ALU that is required to execute the first instruction of the set that is stored in the latch <b>217</b>, and assigns to it either a full capability ALU, the move unit <b>315</b> or nothing if an ALU is not required to execute the instruction. Details of the step <b>413</b> are provided in the flow diagram of FIG. 10, as described below. The steps <b>411</b> and <b>413</b> may be processed in parallel, since they are independent of one another, or, for convenience, may be performed in sequence.</p><p>A next step <b>415</b> asks whether all three instructions of the set stored in latches <b>217</b>-<b>219</b> (FIG. 4) have been assigned resources or held by the control unit <b>87</b>. If not, a step <b>417</b> causes the steps <b>409</b>, <b>411</b> and <b>413</b> to be performed on the next in order of the set of three instructions. In out example, we have only examined the first instruction in the latch <b>217</b>, so the steps <b>409</b>, <b>411</b> and <b>413</b> are then repeated for the second instruction stored in the latch <b>218</b>. Once each of the three instructions of the set have been assigned resources, or designated to be held for a cycle, a final step <b>419</b> indicates that the switching instructions to the various multiplexers in the several pipeline stages will be issued at the appropriate times for processing each of these three instructions as they work there way through the stages of the pipelines. After that is completed, the control unit <b>87</b> returns to the step <b>401</b> by causing the next three instructions to be loaded into the latches <b>217</b>-<b>219</b> in the manner previously described with respect to FIG. <b>4</b>.</p><p>It will be noted that at the time the control unit <b>87</b> is examining and assigning resources to the set of three instructions, other instructions earlier examined are being processed by other pipeline stages. Therefore, the resources that are allocated for a particular instruction are stored by the execution unit <b>87</b> until that instruction has worked its way down to the stage where the resource must be provided. For example, an adder of the AG stage must be provided one cycle time after the assignment is made, so the multiplexers of the AG stage are appropriately switched at that next operational cycle. Similarly, the ALU/move unit that is assigned to a particular instruction is actually not connected to receive the instruction for at least three cycle times since the EX unit is three stages downstream from the IS stage.</p><p>It will be noted from FIGS. 4-7 that the control circuit <b>87</b> provides control signals to the various multiplexers, latches and other components as the result of decoding the instructions being executed. One aspect of the control unit <b>87</b> is described in copending patent application Ser. No. 09/088,226, filed Jun. 1, 1998, which application is expressly incorporated herein in its entirety by this reference.</p><p>Referring to FIG. 9, the algorithm for executing the step <b>411</b> of FIG. 8 is shown in more detail. A step <b>421</b> first determines whether the instruction being examined requires memory access, and thus one of the full capability adders <b>237</b> or <b>239</b>. If so, a next step <b>423</b> determines whether a full capability adder is available. If this is the first or second of the set of three instructions to be examined, then a full capability adder will be available but if it is the third instruction, it needs to be determined whether both full capability adders <b>237</b> and <b>239</b> have already been designated for use by the prior two instructions of the set. If both of those adders are in use, a next step <b>425</b> shows that the instruction is flagged to be held for one operational cycle, in a manner described previously. If one of the full capability adders <b>237</b> or <b>239</b> is available, however, a next step <b>427</b> assigns the first available one to receive the instruction being examined.</p><p>Returning to the initial step <b>421</b> of FIG. 9, if the instruction is such that it does not need a full capability adder, a next step <b>429</b> determines whether the instruction needs the reduced capability adder <b>241</b>. If so, it is then asked whether the adder <b>241</b> is available, in a step <b>431</b>. If not, the processing proceeds to the step <b>425</b> to hold that instruction for the next cycle. If the adder <b>241</b> is available, however, a next step <b>433</b> assigns it to the instruction being examined. Returning to the step <b>429</b>, if the instruction does not need the adder C, then the processing of the step <b>411</b> of FIG. 8 is completed.</p><p>Referring to FIG. 10, a similar flow chart is provided for the step <b>413</b> of FIG. 8. A first step <b>441</b> of FIG. 10 asks whether the instruction being analyzed needs one of the full ALU's <b>311</b> or <b>313</b> to be executed. If so, a next step <b>443</b> asks whether one of them is available and, if so, one is assigned to this instruction by a step <b>445</b>. If neither of the ALU <b>311</b> and <b>313</b> are available, however, because they have previously been assigned to other instructions of the set, then a flag is raised in a step <b>447</b> and that instruction is held within the IS stage to be sent down the pipeline in the next execution cycle.</p><p>Returning to the step <b>441</b>, if the instruction does not need one of the full capability ALUs <b>311</b> or <b>313</b>, a next step <b>449</b> determines whether the instruction requires the move unit <b>315</b> for execution. If not, the processing of the step <b>413</b> of FIG. 8 is completed. But if the instruction does need the move unit <b>315</b>, a next step <b>451</b> asks whether it is available and, if so, assigns it to receive that instruction at the later time, in a step <b>453</b>. However, if the move unit is determined in step <b>451</b> not to be available, because it has been assigned to a previous instruction of the set, processing returns to the step <b>443</b> to ascertain whether one of the full capability ALU's <b>311</b> or <b>313</b> is available to execute the instruction. If so, one of them is assigned to it even though the instruction does not need that much capability, in order to increase the number of instructions that are being processed in parallel at all times.</p><p>As one implemention detail of the microprocessor of FIGS. 3-7, techniques for distributing clock signals to various circuit portions are given in copending patent application entitled \u201cImproved Clock Distribution System,\u201d of Sathyanandan Rajivan, filed Sep. 11, 1998, which application is incorporated herein in its entirety by this reference.</p><p>Although the various aspects of the present invention have been described with respect to its preferred embodiments, it will be understood that the invention is entitled to protection within the full scope of the appended claims.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Kenneth K.", "last_name": "Munson", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "RISE TECHNOLOGY COMPANY"}, {"first_name": "", "last_name": "RISE TECHNOLOGY COMPANY", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F   9/34"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/38        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "712215"}, {"primary": false, "label": "711125"}, {"primary": false, "label": "711149"}, {"primary": false, "label": "712023"}, {"primary": false, "label": "712225"}, {"primary": false, "label": "712E09046"}, {"primary": false, "label": "712E09062"}, {"primary": false, "label": "712E09071"}], "ecla_classes": [{"label": "G06F   9/38P"}, {"label": "G06F   9/38T"}, {"label": "G06F   9/38D"}], "cpc_classes": [{"label": "G06F   9/3824"}, {"label": "G06F   9/3867"}, {"label": "G06F   9/3885"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1998-04-20", "application_date": "1998-09-11", "family_members": [{"ucid": "US-6408377-B2", "titles": [{"lang": "EN", "text": "Dynamic allocation of resources in multiple microprocessor pipelines"}]}, {"ucid": "US-20010016900-A1", "titles": [{"lang": "EN", "text": "Dynamic allocation of resources in multiple microprocessor pipelines"}]}, {"ucid": "US-6341343-B2", "titles": [{"lang": "EN", "text": "Parallel processing instructions routed through plural differing capacity units of operand address generators coupled to multi-ported memory and ALUs"}]}, {"ucid": "US-20010014940-A1", "titles": [{"lang": "EN", "text": "Dynamic allocation of resources in multiple microprocessor pipelines"}]}, {"ucid": "US-20010014939-A1", "titles": [{"lang": "EN", "text": "Dynamic allocation of resources in multiple microprocessor pipelines"}]}, {"ucid": "US-6304954-B1", "titles": [{"lang": "EN", "text": "Executing multiple instructions in multi-pipelined processor by dynamically switching memory ports of fewer number than the pipeline"}]}]}