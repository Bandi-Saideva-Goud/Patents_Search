{"patent_number": "US-6408363-B1", "publication_id": 73065464, "family_id": 24256843, "publication_date": "2002-06-18", "titles": [{"lang": "EN", "text": "Speculative pre-flush of data in an out-of-order execution processor system"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50337593\"><p>Speculative pre-fetching and pre-flushing of additional cache lines minimize cache miss latency and coherency check latency of an out of order instruction execution processor. A pre-fetch/pre-flush slot (DPRESLOT) is provided in a memory queue (MQUEUE) of the out-of-order execution processor. The DPRESLOT monitors the transactions between a system interface, e.g., the system bus, and an address reorder buffer slot (ARBSLOT) and/or between the system interface and a cache coherency check slot (CCCSLOT). When a cache miss is detected, the DPRESLOT causes one or more cache lines in addition to the data line, which caused the current cache miss, to be pre-fetched from the memory hierarchy into the cache memory (DCACHE) in anticipation that the additional data would be required in the near future. When a cache write back is detected as a result of a cache coherency check, the DPRESLOT causes one or more cache lines, in addition to the data line currently being written back, to be pre-flushed out to the memory hierarchy from the respective cache memory (DCACHE) of the processor that owns the line, in anticipation that the additional data would be required by the requesting processor in the near future. A logic included in the DPRESLOT prevents a cache miss request for the additional data when another request has already been made for the data.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00001\" num=\"1\"><claim-text>1. An apparatus for minimizing cache coherency check latency in an out of order instruction execution system having a plurality of processors, comprising:</claim-text><claim-text>at least one cache coherency check mechanism associated with a first one of said plurality of processors, said at least one cache coherency check mechanism being configured to output a presence signal indicating that a first data line being requested by a second one of said plurality of processors is present in a cache memory associated with said first one of said plurality of processors; </claim-text><claim-text>at least one pre-flush slot configured to, upon receipt of said presence signal, determine at least one additional data line to be pre-flushed from said cache memory associated with said first one of said plurality of processors to said second one of said plurality of processors, and </claim-text><claim-text>a logic associated with said at least one pre-flush slot, said logic configured to provide an indication whether said at least one additional data line is already being flushed from said cache memory. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00002\" num=\"2\"><claim-text>2. The apparatus for minimizing cache coherency check latency according to <claim-ref idref=\"US-6408363-B1-CLM-00001\">claim 1</claim-ref>, wherein said at least one pre-flush slot comprises:</claim-text><claim-text>an adjacent address logic configured to provide one or more additional addresses corresponding to said at least one additional data line, said at least one additional data line having a memory location adjacent to said first data line. </claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00003\" num=\"3\"><claim-text>3. The apparatus for minimizing cache coherency check latency according to <claim-ref idref=\"US-6408363-B1-CLM-00002\">claim 2</claim-ref>, wherein:</claim-text><claim-text>said adjacent address logic receives a first address corresponding to said first data line, and provides said one or more additional addresses by inverting one or more bits of said first address. </claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00004\" num=\"4\"><claim-text>4. The apparatus for minimizing cache coherency check latency according to <claim-ref idref=\"US-6408363-B1-CLM-00003\">claim 3</claim-ref>, wherein:</claim-text><claim-text>said one or more bits of said first address comprises a least significant bit of said first address. </claim-text></claim>"}, {"num": 5, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00005\" num=\"5\"><claim-text>5. The apparatus for minimizing cache coherency check latency according to <claim-ref idref=\"US-6408363-B1-CLM-00002\">claim 2</claim-ref>, further comprising:</claim-text><claim-text>a busy latch having a set input and a clear input, said busy latch being configured to output a busy signal, and said busy signal being active when said set input is triggered, and inactive when said clear input is triggered; and </claim-text><claim-text>a register configured to store a cache index and a tag, both of which being derived from an address received from said adjacent address logic, said register receiving said address from said adjacent address logic upon receipt of an update signal, said update signal being produced by inverting said busy signal. </claim-text></claim>"}, {"num": 6, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00006\" num=\"6\"><claim-text>6. The apparatus for minimizing cache coherency check latency according to <claim-ref idref=\"US-6408363-B1-CLM-00005\">claim 5</claim-ref>, further comprising:</claim-text><claim-text>a decode logic for receiving a transaction type, said decode logic being configured to trigger said set input of said busy latch when said received transaction type indicates a receipt of said presence signal. </claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00007\" num=\"7\"><claim-text>7. The apparatus for minimizing cache miss latency according to <claim-ref idref=\"US-6408363-B1-CLM-00006\">claim 6</claim-ref>, wherein:</claim-text><claim-text>said at least one pre-flush slot is configured to determine whether said at least one additional data line is present in said cache memory when said busy signal is active. </claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00008\" num=\"8\"><claim-text>8. The apparatus for minimizing cache miss latency according to <claim-ref idref=\"US-6408363-B1-CLM-00007\">claim 7</claim-ref>, wherein:</claim-text><claim-text>said at least one cache coherency check mechanism is configured to output said presence signal when said first data line has been flushed to said second one of said plurality processor from said cache memory. </claim-text></claim>"}, {"num": 9, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00009\" num=\"9\"><claim-text>9. The apparatus for minimizing cache miss latency according to <claim-ref idref=\"US-6408363-B1-CLM-00007\">claim 7</claim-ref>, wherein:</claim-text><claim-text>said at least one pre-flush slot is configured to cause said at least one additional data line to be flushed to said second one of said plurality processor from said cache memory when said busy signal is active. </claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00010\" num=\"10\"><claim-text>10. The apparatus for minimizing cache miss latency according to <claim-ref idref=\"US-6408363-B1-CLM-00009\">claim 9</claim-ref>, wherein:</claim-text><claim-text>said at least one pre-flush slot is configured to cause said at least one additional data line to be flushed to said second one of said plurality processors from said cache memory if said at least one additional data line is determined to be present in said cache memory. </claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00011\" num=\"11\"><claim-text>11. A method of minimizing cache coherency check latency in an out of order instruction execution system having a plurality of processors, comprising:</claim-text><claim-text>detecting a request for access to a first data line from a memory hierarchy, said request being made by a first one of said plurality of processors; </claim-text><claim-text>determining whether said first data line is present in a cache memory associated with a second one of said plurality of processors; </claim-text><claim-text>calculating an address of at least one additional data line to be pre-flushed from said cache memory to said second one of said plurality of processors; and </claim-text><claim-text>determining whether a previously made request for said at least one additional data line from said cache memory is pending. </claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00012\" num=\"12\"><claim-text>12. The method of minimizing cache coherency check latency in accordance with <claim-ref idref=\"US-6408363-B1-CLM-00011\">claim 11</claim-ref>, wherein said step of calculating said address of said at least one additional data line comprises:</claim-text><claim-text>inverting one or more bits of an address of said first data line. </claim-text></claim>"}, {"num": 13, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00013\" num=\"13\"><claim-text>13. The method of minimizing cache coherency check latency in accordance with <claim-ref idref=\"US-6408363-B1-CLM-00012\">claim 12</claim-ref>, wherein:</claim-text><claim-text>said one or more bits comprises a least significant bit. </claim-text></claim>"}, {"num": 14, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00014\" num=\"14\"><claim-text>14. The method of minimizing cache coherency check latency in accordance with <claim-ref idref=\"US-6408363-B1-CLM-00011\">claim 11</claim-ref>, further comprising:</claim-text><claim-text>if said previously made request is pending, preventing flushing of said at least one additional data line to said second one of said plurality of processors. </claim-text></claim>"}, {"num": 15, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00015\" num=\"15\"><claim-text>15. The method of minimizing cache coherency check latency in accordance with <claim-ref idref=\"US-6408363-B1-CLM-00014\">claim 14</claim-ref>, further comprising:</claim-text><claim-text>if said previously made request is not pending, issuing a request for said at least one additional data line to be flushed to said second one of said plurality of processors. </claim-text></claim>"}, {"num": 16, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00016\" num=\"16\"><claim-text>16. The method of minimizing cache coherency check latency in accordance with <claim-ref idref=\"US-6408363-B1-CLM-00014\">claim 14</claim-ref>, further comprising:</claim-text><claim-text>determining whether said at least one additional data line is present in said cache memory. </claim-text></claim>"}, {"num": 17, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00017\" num=\"17\"><claim-text>17. The method of minimizing cache coherency check latency in accordance with <claim-ref idref=\"US-6408363-B1-CLM-00016\">claim 16</claim-ref>, further comprising:</claim-text><claim-text>if said at least one additional data line is present in said cache memory, issuing a request for said at least one additional data line to be flushed to said second one of said plurality of processors. </claim-text></claim>"}, {"num": 18, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6408363-B1-CLM-00018\" num=\"18\"><claim-text>18. The method of minimizing cache coherency check latency in accordance with <claim-ref idref=\"US-6408363-B1-CLM-00016\">claim 16</claim-ref>, further comprising:</claim-text><claim-text>if said at least one additional data line is not present in said cache memory, preventing said at least one additional data line from being flushed to said second one of said plurality of processors.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53586091\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>RELATED APPLICATION</h4><p>The present application is related to an application for U.S. Letter patent, entitled \u201cSpeculative Pre-fetch of data in an Out-of-order Execution Processor System\u201d by the present inventors, assigned to the assignee of the present application, having U.S. application Ser. No. 09/565,017.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>TECHNICAL FIELD</h4><p>The present invention generally relates to computer processor operations and architectures. More particularly the present invention relates to performance optimization by speculatively pre-fetching and pre-flushing data in a processor system in which instructions may be executed out of order.</p><h4>BACKGROUND ART</h4><p>A high performance processor, e.g., a super-scalar processor in which two or more scalar operations are performed in parallel, may be designed to execute instructions out of order, i.e., in an order that is different from what is defined by the program running on the processor. That is, in this high performance processor system, instructions are executed when they can be executed rather than when they appear in the sequence defined by the program. Typically, after the out of order execution of instructions, the results are ultimately reordered to correspond with the proper instruction order, prior to passing the results back to the program running on the processor.</p><p>Examples of processor architectures that execute instruction out of order are described in U.S. Pat. No. 5,758,178 (issued May 26, 1998, and entitled \u201cMiss Tracking System and Method\u201d), U.S. Pat. No. 5,761,713 (issued Jun. 2, 1998, and entitled \u201cAddress Aggregation System and Method for Increasing Throughput to a Multi-Banked Data Cache From a Processor by Concurrently Forwarding an Address to Each Bank\u201d), U.S. Pat. No. 5,838,942 (issued Nov. 17, 1998, and entitled \u201cPanic Trap System and Method\u201d), U.S. Pat. No. 5,809,275 (issued Sep. 15, 1998, and entitled \u201cStore-to Load Hazard Resolution System and Method for a Processor that Executes Instructions Out of Order\u201d), U.S. Pat. No. 5,799,167 (issued Aug. 25, 1998, and entitled \u201cInstruction Nullification System and Method for a Processor that Executes Instructions Out of Order\u201d), all to Gregg Lesartre who is one of the present inventors, assigned to the present assignee, and all of which are expressly incorporated herein by reference in their entireties.</p><p>As described in more detail in, e.g., U.S. Pat. No. 5,758,178 ('178), an out of order execution processor system may include one or more processors, each having a memory queue (MQUEUE) for receiving and executing instructions that are directed to memory accesses to the cache memory (DCACHE) or the memory hierarchy. The MQUEUE includes a plurality of instruction processing mechanisms for receiving and executing respective memory instructions out of order. Each instruction processing mechanism includes an instruction register for storing an instruction and an address reorder buffer slot (ARBSLOT) for storing the data address of the instruction execution results. Significantly, dependent-on-miss (DM) indicator logic in each ARBSLOT prevents a request from its respective ARBSLOT to the memory hierarchy for miss data that is absent from the DCACHE when another ARBSLOT has already requested from the memory hierarchy the miss data.</p><p>In particular, for example, FIG. 1 shows a block diagram of the relevant portions of the computer system for illustrating the operation of the instruction processing mechanism <b>39</b><i>b </i>portion of the MQUEUE. The MQUEUE includes one or more ARBSLOTs <b>48</b> (only one of which is shown). When an ARBSLOT <b>48</b> requests a cache line from the DCACHE <b>24</b>, the ARBSLOT <b>48</b> asserts signal ACCESS_REQ <b>115</b> accompanied with an address ACCESS_ADDR <b>114</b>. In the event that there is a potential hit in the DCACHE <b>24</b>, the status indicator <b>82</b> (or status indicators if the cache is associative) will reflect a valid cache line or lines. Further, the tag compare mechanism <b>108</b> reads the tag DCACHE_TAG(s) <b>81</b> and compares it to the tag ACCESS_TAG <b>116</b> associated with the access address ACCESS_ADDR <b>114</b>. When there is a match, the tag compare mechanism <b>108</b> concludes that there is a hit and deasserts the signal\u02dcHIT <b>118</b> to indicate a hit, which causes the ARBSLOT <b>48</b> to mark itself done. The result of the operation is held in a rename register (not shown) until the instruction retires, when it is moved to an architectural register (not shown).</p><p>When the cache access results in a cache miss, e.g., based upon a status indicator <b>82</b> indicating an invalid cache line(s), or alternatively, when the tag DCACHE_TAG(s) <b>81</b> does not match the tag ACCESS_TAG <b>116</b>, then the tag compare mechanism <b>108</b> asserts the \u02dcHIT signal <b>118</b> to indicate a miss to the ARBSLOT <b>48</b>. Assuming that this is the first ARBSLOT <b>48</b> to attempt to access this miss data line, the DM indicator logic <b>135</b> causes the miss request signal MISS_REQUEST <b>111</b> to be issued to the miss arbitrator <b>107</b>. The miss arbitrator <b>107</b> arbitrates by prioritizing the various miss requests that can be generated by the various ARBSLOTS <b>48</b>. Eventually, the miss arbitrator <b>107</b> issues a signal MISS_GRANTED <b>112</b> to grant the miss request. This signal is sent to the ARBSLOT <b>48</b>, which in turn asserts the miss control signal MISS_CAV signal <b>101</b> to the system interface control <b>102</b>. The system interface control <b>102</b> in turn makes a memory request to the memory hierarchy (not shown) for the data line based upon the address MISS/COPY_IN ADDR <b>104</b> that is forwarded from the ARBSLOT <b>48</b> to the system interface control <b>102</b>.</p><p>Once the data line is transferred from the memory hierarchy to the system interface control <b>102</b>, the system interface control <b>102</b> passes the data line to the DCACHE <b>24</b>, as indicated by reference arrow <b>105</b>, asserts the control signal COPY_IN to the DCACHE <b>24</b>, and issues the status bits to the DCACHE <b>24</b>. Simultaneously, the system interface control <b>102</b> asserts the control signal COPY_IN <b>103</b> to the ARBSLOTs <b>48</b> and places the associated address on MISS/COPY_IN ADDR <b>104</b> to the ARBSLOTs <b>48</b>.</p><p>If another ARBSLOT <b>148</b> attempts to access the DCACHE <b>24</b> for a miss data line that is currently being requested from memory hierarchy, then the particular ARBSLOT <b>48</b> will be advised by the status indicator <b>82</b>, as the status indicator <b>82</b> will indicate a miss pending status, or that the cache line is being requested by another ARBSLOT <b>48</b>. Thus, a redundant memory request for a data line that has already been requested is avoided. A more detailed description of the memory queue (MQUEUE) and the DM indicator <b>135</b> may be found in the above listed U.S. patents, e.g., the '178 patent.</p><p>While modern day high performance processors, e.g., a super-scalar processor described above, have improved greatly in the instruction execution time, slow memory access time is still a significant impediment to a processor running at its full speed. If requests for data can be fulfilled from the cache memory, delays associated with an access to the slower memory hierarchy\u2014usually referred to as a cache miss latency\u2014can be avoided. Thus, reducing the number of cache misses is a goal in high performance processor designs.</p><p>Moreover, in a multi-processor systems, whenever a processor requests a data line, a coherency check is required to determine if respective caches of the other processors contain the requested data line, and/or whether a writing back (or flushing) of the data line to the memory hierarchy is required, e.g., when the data line was modified by the particular processor that owns the data line. The coherency check adds delays to memory accesses\u2014referred to herein as coherency check latency\u2014.</p><p>Speculative pre-fetching and pre-flushing are based on a well known locality theory, called the spatial locality theory, which observes that when information is accessed by the processor, information whose addresses are nearby the accessed information tend to be accessed as well. This is particularly true when the load or store operation that caused the cache miss is a part of an instruction code sequence, which is accessing a record length longer than a cache line, i.e., when the instruction code sequence references data that spans over multiple data lines. In a system utilizing pre-fetching and/or pre-flushing, rather than fetching (and/or flushing) only currently accessed data into (or from) the cache memory, a block of data (or one or more cache lines) in the vicinity, including the currently accessed data, may be brought into (and/or flushed from) the cache memory. This speculative pre-fetching and pre-flushing of extra data lines into (or from) the data cache before it is required by later memory reference instructions may hide at least some of the cache-miss latency and the coherency check latency, and thus improve the overall performance of the processor system.</p><p>Unfortunately, however, heretofore, no known solutions for implementing pre-fetching and/or pre-flushing data lines in processors that perform out of order execution of instructions exists. In a system employing a speculative pre-fetching and/or pre-flushing described above, each additional memory request resulting from an out of order execution of instructions involves a memory transaction that requires transfer of a number of data lines (rather than a single data line without the pre-fetching or pre-flushing of extra data line(s)), and may result in an even greater increased traffic across the system bus, may exacerbate the excessive utilization of the system interface bandwidth, and thus may compromise system performance.</p><p>Thus, what is needed is an efficient system for and method of pre-fetching one or more data lines from a memory hierarchy to a cache memory without compromising the system performance of an out of order processing system.</p><p>What is also needed is an efficient system and method for prefetching one or more data lines from memory hierarchy to a cache memory while minimizing redundant multiple memory requests in the event of a cache miss in an out of order processing system.</p><p>What is also needed is an efficient system for and method of pre-flushing one or more data lines from a cache memory in a multiple out-of-order instruction execution processors system without adding to the system complexity, and thereby minimizing coherency check latency of the system.</p><h4>SUMMARY OF INVENTION</h4><p>In accordance with the principles of the present invention, an apparatus for minimizing cache coherency check latency in an out of order instruction execution system having a plurality of processors comprises at least one cache coherency check mechanism associated with a first one of the plurality of processors, the at least one cache coherency check mechanism being configured to output a presence signal indicating that a first data line being requested by a second one of the plurality processor is present in a cache memory associated with the first one of the plurality of processors, at least one pre-flush slot configured to, upon receipt of the presence signal, determine at least one additional data line to be pre-flushed from the cache memory associated with the first one of the plurality of processors to the memory hierarchy; and a logic associated with the at least one pre-flush slot, the logic configured to provide an indication whether the at least one additional data line is already being flushed to the memory hierarchy from the cache memory.</p><p>In addition, in accordance with another aspect of the principles of the present invention, a method of minimizing cache coherency check latency in an out of order instruction execution system having a plurality of processors comprises detecting a request for access to a first data line from a memory hierarchy, the request being made by a first one of the plurality of processors, determining whether the first data line is present in a cache memory associated with a second one of the plurality of processors, calculating an address of at least one additional data line to be pre-flushed from the cache memory to the memory hierarchy, and determining whether a previously made request for the at least one additional data line from the cache memory is pending.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>DESCRIPTION OF DRAWINGS</h4><p>Features and advantages of the present invention will become apparent to those skilled in the art from the following description with reference to the drawings, in which:</p><p>FIG. 1 is a block diagram showing the relevant portions of a legacy computer system having an out-of-order instruction execution processor;</p><p>FIG. 2 is a block diagram of an exemplary embodiment of the pre-fetch/pre-flush slot (DPRESLOT) in accordance with the principles of the present invention;</p><p>FIG. 2A is a block diagram of an exemplary embodiment of the cache port arbitration logic in accordance with a preferred embodiment of the present invention;</p><p>FIG. 3 is a flow diagram of an exemplary embodiment of the pre-fetching process in accordance with the principles of the present invention;</p><p>FIG. 4 is a block diagram of an exemplary embodiment of the cache coherency check slot (CCCSLOT) in accordance with the principles of the present invention; and</p><p>FIG. 5 is a flow diagram of an exemplary embodiment of the pre-flushing process in accordance with the principles of the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS</h4><p>For simplicity and illustrative purposes, the principles of the present invention are described by referring mainly to an exemplar embodiment, particularly, with references to an example in which a specific circuit design is implemented. However, one of ordinary skill in the art would readily recognize that the same principles are equally applicable to, and can be implemented in, other circuit designs, and that any such variation would be within such modifications that do not depart from the true spirit and scope of the present invention.</p><p>In accordance with the principles of the present invention, a pre-fetch/pre-flush slot (DPRESLOT) is provided in a memory queue (MQUEUE) of the out-of-order execution processor. The DPRESLOT monitors the transactions between a system interface, e.g., the system bus, and an address reorder buffer slot (ARBSLOT) and/or between the system interface and a cache coherency check slot (CCCSLOT). When a cache miss is detected, the DPRESLOT causes one or more cache lines in addition to the data line, which caused the current cache miss, to be pre-fetched from the memory hierarchy into the cache memory (DCACHE) in anticipation that the additional data would be required in the near future. When a cache write back is detected as a result of a cache coherency check, the DPRESLOT causes one or more cache lines, in addition to the data line currently being written back, to be pre-flushed out to the memory hierarchy from the respective cache memory (DCACHE) of the processor that owns the line, in anticipation that the additional data would be required by the requesting processor in the near future. A logic included in the DPRESLOT prevents a cache miss request for the additional data when another request has already been made for the data. Speculative pre-fetching and pre-flushing of the additional cache lines minimize cache miss latency and coherency check latency of an out of order instruction execution processor.</p><p>In particular, according to a preferred embodiment of the present invention, one or more DPRESLOT(s) is added to the instruction processing mechanism <b>39</b><i>b </i>(FIG. <b>1</b>). In the alternative, one or more of the ARBSLOT shown in FIG. 1 may be modified to perform the functions of the DPRESLOT, which will now be described in more detail.</p><p>FIG. 2 shows a block diagram of an exemplary embodiment of the pre-fetch/pre-flush slot DPRESLOT) <b>200</b> in accordance with the principles of the present invention, which includes a register <b>136</b> for storing a not hit (\u02dcHIT) indicator <b>136</b><i>a </i>set by the signal \u02dcHIT <b>118</b> from the tag compare mechanism <b>108</b> (FIG. <b>1</b>), a cache index <b>136</b><i>b </i>and a real address tag (TAG) <b>136</b><i>c</i>, which are received as an address ADDR <b>128</b> and a TAG <b>134</b>, respectively, from the adjacent address logic <b>213</b>, and, optionally in a preferred embodiment of the present invention, amiss type store <b>136</b><i>d </i>for holding a store flag (STORE) received from a MISS_STORE input <b>214</b>. The single bit flag STORE indicates whether the memory access instruction being processed performs a read or a write operation, and is derived from the instruction currently being processed in the instruction processing mechanism <b>39</b><i>b </i>(FIG. <b>1</b>). The flag STORE is used by the DCACHE <b>24</b> to maintain the cache operation with respect to the pre-fetched data line(s) consistent with the memory access instruction being performed.</p><p>The adjacent address logic <b>213</b> receives the address present on the MISS/COPY_IN ADDR <b>104</b>, which is part of the transactional interface between the instruction processing mechanism <b>39</b><i>b </i>(FIG. 1) and the system interface control <b>102</b> (FIG. <b>1</b>). The adjacent address logic <b>213</b> produces addresses that are adjacently located to the address received from the MISS/COPY_IN ADDR <b>104</b> by, e.g., inverting one or more lower significant bits of the received address or by a use of a counter to generate a number of addresses. In this exemplary embodiment, the least significant bit (LSB) of the received address is inverted to produce a single address having a location immediately next to, i.e., immediately preceding or following, the received address.</p><p>The adjacent address(s) thus produced is output on the ADDR <b>128</b> for storage in the CACHE INDEX <b>136</b><i>b </i>of the register <b>136</b>. The adjacent address logic <b>213</b> also provides the TAG <b>134</b>, which is a real page number (RPN) associated with the adjacent address in the preferred embodiment, for storage in the TAG <b>136</b><i>c </i>of the register <b>136</b>. The register <b>136</b> receives an update signal <b>212</b>. While the update signal <b>212</b> is active, the register <b>136</b> updates its content, i.e., contents of each of the fields, the \u02dcHIT <b>136</b><i>a</i>, the CACHE INDEX <b>136</b><i>b</i>, the TAG <b>136</b><i>c </i>and the STORE <b>136</b><i>d. </i></p><p>The update signal <b>212</b> is output from the inverter <b>219</b>, which receives as its input a BUSY signal <b>204</b> from the busy latch <b>203</b>. The busy latch <b>203</b> may comprise, e.g., a set-and-reset (S-R) flip-flop, and has two inputs, SET <b>205</b> and CLR <b>206</b>, which sets and resets the BUSY signal <b>204</b>, respectively. When the BUSY signal <b>204</b> is set, i.e., active, the update signal <b>212</b> becomes inactive, and thus the updating of the register <b>136</b> is stopped. The SET input <b>205</b> receives a decoded output from the decoder <b>202</b>, which receives input signals, MISS_CAV <b>101</b> and TRANS_TYPE <b>201</b>. The TRANS_TYPE <b>201</b> may be one of but not limited to, a \u201cload miss\u201d resulting from a read instruction, a \u201cstore\u201d miss resulting from a write instruction, and a coherency check response. TRANS_TYPE <b>201</b> is derived from the instruction currently being processed by the instruction processing mechanism <b>39</b><i>b </i>(FIG. 1) and/or from signals received from the system interface control <b>102</b> (FIG. <b>1</b>).</p><p>The decoder <b>202</b> outputs an active SET <b>205</b> signal when the MISS_CAV <b>101</b> indicates a valid address being present on the MISS/COPY_IN ADDR <b>104</b> and when the TRANS_TYPE <b>201</b> indicates that the transaction being processed in the transactional interface between the instruction processing mechanism <b>39</b><i>b </i>(FIG. 1) and the system interface control <b>102</b> (FIG. 1) is a memory access check, which will be described in more detail later.</p><p>The register <b>136</b> continuously updates its content as long as the BUSY signal <b>204</b> remains inactive (i.e., when the update signal <b>212</b> is active). When BUSY signal <b>204</b> becomes active, the register <b>136</b> halts updating its content, and the DPRESLOT <b>200</b> issues an ACCESS_REQ <b>115</b> (shown in FIG. 1) presenting the current contents of the CACHE INDEX <b>136</b><i>b</i>, the TAG <b>136</b><i>c </i>and the STORE <b>136</b><i>d </i>on the ACCESS_ADDR <b>114</b>, the ACCESS TAG_<b>116</b> and the ACCESS_STORE <b>218</b>, respectively, to the DCACHE <b>24</b>.</p><p>In the event that there is a potential hit in the DCACHE <b>24</b>, the status indicator <b>82</b> will reflect a valid cache line(s) as described in more detail in the '178 patent. Further, the tag compare mechanism <b>108</b> reads the tag DCACHE_TAG(s) <b>81</b> and compares it to the tag ACCESS_TAG <b>116</b> associated with the access address ACCESS_ADDR <b>114</b>. When there is a match, the tag compare mechanism <b>108</b> concludes that there is a hit and deasserts the signal \u02dcHIT <b>118</b> to indicate a hit, which causes the CLR input <b>206</b> of the busy latch <b>203</b> to be asserted, causing the BUSY signal <b>204</b> to be deasserted.</p><p>When the cache access misses based upon a status indicator <b>82</b>, or alternatively, when the tag DCACHE_TAG <b>81</b> does not match the tag ACCESS_TAG <b>116</b>, then the tag compare mechanism <b>108</b> asserts the \u02dcHIT signal <b>118</b> to indicate a miss. A compare mechanism <b>145</b> receives a cache index from the address MISS/COPY_IN ADDR <b>104</b>, as indicated by reference arrow <b>146</b>, and compares it to the CACHE INDEX <b>136</b><i>b </i>from the register <b>136</b>, as indicated by reference arrow <b>147</b>. The results of the compare mechanism <b>145</b> are passed to an AND gate <b>214</b>, as indicated by reference arrow <b>149</b>. Provided that the miss control signal MISS_CAV <b>101</b> is asserted, the compare signal <b>149</b> can cause the busy latch <b>203</b> to be reset, causing the BUSY signal <b>204</b> to be deaserted. In this exemplary embodiment, the compare signal <b>149</b> enables the updating of the register <b>136</b> to be resumed after the MISS_GRANTED signal <b>112</b> is received by the DPRESLOT <b>200</b>.</p><p>The busy latch <b>203</b> may also be reset when there is already a pending request for the cache line. If any of the ARBSLOTs <b>148</b> has already requested the same cache line from memory hierarchy (not shown), then the DPRESLOT <b>200</b> will be advised by the status indicator <b>82</b> (FIG. <b>1</b>), as the status indicator <b>82</b> will indicate a miss pending status as described in more detail in the '178 patent. In this case, the tag compare mechanism <b>108</b> asserts the signal HIT_DM <b>121</b> (as shown in FIG. <b>1</b>), which is input, along with a signal ACCESS_+_<b>2</b>, denoted by reference numeral <b>158</b> representing two cycles after the signal ACCESS_REQ <b>115</b> (FIG. <b>3</b>), to the AND logic gate <b>211</b>, which causes the BUSY signal <b>204</b> to be deasserted.</p><p>Yet another occasion in which the busy latch <b>203</b> may be cleared is when a signal indicative of an occurrence of an unexpected catastrophic event is received from the input <b>208</b> of the OR logic gate <b>207</b>. An unexpected catastrophic event may be, e.g., a CPU trap.</p><p>Since the BUSY signal <b>204</b> is input to the AND logic gate <b>137</b>, when it is inactive, i.e., in a deasserted state, the DPRESLOT <b>200</b> is precluded from making a MISS_REQUEST <b>111</b>. The deasserted BUSY signal <b>204</b> also causes the register <b>136</b> to resume update of its contents.</p><p>If, on the other hand, the ACCESS REQ<b>115</b> of this adjacent cache line caused a miss, i.e., \u02dcHIT signal <b>139</b> and the BUSY signal <b>204</b> are asserted, then the AND logic gate <b>137</b> will issue the miss request signal MISS_REQUEST <b>111</b> to the miss arbitrator <b>107</b> (FIG. <b>1</b>). The miss arbitrator <b>107</b> arbitrates by prioritizing the various miss requests that can be generated by the various ARBSLOTS <b>48</b> and/or the DPRESLOT <b>200</b>. Eventually, the miss arbitrator <b>107</b> issues a signal MISS_GRANTED <b>112</b> to grant the miss request. This signal is sent to the driver <b>213</b> in the DPRESLOT <b>200</b>, which in turn asserts the miss control signal MISS_CAV signal <b>101</b> to the system interface control <b>102</b>. The system interface control <b>102</b> in turn makes a memory request to the memory hierarchy (not shown) for the data line based upon the address MISS/COPY_IN ADDR <b>104</b>.</p><p>FIG. 2A shows an exemplary block diagram of the relevant portions of the cache port arbitration logic in accordance with a preferred embodiment of the present invention, in which three drivers <b>220</b> are added, each of which are enabled, i.e., allowed to output the signal presented in the respective inputs, when the CACHE_GRANT signal <b>221</b> is asserted by the cache port arbiter <b>222</b>, which may be a part of the DCACHE <b>24</b>. The CACHE_GRANT signal <b>221</b> is asserted upon a receipt, and an aribitration, of the the CACHE_REQ signal <b>223</b>, which is received from the AND logic gate <b>224</b>. The AND logic gate <b>224</b> in turn receives, as its inputs, the clock pulse <b>225</b>, the BUSY signal <b>204</b>, the \u02dcACCESS+<b>1</b> (i.e., the complement of one clock cycles after the ACCESS_REQ <b>115</b>) <b>226</b> and \u02dcACCESS+<b>2</b> (i.e., the complement of two clock cycles after the ACCESS_REQ <b>115</b>) <b>158</b>.</p><p>The process of the inventive pre-fetch operation will now be described with reference to an exemplary flow diagram shown in FIG. <b>3</b>. In step <b>301</b>, the transactional interface between the instruction processing mechanism <b>39</b><i>b </i>and the system interface control <b>102</b> (which will be referred to as simply the \u201ctransactional interface\u201d hereafter) is continuously monitored for a presence of any transaction, which may be accomplished by, for example, by monitoring for an assertion of the MISS_CAV <b>101</b> in the exemplary DPRESLOT <b>200</b> shown in FIG. <b>2</b>.</p><p>Once a transaction is detected, a determination is made, in step <b>302</b>, whether there is a valid address present in the transactional interface. In the example shown in FIG. 2, the presence of a valid address may be presumed, e.g., when the MISS_CAV signal <b>101</b> is asserted. When it is determined that a valid address is not present on the transactional interface, then the process returns to step <b>301</b>, i.e., the monitoring of the transactional interface continues.</p><p>On the other hand, if a valid address is detected, the process proceeds to step <b>303</b>, during which a determination is made whether the transaction is a memory access request resulting from a cache miss. In the example of FIG. 2, this determination can be made based on the TRANS_TYPE <b>201</b>. When it is determined that the transaction is not a cache miss, then the process returns to step <b>301</b>, i.e., the monitoring of the transactional interface continues.</p><p>However, if the transaction is a memory access request resulting from a cache miss, then, in step <b>304</b>, the monitoring of the transactional interface is halted. In the DPRESLOT <b>200</b>, for example, the updating of the register <b>136</b> is halted by setting the busy latch <b>203</b>. Then, in step <b>305</b>, one or more address(s) of data lines to be pre-fetched are calculated. For example, in the DPRESLOT <b>200</b>, the adjacent address logic <b>213</b> calculates the to-be-pre-fetched addresses by inverting one or more bits (e.g., the least significant bit (LSB)) of the address of the data line, the attempted access of which has caused the cache miss, present on the MISS/COPY_IN ADDR <b>104</b>.</p><p>In step <b>306</b>, a cache look-up operation is performed for the addresses calculated during the above step <b>305</b>. For example, in the example of FIG. 2, the DPRESLOT <b>200</b> issues an ACCESS_REQ <b>115</b> presenting the current contents of the CACHE INDEX <b>136</b><i>b</i>, the TAG <b>136</b><i>c </i>and the STORE <b>136</b><i>d </i>on the ACCESS_ADDR <b>114</b>, the ACCESS_TAG <b>116</b> and the ACCESS_STORE <b>218</b>, respectively, to the DCACHE <b>24</b>.</p><p>In step <b>307</b>, the result of the cache look-up operation is examined to determine whether the to-be-pre-fetched data lines are already present in the cache memory, i.e., a cache hit occurs. For example, in the FIG. 2 example, the DPRESLOT <b>200</b> determines that a cache hit has occurred by observing the \u02dcHIT <b>118</b> being deasserted by the tag compare mechanism <b>108</b>. If a cache hit has occurred, the process returns to step <b>301</b>, and the monitoring of the transactional interface is resumed.</p><p>If, however, in step <b>307</b>, a cache miss is detected, the process proceeds to step <b>308</b>, in which a determination whether a request for the to-be-pre-fetched data line(s) is already made, e.g., by a ARBSLOT <b>48</b> in the example shown in FIG. <b>2</b>. In the example of FIG. 2, a pending request for the data line may be detected from the HIT_DM <b>121</b>. If it is determined that a request for the data line is already pending, then the process returns to step <b>301</b>, and the monitoring of the transactional interface is resumed.</p><p>Finally, in step <b>309</b>, if no prior requests for the data line is pending, a request for the to-be-pre-fetched data line is issued, e.g., by issuing the MISS_REQUEST <b>111</b> in the example of FIG. 2, which eventually leads to MISS_CAV <b>101</b> being asserted, and causes a memory hierarchy access for the data line(s). In a preferred embodiment, once the request for the to-be-pre-fetched data line(s) is issued (MISS_CAV <b>101</b> fires), the process immediately returns to step <b>301</b>, and the entire process is continuously repeated. In FIG. 2, for example, the system interface control <b>102</b> advantageously handles the actual access of the memory hierarchy, allowing the DPRESLOT <b>200</b> to continue the above described process. When the address of the to-be-pre-fetched data line is placed on the MISS/COPY_IN ADDR <b>104</b> as a part of the request to the system control interface <b>102</b>, the compare <b>145</b> receives identical cache index on both of its inputs <b>146</b> and <b>147</b>, and thus the BUSY signal <b>204</b> is deasserted, causing the register <b>136</b> to resume updating of its contents.</p><p>If a miss request is initiated by an instruction in ARBSLOT <b>48</b>, that matches the address on the compare input <b>147</b> before DPRESLOT <b>200</b> receives the MISS_GRANTED signal <b>112</b>, the BUSY signal <b>204</b> will still be deasserted, and updating the register <b>136</b> will still resume.</p><p>The inventive cache pre-flushing system and method in accordance with the principles of the present invention will now be described with references to exemplary embodiments shown in FIGS. 4 and 5.</p><p>According to a preferred embodiment of the present invention, one or more cache coherency check slot (CCCSLOT) is added to the instruction processing mechanism <b>39</b><i>b </i>(FIG. <b>1</b>). In the alternative, one or more of the ARBSLOTs shown in FIG. 1 may be modified to assume the functions of the CCCSLOT, which will now be described in more detail.</p><p>In particular, FIG. 4 shows a block diagram of an exemplary embodiment of the cache coherency check slot (CCCSLOT), which may appear and functions in much similar way as an ARBSLOT <b>48</b>, as described in the '178 patent, with the key differences being, inter alia, the addition of the done latch <b>402</b> and the driver <b>407</b>, and that the address <b>128</b> and the tag <b>134</b> are received, rather than from the address calculator <b>58</b>, from the system interface control <b>102</b>.</p><p>When any of the multiple processors in a multiple processors computing system requests one or more data line(s) from the memory hierarchy, the memory request and the address(s) of the one or more data lines(s) appear on the system interface <b>22</b> (FIG. <b>1</b>). Each processor's system interface <b>102</b>, upon detecting the memory request, issues a CCC_INSERT signal <b>401</b> to its instruction processing mechanism <b>39</b>B.</p><p>To this end, in the preferred embodiment of the present invention, the respective CCCSLOT <b>400</b> of each of the processors receives the ADDR <b>128</b>, the TAG <b>134</b> and the CCC_INSERT signal <b>401</b> from the system interface control <b>102</b>, the ADDR <b>128</b> and the TAG <b>134</b> being associated with the data line being requested by another processor in the system. The CCC_INSERT signal <b>401</b> serves as a clock signal to the register <b>136</b> of the CCCSLOT <b>400</b>, thus allowing the register <b>136</b> to update its CACHE INDEX <b>136</b><i>b </i>and the TAG <b>136</b><i>c </i>with the ADDR <b>128</b> and the TAG <b>134</b>, respectively. The CCC_INSERT signal <b>401</b> is also input to the clear (CLR) input of the done latch <b>402</b>, which may be, e.g., a set-and-reset (S-R) flip-flop. When the CLR input is received, the output of the done latch <b>402</b> becomes inactive. The inverter <b>410</b> inverts the output signal of the done latch <b>401</b>, thus presenting an active signal \u02dcDONE to the input of the AND logic gate <b>137</b> as shown.</p><p>Upon receipt of the CCC_INSERT signal <b>401</b>, the CCCSLOT <b>400</b> issues a ACCESS_REQ signal <b>115</b> to the DCACHE <b>24</b>, and places the CACHE INDEX <b>136</b><i>b </i>and the TAG <b>134</b> on the ACCESS_ADDR <b>114</b> and the ACCESS_TAG <b>116</b>, respectively. In response, the DCACHE <b>24</b> issues the DCACHE TAG(s) <b>81</b> and the STATUS(s) <b>82</b> as shown in FIG. <b>1</b>. In much similar manner as previously described in the '178 patent with regard to the ARBSLOT <b>48</b>, a MISS_REQUEST <b>111</b> is generated when the data line corresponding to the ADDR <b>128</b> and the TAG <b>134</b> is absent from DCACHE <b>24</b>, and when no other request for the same data line is pending. When the MISS_ARBITRATOR <b>107</b> returns the MISS_GRANTED signal <b>112</b> in response to the MISS_REQUEST <b>111</b>, the MISS_GRANTED signal <b>112</b> is input to the SET input of the done latch <b>402</b>, thus producing an active DONE signal to prevent any further MISS-REQUEST <b>111</b> being issued.</p><p>The MISS_GRANTED signal <b>112</b> also enables the driver <b>407</b> to pass the current content of the \u02dcHIT <b>136</b><i>a </i>of the register <b>136</b> onto the CCC_MISS/HIT signal <b>408</b>, which is sent to the system interface control <b>102</b>. Based on the received CCC_MISS/HIT signal <b>408</b> and the STATUS <b>82</b>, the system interface control <b>102</b> determines whether writing back, or flushing, of the data line (i.e., being pointed to by the MISS/COPY_IN ADDR <b>104</b>) from the DCACHE <b>24</b> to the memory hierarchy (not shown) is required. In an embodiment of the present invention, whenever the data line is found in the DCACHE <b>24</b>, i.e., the CCC_MISS/HIT <b>408</b> is inactive, and the STATUS <b>82</b> indicate that the cache line is dirty, the system interface control <b>102</b> causes the data line (i.e., being pointed to by the MISS/COPY_IN ADDR <b>104</b>) to be written out to the processor that requested the cache line.</p><p>When the DPRESLOT <b>200</b> receives the indicated cache coherency check result on the TRANS _TYPE input <b>201</b> driven by the CCCSLOT <b>400</b>, the DPRESLOT <b>200</b> initiates a pre-flushing operation in accordance with the principles of the present invention, which will now be described with references to FIGS. 2 and 5.</p><p>In particular, FIG. 5 shows a flow diagram of an exemplary embodiment of the pre-flushing process, in step <b>501</b> of which, the transactional interface between the instruction processing mechanism <b>39</b><i>b </i>and the system interface control <b>102</b> (which will be referred to as simply the \u201ctransactional interface\u201d hereafter) is continuously monitored for a presence of any transaction, which may be accomplished by, for example, by monitoring for an assertion of the MISS_CAV <b>101</b> in the exemplary DPRESLOT <b>200</b> shown in FIG. <b>2</b>.</p><p>Once a transaction is detected, a determination is made whether there is a valid address present in the transactional interface, e.g., by detecting the MISS_CAV signal <b>101</b> being asserted (step <b>502</b>). When it is determined that a valid address is not present on the transactional interface, then the process returns to step <b>501</b>, i.e., the monitoring of the transactional interface continues.</p><p>On the other hand, if a valid address is detected, the process proceeds to step <b>503</b>, during which a determination is made whether the transaction is a coherency response resulting from a cache coherency check. When it is determined that the transaction is not a cache coherency response, then the process returns to step <b>501</b>, i.e., the monitoring of the transactional interface continues.</p><p>However, if the transaction is a coherency response, e.g., a coherency response transaction requiring the copy of dirty data as indicated by STATUS <b>82</b>, the monitoring of the transactional interface is halted, e.g., by setting the busy latch <b>203</b> to halt the updating of the register <b>136</b>. Then, in step <b>505</b>, one or more address(s) of data lines to be pre-flushed are calculated. The adjacent address logic <b>213</b> calculates the to-be-pre-flushed addresses by inverting one or more bits (e.g., the least significant bit (LSB)) of the address of the data line present on the MISS/COPY_IN ADDR <b>104</b>.</p><p>In step <b>506</b>, a cache look-up operation is performed for the addresses calculated during the above step <b>505</b>. The DPRESLOT <b>200</b> issues an ACCESS_REQ <b>115</b> presenting the current contents of the CACHE INDEX <b>136</b><i>b</i>, the TAG <b>136</b><i>c </i>and the STORE <b>136</b><i>d </i>on the ACCESS_ADDR <b>114</b>, the ACCESS_TAG <b>116</b> and the ACCESS_STORE <b>218</b>, respectively, to the DCACHE <b>24</b>.</p><p>In step <b>507</b>, the result of the cache look-up operation is examined to determine whether the to-be-pre-flushed data line(s) is present in the cache memory; i.e., the DPRESLOT <b>200</b> determines that a cache hit has occurred by observing the \u02dcHIT <b>118</b> being deasserted by the tag compare mechanism <b>108</b>. If a cache miss has occurred, the process returns to step <b>501</b>, and the monitoring of the transactional interface is resumed.</p><p>If, however, in step <b>507</b>, a cache hit is detected, the process proceeds to step <b>508</b>, in which a determination whether a request for the to-be-pre-flushed data line(s) is already made, e.g., by a ARBSLOT <b>48</b> shown in FIG. 1, by observing a HIT_DM <b>121</b>. If it is determined that a request for the data line is already pending, then the process returns to step <b>501</b>, and the monitoring of the transactional interface is resumed.</p><p>Finally, in step <b>509</b>, if no prior requests for the data line is pending, flush transaction for the to-be-pre-flushed data line is issued, e.g., by issuing the MISS_REQUEST <b>111</b>, which causes a memory hierarchy access by the system interface control <b>102</b> to write the data line(s) from DCACHE <b>24</b> to the memory hierarchy. To this end, the \u02dcHIT input to the AND logic gate <b>137</b> may be inverted for the purpose of using the DPRESLOT <b>200</b> for a pre-flushing operation, e.g., when the TRANS_TYPE <b>201</b> indicates a cache coherency check. In a preferred embodiment of the present invention, the STATUS <b>82</b> is consulted, and the to-be-pre-flushed data line is flushed only if the status of the to-be-pre-flushed data line indicates that the data is dirty. In the alternative, the to-be-pre-flushed data line may be flushed without regard to its status. In a preferred embodiment, once the request for the to-be-pre-flushed data line(s) is issued, the process immediately returns to step <b>501</b>, and the entire process is continuously repeated.</p><p>As can be appreciated, an efficient system for pre-fetching and/or pre-flushing one or more data lines, which does not affect the other components of, and thus can be easily integrated into, an out of order processing system, and which also minimizes redundant multiple memory requests, has been described.</p><p>While the invention has been described with reference to the exemplary embodiments thereof, those skilled in the art will be able to make various modifications to the described embodiments of the invention without departing from the true spirit and scope of the invention. The terms and descriptions used herein are set forth by way of illustration only and are not meant as limitations. In particular, although the method of the present invention has been described by examples, the steps of the method maybe performed in a different order than illustrated or simultaneously. Those skilled in the art will recognize that these and other variations are possible within the spirit and scope of the invention as defined in the following claims and their equivalents.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Gregg B", "last_name": "Lesartre", "name": ""}, {"first_name": "David Jerome", "last_name": "Johnson", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "HEWLETT-PACKARD COMPANY"}, {"first_name": "", "last_name": "HEWLETT-PACKARD DEVELOPMENT COMPANY, L.P.", "name": ""}, {"first_name": "", "last_name": "HEWLETT-PACKARD COMPANY", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMDE"}, {"label": "G06F   9/38        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711135"}, {"primary": false, "label": "711E12026"}, {"primary": false, "label": "712E09048"}, {"primary": false, "label": "711143"}, {"primary": false, "label": "711146"}, {"primary": false, "label": "711141"}], "ecla_classes": [{"label": "S06F212:507"}, {"label": "Y02B60:12F"}, {"label": "G06F  12/08B4P"}, {"label": "G06F   9/38D4"}], "cpc_classes": [{"label": "G06F  12/0815"}, {"label": "G06F2212/507"}, {"label": "Y02D  10/00"}, {"label": "G06F2212/507"}, {"label": "G06F   9/3834"}, {"label": "G06F  12/0815"}, {"label": "Y02D  10/00"}, {"label": "G06F   9/3834"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "2000-05-04", "application_date": "2000-05-04", "family_members": [{"ucid": "DE-10113191-A1", "titles": [{"lang": "EN", "text": "Computer power optimization by speculative calling-up and clearing of data in a computer processor cache memory bank, in which orders can be carried out in an un-ordered fashion so reducing cache coherence latency"}, {"lang": "DE", "text": "Spekulatives Vorausr\u00e4umen von Daten in einem Prozessorsystem mit einer ungeordneten Ausf\u00fchrung"}]}, {"ucid": "US-6408363-B1", "titles": [{"lang": "EN", "text": "Speculative pre-flush of data in an out-of-order execution processor system"}]}, {"ucid": "DE-10113191-B4", "titles": [{"lang": "EN", "text": "Speculative preprocessing of data in a processor system with a disorderly execution"}, {"lang": "DE", "text": "Spekulatives Vorausr\u00e4umen von Daten in einem Prozessorsystem mit einer ungeordneten Ausf\u00fchrung"}]}]}