{"patent_number": "US-6202139-B1", "publication_id": 72607519, "family_id": 22279045, "publication_date": "2001-03-13", "titles": [{"lang": "EN", "text": "Pipelined data cache with multiple ports and processor with load/store unit selecting only load or store operations for concurrent processing"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72525151\"><p>A computer system includes a processor having a cache which includes multiple ports, although a storage array included within the cache may employ fewer physical ports than the cache supports. The cache is pipelined and operates at a clock frequency higher than that employed by the remainder of a microprocessor including the cache. In one embodiment, the cache preferably operates at a clock frequency which is at least a multiple of the clock frequency at which the remainder of the microprocessor operates. The multiple is equal to the number of ports provided on the cache (or the ratio of the number of ports provided on the cache to the number of ports provided internally, if more than one port is supported internally). Accordingly, the accesses provided on each port of the cache during a clock cycle of the microprocessor clock can be sequenced into the cache pipeline prior to commencement of the subsequent clock cycle. In one particular embodiment, the load/store unit of the microprocessor is configured to select only load memory operations or only store memory operations for concurrent presentation to the data cache. Accordingly, the data cache may be performing only reads or only writes to its internal array during a clock cycle. The data cache may implement several techniques for accelerating access time based upon this feature. For example, the bit lines within the data cache array may be only balanced between accesses instead of precharging (and potentially balancing).</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00001\" num=\"1\"><claim-text>1. A cache comprising:</claim-text><claim-text>a plurality of ports operating, during use, at a first clock frequency, each of said plurality of ports configured to concurrently receive a different cache access according to a first clock signal having said first clock frequency; and </claim-text><claim-text>a pipeline coupled to said plurality of ports, wherein said pipeline comprises a plurality of pipeline stages and is configured to select, into the pipeline, one cache access per clock cycle of a second clock signal having a second clock frequency, said second clock frequency being a multiple of said first clock frequency, the cache accesses selected in consecutive clock cycles of the second clock signal being selected from different ones of the plurality of ports, and wherein the multiple is greater than one, and wherein each of said plurality of pipeline stages is coupled to receive said second clock signal and is configured to operate responsive to the second clock signal. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00002\" num=\"2\"><claim-text>2. The cache as recited in claim <b>1</b> wherein said pipeline comprises a mux and sequence unit coupled to said plurality of ports, wherein said mux and sequence unit is configured to sequence cache accesses presented to said plurality of ports at a rate of one access per clock cycle of said second clock signal.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00003\" num=\"3\"><claim-text>3. The cache as recited in claim <b>2</b> wherein said mux and sequence unit is included in a first stage of said plurality of pipeline stages.</claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00004\" num=\"4\"><claim-text>4. The cache as recited in claim <b>3</b> wherein said mux and sequence unit is configured to receive a set of capture and send pulses, and wherein said mux and sequence unit is configured to mux and sequence one of said cache accesses per capture and send pulse received.</claim-text></claim>"}, {"num": 5, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00005\" num=\"5\"><claim-text>5. The cache as recited in claim <b>4</b>, wherein said pipeline further comprises a row decoder coupled to said mux and sequence unit, wherein said row decoder is configured to decode an address of said one of said cache accesses provided by said mux and sequence unit, and wherein said row decoder is included in a second stage of said plurality of pipeline stages.</claim-text></claim>"}, {"num": 6, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00006\" num=\"6\"><claim-text>6. The cache as recited in claim <b>5</b> wherein said pipeline further comprises a set of row drivers coupled to said row decoder, an array coupled to said set of row drivers, and a set of sense amplifiers coupled to said array, wherein said set of row drivers, said array, and said set of sense amplifiers are included in a third stage of said plurality of pipeline stages.</claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00007\" num=\"7\"><claim-text>7. The cache as recited in claim <b>6</b> wherein said array comprises a pair of bit lines for each bit stored in a row, and wherein said array is configured to balance the pairs of bit lines between cache accesses concurrently provided to said plurality of ports instead of precharging the pairs of bit lines.</claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00008\" num=\"8\"><claim-text>8. The cache as recited in claim <b>7</b> wherein said array is configured to precharge the pairs of bit lines subsequent to access by the cache accesses concurrently provided to said plurality of ports and prior to subsequent cache accesses.</claim-text></claim>"}, {"num": 9, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00009\" num=\"9\"><claim-text>9. The cache as recited in claim <b>6</b> wherein said pipeline further comprises a plurality of outputs coupled to said set of sense amplifiers, wherein said plurality of outputs are configured to convey data read in response to load memory operations on said plurality of ports, wherein said plurality of outputs are included within a fourth stage of said pipeline.</claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00010\" num=\"10\"><claim-text>10. The cache as recited in claim <b>9</b> wherein said multiple is four and the number of said plurality of ports is four.</claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00011\" num=\"11\"><claim-text>11. A processor comprising:</claim-text><claim-text>a data cache having a plurality of ports; and </claim-text><claim-text>a load/store unit coupled to said data cache, said load/store unit configured to select a memory operation for each of said plurality of ports, wherein said load/store unit is configured to select a first load memory operation for presentation on a first one of said plurality of ports and to select only load memory operations for concurrent presentation on remaining ones of said plurality of ports responsive to selecting said first load memory operation for presentation on said first one of said plurality of ports, and wherein said load store unit is configured to select a first store memory operation for presentation on said first one of said plurality of ports and to select only store memory operations for concurrent presentation on said remaining ones of said plurality of ports responsive to selecting said first store memory operation for presentation of said first one of said plurality of ports. </claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00012\" num=\"12\"><claim-text>12. The processor as recited in claim <b>11</b> wherein said data cache comprises an array having a number of ports less than the number of said plurality of ports of said data cache, said array operating, during use, at a first clock frequency which is a multiple of a second clock frequency at which said load/store unit operates, during use.</claim-text></claim>"}, {"num": 13, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00013\" num=\"13\"><claim-text>13. The processor as recited in claim <b>12</b> wherein said data cache is pipelined in stages operating, during use, at said first clock frequency.</claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00014\" num=\"14\"><claim-text>14. The processor as recited in claim <b>13</b> wherein said data cache includes a first pipeline stage comprising a mux and sequence unit coupled to said plurality of ports, wherein said mux and sequence unit is configured to sequence memory operations presented to said plurality of ports at a rate of one memory operation per clock cycle of a first clock signal operating, during use, at said first clock frequency.</claim-text></claim>"}, {"num": 15, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00015\" num=\"15\"><claim-text>15. The processor as recited in claim <b>14</b> wherein said data cache includes a second pipeline stage comprising a row decoder coupled to said mux and sequence unit, wherein said row decoder is configured to decode an address of one of said memory operations provided by said mux and sequence unit.</claim-text></claim>"}, {"num": 16, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00016\" num=\"16\"><claim-text>16. The processor as recited in claim <b>15</b> wherein said data cache includes a third pipeline stage comprising a set of row drivers coupled to said row decoder, an array coupled to said set of row drivers, and a set of sense amplifiers coupled to said array.</claim-text></claim>"}, {"num": 17, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00017\" num=\"17\"><claim-text>17. The processor as recited in claim <b>16</b> wherein said data cache includes a fourth pipeline stage comprising a plurality of outputs coupled to said set of sense amplifiers, wherein said plurality of outputs are configured to convey data read in response to load memory operations on said plurality of ports.</claim-text></claim>"}, {"num": 18, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00018\" num=\"18\"><claim-text>18. The processor as recited in claim <b>16</b> wherein said array comprises a pair of bit lines for each bit stored in a row, and wherein said array is configured to balance the pairs of bit lines between memory operations concurrently provided to said plurality of ports instead of precharging the pairs of bit lines.</claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00019\" num=\"19\"><claim-text>19. The processor as recited in claim <b>18</b> wherein said array is configured to precharge the pairs of bit lines subsequent to access by the memory operations concurrently provided to said plurality of ports and prior to subsequent memory operations.</claim-text></claim>"}, {"num": 20, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00020\" num=\"20\"><claim-text>20. A computer system comprising:</claim-text><claim-text>a processor including a data cache, said data cache having a plurality of ports, and a load/store unit configured to select a memory operation for each of said plurality of ports, wherein said load/store unit is configured to select only load memory operations for concurrent presentation on said plurality of ports or only store memory operations for concurrent presentation on said plurality of ports, and wherein said data cache comprises an array having a number of ports less than the number of said plurality of ports of said data cache, said array operating, during use, at a first clock frequency which is a multiple of a second clock frequency at which said load/store unit operates, during use, and wherein said data cache is pipelined in stages operating, during use, at said first clock frequency to provide access from each of said plurality of ports; and </claim-text><claim-text>a peripheral device configured to provide communication external to said computer system. </claim-text></claim>"}, {"num": 21, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00021\" num=\"21\"><claim-text>21. The cache as recited in claim <b>1</b> wherein the multiple is equal to the number of said plurality of ports.</claim-text></claim>"}, {"num": 22, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00022\" num=\"22\"><claim-text>22. A computer system comprising:</claim-text><claim-text>a processor comprising: </claim-text><claim-text>a data cache having a plurality of ports; and </claim-text><claim-text>a load/store unit coupled to said data cache, said load/store unit configured to select a memory operation for each of said plurality of ports, wherein said load/store unit is configured to select a first load memory operation for presentation on a first one of said plurality of ports and to select only load memory operations for concurrent presentation on remaining ones of said plurality of ports responsive to selecting said first load memory operation for presentation on said first one of said plurality of ports, and wherein said load store unit is configured to select a first store memory operation for presentation on said first one of said plurality of ports and to select only store memory operations for concurrent presentation on said remaining ones of said plurality of ports responsive to selecting said first store memory operation for presentation of said first one of said plurality of ports; and </claim-text><claim-text>a peripheral device configured to provide communication external to said computer system. </claim-text></claim>"}, {"num": 23, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00023\" num=\"23\"><claim-text>23. A method comprising:</claim-text><claim-text>receiving a first cache access on a first port of a cache during a first clock cycle of a first clock signal, the first port operating responsive to said first clock signal which has a first clock frequency; </claim-text><claim-text>receiving a second cache access on a second port of a cache during the first clock cycle, the second port operating responsive to said first clock signal; </claim-text><claim-text>selecting said first cache access from said first port into a pipeline of said cache during a second clock cycle of the second clock signal, the pipeline comprising a plurality of pipeline stages for performing cache accesses, the plurality of pipeline stages operating responsive to the second clock signal which has a second clock frequency which is a multiple of the first clock frequency, the multiple being greater than one; and </claim-text><claim-text>selecting said second cache access from said second port into said pipeline during a third clock cycle of the second clock signal consecutive to the second clock cycle. </claim-text></claim>"}, {"num": 24, "parent": 23, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00024\" num=\"24\"><claim-text>24. The method as recited in claim <b>23</b> wherein said selecting said first cache access and said selecting said second cache access are performed in a first stage of the plurality of pipeline stages.</claim-text></claim>"}, {"num": 25, "parent": 24, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00025\" num=\"25\"><claim-text>25. The method as recited in claim <b>24</b> further comprising decoding an address in a second stage of the plurality of pipeline stages.</claim-text></claim>"}, {"num": 26, "parent": 25, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00026\" num=\"26\"><claim-text>26. The method as recited in claim <b>25</b> further comprising accessing an array in a third stage of the plurality of pipeline stages.</claim-text></claim>"}, {"num": 27, "parent": 26, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00027\" num=\"27\"><claim-text>27. The method as recited in claim <b>26</b> further comprising balancing pairs of bit lines in said array between cache accesses instead of precharging.</claim-text></claim>"}, {"num": 28, "parent": 27, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00028\" num=\"28\"><claim-text>28. The method as recited in claim <b>27</b> further comprising precharging the pairs of bit lines prior to receiving a third cache access on said first port during a fourth clock cycle of said first clock signal, the fourth clock cycle being consecutive to the first clock cycle.</claim-text></claim>"}, {"num": 29, "parent": 26, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00029\" num=\"29\"><claim-text>29. The method as recited in claim <b>26</b> further comprising driving array output data accessed from said array in a fourth stage of said plurality of pipeline stages.</claim-text></claim>"}, {"num": 30, "parent": 23, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00030\" num=\"30\"><claim-text>30. The method as recited in claim <b>23</b> wherein said multiple is equal to a number of ports on the cache.</claim-text></claim>"}, {"num": 31, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00031\" num=\"31\"><claim-text>31. A method comprising:</claim-text><claim-text>selecting either a first load memory operation or a first store memory operation for presentation on a first port of a plurality of ports on a cache; </claim-text><claim-text>selecting only additional load memory operations for concurrent presentation on remaining ones of said plurality of ports responsive to selecting said first load memory operation; and </claim-text><claim-text>selecting only additional store memory operations for concurrent presentation on said remaining ones of said plurality of ports responsive to selecting said first store memory operation. </claim-text></claim>"}, {"num": 32, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00032\" num=\"32\"><claim-text>32. A processor comprising:</claim-text><claim-text>a data cache having a plurality of ports; and </claim-text><claim-text>a load/store unit coupled to said data cache, said load/store unit configured to select a memory operation for each of said plurality of ports, wherein said load/store unit is configured to select only load memory operations for concurrent presentation on said plurality of ports or only store memory operations for concurrent presentation on said plurality of ports; </claim-text><claim-text>wherein said data cache comprises an array having a number of ports less than the number of said plurality of ports of said data cache, said array operating, during use, at a first clock frequency which is a multiple of a second clock frequency at which said load/store unit operates, during use. </claim-text></claim>"}, {"num": 33, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00033\" num=\"33\"><claim-text>33. The processor as recited in claim <b>32</b> wherein said data cache is pipelined in stages operating, during use, at said first clock frequency.</claim-text></claim>"}, {"num": 34, "parent": 33, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00034\" num=\"34\"><claim-text>34. The processor as recited in claim <b>33</b> wherein said data cache includes a first pipeline stage comprising a mux and sequence unit coupled to said plurality of ports, wherein said mux and sequence unit is configured to sequence memory operations presented to said plurality of ports at a rate of one memory operation per clock cycle of a first clock signal operating, during use, at said first clock frequency.</claim-text></claim>"}, {"num": 35, "parent": 34, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00035\" num=\"35\"><claim-text>35. The processor as recited in claim <b>34</b> wherein said data cache includes a second pipeline stage comprising a row decoder coupled to said mux and sequence unit, wherein said row decoder is configured to decode an address of one of said memory operations provided by said mux and sequence unit.</claim-text></claim>"}, {"num": 36, "parent": 35, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00036\" num=\"36\"><claim-text>36. The processor as recited in claim <b>35</b> wherein said data cache includes a third pipeline stage comprising a set of row drivers coupled to said row decoder, an array coupled to said set of row drivers, and a set of sense amplifiers coupled to said array.</claim-text></claim>"}, {"num": 37, "parent": 36, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00037\" num=\"37\"><claim-text>37. The processor as recited in claim <b>36</b> wherein said data cache includes a fourth pipeline stage comprising a plurality of outputs coupled to said set of sense amplifiers, wherein said plurality of outputs are configured to convey data read in response to load memory operations on said plurality of ports.</claim-text></claim>"}, {"num": 38, "parent": 36, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00038\" num=\"38\"><claim-text>38. The processor as recited in claim <b>36</b> wherein said array comprises a pair of bit lines for each bit stored in a row, and wherein said array is configured to balance the pairs of bit lines between memory operations concurrently provided to said plurality of ports instead of precharging the pairs of bit lines.</claim-text></claim>"}, {"num": 39, "parent": 38, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202139-B1-CLM-00039\" num=\"39\"><claim-text>39. The processor as recited in claim <b>38</b> wherein said array is configured to recharge the pairs of bit lines subsequent to access by the memory operations concurrently provided to said plurality of ports and prior to subsequent memory operations.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54510771\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>The present invention is related to the field of microprocessors and, more particularly, to data caches employed within microprocessors.</p><p>2. Description of the Related Art</p><p>Superscalar microprocessors attempt to achieve high performance by issuing/executing multiple instructions concurrently. To the extent that superscalar microprocessors are successful at issuing/executing multiple instructions concurrently, high performance may be realized. Several factors may influence the successful concurrent issue/execution of instructions. For example, a first instruction which is dependent upon a second instruction (e.g. for a source operand) generally does not issue/execute concurrently with the first instruction. Still further, the frequency of branch instructions (which determine which instructions will be fetched next from a variety of sources) may impact the number of instructions available for issue and hence the number of instructions issued concurrently.</p><p>In the continuing evolution of superscalar microprocessors, the maximum issue rate (i.e. the number of instructions which can be concurrently issued) has been increasing. In other words, a trend toward wider issue superscalar microprocessors has been occurring. While additional performance gains may be realized by allowing for larger numbers of instructions to concurrently issue, wider issue microprocessors may face additional design challenges as well.</p><p>Among the additional design challenges is providing sufficient data cache ports for the number of memory operations which may be concurrently issued. As used herein, the term \u201cport\u201d, in connection with a cache, refers to a facility for accessing the cache in response to one memory operation. Other memory operations use other ports for accessing the cache concurrently. Superscalar microprocessors generally include data caches to decrease the latency of access to memory operands. Instruction sequences include a certain number of memory operations to access and/or update memory operands. Generally speaking, a memory operation specifies the transfer of data between the microprocessor and a memory external to the microprocessor (although the transfer may be completed via an internal cache). Load memory operations specify the transfer of data from a memory to the microprocessor, while store memory operations specify the transfer of data from the microprocessor to the memory. Memory operations may be explicit instructions, or an implicit part of another instruction specifying a memory operand, depending upon the instruction set architecture employed by the microprocessor.</p><p>As issue rates increase, the number of memory operations for which concurrent access to a cache is desired increases as well. If concurrent access is not provided (by providing sufficient data cache ports), then performance generally degrades. For example, many instructions are dependent upon load memory operations (either directly or indirectly) for source operands. Such dependent instructions typically cannot execute if the load memory operations are stalled due to a lack of available cache ports. Additionally, pipeline stalls may develop if subsequent memory operations attempt to issue prior to execution of prior memory operations and the available resources for queuing memory operations become full.</p><p>Various methods for multiporting data caches have been employed in the past. For example, the cache arrays may be physically multiported (allowing for concurrent access to any storage location within the array from each port in parallel with access to any other storage location from the other ports). Unfortunately, physically multiporting the array typically leads to large increases in the microprocessor chip area occupied by the array. The size of the chip is important to chip yields and number of chips per semiconductor wafer, and hence to the cost of producing the microprocessor. Accordingly, increase in the area occupied by a cache array is generally undesirable.</p><p>Another method employed to provide multiported cache access is to bank the cache. Each port may access one of the banks in parallel with a different port accessing a different bank. If two or more memory operations which would otherwise concurrently access the data cache actually access data within the same bank, one of the memory operations completes and the others are inhibited. Unfortunately, even with a large number of available ports, concurrent access to the data cache may not be achieved due to the occurrence of bank conflicts. Accordingly, a solution to multiporting a data cache which does not incur the disadvantages of physically multiporting the array or banking the cache is desired.</p><h4>SUMMARY OF THE INVENTION</h4><p>The problems outlined above are in large part solved by a cache in accordance with the present invention. The cache includes multiple ports, although the storage array included within the cache employs fewer ports than the cache supports. The cache is pipelined and operates at a clock frequency higher than that employed by the remainder of a microprocessor including the cache. Advantageously, the multiple accesses can be pipelined through the cache and the cache may internally have fewer ports than the number of ports actually supported by the cache. Accordingly, the cache may be implementable in a smaller area then a cache supporting more ports internally. Additionally, since the accesses are pipelined instead of applied to separate banks, the performance losses due to bank conflicts may be avoided. The cache may provide multiport access to support wide issue superscalar microprocessors in a small area and with high performance.</p><p>In one embodiment, the cache preferably operates at a clock frequency which is at least a multiple of the clock frequency at which the remainder of the microprocessor operates. The multiple is equal to the number of ports provided on the cache (or the ratio of the number of ports provided on the cache to the number of ports provided internally, if more than one port is supported internally). Accordingly, the accesses provided on each port of the cache during a clock cycle of the microprocessor clock can be sequenced into the cache pipeline prior to commencement of the subsequent clock cycle.</p><p>In one particular embodiment, the load/store unit of the microprocessor is configured to select only load memory operations or only store memory operations for concurrent presentation to the data cache. Accordingly, the data cache may be performing only reads or only writes to its internal array during a clock cycle. The data cache may implement several techniques for accelerating access time based upon this feature. For example, the bit lines within the data cache array may be only balanced between accesses instead of precharging (and potentially balancing).</p><p>Broadly speaking, the present invention contemplates a cache comprising a plurality of ports and a pipeline. The plurality of ports are operable at a first clock frequency and each of the plurality of ports is configured to concurrently receive a different cache access according to a first clock signal operable at the first clock frequency. Coupled to the plurality of ports, the pipeline is configured to perform one cache access per clock cycle of a second clock signal operable at a second clock frequency. The second clock frequency is at least a multiple of the first clock frequency, wherein the multiple is equal to a number of the plurality of ports.</p><p>The present invention further contemplates a processor comprising a data cache having a plurality of ports and a load/store unit. Coupled to the data cache, the load/store unit is configured to select a memory operation for each of the plurality of ports. The load/store unit is configured to select only load memory operations for concurrent presentation on the plurality of ports or only store memory operations for concurrent presentation on the plurality of ports.</p><p>Moreover, the present invention contemplates a computer system comprising a processor and an input/output (I/O) device. The processor includes a data cache having a plurality of ports and a load/store unit configured to select a memory operation for each of the plurality of ports. The load/store unit is additionally configured to select only load memory operations for concurrent presentation on the plurality of ports or only store memory operations for concurrent presentation on the plurality of ports. The data cache comprises an array having a number of ports less than a number of the plurality of ports. The array is operable at a first clock frequency which is a multiple of a second clock frequency at which the load/store unit is operable. The data cache is pipelined in stages operable at the first clock frequency to provide access from each of the plurality of ports. The I/O device is configured to provide communication between the computer system and another computer system to which the I/O device is coupled.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Other objects and advantages of the invention will become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p><p>FIG. 1 is a block diagram of a microprocessor.</p><p>FIG. 2 is a flow chart illustrating operation of one embodiment of a load/store unit shown in FIG. <b>1</b>.</p><p>FIG. 3 is a block diagram of one embodiment of a data cache shown in FIG. <b>1</b>.</p><p>FIG. 4 is a block diagram of a portion of an array shown in FIG. <b>3</b>.</p><p>FIG. 5 is a circuit diagram of one embodiment of a multiplexer (mux) and sequence unit shown in FIG. <b>3</b>.</p><p>FIG. 6 is a timing diagram illustrating capture and send pulses provided to the circuit shown in FIG. <b>5</b>.</p><p>FIG. 7 is a timing diagram illustrating operation of one embodiment of the load/store unit shown in FIG. <b>1</b> and the data cache shown in FIG. <b>3</b>.</p><p>FIG. 8 is a block diagram of a computer system including the microprocessor shown in FIG. <b>1</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><p>While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><h4>DETAILED DESCRIPTION OF THE INVENTION</h4><p>Turning now to FIG. 1, a block diagram of one embodiment of a microprocessor <b>10</b> is shown. Other embodiments are possible and contemplated. As shown in FIG. 1, microprocessor <b>10</b> includes an instruction cache <b>12</b>, a data cache <b>14</b>, a decode unit <b>16</b>, a plurality of reservation stations including reservation stations <b>17</b>A-<b>17</b>F, a plurality of execute units <b>18</b>A-<b>18</b>E, a load/store unit <b>20</b>, a reorder buffer <b>22</b>, a register file <b>24</b>, a microcode unit <b>28</b>, and a bus interface unit <b>32</b>. Elements referred to herein with a particular reference number followed by a letter will be collectively referred to by the reference number alone. For example, the plurality of execute units will be collectively referred to herein as execute units <b>18</b>. Execute units <b>18</b> may include more or fewer execute units than execute units <b>18</b>A-<b>18</b>E shown in FIG. <b>1</b>.</p><p>As shown in FIG. 1, instruction cache <b>12</b> is coupled to bus interface unit <b>32</b> and to decode unit <b>16</b>, which is further coupled to reservation stations <b>17</b>, reorder buffer <b>22</b>, register file <b>24</b>, and microcode unit <b>28</b>. Reorder buffer <b>22</b>, execute units <b>18</b>, and data cache <b>14</b> are each coupled to a result bus <b>30</b> for forwarding of execution results. Furthermore, reservation stations <b>17</b>A-<b>17</b>E are coupled to a respective execute unit <b>18</b>A-<b>18</b>E, while reservation station <b>17</b>F is coupled to load/store unit <b>20</b>. Each reservation station <b>17</b> is coupled to receive operand information from reorder buffer <b>22</b>. Load/store unit <b>20</b> is coupled to data cache <b>14</b>, which is further coupled to bus interface unit <b>32</b>. Bus interface unit <b>32</b> is coupled to a CPU bus <b>224</b>.</p><p>Generally speaking, microprocessor <b>10</b> employs a multiported data cache <b>14</b>, allowing for multiple memory operations to be performed in parallel. The array within data cache <b>14</b> is physically single ported, but data cache <b>14</b> is pipelined into multiple stages. The pipeline within data cache <b>14</b> is operated at a clock frequency which is a multiple of the clock frequency at which the remainder of microprocessor <b>10</b> operates. The multiple of the clock frequency may be greater than or equal to the number of ports provided on data cache <b>14</b>. The operations from the various ports are sequenced into the pipeline to provide multiple concurrent access (as viewed by the remainder of microprocessor <b>10</b> operating at the lower clock frequency). Before the end of the clock cycle in which multiple memory operations are presented for cache access, each of the memory operations has been sequenced into the pipeline. Accordingly, a subsequent set of memory operations may be presented in the next clock cycle to the data cache ports of data cache <b>14</b>. Advantageously, multiple ports are supported without physically multiporting the array within data cache <b>14</b>. Furthermore, data cache <b>14</b> may be a non-banked structure. Therefore, bank conflicts are eliminated. Memory operations may be selected for access without regard to which memory locations are accessed within data cache <b>14</b>, and the operations may complete cache access uninhibited.</p><p>Microprocessor <b>10</b> as shown in FIG. 1 is a wide-issue superscalar microprocessor and hence performing multiple memory operations via multiple ports on data cache <b>14</b> may support high execution rates by rapidly providing memory operands for instructions. Load/store unit <b>20</b> receives memory operations corresponding to any instructions which may be dispatched to execute units <b>18</b> as well as any instructions which may be purely memory operations (i.e. load/store instructions). Load/store unit <b>20</b> may be configured to generate addresses for memory operations, or may be configured to receive addresses from separate address generation units. In yet another alternative, load/store unit <b>20</b> may be configured to receive addresses from execute units <b>18</b>.</p><p>In one embodiment, load/store unit <b>20</b> is configured to select only load memory operations or only store memory operations for concurrent presentation to data cache <b>14</b>. Accordingly, data cache <b>14</b> may be performing only reads or only writes to its internal array during a clock cycle. Data cache <b>14</b> may implement several techniques for accelerating access time based upon this feature, as described in more detail below. In one exemplary embodiment, data cache <b>14</b> may include four ports and hence may perform up to four load memory operations or four store memory operations per clock cycle. Store memory operations may be performed via two accesses, according to one particular embodiment. During the first access, the affected cache lines are read from data cache <b>14</b>. During the second access, the affected cache lines are updated with the store data. The store memory operations may be speculatively performed without first determining a hit/miss status within data cache <b>14</b>. If a store memory operation is a miss, the data read in the first access for the store may be written to memory (if updated while in data cache <b>14</b>) and the remainder of the cache line updated by the store memory operation may be read from memory.</p><p>As used herein, the term \u201cclock frequency\u201d refers to the inverse of the period of a clock signal used to clock a pipeline. For example, the pipelines employed within microprocessor <b>10</b> for instruction processing are clocked at a first clock frequency corresponding to an ICLK signal as discussed in more detail below. The pipeline within data cache <b>14</b> is clocked at a second clock frequency corresponding to a CLK<b>1</b> signal and a CLK<b>2</b> signal as discussed in more detail below. The second clock frequency is at least a multiple of the first clock frequency, wherein the multiple is equal to the number of ports on data cache <b>14</b>. One period of a clock signal is referred to as a \u201cclock cycle\u201d as used herein, and describes the amount of time within which each stage of a pipeline completes its work upon a particular item and is therefore ready for a new item at commencement of the next clock cycle. Generally, a pipeline is a set of two or more stages designed to accomplish a function, wherein multiple items may be in the pipeline (one per stage) such that processing of the function upon each item may be overlapped with the processing of other items. Each stage within the pipeline performs a predetermined portion of the overall function upon each item conveyed to that stage.</p><p>Instruction cache <b>12</b> is a high speed cache memory for storing instructions. It is noted that instruction cache <b>12</b> may be configured into a set-associative or direct mapped configuration. Instruction cache <b>12</b> may additionally include a branch prediction mechanism for predicting branch instructions as either taken or not taken. Instructions are fetched from instruction cache <b>12</b> and conveyed to decode unit <b>16</b> for decoding and dispatch to a reservation station <b>17</b>.</p><p>Decode unit <b>16</b> decodes each instruction fetched from instruction cache <b>12</b>. Decode unit <b>16</b> dispatches the instruction to one or more of reservation stations <b>17</b> depending upon the type of instruction detected. For example, if a given instruction includes a memory operand, decode unit <b>16</b> may signal load/store unit <b>20</b> to perform a load/store (i.e. read/write) memory operation in response to the given instruction.</p><p>Decode unit <b>16</b> also detects the register operands used by the instruction and requests these operands from reorder buffer <b>22</b> and register file <b>24</b>. In one embodiment, execute units <b>18</b> are symmetrical execution units. Symmetrical execution units are each configured to execute a particular subset of the instruction set employed by microprocessor <b>10</b>. The subsets of the instruction set executed by each of the symmetrical execution units are the same. In another embodiment, execute units <b>18</b> are asymmetrical execution units configured to execute dissimilar instruction subsets. For example, execute units <b>18</b> may include a branch execute unit for executing branch instructions, one or more arithmetic/logic units for executing arithmetic and logical instructions, and one or more floating point units for executing floating point instructions. Decode unit <b>16</b> dispatches an instruction to a reservation station <b>17</b> which is coupled to an execute unit <b>18</b> or load/store unit <b>20</b> which is configured to execute that instruction.</p><p>Microcode unit <b>28</b> is included for handling instructions for which the architecturally defined operation is more complex than the hardware employed within execute units <b>18</b> and load/store unit <b>20</b> may handle. Microcode unit <b>28</b> parses the complex instruction into multiple instructions which execute units <b>18</b> and load/store unit <b>20</b> are capable of executing.</p><p>Load/store unit <b>20</b> provides an interface between execute units <b>18</b> and data cache <b>14</b>. Load and store memory operations are performed by load/store unit <b>20</b> to data cache <b>14</b>. Additionally, memory dependencies between load and store memory operations are detected and handled by load/store unit <b>20</b>.</p><p>Reservation stations <b>17</b> are configured to store instructions whose operands have not yet been provided. An instruction is selected from those stored in a reservation station <b>17</b>A-<b>17</b>F for execution if: (1) the operands of the instruction have been provided, and (2) the instructions within the reservation station <b>17</b>A-<b>17</b>F which are prior to the instruction being selected in program order have not yet received operands. It is noted that a centralized reservation station may be included instead of separate reservations stations. The centralized reservation station is coupled between decode unit <b>16</b>, execute units <b>18</b>, and load/store unit <b>20</b>. Such an embodiment may perform the dispatch function within the centralized reservation station.</p><p>Microprocessor <b>10</b> supports out of order execution, and employs reorder buffer <b>22</b> for storing execution results of speculatively executed instructions and storing these results into register file <b>24</b> in program order, for performing dependency checking and register renaming, and for providing for mispredicted branch and exception recovery. When an instruction is decoded by decode unit <b>16</b>, requests for register operands are conveyed to reorder buffer <b>22</b> and register file <b>24</b>. In response to the register operand requests, one of three values is transferred to the reservation station <b>17</b>A-<b>17</b>F which receives the instruction: (1) the value stored in reorder buffer <b>22</b>, if the value has been speculatively generated; (2) a tag identifying a location within reorder buffer <b>22</b> which will store the result, if the value has not been speculatively generated; or (3) the value stored in the register within register file <b>24</b>, if no instructions within reorder buffer <b>22</b> modify the register. Additionally, a storage location within reorder buffer <b>22</b> is allocated for storing the results of the instruction being decoded by decode unit <b>16</b>. The storage location is identified by a tag, which is conveyed to the unit receiving the instruction. It is noted that, if more than one reorder buffer storage location is allocated for storing results corresponding to a particular register, the value or tag corresponding to the last result in program order is conveyed in response to a register operand request for that particular register.</p><p>When execute units <b>18</b> or load/store unit <b>20</b> execute an instruction, the tag assigned to the instruction by reorder buffer <b>22</b> is conveyed upon result bus <b>30</b> along with the result of the instruction. Reorder buffer <b>22</b> stores the result in the indicated storage location. Additionally, reservation stations <b>17</b> compare the tags conveyed upon result bus <b>30</b> with tags of operands for instructions stored therein. If a match occurs, the reservation station captures the result from result bus <b>30</b> and stores it with the corresponding instruction. In this manner, an instruction may receive the operands it is intended to operate upon. Capturing results from result bus <b>30</b> for use by instructions is referred to as \u201cresult forwarding\u201d.</p><p>Instruction results are stored into register file <b>24</b> by reorder buffer <b>22</b> in program order. Storing the results of an instruction and deleting the instruction from reorder buffer <b>22</b> is referred to as \u201cretiring\u201d the instruction. By retiring the instructions in program order, recovery from incorrect speculative execution may be performed. For example, if an instruction is subsequent to a branch instruction whose taken/not taken prediction is incorrect, then the instruction may be executed incorrectly. When a mispredicted branch instruction or an instruction which causes an exception is detected, reorder buffer <b>22</b> discards the instructions subsequent to the mispredicted branch instructions. Instructions thus discarded are also flushed from reservation stations <b>17</b>, execute units <b>18</b>, load/store unit <b>20</b>, and decode unit <b>16</b>.</p><p>Register file <b>24</b> includes storage locations for each register defined by the microprocessor architecture employed by microprocessor <b>10</b>. For example, microprocessor <b>10</b> may employ the \u00d786 microprocessor architecture. For such an embodiment, register file <b>24</b> includes locations for storing the EAX, EBX, ECX, EDX, ESI, EDI, ESP, and EBP register values.</p><p>Data cache <b>14</b> is a high speed cache memory configured to store data to be operated upon by microprocessor <b>10</b>. It is noted that data cache <b>14</b> may be configured into a set-associative or direct-mapped configuration. Data cache <b>14</b> allocates and deallocates storage for data in cache lines. In other words, a cache line is a block of contiguous bytes which is allocated and deallocated from a cache as a unit.</p><p>Bus interface unit <b>32</b> effects communication between microprocessor <b>10</b> and devices coupled thereto via CPU bus <b>224</b>. For example, instruction fetches which miss instruction cache <b>12</b> may be transferred from a main memory by bus interface unit <b>32</b>. Similarly, data requests performed by load/store unit <b>20</b> which miss data cache <b>14</b> may be transferred from main memory by bus interface unit <b>32</b>. Additionally, data cache <b>14</b> and/or a victim cache (not shown) may discard a cache line of data which has been modified by microprocessor <b>10</b>. Bus interface unit <b>32</b> transfers the modified line to the main memory.</p><p>It is noted that decode unit <b>16</b> may be configured to dispatch an instruction to more than one execution unit. For example, in embodiments of microprocessor <b>10</b> which employ the \u00d786 microprocessor architecture, certain instructions may operate upon memory operands. Executing such an instruction involves transferring the memory operand from data cache <b>14</b>, executing the instruction, and transferring the result to memory (if the destination operand is a memory location) or data cache <b>14</b>. Load/store unit <b>20</b> performs the memory operations, and an execute unit <b>18</b> performs the execution of the instruction.</p><p>Turning now to FIG. 2, a flowchart is shown illustrating operation of one embodiment of load/store unit <b>20</b>. Other embodiments are possible and contemplated. The steps shown in FIG. 2 are illustrated in a particular order for ease of understanding, but any suitable order may be used. Furthermore, while steps may be shown in serial order in FIG. 2, the steps may be performed in parallel in the circuitry of load/store unit <b>20</b> as desired.</p><p>Load/store unit <b>20</b> scans the memory operations within reservation station <b>17</b>F to select a first memory operation for access to data cache <b>14</b> (step <b>40</b>). A variety of methods may be used to select a first memory operation. For example, load/store unit <b>20</b> may select the oldest instruction (in program order) for which all the address operands have been provided (or for which the address has been provided if load/store unit <b>20</b> does not generate addresses). On the other hand, load/store unit <b>20</b> may be configured to select load memory operations which have received address operands or addresses until the number of store memory operations which have received address operands or addresses and store data is equal to the number of ports on data cache <b>14</b>. Using this method, load memory operations (which are typically more frequent and the delay of which may be performance limiting since subsequent instructions may depend on the load memory operations) are performed until a number of stores ready for data cache access is sufficient to occupy each of the ports of data cache <b>14</b>.</p><p>Once a first memory operation has been selected, load/store unit <b>20</b> selects additional memory operations for access to data cache <b>14</b> based upon the type of memory operations selected in step <b>40</b> (decision block <b>42</b>). If the first memory operation is a load memory operation, load/store unit <b>20</b> selects additional load memory operations up to the number of ports provided on data cache <b>14</b> (step <b>44</b>). On the other hand, if the first memory operation is a store memory operation, load/store unit <b>20</b> selects additional store memory operations for access to data cache <b>14</b> (step <b>46</b>).</p><p>As mentioned above, store memory operations may be performed via two accesses to data cache <b>14</b> (the first access to read the affected cache line, and the second access to update the affected cache line). In such an embodiment, if store memory operations are selected for data cache access during a clock cycle, load/store unit <b>20</b> does not select memory operations during the succeeding clock cycle.</p><p>Turning now to FIG. 3, a block diagram of one embodiment of the data portion <b>50</b> of data cache <b>14</b> is shown. The tag portion of data cache <b>14</b> may be organized in a similar fashion, along with tag comparators to determine if the memory operations are hits or misses within data cache <b>14</b>. Other embodiments are possible and contemplated. In the embodiment of FIG. 3, data portion <b>50</b> includes a mux and sequence unit <b>52</b>, a row decoder <b>54</b>, a set of row drivers <b>56</b>, an array <b>58</b>, a set of sense amplifiers (amps) <b>60</b>, and a self timed pulse generator and clock multiplier <b>62</b>. Additionally, data portion <b>50</b> includes four ports <b>64</b>A, <b>64</b>B, <b>64</b>C, and <b>64</b>D. A corresponding set of data outputs <b>66</b>A, <b>66</b>B, <b>66</b>C, and <b>66</b>D are provided for forwarding load data in response to accesses on the ports <b>64</b>A-<b>64</b>D. A corresponding set of data inputs (not shown) are also provided for conveying store data and/or fill data into data portion <b>50</b>. An ICLK line <b>68</b> is coupled to self timed pulse generator and clock multiplier <b>62</b> and to a plurality of storage devices <b>70</b>A-<b>70</b>D. Storage devices <b>70</b> are coupled to mux and sequence unit <b>52</b>, which is also coupled to receive a set of capture and send pulses from self timed pulse generator and clock multiplier <b>62</b>. Mux and sequence unit <b>52</b> is coupled through a pipeline storage device <b>72</b>A to row decoder <b>54</b>, which is further coupled through pipeline storage device <b>72</b>B to row drivers <b>56</b>. Row drivers <b>56</b> are coupled to array <b>58</b>, which is further coupled to sense amps <b>60</b>. Sense amps <b>60</b> are coupled to output storage devices <b>74</b>A-<b>74</b>D, which are further coupled to outputs <b>66</b>A-<b>66</b>D, respectively. Self timed pulse generator and clock multiplier <b>62</b> is configured to provide CLK<b>1</b> and CLK<b>2</b> signals upon a CLK<b>1</b>/CLK<b>2</b> line <b>76</b> to storage devices <b>72</b>A-<b>72</b>B, row drivers <b>56</b>, array <b>58</b>, and sense amps <b>60</b>. Storage devices <b>70</b>, <b>72</b>, and <b>74</b> may be any suitable clocked storage device. For example, latches, registers, and flip-flops may be suitable.</p><p>Load/store unit <b>20</b> provides memory operations on ports <b>64</b>A-<b>64</b>D during each ICLK clock cycle in which memory operations are available. The ICLK signal provided upon ICLK line <b>68</b> is the clock signal received by portions of microprocessor <b>10</b> outside of data cache <b>14</b>. For the present embodiment, the memory operations provided on ports <b>64</b>A-<b>64</b>D are captured in storage devices <b>70</b>A-<b>70</b>D. Storage devices <b>70</b> retain the memory operations while mux and sequence unit <b>52</b> sequences the memory operations into the data cache storage pipeline according to capture and send pulses provided by self timed pulse generator and clock multiplier <b>62</b>. A capture pulse and a send pulse are provided for each port. The capture pulse directs mux and sequence unit <b>52</b> to capture the value from the corresponding port, while the send pulse directs mux and sequence unit <b>52</b> to send the captured value to storage device <b>72</b>A. The capture and send pulses are generated by self timed pulse generator and clock multiplier <b>62</b> as analog self timed delays from an edge of the ICLK signal. In this manner, the memory operation provided on each port <b>64</b>A-<b>64</b>D is sequenced in turn into the single pipeline provided within data portion <b>50</b>.</p><p>Self timed pulse generator and clock multiplier <b>62</b> is also configured to multiply the ICLK signal to produce the CLK<b>1</b> and CLK<b>2</b> signals. CLK<b>1</b> and CLK<b>2</b> may be non-overlapping clock signals operating at a multiple of the frequency of the ICLK signal. The frequency of the CLK<b>1</b> and CLK<b>2</b> signals may be any frequency which can be supported by the pipeline stages of data portion <b>50</b> and which sequences the memory operations on each port <b>64</b>A-<b>64</b>D into data portion <b>50</b> within one period of the ICLK signal. Preferably, the frequency of the CLK<b>1</b> and CLK<b>2</b> signals may be a multiple of the frequency of the ICLK signal, wherein the multiple is equal to the number of ports <b>64</b>.</p><p>Row decoder <b>54</b> decodes the address of each memory operation as provided by storage device <b>72</b>A in order to select a row of array <b>58</b> in which data corresponding to the address may be stored. Row decoder <b>54</b> produces a set of word line signals which are used to activate the selected row within array <b>58</b> and deactivate the non-selected rows. The word line signals are stored in storage device <b>72</b>B and conveyed to row drivers <b>56</b> during the subsequent clock cycle as defined by the CLK<b>1</b> and CLK<b>2</b> signals.</p><p>Row drivers <b>56</b> receive the word line signals and drive each word line signal to the corresponding row within array <b>58</b>. Row drivers <b>56</b> are configured to assert word line signals during the active phase of CLK<b>1</b> and to deassert word line signals during active phase of CLK<b>2</b>. Accordingly, the read (or write) of the selected row of array <b>58</b> is performed during the active phase of CLK<b>1</b>. As will be described below, the bit lines within array <b>58</b> may be precharged and/or balanced during the active phase of CLK<b>2</b>.</p><p>A read memory operation may proceed as follows: The selected row within array <b>58</b> is enabled, and the non-selected rows disabled, according to the word line signals provided by row decoder <b>54</b> and driven by row drivers <b>56</b>. The storage cells within the selected row are each connected to a pair bit lines, one of which is discharged according to the value stored in the storage cells. Sense amps <b>60</b> detect the differential between the voltages upon the pair bit lines to determine the value of the bit stored in that storage cell. Sense amps <b>60</b> thereby capture the value from the selected row.</p><p>In one embodiment, a sense amp may be provided for each bit within a row of array <b>58</b> and for each port. Accordingly, the embodiment of FIG. 3 may include four sense amps per bit (one for each of the four ports). Each sense amp is coupled to an output storage device <b>74</b>A-<b>74</b>D, which is further coupled to an output <b>66</b>A-<b>66</b>D. Outputs <b>66</b> convey data to result bus <b>30</b> (possibly passing through a rotator/sign extend structure).</p><p>It is noted that, because of the pipeline employed by data portion <b>50</b>, data is sensed by sense amps <b>60</b> at different points in time for accesses performed on different ports. Accordingly, output storage devices <b>74</b>A-<b>74</b>D each receive a different capture pulse as generated by self timed pulse generator and clock multiplier <b>62</b> in order to capture the data corresponding to that port. Send pulses may be eliminated from storage devices <b>74</b>, because storage devices <b>74</b> provide data to pipeline stages which are clocked according to the ICLK signal. Accordingly, it is desirable for the data to be available upon outputs <b>66</b>A-<b>66</b>D for a clock period of the ICLK signal.</p><p>A write sequence may perform a read as described above, and then an update sequence which proceeds as follows: The selected row within array <b>58</b> is enabled, and the non-selected rows disabled, according to the word line signals provided by row decoder <b>54</b> and driven by row drivers <b>56</b>. The pair of bit lines are driven to the value intended to be stored in the storage cell, which overwrites the previously stored value.</p><p>In the embodiment of FIG. 3, four pipeline stages are employed. Mux and sequence unit <b>52</b> may operate upon a memory operation during the first pipeline stage. The second pipeline stage may include operation of row decoder <b>54</b>, followed by row drive, array access, and sensing in the third pipeline stage. Output forwarding via storage devices <b>74</b>A-<b>74</b>D may comprise the fourth pipeline stage. Thus mux and sequence unit <b>52</b>, row decoder <b>54</b>, row drivers <b>56</b>, array <b>58</b>, sense amps <b>60</b>, storage devices <b>74</b>A-<b>74</b>D, and pipeline storage devices <b>72</b>A-<b>72</b>B may form an exemplary pipeline <b>78</b>.</p><p>While in the above description, data portion <b>50</b> has been described as including a single pipeline, embodiments are contemplated in which multiple parallel pipelines are provided. Multiple parallel pipelines might be used, for example, if physically adding multiple ports to array <b>58</b> is desirable in combination with supporting even a larger number of ports than the multiple physical ports via the pipeline of the accesses described above. For example, eight ports could be supported by providing a dual ported array and two parallel pipelines operating at four times the frequency of the ICLK signal.</p><p>Turning next to FIG. 4, a block diagram of a portion of one embodiment of array <b>58</b> is shown. Other embodiments are possible and contemplated. In the embodiment of FIG. 4, a storage cell <b>80</b> is shown coupled to a word line <b>82</b> and a pair bit lines <b>84</b>A-<b>84</b>B. Bit lines <b>84</b>A-<b>84</b>B are coupled to a precharge and balance unit <b>86</b>, which is further coupled to CLK<b>1</b>/CLK<b>2</b> line <b>76</b>. Other storage cells coupled to the same wordline <b>82</b> and different bit lines form a row of an array <b>58</b>. Other storage cells coupled to different word lines form other rows. One storage cell from each row may be coupled to bit lines <b>84</b>.</p><p>If storage cell <b>80</b> is within the selected row, row drivers <b>56</b> activates word line <b>82</b>. As described above, in the present embodiment, word line <b>82</b> is activated during the active phase of the CLK<b>1</b> signal. Upon activation of word line <b>82</b>, the nodes within storage cell <b>80</b>, which store the true and complement of the value stored within storage cell <b>80</b>, are coupled to the lines <b>84</b>A and <b>84</b>B, respectively. Sense amps <b>60</b> are coupled to bit lines <b>84</b>A-<b>84</b>B, and are configured to sense the differential between the voltages upon bit lines <b>84</b>A-<b>84</b>B.</p><p>Precharge and balance unit <b>86</b> is configured to either precharge bit lines <b>84</b>A-<b>84</b>B or to balance bit lines <b>84</b>A-<b>84</b>B. Because load/store unit <b>20</b> is configured to provide only load memory operations or only store memory operations concurrently on the ports of data cache <b>14</b>, and because sense amps <b>60</b> sense the differential between bit lines <b>84</b>A-<b>84</b>B, precharge and balance unit <b>86</b> need not fully precharge bit lines <b>84</b>A-<b>84</b>B between each memory operation which was concurrently provided to the ports of data cache <b>14</b>. If load memory operations and store memory operations were concurrently provided to the ports of data cache <b>14</b>, a store memory operation (in which one of the bit lines <b>84</b>A-<b>84</b>B is driven to a logical low state and the other bit line <b>84</b>A-<b>84</b>B is driven to a logical high state) performed before a load memory operation would require a precharge. Typically, a balancing operation is provided subsequent to the precharge. The balancing operation ensures that substantially equal charge is provided upon both bit lines <b>84</b>A and <b>84</b>B.</p><p>Precharge and balance unit <b>86</b> is configured to precharge bit lines <b>84</b>A-<b>84</b>B upon completion of the memory operation provided to port <b>64</b>D. Between memory operations provided to the other ports, precharge and balance unit <b>86</b> is configured to perform only a balancing of the differential between bit lines <b>84</b>A and <b>84</b>B. Accordingly, the amount of time needed to perform a precharge is eliminated. The frequency at which the pipeline of data cache <b>14</b> can be operated may be increased.</p><p>As mentioned above, row drivers <b>56</b> are configured to assert word line <b>82</b> during the active phase of the CLK<b>1</b> signal. Accordingly, precharge and balance unit <b>86</b> receives CLK<b>1</b>/CLK<b>2</b> line <b>76</b> and performs balancing during the active phase of the CLK<b>2</b> signal. Additionally, precharge and balance unit <b>86</b> is configured to precharge bit lines <b>84</b>A-<b>84</b>B subsequent to completing the memory operation provided to port <b>64</b>D.</p><p>Turning next to FIG. 5, a circuit diagram of a portion of one embodiment of mux and sequence unit <b>52</b>. Other embodiments are possible and contemplated. The portion shown in FIG. 5 illustrates the mux and sequencing of one bit from each of the ports of data cache <b>14</b> to row decoder <b>54</b>. Other bits may be handled in parallel in a similar fashion. It is noted that the embodiment of mux and sequence unit <b>52</b> illustrated in FIG. 5 integrates the storage functionality of storage device <b>72</b>A as shown in FIG. 3, and hence storage device <b>72</b>A may be deleted from an embodiment employing the embodiment of mux and sequence unit <b>52</b> shown in FIG. <b>5</b>.</p><p>Mux and sequence unit <b>52</b> includes a flip-flop <b>90</b>A for capturing a bit from port A (through storage device <b>70</b>A in the embodiment of FIG. 3) and sending the bit to row decoder <b>54</b> through a domino-style gate <b>92</b>. Similarly, a flip-flop <b>90</b>B is provided for capturing a bit from port B and sending the bit to row decoder <b>54</b> through gate <b>92</b>; a flip-flop <b>90</b>C is provided for capturing a bit from port C and sending the bit to row decoder <b>54</b> through gate <b>92</b>; a flip-flop <b>90</b>D is provided for capturing a bit from port D and sending the bit to row decoder <b>54</b> through gate <b>92</b>. Each of flip-flops <b>90</b> is coupled to receive a different capture and send pulse for capturing the value from a port and for sending the value to row decoder <b>54</b>.</p><p>FIG. 6 is a timing diagram illustrating the capture and send pulses shown on the circuit diagram of FIG. 5 as generated by one embodiment of self timed pulse generator and clock multiplier <b>62</b>. Additionally, a precharge pulse for precharging gate <b>92</b> is illustrated. Vertical dotted lines illustrate timing relationships between the precharge pulse and the capture and send pulses. The ICLK signal is illustrated at the top of the timing diagram to show that the capture and send pulses complete within one period of the ICLK signal.</p><p>Turning now to FIG. 7, a timing diagram illustrating a flow of an exemplary set of memory operations through one embodiment of data cache <b>14</b> is shown. Other embodiments are possible and contemplated. Solid vertical lines on the timing diagram of FIG. 7 delimit clock cycles as defined by the ICLK signal. Any two vertical lines (solid or dashed) delimit clock cycles as defined by the CLK<b>1</b>/CLK<b>2</b> clock signals.</p><p>During the first clock cycle or period of the ICLK signal as shown in FIG. 7, memory operations A<b>1</b>, B<b>1</b>, C<b>1</b>, and D<b>1</b> are selected by load/store unit <b>20</b> and are presented on the ports of data cache <b>14</b>. During the second clock cycle of the ICLK signal as shown in FIG. 7, memory operations A<b>2</b>, B<b>2</b>, C<b>2</b>, and D<b>2</b> are selected by load/store unit <b>20</b> and are presented on the ports of a cache <b>14</b>.</p><p>As illustrated horizontally across the timing diagram of FIG. 7 next to the label Mux/Seq., memory operations A<b>1</b>, B<b>1</b>, C<b>1</b>, and DI are sequenced through mux and sequence unit <b>52</b> during consecutive clock cycles as defined by the CLK<b>1</b> and CLK<b>2</b> signals. The sequencing occurs during the second clock cycle as defined by the ICLK signal. Similarly, memory operations A<b>2</b>, B<b>2</b>, C<b>2</b>, and D<b>2</b> are sequenced through mux and sequence unit <b>52</b> during consecutive clock cycles as defined by the CLK<b>1</b> and CLK<b>2</b> clock signals during the third clock cycle as defined by the ICLK signal. Subsequently, each memory operation flows through the row decoder stage (illustrated horizontally in FIG. 7 next to the label Row Dec.) and into the Array stage (illustrated horizontally in FIG. 7 next to the Array label). The row decoder stage is the stage in which row decoder <b>54</b> operates, while the array stage is the stage in which row drivers <b>56</b>, array <b>58</b>, and sense amps <b>60</b> operate.</p><p>FIG. 7 further illustrates providing the result of each memory operation at the outputs of data cache <b>14</b>. Output <b>66</b>A is illustrated horizontally next to the label Result A. Similarly, output <b>66</b>B is illustrated horizontally next to the label Result B; output <b>66</b>C is illustrated horizontally next to the label Result C; and output <b>66</b>D is illustrated horizontally next to the label Result D. As FIG. 7 shows, data corresponding to a memory operation that is presented on each port is available for the full period of the ICLK signal, but at different points in time within the clock cycle. Accordingly, sampling of the results may be timed according to the port, or timed such that the result from any port may be sampled.</p><p>Turning now to FIG. 8, a block diagram of one embodiment of a computer system <b>200</b> including microprocessor <b>10</b> coupled to a variety of system components through a bus bridge <b>202</b> is shown. Other embodiments are possible and contemplated. In the depicted system, a main memory <b>204</b> is coupled to bus bridge <b>202</b> through a memory bus <b>206</b>, and a graphics controller <b>208</b> is coupled to bus bridge <b>202</b> through an Advanced Graphics Port (AGP) bus <b>210</b>. Finally, a plurality of Peripheral Component Intercorret (PCI) devices <b>212</b>A-<b>212</b>B are coupled to bus bridge <b>202</b> through a PCI bus <b>214</b>. A secondary bus bridge <b>216</b> may further be provided to accommodate an electrical interface to one or more Extended Industry Standard Architecture (EISA) or ISA, Industry Standard Architecture (ISA) devices <b>218</b> through an EISA/ISA bus <b>220</b>. Microprocessor <b>10</b> is coupled to bus bridge <b>202</b> through a Central Processing Unit (CPU) bus <b>224</b>.</p><p>Bus bridge <b>202</b> provides an interface between microprocessor <b>10</b>, main memory <b>204</b>, graphics controller <b>208</b>, and devices attached to PCI bus <b>214</b>. When an operation is received from one of the devices connected to bus bridge <b>202</b>, bus bridge <b>202</b> identifies the target of the operation (e.g. a particular device or, in the case of PCI bus <b>214</b>, that the target is on PCI bus <b>214</b>). Bus bridge <b>202</b> routes the operation to the targeted device. Bus bridge <b>202</b> generally translates an operation from the protocol used by the source device or bus to the protocol used by the target device or bus.</p><p>In addition to providing an interface to an ISA/EISA bus for PCI bus <b>214</b>, secondary bus bridge <b>216</b> may further incorporate additional functionality, as desired. For example, in one embodiment, secondary bus bridge <b>216</b> includes a master PCI arbiter (not shown) for arbitrating ownership of PCI bus <b>214</b>. An input/output controller (not shown), either external from or integrated with secondary bus bridge <b>216</b>, may also be included within computer system <b>200</b> to provide operational support for a keyboard and mouse <b>222</b> and for various serial and parallel ports, as desired. An external cache unit (not shown) may further be coupled to CPU bus <b>224</b> between microprocessor <b>10</b> and bus bridge <b>202</b> in other embodiments. Alternatively, the external cache may be coupled to bus bridge <b>202</b> and cache control logic for the external cache may be integrated into bus bridge <b>202</b>.</p><p>Main memory <b>204</b> is a memory in which application programs are stored and from which microprocessor <b>10</b> primarily executes. A suitable main memory <b>204</b> comprises DRAM (Dynamic Random Access Memory), and preferably a plurality of banks of SDRAM (Synchronous DRAM).</p><p>PCI devices <b>212</b>A-<b>212</b>B are illustrative of a variety of peripheral devices such as, for example, network interface cards, video accelerators, audio cards, hard or floppy disk drives or drive controllers, SCSI (Small Computer Systems Interface) adapters and telephony cards. Similarly, ISA device <b>218</b> is illustrative of various types of peripheral devices, such as a modem, a sound card, and a variety of data acquisition cards such as General Purpose Interface (GPIB) or field bus interface cards.</p><p>Graphics controller <b>208</b> is provided to control the rendering of text and images on a display <b>226</b>. Graphics controller <b>208</b> may embody a typical graphics accelerator generally known in the art to render three-dimensional data structures which can be effectively shifted into and from main memory <b>204</b>. Graphics controller <b>208</b> may therefore be a master of AGP bus <b>210</b> in that it can request and receive access to a target interface within bus bridge <b>202</b> to thereby obtain access to main memory <b>204</b>. A dedicated graphics bus accommodates rapid retrieval of data from main memory <b>204</b>. For certain operations, graphics controller <b>208</b> may further be configured to generate PCI protocol transactions on AGP bus <b>210</b>. The AGP interface of bus bridge <b>202</b> may thus include functionality to support both AGP protocol transactions as well as PCI protocol target and initiator transactions. Display <b>226</b> is any electronic display upon which an image or text can be presented. A suitable display <b>226</b> includes a cathode ray tube (\u201cCRT\u201d), a liquid crystal display (\u201cLCD\u201d), etc.</p><p>It is noted that, while the AGP, PCI, and ISA or EISA buses have been used as examples in the above description, any bus architectures may be substituted as desired. It is further noted that computer system <b>200</b> may be a multiprocessing computer system including additional microprocessors (e.g. microprocessor <b>10</b><i>a </i>shown as an optional component of computer system <b>200</b>). Microprocessor <b>10</b><i>a </i>may be similar to microprocessor <b>10</b>. More particularly, microprocessor <b>10</b><i>a </i>may be an identical copy of microprocessor <b>10</b>. Microprocessor <b>10</b><i>a </i>may share CPU bus <b>224</b> with microprocessor <b>10</b> (as shown in FIG. 8) or may be connected to bus bridge <b>202</b> via an independent bus.</p><p>In accordance with the above disclosure, a data cache has been shown which provides multiporting through pipelining of the data cache and operating the pipeline at a multiple of the clock frequency employed within the remainder of the microprocessor. Advantageously, multiple accesses per clock cycle may be accomplished without physical multiporting of the cache or banking of the cache.</p><p>Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "David B.", "last_name": "Witt", "name": ""}, {"first_name": "James K.", "last_name": "Pickett", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "ADVANCED MICRO DEVICES, INC."}, {"first_name": "", "last_name": "ADVANCED MICRO DEVICES, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  13/00"}, {"primary": false, "label": "G06F   1/04"}, {"primary": false, "label": "G06F   9/312"}, {"primary": false, "label": "G11C  11/413"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/38        20060101A I20051008RMEP"}, {"label": "G11C   7/10        20060101A I20051008RMEP"}, {"label": "G11C   8/16        20060101A I20051008RMEP"}, {"label": "G06F  12/08        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711169"}, {"primary": false, "label": "711E12048"}, {"primary": false, "label": "712E09046"}, {"primary": false, "label": "711149"}, {"primary": false, "label": "711131"}, {"primary": false, "label": "36523005"}, {"primary": false, "label": "711167"}, {"primary": false, "label": "710039"}, {"primary": false, "label": "711140"}, {"primary": false, "label": "710006"}, {"primary": false, "label": "711E12049"}, {"primary": false, "label": "36518905"}], "ecla_classes": [{"label": "G06F  12/08B6N"}, {"label": "G06F  12/08B6P"}, {"label": "G06F   9/38D"}, {"label": "G11C   7/10M5"}, {"label": "G11C   8/16"}, {"label": "G11C   7/10S"}], "cpc_classes": [{"label": "G11C   7/1039"}, {"label": "G06F   9/3824"}, {"label": "G06F   9/3869"}, {"label": "G06F  12/0853"}, {"label": "G06F  12/0855"}, {"label": "G11C   7/1072"}, {"label": "G11C   8/16"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1998-06-19", "application_date": "1998-06-19", "family_members": [{"ucid": "US-6202139-B1", "titles": [{"lang": "EN", "text": "Pipelined data cache with multiple ports and processor with load/store unit selecting only load or store operations for concurrent processing"}]}]}