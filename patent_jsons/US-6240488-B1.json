{"patent_number": "US-6240488-B1", "publication_id": 72673678, "family_id": 23625096, "publication_date": "2001-05-29", "titles": [{"lang": "EN", "text": "Prefetching hints"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11184969\" source=\"national office\"><p>A processor capable of executing prefetching instructions containing hint fields is provided. The hint fields contain a first portion which enables the selection of a destination indicator for refill operations, and a second portion which identifies a destination.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72562339\"><p>A processor capable of executing prefetching instructions containing hint fields is provided. The hint fields contain a first portion which enables the selection of a destination indicator for refill operations, and a second portion which identifies a destination.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00001\" num=\"1\"><claim-text>1. In a processor with an n-way, set associative cache, a method comprising:</claim-text><claim-text>addressing the cache using address information contained in an instruction, said instruction including a hint field; </claim-text><claim-text>indicating a cache miss for desired data that is addressed by said instruction; </claim-text><claim-text>specifying a refill destination for said desired data with a first portion of said hint field, wherein said destination is a first way in said cache when said desired data is a first type of data and a second way in said cache when said desired data is a second type of data, said first portion being operable in subsequent instructions to consistently specify such way destinations for subsequent first and second types of data; and </claim-text><claim-text>refilling said cache with said desired data. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00002\" num=\"2\"><claim-text>2. The method of claim <b>1</b> further comprising the step of enabling said specifying step using a second portion of said hint field.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00003\" num=\"3\"><claim-text>3. The method of claim <b>2</b> wherein said refilling step refills said first way of said cache and further comprising a third portion of said hint field which indicates whether said desired data is expected to be modified.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00004\" num=\"4\"><claim-text>4. The method of claim <b>1</b> further comprising the step of disabling said specifying step using a second portion of said hint field.</claim-text></claim>"}, {"num": 5, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00005\" num=\"5\"><claim-text>5. The method of claim <b>4</b> further comprising the step of refilling said cache based on a least-recently-used protocol.</claim-text></claim>"}, {"num": 6, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00006\" num=\"6\"><claim-text>6. A processor comprising:</claim-text><claim-text>a first n-way, set associative cache containing a first-cache line that is addressed using address information contained in an instruction, said instruction also containing a hint field; </claim-text><claim-text>a first comparator, coupled to said first n-way cache, for indicating a first-cache miss when said first-cache line is addressed and does not contain desired data; and </claim-text><claim-text>a first multiplexer, coupled to said first n-way cache, for choosing a first destination indicator used to direct refilling of said first-cache line with said desired data, said first destination indicator being chosen from a first portion of said hint field and a first alternative indicator, said first portion specifying a first way for desired data identified as a first data type and a second way for desired data identified as a second data type, and said first alternative indicator specifying a destination way for desired data without regard to data type, said first portion being operable in subsequent refill operations to consistently specify first and second way destinations for subsequent first data type and second data type data, respectively. </claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00007\" num=\"7\"><claim-text>7. The processor of claim <b>6</b> further comprising a means for disabling a refill operation of said cache line when said first way is unavailable.</claim-text></claim>"}, {"num": 8, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00008\" num=\"8\"><claim-text>8. The processor of claim <b>6</b> further comprising:</claim-text><claim-text>a second n-way cache, coupled to said first cache, containing a second cache line that is addressed using said address information; </claim-text><claim-text>a second comparator, coupled to said second cache, for indicating a second-cache miss when said second cache line is addressed; and </claim-text><claim-text>a second multiplexer, coupled to said second cache, for choosing a second destination indicator used to direct refilling of said second cache line with said desired data, said second destination indicator being chosen from said first portion of said hint field and a second alternative indicator, said first portion being operable to consistently specify said first way for desired data identified as first data type and said second way for desired data identified as second data type, and said second alternative indicator specifying a destination way for desired data without regard to data type. </claim-text></claim>"}, {"num": 9, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00009\" num=\"9\"><claim-text>9. A processor comprising:</claim-text><claim-text>a queue for holding an instruction, said instruction including a hint field and being used to refill desired data to a first n-way, set associative, cache; and </claim-text><claim-text>a first multiplexer, coupled to said queue, for choosing a first destination indicator used to direct refilling of said first cache with said desired data, said first destination indicator being chosen from a first portion of said hint field and an alternative indicator under control of a second portion of said hint field, said first portion specifying a first way for desired data identified as a first data type and a second way for desired data identified as a second data type, said first portion being operable in subsequent refill operations to consistently specify first and second way destinations for subsequent first data type and second data type data, respectively. </claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00010\" num=\"10\"><claim-text>10. The processor of claim <b>9</b> wherein said first portion of said hint field points to said first way of said first cache, wherein said first way contains no desired data identified as second data type.</claim-text></claim>"}, {"num": 11, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00011\" num=\"11\"><claim-text>11. The processor of claim <b>9</b> wherein said first portion of said hint field points to said second way of said first cache, wherein said second way contains no desired data identified as first data type.</claim-text></claim>"}, {"num": 12, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00012\" num=\"12\"><claim-text>12. The processor of claim <b>9</b> further comprising:</claim-text><claim-text>a second multiplexer, coupled to said queue, for choosing a second destination indicator used to direct reading of said desired data from a second cache. </claim-text></claim>"}, {"num": 13, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00013\" num=\"13\"><claim-text>13. The processor of claim <b>9</b> wherein said alternative indicator is a randomly generated bit.</claim-text></claim>"}, {"num": 14, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00014\" num=\"14\"><claim-text>14. The method of claim <b>1</b> wherein said first type of data and said second type of data are streamed data and retained data, respectively.</claim-text></claim>"}, {"num": 15, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00015\" num=\"15\"><claim-text>15. The processor of claim <b>6</b> wherein said hint field includes a second portion which controls said first multiplexer.</claim-text></claim>"}, {"num": 16, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00016\" num=\"16\"><claim-text>16. The processor of claim <b>6</b> wherein said first data type and said second data type are streamed data and retained data, respectively.</claim-text></claim>"}, {"num": 17, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00017\" num=\"17\"><claim-text>17. The processor of claim <b>9</b> wherein said first data type and said second data type are streamed data and retained data, respectively.</claim-text></claim>"}, {"num": 18, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00018\" num=\"18\"><claim-text>18. The processor of claim <b>9</b> wherein said alternative indicator is an LRU bit.</claim-text></claim>"}, {"num": 19, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00019\" num=\"19\"><claim-text>19. The processor of claim <b>6</b> wherein said first alternative indicator is an LRU bit.</claim-text></claim>"}, {"num": 20, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00020\" num=\"20\"><claim-text>20. The processor of claim <b>6</b> wherein said first alternative indicator is a randomly generated bit.</claim-text></claim>"}, {"num": 21, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00021\" num=\"21\"><claim-text>21. In a processor with an n-way, set associative, cache, a method comprising:</claim-text><claim-text>issuing an instruction, wherein said instruction includes a hint field; and </claim-text><claim-text>specifying a refill destination for desired data within the cache using said hint field, said destination being a first way in said cache when said desired data is a first type of data and a second way in said cache when said desired data is a second type of data, wherein subsequent hint fields in subsequent instructions are operable to consistently specify such way destinations for subsequent first and second types of data. </claim-text></claim>"}, {"num": 22, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00022\" num=\"22\"><claim-text>22. A processor comprising:</claim-text><claim-text>an n-way, set associative, cache containing a cache line that is addressed using address information contained in an instruction, said instruction also containing a hint field; and </claim-text><claim-text>a multiplexer, coupled to said n-way cache, for choosing a destination indicator that is used to direct refilling of said cache line with desired data, said destination indicator being chosen from information contained in said hint field and an alternative indicator, said information specifying a first way for desired data identified as a first data type and a second way for desired data identified as a second data type, and wherein subsequent hint fields disposed in subsequent instructions contain subsequent information that is operable to consistently specify first and second way destinations for subsequent first data type and second data type data, respectively, in subsequent refill operations. </claim-text></claim>"}, {"num": 23, "parent": 22, "type": "dependent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00023\" num=\"23\"><claim-text>23. The processor of claim <b>22</b> wherein said alternative indicator specifies said first way and second way randomly.</claim-text></claim>"}, {"num": 24, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6240488-B1-CLM-00024\" num=\"24\"><claim-text>24. A processor comprising:</claim-text><claim-text>a queue for holding an instruction, said instruction including a hint field; and </claim-text><claim-text>an n-way, set associative, cache coupled to said queue that is operable to receive desired data in accordance with said hint field, wherein said hint field specifies a first way for receiving desired data identified as a first data type and a second way for receiving desired data identified as a second data type, and subsequent instructions contain hint fields that are operable to consistently specify first and second way destinations for subsequent first data type and second data type data, respectively. </claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54552060\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><p>This application is a continuation of Ser. No. 08/410,524, filed Mar. 24, 1995, now U.S. Pat. No. 5,732,242.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><p>A preferred embodiment of the present invention is incorporated in a superscalar processor identified as \u201cR10000, \u201d which was developed by Silicon Graphics, Inc., of Mountain View, Calif. Various aspects of the R10000 are described in commonly-owned copending patent applications having Ser. No.: 08/324,124 (\u201cCache Memory\u201d), Ser. No. 08/324,127 (\u201cRedundant Mapping Tables\u201d), Ser. No. 08/324,128 (\u201cMemory Translation\u201d) Ser. No. 08/324,129 (\u201cAddress Queue\u201d) and Ser. No. 08/404,625 (attorney docket number 012178-563-1, filed Mar. 14, 1995, entitled \u201cAddress Queue\u201d), which are hereby incorporated by reference in their entirety for all purposes.</p><h4>BACKGROUND OF THE INVENTION</h4><p>This invention relates in general to computer processors capable of executing prefetch instructions and, in particular, to a processor capable of tailoring prefetch operations to accommodate certain types of data held in cache memories.</p><p>Modern computer processors are typically configured with a memory system consisting of multiple levels of memory having different speeds and sizes (main memory being the largest and slowest). The fastest memories are usually smaller in size since they cost more per bit than slower memories. To improve access time to main memory, one or more smaller, faster memories may be disposed between the main memory and the processor. Such memories, referred to as cache memories, serve as buffers between lower-speed main memory and the processor.</p><p>In some architectures, a hierarchy of caches may be disposed between the processor and main memory. See, J. Heinrich, <i>MIPS R</i>4000 <i>Microprocessor User's Manual</i>, p. 244 (PTR Prentice Hall 1993). Such a hierarchy may include, for example, a primary cache and secondary cache. Primary cache typically is the smallest cache memory having the fastest access time. Secondary cache is generally larger and slower than the primary cache but smaller and faster than main memory. Secondary cache serves as a backup to primary cache in the event of a primary cache miss.</p><p>To facilitate cache operation, a memory controller (part of the processor) is typically used to fetch instructions and/or data that are required by the processor and store them in the cache. When a controller fetches instructions or data, it first checks the cache. Control logic determines if the desired information is stored in the cache (i.e., cache hit). If a cache hit occurs, the processor simply retrieves the desired information from the cache.</p><p>However, if the desired data is not in the cache (i.e., cache miss), the controller accesses main memory (or the next level of cache memory) to load the accessed cache with the desired data. This loading operation is referred to as a \u201crefill.\u201d Since cache size is limited, a refill operation usually forces some portion of data out of the cache to make room for the desired data. The displaced data may be written back to main memory to preserve its state before the desired data is refilled into the cache.</p><p>Processor performance is improved when desired data is found in a cache. A processor will operate at the speed of its fastest memory that contains desired data. When forced to access a slower memory (i.e., secondary cache or main memory) as a result of a miss, processor operations slow down thereby impeding performance. A cache-induced reduction in processor performance may be quantified as the function of a cache miss rate and average latency (i.e., delay) per miss to retrieve data from a slower memory; i.e., (miss rate)\u00d7(average latency per miss). Processor performance is improved by minimizing this product (i.e., reducing the miss rate and/or average latency per miss).</p><p>Cache miss rate may be reduced by controlling data flow in a cache (i.e., choosing what goes in and comes out of the cache). Ideally, a cache should contain useful (i.e., desired) data and discard useless data.</p><p>Latency may be reduced through the use of prefetching; i.e., the retrieval of data before it is required by a program. A prefetch instruction may initiate a cache refill but the processor need not wait for data to return from memory before proceeding with other instructions. Since prefetching accesses data before it is needed and in parallel with other processor operations, the latency associated with prefetched data is hidden.</p><p>Prefetching is possible when data patterns can be predicted (i.e., such as when processing matrices and arrays). Because prefetching is programmable, a compiler (or programmer or operating system) can judiciously use this instruction when warranted by the data (i.e., the compiler will consider the current pattern of memory references to determine whether it can predict future references).</p><p>In summary, the performance of a processor which uses a cache memory will be increased to the extent that data flow in the cache may be controlled to reduce the cache miss rate, and prefetching may be utilized to reduce the average latency per miss.</p><p>In some applications, certain data stored in a cache is reused extensively while other data is not. To minimize repeated refill operations, data that is reused extensively should not be replaced with data that is used infrequently. Accordingly, extensively reused data should be \u201cretained\u201d in the cache to the extent possible, while data that is not reused extensively should be allowed to pass or \u201cstream\u201d through the cache without restriction. (Such data is referred to herein as \u201cretained data\u201d and \u201cstreamed data,\u201d respectively.)</p><p>In addition to restricting the replacement of retained data, it is also desirable to hide the latency (i.e., delay) of accessing streamed data. (The latency of retained data is inherently hidden since this data is generally kept in the cache.)</p><p>The use of retained and streamed data, as defined above, arises in such cases as blocked matrix algorithms (where the \u201cblocked\u201d data should stay in the cache and not be replaced by \u201cnon-blocked\u201d data; see, Lam et al., \u201cThe Cache Performance and Optimizations of Blocked Algorithms,\u201d <i>Fourth International Conference on Architectural Support for Programming Languages and Operating Systems </i>(ASPLOS IV), Palo Alto, Calif., Apr. 9-11, 1991), DSP algorithms (where the filter coefficients should stay in the cache and not be replaced by the stream of signal data), and operating system operations such a \u201cbzero\u201d (i.e., zero out a block of memory) and \u201cbcopy\u201d (copy a block of memory from one location to another).</p><p>One solution to restricting replacement of retained data is to \u201clock down\u201d specific parts of the cache (i.e., bring the retained data into the cache and then lock it down so that it cannot be replaced by the streamed data). This \u201clock down\u201d approach is undesirable, however, because it adds a special state to the cache (complicating operations such as context switching) and requires new instructions for the user (i.e., for specifying the portion of the cache to be locked and unlocked).</p><p>Another solution to restricting replacement of retained data that also hides the latency of accessing streamed data is to \u201cprefetch\u201d streamed data. In general, prefetching memory blocks into primary and secondary caches can increase performance by reducing delays required to refill caches. Such operation has no effect on the logical operation of a program and can significantly improve programs that have predictable memory accesses but have a high cache miss ratio. However, improper use of such prefetching operation can reduce performance by interfering with normal memory accesses.</p><p>Prefetching streamed data has been suggested through the use of an \u201cuncached prefetch\u201d instruction. This instruction segregates streamed data into a separate target buffer rather than storing such data in the normal cache memory (thereby preventing streamed data from displacing retained data held in the cache). However, uncached prefetches are undesirable because data must be buffered somewhere other than a cache or primary cache. Placing the prefetched data in a secondary cache but not the primary cache is undesirable because latency is not fully hidden. Further, placing the prefetched data in a special buffer off to the side of a primary data cache is also undesirable since it complicates multiprocessor snooping and, in fact, creates another primary cache.</p><p>Accordingly, there is a need to control the destination of retained and streamed data flowing into a cache system to ensure that one type of data does not displace the other type of data during refill operations, and a need to minimize the latency associated with accessing such data.</p><h4>SUMMARY OF THE INVENTION</h4><p>The present invention provides for an apparatus and method that allows the use of indicators (i.e., prefetching \u201chint\u201d bits) within an instruction format to control which way in an n-way set-associative cache prefetched data should be placed.</p><p>In one embodiment, the present invention provides for a processor that includes a decoder for decoding a prefetch instruction; an address queue for holding a decoded prefetched instruction, wherein the prefetched instruction includes a hint field and address information; a first n-way set-associative cache, coupled to the address queue, containing a first-cache line that is addressed using the address information; a first comparator means, coupled to the first cache, for indicating a first-cache miss when the first-cache line is addressed and does not contain desired data; and a first selection means, coupled to the first cache, for choosing a destination indicator for refilling the first-cache line with the desired data based on a first portion of the hint field.</p><p>A better understanding of the nature and advantages of the present invention may be had with reference to the detailed description and the drawings below.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>FIG. 1 is a block level diagram of a processor system employing prefetching hints in accordance with an embodiment of the invention;</p><p>FIG. 2 is a block diagram of the addressing architecture of the system of FIG. 1;</p><p>FIG. 3 is a flow chart illustrating a refill operation using prefetching hints;</p><p>FIGS. 4 and 5 illustrate the effect of cache block availability on prefetching operations;</p><p>FIGS. 6 and 7 illustrate formats of two prefetch instructions used by the system of FIG. 1;</p><p>FIG. 8 illustrates predecode and decode operations as they relate to the instructions of FIGS. 6 and 7;</p><p>FIGS. 9, <b>10</b> and <b>11</b> illustrate components of the primary cache control unit of FIG. 1; and</p><p>FIGS. 12 and 13 illustrate components and operation of the secondary cache control unit of FIG. <b>1</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF THE PREFERRED EMBODIMENT</h4><h4>Contents</h4><p>I. SYSTEM</p><p>II. PREDECODE AND DECODE</p><p>III. PRIMARY CACHE CONTROL</p><p>IV. SECONDARY CACHE CONTROL</p><h4>I. SYSTEM</h4><p>FIG. 1 provides a block-level diagram of processor <b>100</b> employing prefetching hints in accordance with an embodiment of the invention. The architecture of processor <b>100</b> as shown in FIG. 1 is implemented in the R10000 Super-scalar Microprocessor developed by Silicon Graphics, Inc., of Mountain View, Calif. The processor is further described in J. Heinrich, <i>MIPS R</i>10000 <i>Microprocessor User's Manual</i>, MIPS Technologies, Inc., (1994), which is hereby incorporated by reference in its entirety for all purposes.</p><p>A. Architecture</p><p>1. System of FIG. 1</p><p>Referring to FIG. 1, processor <b>100</b> includes predecoder <b>12</b>, whose inputs are coupled to secondary cache <b>70</b> and main memory <b>80</b> (connections not shown). Predecoder <b>12</b> receives instructions from these memory units, including prefetch instruction <b>10</b> which contains prefetching hint bits <b>5</b> (i.e., h<b>2</b>, h<b>1</b> and h<b>0</b>).</p><p>The outputs of predecoder <b>12</b> are coupled to instruction cache <b>14</b>, which receives \u201cpredecoded\u201d instructions. Instruction cache <b>14</b> is further coupled to decoder <b>16</b>, which receives instructions issued from cache <b>14</b>. Decoder <b>16</b> is coupled to address queue <b>22</b>, which receives and temporarily holds decoded memory-access instructions (e.g., load, store and prefetch instructions) until they can be executed.</p><p>Address queue <b>22</b> is coupled to primary cache control <b>26</b>, secondary cache control <b>28</b> and system interface control <b>30</b> via bus <b>24</b>, as shown in FIG. <b>1</b>. Address queue <b>22</b> is also coupled to control <b>26</b> through lines <b>84</b> and <b>82</b>, which convey status information of an addressed block. Primary cache control <b>26</b>, which controls access to primary cache <b>34</b>, is coupled to secondary cache control <b>28</b> through primary miss line <b>54</b>, and is coupled to primary cache <b>34</b> via control lines <b>44</b>, <b>46</b>, <b>48</b>, <b>50</b> and <b>52</b>.</p><p>Secondary cache control <b>28</b>, which controls access to secondary cache <b>70</b>, is coupled to system interface control <b>30</b> through secondary miss line <b>29</b>, and to secondary cache <b>70</b> through control bus <b>58</b> and data bus <b>57</b>.</p><p>System interface control <b>30</b>, which controls access to main memory <b>80</b>, is coupled to the main memory through data bus <b>74</b> and control bus <b>72</b>. Data bus <b>74</b> is coupled to data bus <b>57</b> through bus <b>55</b>. Further, data bus <b>74</b> and control bus <b>72</b> are coupled to system bus <b>76</b> which is, in turn, coupled to input/output bus <b>78</b>. This latter bus is coupled to main memory <b>80</b>.</p><p>Although not shown in FIG. 1, registers for temporarily holding data or control information are periodically disposed within the buses of the system shown in this figure.</p><p>Primary cache <b>34</b> is two-way set-associative (i.e., two cache blocks are assigned to each set). Way 0 of cache <b>34</b> includes data array <b>40</b> and tag array <b>42</b>. Similarly, way 1 includes data array <b>36</b> and tag array <b>38</b>. This cache is indexed with a virtual address and tagged with a physical address. A more detailed description of primary cache <b>34</b> may be found in co-pending U.S. patent application Ser. No. 08/324,124 which, as noted above, is incorporated by reference in its entirety for all purposes.</p><p>Secondary cache <b>70</b> is also two-way set-associative. Way 0 of cache <b>70</b> includes data array <b>66</b> and tag array <b>68</b>. Similarly, way 1 of cache <b>70</b> includes data array <b>62</b> and tag array <b>64</b>. This cache is indexed with a physical address and tagged with a physical address.</p><p>2. Addressing Architecture</p><p>The addressing architecture for the system of FIG. 1 is illustrated in FIG. <b>2</b>. Address information held in queue <b>22</b> (e.g., offset value and/or register numbers) is forwarded to an integer register file <b>1011</b> and address calculate unit <b>1012</b>, which generates a virtual address (i.e., V[all]; a complete virtual address) on line <b>1014</b>. This virtual address is converted to a physical address (i.e., P[all]; a complete physical address) through translation lookaside buffer (TLB) <b>1020</b>. A more detailed description of this architecture is provided in U.S. patent application Ser. Nos. 08/324,128, 08/324,129, and 08/404,625 (attorney docket number 012178-563-1, filed Mar. 14, 1995, entitled \u201cAddress Queue\u201d), which, as noted above, are incorporated herein by reference in their entirety for all purposes.</p><p>Referring to FIG. 2, a portion of the address on line <b>1014</b> (i.e., V[part]) is applied as an index to primary cache <b>34</b> on line <b>1016</b>. V[part] identifies a set in primary cache <b>34</b> containing two cache blocks (i.e., one in way 0 and another in way 1). Each cache block contains a data portion (held in arrays <b>40</b> and <b>36</b>) and a tag portion (held in arrays <b>42</b> and <b>38</b>). Within the tag portions is a physical tag <b>909</b> (way 0) and <b>911</b> (way 1) (FIG. 9) which uniquely identifies each block within a set.</p><p>The physical tag of each cache block identified by V[part] is output to comparators <b>1026</b> and <b>1035</b>, which are coupled to tag arrays <b>38</b> and <b>42</b>, respectively. A portion of the physical address generated by TLB <b>1020</b> (i.e., P[part1]) is also input to these comparators, as shown in FIG. <b>2</b>. If the physical tag from either array matches P[part1], the corresponding signal line <b>1030</b> or <b>1036</b> goes high (i.e., logic 1). Alternatively, if neither comparator identifies a match, lines <b>1030</b> and <b>1036</b> remain low (i.e., logic 0). These lines are ORed together at gate <b>1037</b> and inverted by inverter <b>1054</b> thereby creating a signal on primary miss line <b>54</b> (i.e., high=primary-cache \u201cmiss,\u201d low=primary cache \u201chit\u201d).</p><p>As shown in FIG. 2, similar architecture is used to address secondary cache <b>70</b>. In this case, however, a portion of the physical address P[all] on line <b>1022</b> is used to index the cache on line <b>1038</b> (i.e., P[part2]) and thereby identify two cache blocks held in a single set (i.e., one block in way 0 and another in way 1). Another portion of P[all] is used to check a physical tag associated with each addressed block (i.e., P[part3]) through comparators <b>1051</b> (way 0) and <b>1041</b> (way 1). The output of these comparators (where a high signal represents a hit and a low signal represents a miss) are ORed together by gate <b>1053</b> and inverted by inverter <b>1056</b>, thereby creating a signal for secondary miss line <b>29</b> (i.e., high=secondary-cache \u201cmiss\u201d).</p><p>The portions of virtual address V[all] and physical address P[all] used to index and tag-check cache entries is application specific. This two-level (i.e., primary and secondary cache), two-way, set-associative cache structure is well known to those having ordinary skill in the art.</p><p>B. Operation of System</p><p>1. Overall Operation</p><p>The system of FIG. 1 enables the use of prefetching hints in accordance with an embodiment of the invention. More specifically, prefetch instruction <b>10</b> (retrieved from main memory or secondary cache) containing prefetching hint bits <b>5</b> is partially decoded in predecoder <b>12</b> as it is written into instruction cache <b>14</b> during an instruction cache refill operation. Predecoding rearranges fields within the instruction to facilitate later decoding. In addition, the high three bits of the opcode of an instruction are modified during predecode to specify a register destination (if any) of the instruction. A prefetch instruction issuing from instruction cache <b>14</b> is decoded in decoder <b>16</b> and subsequently loaded into address queue <b>22</b>.</p><p>Prefetching hint bits <b>5</b> contained in prefetch instruction <b>10</b> are processed through predecoding and decoding operations and output on lines <b>20</b> (indicating a prefetch destination (h<b>1</b>) and enabling the use of this indicator (h<b>2</b>)) and <b>18</b> (indicating whether a prefetch request is for a load or a store (h<b>0</b>)). (These prefetching hint bits are discussed in greater detail below.) An additional bit resulting from the decoding operation and shown in FIG. 1 is an \u201caccess request\u201d bit on line <b>19</b> (indicating a memory access request such as prefetch, load or store). These bits are temporarily held in address queue <b>22</b> until the prefetch instruction is issued from the queue for purposes of execution. At which point, all four bits are forwarded to primary cache control <b>26</b> and secondary cache control <b>28</b> over line <b>24</b>.</p><p>Other bits held in address queue <b>22</b> include instruction operation codes and address information (e.g., offset values and register numbers). As described above in connection with FIG. 2, address-related bits are output on line <b>1010</b> to an integer register file <b>1011</b> and an address calculate unit <b>1012</b> to generate a virtual address. This address is subsequently forwarded to TLB <b>1020</b> to generate a physical address. The use of virtual and physical addresses in connection with primary cache <b>34</b> and secondary cache <b>70</b> is described above in connection with FIG. <b>2</b>.</p><p>Returning to FIG. 1, when prefetch instruction <b>10</b> is issued from queue <b>22</b>, processor <b>100</b> calculates an address associated with this instruction (in this embodiment, virtual and physical addresses are calculated as shown in FIG. 2) and applies this address to primary cache <b>34</b>, secondary cache <b>70</b> and main memory <b>80</b>.</p><p>Should primary cache <b>34</b> contain the desired data, a primary-cache \u201chit\u201d occurs (as described above) and the prefetch instruction causes no action. In short, instruction <b>10</b> is considered \u201cdone\u201d by processor <b>100</b> and removed from the system. No further processing need be performed.</p><p>Conversely, if primary cache <b>34</b> does not contain the data, a primary-cache \u201cmiss\u201d occurs (as described above) and primary cache control <b>26</b> notifies secondary cache control <b>28</b> through miss line <b>54</b> (i.e., logic 1). In response, secondary cache <b>70</b> is addressed for the missing data. If the desired data is found in the secondary cache (i.e., a secondary-cache hit; see above), this data is used to \u201crefill\u201d primary cache <b>34</b> over data busses <b>56</b> and <b>57</b>. Significantly, in certain circumstances, the prefetching hint bits provided on line <b>20</b> may be used to identify which way of primary cache <b>34</b> should be refilled.</p><p>If, however, secondary cache <b>70</b> does not contain the desired data, a secondary-cache \u201cmiss\u201d occurs (see above) and secondary cache control <b>28</b> notifies system interface control <b>30</b> through miss line <b>29</b> (i.e., logic 1). In response, main memory <b>80</b> is addressed for the missing data. When the data is found, it is used to \u201crefill\u201d secondary cache <b>70</b> and primary cache <b>34</b> over data buses <b>74</b>, <b>55</b>, <b>57</b> and <b>56</b>. Again, in certain circumstances, the prefetching hint bits provided on line <b>20</b> may be used to identify which way of primary cache <b>34</b> and secondary cache <b>70</b> should be refilled.</p><p>More specifically, the hint bits provided on line <b>20</b> (and ultimately conveyed to secondary cache control <b>28</b> and primary cache control <b>26</b> as described herein) may be used to direct the flow of data into way 0 or way 1 of either cache and thereby prevent streamed data from displacing retained data during refill operations.</p><p>2. Refill Operation</p><p>The use of prefetching hint bits in accordance with the system of FIG. 1 to carry out refill operations is illustrated in the flow chart <b>300</b> of FIG. <b>3</b>. At block <b>302</b>, prefetch instruction <b>10</b> has already been predecoded and loaded into instruction cache <b>14</b>. At block <b>304</b>, hint bits held in prefetch instruction <b>10</b> are decoded in decoder <b>16</b>. These bits are then forwarded to address queue <b>22</b> in accordance with block <b>306</b>, and thereafter issued to primary cache control <b>26</b> pursuant to block <b>308</b>.</p><p>At about the same time, a virtual memory address calculated from information held within prefetch instruction <b>10</b> is forwarded to primary cache <b>34</b> to determine whether there is a hit or a miss. In the event of a cache hit, no prefetching operation is required and the prefetch operation is terminated without effect, as indicated by blocks <b>310</b> and <b>320</b>.</p><p>Similarly, if an addressed cache line (i.e., cache location holding a cache block) is \u201cunavailable\u201d (discussed below), or there is an exception resulting from a prefetch instruction, the prefetch operation is terminated without effect, as indicated by blocks <b>310</b> and <b>320</b>. (The architecture of processor <b>100</b> may ignore prefetching hint bits\u2014or the prefetch instructions entirely\u2014since prefetch instructions do not effect the results of a program. Accordingly, if any problem is encountered, these instructions are simply aborted without generating any exceptions.)</p><p>On the other hand, if there is a primary-cache miss on an available cache line, a miss signal is sent to secondary cache control <b>28</b> to enable secondary cache access in accordance with block <b>312</b> (using the prefetching hint bits on line <b>24</b> and a physical address generated by TLB <b>1020</b> (FIG. <b>1</b>)). If the secondary cache hits, the addressed line is copied from secondary cache <b>70</b> to primary cache <b>34</b> (i.e., cache <b>34</b> is refilled with data conveyed over data buses <b>57</b> and <b>56</b> (FIG. <b>1</b>)) in the way selected by the hint bits held in prefetch instruction <b>10</b>, pursuant to blocks <b>314</b> and <b>318</b> of FIG. <b>3</b>. During this time, the refill status is recorded in the state field (i.e., <b>910</b> or <b>906</b> of FIG. 9) of the cache tag associated with the selected way.</p><p>Conversely, if there is a secondary-cache miss, the subject line is copied from main memory <b>80</b> to secondary cache <b>70</b> and primary cache <b>34</b> (i.e., caches <b>34</b> and <b>70</b> are refilled with data conveyed over buses <b>74</b>, <b>55</b>, <b>57</b> and <b>56</b> (FIG. <b>1</b>)) in the way selected by the hint bits held in prefetch instruction <b>10</b>, in accordance with blocks <b>314</b> and <b>316</b>. System interface control <b>30</b> includes conventional control logic (not shown) used to retrieve data from main memory (i.e., \u201cmain memory access logic\u201d). After the desired information has been refilled into the appropriate cache(s), prefetch operation is terminated in accordance with block <b>320</b>.</p><p>3. Availability</p><p>The availability of a primary cache line is indicated by state bits held in address queue <b>22</b> and tag arrays <b>38</b> and <b>42</b> of primary cache <b>34</b>. Referring to FIG. 9, tag arrays <b>38</b> and <b>42</b> hold state fields <b>906</b> and <b>910</b>, respectively. Further, address queue <b>22</b> holds \u201clock\u201d and \u201cuse\u201d bits indicating that a particular block is reserved by another instruction held in the queue. These values are forwarded to primary cache control <b>26</b> over lines <b>84</b> and <b>82</b>, respectively, as shown in FIG. <b>1</b>. (A discussion of lock and use bits is provided in copending U.S. application Ser. Nos. 08/324,129 and 08/404,625 (attorney docket number 012178-563-1, filed Mar. 14, 1995, entitled \u201cAddress Queue\u201d) which, as noted above, are incorporated herein by reference in their entirety for all purposes.) The values held by these state bits are used to determine whether a refill operation may proceed (discussed below).</p><p>A number of conditions may make a primary cache block unavailable for refill. For example, if the addressed block is already in a refill state (indicated by state fields <b>906</b> or <b>910</b>), it is unavailable for additional refill activities. Further, if another entry in address queue <b>22</b> has flagged the addressed block as \u201clocked\u201d or \u201cused\u201d (i.e., the block is needed for another memory-access operation), the block (i.e., way) is unavailable for refill.</p><p>FIGS. 4 and 5 graphically illustrate the effect of block availability. In FIG. 4, prefetching hint bits direct way 0 to be refilled. Should this way be available for a selected line (<b>402</b>), refill will proceed (<b>404</b>). Conversely, should way 0 of the selected line be unavailable (<b>406</b>), the prefetching operation is terminated without effect and the subject block remains unavailable (<b>408</b>). FIG. 5 illustrates the same effect when prefetching hint bits direct way 1 to be refilled.</p><p>C. Prefetch Instruction Format</p><p>The system of FIG. 1 uses two formats of prefetch instructions; PREF (FIG. 6) and PREFX (FIG. <b>7</b>). PREF <b>600</b> adds a 16-bit signed offset <b>608</b> to the contents of a register identified by base field <b>604</b> to form a virtual address. Hint field <b>606</b> is a 5-bit field holding prefetching hint bits that operate as described herein. These bits may be set by a programmer, compiler or operating system. The PREF operation code is identified in field <b>602</b>.</p><p>PREFX <b>700</b> adds the contents of a register identified by base field <b>704</b> to the contents of a second register identified by index field <b>706</b> to form a virtual address. Hint field <b>708</b> is a 5-bit field holding prefetching hint bits that operate as described herein. These bits may be set by the programmer, compiler or operating system. The PREFX operation code is identified in field <b>710</b>. Both instruction formats are further described in C. Price, <i>MIPS R</i>10000<i>\u2014Mips IV ISA Manual</i>, MIPS Technologies, Inc. (1994), which is hereby incorporated by reference in its entirety for all purposes.</p><p>D. Prefetching Hint Bits</p><p>As discussed generally above, prefetching hint bits contained in a prefetch instruction indicate what prefetching operation is expected (e.g., load/store) and possibly the destination (i.e., cache way) for associated data. Although the prefetch instructions in FIGS. 6 and 7 contain 5-bit hint fields, the system of FIG. 1 uses only three of these five bits. The specific use of each bit is illustrated in Tables 1 and 2, discussed below.</p><p><tables id=\"TABLE-US-00001\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 1</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Bit-Specific Action for Bits Held in Hint Field</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"1\" colwidth=\"49PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"49PT\"></colspec><colspec align=\"left\" colname=\"3\" colwidth=\"119PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\">Hint</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Bits</entry><entry morerows=\"0\" valign=\"top\">Defini-</entry><entry morerows=\"0\" valign=\"top\">Description of Action (applies to</entry></row><row><entry morerows=\"0\" valign=\"top\">h4h3h2h1h0</entry><entry morerows=\"0\" valign=\"top\">tion</entry><entry morerows=\"0\" valign=\"top\">Primary and/or Secondary Caches)</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"3\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">x x x x 0</entry><entry morerows=\"0\" valign=\"top\">Prefetch</entry><entry morerows=\"0\" valign=\"top\">Cache Hit: no action.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Shared</entry><entry morerows=\"0\" valign=\"top\">Cache Miss: refill cache with a</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(load)</entry><entry morerows=\"0\" valign=\"top\">\u201cshared\u201d (i.e, load) memory read.</entry></row><row><entry morerows=\"0\" valign=\"top\">x x x x 1</entry><entry morerows=\"0\" valign=\"top\">Prefetch</entry><entry morerows=\"0\" valign=\"top\">Cache Hit on writable block: no</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Exclusive</entry><entry morerows=\"0\" valign=\"top\">action.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(store)</entry><entry morerows=\"0\" valign=\"top\">Cache Hit on non-writable block:</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">request upgrade to writable.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Cache Miss: refill cache with an</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u201cexclusive\u201d (i.e., store) memory</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">read.</entry></row><row><entry morerows=\"0\" valign=\"top\">x x 0 0 x</entry><entry morerows=\"0\" valign=\"top\">Prefetch</entry><entry morerows=\"0\" valign=\"top\">Refill either way of the cache,</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Normal</entry><entry morerows=\"0\" valign=\"top\">using normal \u201cLeast Recently Used\u201d</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(LRU) method.</entry></row><row><entry morerows=\"0\" valign=\"top\">x x 0 1 x</entry><entry morerows=\"0\" valign=\"top\">(undefined)</entry></row><row><entry morerows=\"0\" valign=\"top\">x x 1 0 x</entry><entry morerows=\"0\" valign=\"top\">Prefetch</entry><entry morerows=\"0\" valign=\"top\">Refill only way 0 of the cache.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Way 0</entry></row><row><entry morerows=\"0\" valign=\"top\">x x 1 1 x</entry><entry morerows=\"0\" valign=\"top\">Prefetch</entry><entry morerows=\"0\" valign=\"top\">Refill only way 1 of the cache.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Way 1</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"3\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>As shown in Table 1, the system of FIG. 1 uses only the low three bits of the hint field. If bit <b>0</b> (i.e, h<b>0</b>) is set, the instruction will request an exclusive copy of the cache block (i.e., a store operation), which can be written. Otherwise, if h<b>0</b> is clear, the cache will request a shared copy of the cache block (i.e., a load operation).</p><p>Further, bit <b>2</b> (i.e., h<b>2</b>) enables the hint field to direct data to way 0 or way 1 of the cache in accordance with bit <b>1</b> (i.e., h<b>1</b>). Specifically, if h<b>2</b> is set, h<b>1</b> selects which way is refilled if there is a cache miss. If h<b>2</b> is clear, prefetch operation proceeds normally, selecting a cache way in accordance with the default way-selection method of the processor (in this case, using a \u201cleast recently used\u201d (LRU) method). The corresponding data use and operation for each combination of bits h<b>2</b>, h<b>1</b> and h<b>0</b> is illustrated in Table 2 below.</p><p><tables id=\"TABLE-US-00002\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 2</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Data Use and Operation for Select Hint Bit</entry></row><row><entry morerows=\"0\" valign=\"top\">Combinations</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"1\" colwidth=\"42PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"63PT\"></colspec><colspec align=\"left\" colname=\"3\" colwidth=\"112PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\">Hint</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Bits</entry><entry morerows=\"0\" valign=\"top\">Prefetch</entry></row><row><entry morerows=\"0\" valign=\"top\">h2h1h0</entry><entry morerows=\"0\" valign=\"top\">Operation</entry><entry morerows=\"0\" valign=\"top\">Data use and operation</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"3\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">0 0 0</entry><entry morerows=\"0\" valign=\"top\">Load using LRU</entry><entry morerows=\"0\" valign=\"top\">Data is expected to be loaded (not</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">way</entry><entry morerows=\"0\" valign=\"top\">modified).</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Fetch data as if for a load.</entry></row><row><entry morerows=\"0\" valign=\"top\">0 0 1</entry><entry morerows=\"0\" valign=\"top\">Store using</entry><entry morerows=\"0\" valign=\"top\">Data is expected to be stored or</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LRU way</entry><entry morerows=\"0\" valign=\"top\">modified.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Fetch data as if for a store.</entry></row><row><entry morerows=\"0\" valign=\"top\">0 1 0</entry><entry morerows=\"0\" valign=\"top\">Undefined</entry></row><row><entry morerows=\"0\" valign=\"top\">0 1 1</entry></row><row><entry morerows=\"0\" valign=\"top\">1 0 0</entry><entry morerows=\"0\" valign=\"top\">Load streamed</entry><entry morerows=\"0\" valign=\"top\">Data is expected to be loaded (not</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">way 0</entry><entry morerows=\"0\" valign=\"top\">modified) but not reused</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">extensively; it will \u201cstream\u201d</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">through cache.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Fetch data as if for a load and</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">place it in the cache so that it</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">will not displace data prefetched</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">as \u201cretained\u201d.</entry></row><row><entry morerows=\"0\" valign=\"top\">1 0 1</entry><entry morerows=\"0\" valign=\"top\">Store streamed</entry><entry morerows=\"0\" valign=\"top\">Data is expected to be stored or</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">way 0</entry><entry morerows=\"0\" valign=\"top\">modified but not reused</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">extensively; it will \u201cstream\u201d</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">through cache.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Fetch data as if for a store and</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">place it in the cache so that it</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">will not displace data prefetched</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">as \u201cretained\u201d.</entry></row><row><entry morerows=\"0\" valign=\"top\">1 1 0</entry><entry morerows=\"0\" valign=\"top\">Load retained</entry><entry morerows=\"0\" valign=\"top\">Data is expected to be loaded (not</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">way 1</entry><entry morerows=\"0\" valign=\"top\">modified) and reused extensively;</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">it should be \u201cretained\u201d in the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">cache.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Fetch data as if for a load and</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">place it in the cache so that it</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">will not be displaced by data</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">prefetched as \u201cstreamed\u201d.</entry></row><row><entry morerows=\"0\" valign=\"top\">1 1 1</entry><entry morerows=\"0\" valign=\"top\">Store retained</entry><entry morerows=\"0\" valign=\"top\">Data is expected to be stored or</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">way 1</entry><entry morerows=\"0\" valign=\"top\">modified and reused extensively; it</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">should be \u201cretained\u201d in the cache.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Fetch data as if for a store and</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">place it in the cache so that it</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">will not be displaced by data</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">prefetched as \u201cstreamed\u201d.</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"3\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><h4>II. PREDECODE AND DECODE</h4><p>FIG. 8 illustrates predecode and decode operations as they relate to prefetch instructions PREF <b>600</b> and PREFX <b>700</b>. As illustrated in FIG. 8, predecoding of both instructions essentially results in the rearrangement of fields within each instruction format; i.e., PREF <b>600</b> becomes <b>600</b>\u2032 and PREFX <b>700</b> becomes <b>700</b>\u2032. As noted above, predecoding facilitates later decoding.</p><p>The function code of instruction <b>700</b>\u2032 is decoded through inverters <b>802</b>, <b>804</b> and AND gate <b>806</b>, as shown in FIG. <b>8</b>. The resulting signal \u201cA\u201d is high (i.e., logic 1) when the function code of the instruction being decoded is the PREFX code of field <b>710</b> (see FIG. <b>7</b>). This signal is forwarded to the control inputs of MUXs <b>814</b>, <b>816</b> and to an input of OR gate <b>820</b>.</p><p>MUX <b>814</b> selects bits h<b>2</b> and h<b>1</b> of instruction <b>700</b>\u2032 when signal A is high, and bits h<b>2</b> and h<b>1</b> of instruction <b>600</b>\u2032 when signal A is low. The output of MUX <b>814</b> is coupled to line <b>20</b>, which feeds bits h<b>2</b>,hl to address queue <b>22</b> (FIG. <b>1</b>). Similarly, MUX <b>816</b> selects bit ho of instruction <b>700</b>\u2032 when signal A is high, and bit h<b>0</b> of instruction <b>600</b>\u2032 when signal A is low. The output of MUX <b>816</b> is fed into MUX <b>818</b>, which is described below.</p><p>The predecoded operation code <b>602</b>\u2032 of instruction <b>600</b>\u2032 is decoded through inverter <b>810</b> and AND gate <b>812</b>, as shown in FIG. <b>8</b>. The resulting signal \u201cB\u201d is high (i.e., logic 1) when the operation code of the instruction being decoded is the predecoded PREF opcode of field <b>602</b>\u2032 (see FIG. <b>8</b>). This signal is forwarded to an input of OR gate <b>820</b>.</p><p>Output <b>821</b> of OR gate <b>820</b> is coupled to the select input of MUX <b>818</b>. If the instruction being decoded is either PREF or PREFX, output <b>821</b> is high and the output of MUX <b>816</b> (i.e., h<b>0</b> of PREF or PREFX) is selected by MUX <b>818</b>. In such case, the output of MUX <b>818</b> is coupled to line <b>18</b>, which will feed selected bit h<b>0</b> to address queue <b>22</b> (FIG. <b>1</b>). Conversely, if output <b>821</b> is low, line IDODOvFd is selected, which represents a signal associated with floating point operations and unrelated to prefetching hints.</p><p>Output <b>821</b> is also coupled to the input of OR gate <b>822</b>. If the instruction being decoded is a prefetch, output <b>821</b> is high and this high value passes through OR gate <b>822</b> to line <b>19</b>. As noted above, line <b>19</b> represents an \u201caccess request bit\u201d \u2014indicating a memory access request such as prefetch, load or store. Alternatively, load indicator on line <b>824</b> and store indicator on line <b>826</b> are high (like output <b>821</b>) when the instruction being decoded is a load or store, respectively. Accordingly, load and store instructions also generate a high signal on line <b>19</b> (\u201caccess request\u201d).</p><h4>III. PRIMARY CACHE CONTROL</h4><p>FIGS. 9, <b>10</b> and <b>11</b> illustrate components of primary cache control <b>26</b> which facilitate way selection (using prefetching hint bits) for a primary cache refill operation. Referring to FIG. 9, control <b>26</b> includes a decode circuit <b>902</b> and data control circuit <b>904</b>. Block diagrams of these circuits are provided in FIGS. 10 and 11, respectively.</p><p>Circuit <b>902</b> receives bits h<b>2</b>, h<b>1</b>, h<b>0</b> and \u201caccess request\u201d on line <b>24</b>, cache information from tag arrays <b>42</b> and <b>38</b>, and state information from queue <b>22</b>. In particular, tag array <b>42</b> provides state information of an addressed cache block (i.e., way 0) on line <b>52</b> and an LRU value for the associated set on line <b>50</b>. Similarly, tag array <b>38</b> provides state information of an addressed cache block (i.e., way 1) on line <b>46</b>. Address queue <b>22</b> provides status bits (i.e., lock and use) associated with an addressed block on lines <b>84</b> and <b>82</b>, respectively.</p><p>Referring to FIG. 10, bit h<b>2</b> from line <b>24</b> is applied to the select input of MUX <b>924</b>. This MUX selects between bit h<b>1</b> on line <b>922</b> and the LRU bit on line <b>50</b>. If h<b>2</b> is high, prefetching hints are active (see Tables 1 and 2) and bit h<b>1</b> is selected. Alternatively, if h<b>2</b> is low, prefetching hints are inactive and way selection is determined through an LRU bit held in the tag array for the addressed set.</p><p>State information for way 0 is conveyed on line <b>52</b> to combinatorial logic <b>940</b>. Similarly, status bits from address queue <b>22</b> (i.e., lock and use bits) corresponding to the addressed block in way 0 are forwarded to logic <b>940</b>. If the addressed block is locked, used or undergoing refill, lines <b>84</b>, <b>82</b> or <b>52</b>, respectively, will indicate this status with a high logic state (i.e., logic 1). In such instance, logic <b>940</b> will force line <b>936</b> low (i.e., logic 0) indicating way 0 is unavailable. Alternatively, if way 0 is available, line <b>936</b> will be high.</p><p>Similarly, state information for way 1 is conveyed on line <b>46</b> to combinational logic <b>942</b>. Again, status bits from queue <b>22</b> corresponding to the addressed block in way 1 are forwarded to logic <b>942</b>. If the addressed block is locked, used or undergoing refill, lines <b>84</b>, <b>82</b> or <b>46</b>, respectively, will indicate this status with a high logic state (i.e., logic 1). In such instance, logic <b>942</b> will force line <b>938</b> low indicating way 1 is unavailable. Alternatively, if way 1 is available, line <b>938</b> will be high.</p><p>As shown in FIG. 10, the output of MUX <b>924</b> is inverted by inverter <b>928</b> and ANDed with line <b>936</b> in AND gate <b>930</b>. MUX <b>924</b> output is also ANDed with line <b>938</b> in AND gate <b>932</b>. Should either gate <b>930</b> or <b>932</b> output a high signal, this signal will pass through OR gate <b>934</b> and result in a high state on line <b>918</b>. A high signal on <b>918</b> enables a refill sequence to begin.</p><p>As shown in FIG. 11, the signals on lines <b>918</b>, <b>936</b>, <b>938</b> (FIG. <b>10</b>), and <b>54</b> (FIGS. 1 and 2) are combined in AND gates <b>1102</b> and <b>1104</b>. The signal on line <b>916</b> is input directly into AND gate <b>1102</b>, but is inverted by inverter <b>1106</b> before being input into AND gate <b>1104</b>. The outputs of <b>1102</b> and <b>1104</b> are forwarded to primary cache <b>34</b>, conveying a write enable signal to way 1 (line <b>44</b>) and way 0 (line <b>48</b>), respectively, thereby enabling a refill operation to a particular way. As described above, data used for primary cache refill operations may come from secondary cache <b>70</b> or main memory <b>80</b>.</p><p>Processor <b>100</b> defaults to an LRU method for identifying cache ways in a prefetch operation when prefetching hints h<b>1</b> and h<b>2</b> are inactive (i.e., h<b>2</b> is low) and both ways are valid. If either way is \u201cinvalid\u201d (i.e., empty), a new block can be loaded without invalidating any previous block. Specifically, if block 0 is invalid, it is replaced. Otherwise, if block 1 is invalid, it is replaced.</p><p>For any particular set in primary cache <b>34</b>, LRU bit <b>908</b> (FIG. 9) is set according to Table 3 for purposes of selecting the next block (i.e., way) in a cache refill.</p><p><tables id=\"TABLE-US-00003\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 3</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">LRU States</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"4\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"28PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"42PT\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"28PT\"></colspec><colspec align=\"left\" colname=\"4\" colwidth=\"119PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Cache</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Old</entry><entry morerows=\"0\" valign=\"top\">Tag</entry><entry morerows=\"0\" valign=\"top\">New</entry></row><row><entry morerows=\"0\" valign=\"top\">LRU</entry><entry morerows=\"0\" valign=\"top\">Check</entry><entry morerows=\"0\" valign=\"top\">LRU</entry><entry morerows=\"0\" valign=\"top\">Descrition</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"4\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">x</entry><entry morerows=\"0\" valign=\"top\">Refill</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">If either way of the cache is refilled,</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Way 0</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">the LRU bit is set equal to the opposite</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">way.</entry></row><row><entry morerows=\"0\" valign=\"top\">x</entry><entry morerows=\"0\" valign=\"top\">Refill</entry><entry morerows=\"0\" valign=\"top\">0</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Way 1</entry></row><row><entry morerows=\"0\" valign=\"top\">x</entry><entry morerows=\"0\" valign=\"top\">Hit Way</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">If the processor gets a cache hit on</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">either way of the cache, the LRU bit is</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">set equal to the opposite way, because</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">it is now \u201cleast recently used.\u201d</entry></row><row><entry morerows=\"0\" valign=\"top\">x</entry><entry morerows=\"0\" valign=\"top\">Hit Way 0</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">1</entry></row><row><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">Miss or</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">If the processor gets a cache miss, or</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">other</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">on other cycles, the LRU bit is not</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">changed.</entry></row><row><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">Miss or</entry><entry morerows=\"0\" valign=\"top\">1</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">other</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"4\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>More specifically, LRU bit <b>908</b> indicates which block within each set of cache <b>34</b> was least recently used. When it is necessary to replace one of two valid blocks, an LRU block is selected because it is statistically less likely to be used next. Under prefetch conditions, this bit is used when three requirements are satisfied: (1) processor <b>100</b> initiates a cache prefetch refill, (2) prefetching hint bits h<b>2</b> and h<b>1</b> are inactive, and (3) both blocks in the selected cache set are valid.</p><p>As shown in FIG. 9, tag arrays <b>38</b> and <b>42</b> contain one LRU bit <b>908</b> per set of the cache (i.e., per two cache blocks). While LRU bit <b>908</b> is held by the same hardware that makes up tag arrays <b>38</b> and <b>42</b>, it is not considered to be exclusively held by either array and therefore is shown connected to both arrays in FIG. 9 with dotted lines.</p><p>Whenever processor <b>100</b> gets a hit on a block in primary cache <b>34</b>, this bit is updated to select the other block in this set. That is, this block has been used more recently than the other block. When there is a miss, and a new block is refilled into the cache, this bit is updated to select the other block.</p><p>There is a separate write enable for each tag array (i.e., <b>38</b> and <b>42</b>) and LRU bit <b>908</b>. Most tag check operations result in a hit and therefore tag bits <b>909</b> and <b>911</b> are rarely written. However, LRU bit <b>908</b> is written each time a tag is read. Accordingly, the LRU bit is fabricated using a special dual-port RAM cell. The first port enables the reading of LRU bit <b>908</b> during phase 1 (i.e., \u03c61) of any processor tag-check clock cycle. The second port is used to update the LRU bit during phase 2 (i.e., \u03c62) of such cycle, if there was a cache hit. Additional information regarding the LRU bit is provided in copending U.S. patent application Ser. No. 08/324,124 which, as noted above, is incorporated herein by reference in its entirety for all purposes.</p><p>An alternative method to LRU is the well-known pseudo random replacement method. In this method, the decision as to which way is replaced is chosen randomly; no access patterns are considered. This may be implemented using a counter that is incremented every clock cycle. The lower bits of the counter may be used to select the way to be refilled.</p><h4>IV. SECONDARY CACHE CONTROL</h4><p>FIGS. 12 and 13 illustrate components and operations of secondary cache control <b>28</b> which facilitate way selection (using prefetching hint bits) for a secondary cache refill operation. Referring to FIG. 12, control <b>28</b> receives bits h<b>2</b>, h<b>1</b>, h<b>0</b> and \u201caccess request\u201d from line <b>24</b>, hit/miss indication from primary miss line <b>54</b>, and a \u201cmost recently used\u201d (MRU) bit from MRU Table <b>32</b> on line <b>1206</b>. The lowest 13 bits of the physical address P[part2] generated by TLB <b>1020</b> (FIG. 2) is used to index MRU Table 32.</p><p>Control <b>28</b> processes selective information to generate a \u201cmost significant bit\u201d (MSB) <b>1203</b> of address <b>1202</b>, which is used to access the secondary cache. The logical state of MSB <b>1203</b> (i.e., logic 0 or 1) indicates the cache way to be addressed (i.e., way 0 or way 1) for refilling (and reading) operations.</p><p>Referring to FIG. 13, bit h<b>2</b> from line <b>24</b> is applied to the select input of MUXs <b>1306</b> and <b>1314</b>. MUX <b>1306</b> selects between bit h<b>1</b> on line <b>1310</b> and the MRU bit on line <b>1206</b>. The output of MUX <b>1306</b> selects a cache way for prefetching data from (i.e., reading) secondary cache <b>70</b>. Conversely, MUX <b>1314</b> selects between bit h<b>1</b> on line <b>1310</b> and a complemented MRU bit (i.e., an LRU bit) on line <b>1326</b>. The output of MUX <b>1314</b> selects a cache way for prefetching data into (i.e., refilling) secondary cache <b>70</b>. As shown in FIG. 13, line <b>1206</b> (MRU Way) is coupled to inverter <b>1316</b> which is, in turn, coupled to MUX <b>1314</b> through line <b>1326</b>. The output of inverter <b>1316</b> is an LRU bit.</p><p>Referring to MUXs <b>1306</b> and <b>1314</b>, if h<b>2</b> is high, prefetching hints are active (see Tables 1 and 2) and bit hi is selected in each MUX. Alternatively, if h<b>2</b> is low, prefetching hint bits are inactive. Accordingly, way selection is determined through an MRU bit held in MRU Table <b>32</b>.</p><p>As mentioned above, the way-select values output by MUX <b>1306</b> (i.e., h<b>1</b> or MRU) are used for prefetching data from secondary cache <b>70</b>. When the secondary cache is available, line <b>1308</b> will be high. Accordingly, when a primary cache miss signal is received on line <b>54</b>, a high signal is applied to the select input of MUX <b>1304</b> allowing the output of MUX <b>1306</b> to pass through to line <b>1204</b>. The value on line <b>1204</b> is concatenated to address <b>1202</b> at the MSB location. This MSB (i.e., MSB <b>1203</b>) selects way 1 or way 0 for address <b>1202</b> of the current operation (i.e., reading).</p><p>The value on line <b>1204</b> remains active for one processor clock cycle. At the end of the cycle, if the secondary cache remains busy, line <b>1308</b> drops to a low forcing the output of AND gate <b>1302</b> low. Accordingly, the select input to MUX <b>1304</b> chooses the 0 input (line <b>1328</b>). It is from this input that line <b>1204</b> will receive all subsequent values so long as secondary cache <b>70</b> remains busy.</p><p>The output of MUX <b>1306</b> is also forwarded to \u201cDelayed Way Select\u201d <b>1322</b> via line <b>1320</b>. Delayed way select is a storage circuit with control logic (i.e., including RAM or register circuits) for temporarily holding, among other things, MRU, LRU and hint bits output from MUXs <b>1306</b> and <b>1314</b>. Once a secondary cache is accessed, all way select values output to line <b>1204</b> during subsequent processor clock cycles are output from delayed way select <b>1322</b> until the values in select <b>1322</b> are exhausted (at which time the secondary cache will no longer be busy and <b>1308</b> will go high).</p><p>During a secondary cache read, the first way accessed is the way identified by the h<b>1</b> or MRU bit output from MUX <b>1306</b>, as discussed above. The second way is accessed in a subsequent cycle.</p><p>When secondary cache <b>70</b> becomes busy (i.e., facilitating a read or waiting for data from main memory to perform a refill), line <b>1308</b> goes low during a cycle transition from a first cycle (i.e, initial access to available cache) to a second cycle (i.e., subsequent access to cache). Accordingly the output of AND gate <b>1302</b> is also pulled low (i.e., logic 0). As such, the 0 input to MUX <b>1304</b> (i.e., line <b>1328</b>) becomes selected. While cache <b>70</b> remains in a busy state, all new way select values from MUX <b>1306</b> are forwarded exclusively to delayed way select <b>1322</b> for access to cache <b>70</b> through input 0 of MUX <b>1304</b>.</p><p>While MUX <b>1306</b> carries out its way selection for reading operations, MUX <b>1314</b> concurrently chooses way-select values h<b>1</b> (line <b>1310</b>) or LRU (line <b>1326</b>) based on the same h<b>2</b> bit (line <b>1312</b>) for refilling operations. As shown in FIG. 13, this value is forwarded to delayed way select <b>1322</b> on line <b>1318</b> for temporary storage. Should a corresponding read operation result in a secondary cache <b>70</b> miss (after both ways are accessed), line <b>1308</b> remains low while system interface control <b>30</b> is accessed to retrieve refilling data from main memory <b>80</b>. In the meantime, the value selected by MUX <b>1314</b> is output from select <b>1322</b> and forwarded to line <b>1204</b> through the 0 input of MUX <b>1304</b>.</p><p>As described above for a secondary-cache read, the value output on line <b>1204</b> is concatenated to address <b>1202</b> at the MSB location. This MSB (i.e., MSB <b>1203</b>) selects way 1 or way 0 for address <b>1202</b>. Accordingly, data retrieved from main memory <b>80</b> for refilling operations will be directed to the appropriate way using MSB <b>1203</b> (which is the same h<b>1</b> value or the inverse of the MRU value used for reading). Concurrently, primary cache control <b>26</b> is used to perform refill operations for primary cache <b>34</b> using the same data accessed from main memory <b>80</b>.</p><p>In this embodiment, delayed way select <b>1322</b> serves as a temporary buffer allowing way values for secondary-cache reads and subsequent refills to sequentially track these operations as cache <b>70</b> becomes available. When a read attempt results in a miss, the corresponding refill way value (i.e., prefetching hint bit h<b>1</b> or an LRU bit) is accessed from delayed way select <b>1322</b> and forwarded to line <b>1204</b> through MUX <b>1304</b>. In this regard, delayed way select <b>1322</b> functions like a first-in-first-out buffer (i.e., data is transferred asynchronously; piling up as it comes in and releasing it in the same order when cache <b>70</b> becomes available).</p><p>As the foregoing illustrates, in a system having at least a two-way set-associative primary cache, a user may specify hints that control which way prefetched data is placed. The two hints described above may be referred to as \u201cprefetch retained\u201d and \u201cprefetched streamed\u201d. These hints correspond to particular ways in a set-associative cache. For example, in a two-way set-associative cache, prefetched retained might correspond to always placing prefetched data in way 1. Prefetched streamed, on the other hand, might correspond to placing such data in way 0. Normal prefetches (i.e., without either of these hints) and normal loads and stores rely on the normal replacement algorithm (e.g., LRU) to decide where data should be placed.</p><p>As an example of how this works, consider a blocked matrix multiply algorithm. The \u201cblocked\u201d portion of the data will be prefetched into the cache using the prefetch retained hint, and more importantly, the nonblocked data will be prefetched using the prefetched streamed hint. Therefore, the blocked data are less likely to be replaced from the cache and the latency will be essentially hidden.</p><p>A related example is the multiplication of a matrix by a vector. In this case, each element of the vector will be visited many times to carry out the multiplication while each element of the matrix will be visited but once. Accordingly, in this case, the vector will be prefetched into the cache using the prefetch retained hint, and the matrix will be prefetched using the prefetched streamed hint.</p><p>Similarly, in a DSP algorithm the filter coefficients would be prefetched using prefetched retained, and signal data would be prefetched using prefetched streamed. Further, operating system operations such as bcopy and bzero would only use prefetched streamed hints.</p><p>One advantage of prefetched hints is that it involves no cache state whatsoever. A hint is only kept in an instruction queue (or, perhaps, in the cache controller) while the prefetch is outstanding, and once the prefetched data is placed in the cache, the hint may be discarded. Another advantage of prefetch hints is that such hints only affect cases where the programmer, compiler or operating system has a strong reason to believe that data should go into a particular subset (i.e., way) of the cache. In all other cases, the full cache will be used in the normal way.</p><p>In contrast, if a programmer provided \u201cway hints\u201d for all loads and stores, it is likely that processor performance could actually get worse. However, prefetching hints in accordance with the principals of the invention may be selectively used based upon, for example, the access patterns for certain types of data.</p><p>As another example, consider what happens on a context switch. Since this is a stateless operation, the context switch itself occurs as normal. When a context is restarted, all of its data may have been flushed from the cache, but the processor will not realize this. Therefore, the processor will continue using \u201cprefetched streamed\u201d instructions to prefetch the streamed data, but the \u201cretained\u201d data will not be prefetched again. However, once the \u201cretained\u201d data manages to be fetched back into the appropriate cache way through the normal replacement mechanism (i.e., \u201cLRU\u201d in this case), it will stay there. Therefore, after an initial dip in performance resulting from a context switch, the performance should quickly climb back up to optimal level.</p><p>The mapping of the \u201cretained\u201d and \u201cstreamed\u201d hints to particular ways in an associative cache should be consistent across all applications. For example, if blocked matrix multiply was fetching \u201cblocked\u201d data into way 0, and \u201cnon-blocked\u201d data into way 1, the operating system should not suddenly run a bcopy operation that clears everything in way 0 (instead, it should clear out way 1). Therefore, rather than identifying a prefetch operation with a particular way, it is better to identify prefetching based upon data types (i.e., retained and streamed) and to-permanently bind this name to particular ways.</p><p>In multiple-way set-associative caches (such as a 4-way set-associative cache) it would be desirable to put streamed data in a smaller number of ways than retained data. For example, in a 4-way set-associative cache only one of the four ways should hold streamed data while the remaining three of the four ways should hold retained data.</p><p>While the above is a complete description of the preferred embodiment of the invention, various modifications, alternatives and equivalents may be used. Therefore, the above description should not be taken as limiting the scope of the invention which is defined by the appended claims.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Todd C.", "last_name": "Mowry", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "MIPS TECHNOLOGIES, INC."}, {"first_name": "", "last_name": "ARM FINANCE OVERSEAS LIMITED", "name": ""}, {"first_name": "", "last_name": "BRIDGE CROSSING, LLC", "name": ""}, {"first_name": "", "last_name": "MIPS TECHNOLOGIES, INC.", "name": ""}, {"first_name": "", "last_name": "JEFFERIES FINANCE LLC, AS COLLATERAL AGENT", "name": ""}, {"first_name": "", "last_name": "MIPS TECHNOLOGIES, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/08"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMUS"}, {"label": "G06F   9/38        20060101A I20051008RMEP"}, {"label": "G06F  12/12        20060101A N20051008RMEP"}], "national_classes": [{"primary": true, "label": "711128"}, {"primary": false, "label": "711E12057"}, {"primary": false, "label": "711129"}, {"primary": false, "label": "712E09047"}, {"primary": false, "label": "711E12018"}], "ecla_classes": [{"label": "S06F212:6028"}, {"label": "S06F12:10L4"}, {"label": "G06F   9/38D2"}, {"label": "S06F12:12B6"}, {"label": "G06F  12/08B8"}, {"label": "G06F  12/08B10"}], "cpc_classes": [{"label": "G06F  12/0864"}, {"label": "G06F  12/1045"}, {"label": "G06F  12/126"}, {"label": "G06F  12/126"}, {"label": "G06F  12/1045"}, {"label": "G06F  12/0864"}, {"label": "G06F  12/0862"}, {"label": "G06F2212/6028"}, {"label": "G06F  12/0862"}, {"label": "G06F   9/383"}, {"label": "G06F   9/383"}, {"label": "G06F2212/6028"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1995-03-24", "application_date": "1997-12-01", "family_members": [{"ucid": "US-20020010838-A1", "titles": [{"lang": "EN", "text": "Prefetching hints"}]}, {"ucid": "US-20060149904-A1", "titles": [{"lang": "EN", "text": "Prefetching hints"}]}, {"ucid": "US-6240488-B1", "titles": [{"lang": "EN", "text": "Prefetching hints"}]}, {"ucid": "US-7386701-B2", "titles": [{"lang": "EN", "text": "Prefetching hints"}]}, {"ucid": "US-5732242-A", "titles": [{"lang": "EN", "text": "Consistently specifying way destinations through prefetching hints"}]}, {"ucid": "US-7127586-B2", "titles": [{"lang": "EN", "text": "Prefetching hints"}]}]}