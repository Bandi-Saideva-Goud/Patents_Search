{"patent_number": "US-6185675-B1", "publication_id": 72561214, "family_id": 26742679, "publication_date": "2001-02-06", "titles": [{"lang": "EN", "text": "Basic block oriented trace cache utilizing a basic block sequence buffer to indicate program order of cached basic blocks"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11133497\" source=\"national office\"><p>A cache memory configured to access stored instructions according to basic blocks is disclosed. Basic blocks are natural divisions in instruction streams resulting from branch instructions. The start of a basic block is a target of a branch, and the end is another branch instruction. A microprocessor configured to use a basic block oriented cache may comprise a basic block cache and a basic block sequence buffer. The basic block cache may have a plurality of storage locations configured to store basic blocks. The basic block sequence buffer also has a plurality of storage locations, each configured to store a block sequence entry. The block sequence entry may comprise an address tag and one or more basic block pointers. The address tag corresponds to the fetch address of a particular basic block, and the pointers point to basic blocks that follow the particular basic block in a predicted order. A system using the microprocessor and a method for caching instructions in a block oriented manner rather than conventional power-of-two memory blocks are also disclosed.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72507624\"><p>A cache memory configured to access stored instructions according to basic blocks is disclosed. Basic blocks are natural divisions in instruction streams resulting from branch instructions. The start of a basic block is a target of a branch, and the end is another branch instruction. A microprocessor configured to use a basic block oriented cache may comprise a basic block cache and a basic block sequence buffer. The basic block cache may have a plurality of storage locations configured to store basic blocks. The basic block sequence buffer also has a plurality of storage locations, each configured to store a block sequence entry. The block sequence entry may comprise an address tag and one or more basic block pointers. The address tag corresponds to the fetch address of a particular basic block, and the pointers point to basic blocks that follow the particular basic block in a predicted order. A system using the microprocessor and a method for caching instructions in a block oriented manner rather than conventional power-of-two memory blocks are also disclosed.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00001\" num=\"1\"><claim-text>1. A microprocessor configured to cache basic blocks of instructions comprising:</claim-text><claim-text>a basic block cache configured to store basic blocks of instructions, wherein each basic block begins with an instruction that is a target of a branch instruction and ends with a subsequent branch instruction; </claim-text><claim-text>a basic block sequence buffer coupled to said basic block cache and configured to store block sequence entries comprising an address tag and one or more basic block pointers, wherein said address tag corresponds to the fetch address of a particular basic block stored in said basic block cache, and wherein said pointers point to basic blocks that follow said particular basic block in program order; and </claim-text><claim-text>decoding logic coupled to said basic block cache, wherein said decoding logic is configured to form basic blocks having a predetermined number of instructions by padding instruction sequences having less than said predetermined number of instructions with NULL instructions. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00002\" num=\"2\"><claim-text>2. The microprocessor as recited in claim <b>1</b>, wherein said decoding logic is configured to form basic blocks having said predetermined number of instructions by dividing instruction sequences having more than said predetermined number of instructions into multiple basic blocks, wherein all but the last of said multiple basic blocks end with NULL instructions.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00003\" num=\"3\"><claim-text>3. The microprocessor as recited in claim <b>2</b>, wherein said decoding logic is further configured to generate and store operand register dependency information with each basic block in said basic block cache.</claim-text></claim>"}, {"num": 4, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00004\" num=\"4\"><claim-text>4. The microprocessor as recited in claim <b>2</b>, wherein said basic block cache is multi-ported and configured to receive at least two pointers from said basic block sequencer buffer and output at least two basic blocks in parallel.</claim-text></claim>"}, {"num": 5, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00005\" num=\"5\"><claim-text>5. The microprocessor as recited in claim <b>2</b>, further comprising:</claim-text><claim-text>reorder buffer logic coupled to receive said basic blocks from said basic block cache; and </claim-text><claim-text>a plurality of functional units coupled to said reorder buffer logic, wherein the number of functional units is equal to at least twice the predetermined number of instructions in each basic block. </claim-text></claim>"}, {"num": 6, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00006\" num=\"6\"><claim-text>6. The microprocessor as recited in claim <b>5</b>, further comprising a level one instruction cache coupled to receive said fetch address and output a plurality of instruction bytes to said decoding logic in the event said fetch address misses in said basic block sequence buffer.</claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00007\" num=\"7\"><claim-text>7. The microprocessor as recited in claim <b>6</b>, wherein said level one instruction cache and said basic block cache maintain coherency.</claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00008\" num=\"8\"><claim-text>8. The microprocessor as recited in claim <b>7</b>, further comprising:</claim-text><claim-text>an overlap tag array coupled to said basic block sequence buffer and configured to store copies of the address tags stored within said basic block sequence buffer, and </claim-text><claim-text>comparison logic coupled to said overlap tag array and configured to receive a subset of the pointers output by the basic block sequence buffer, wherein said comparison logic is configured to compare said received pointers with the tags stored within the overlap tag array. </claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00009\" num=\"9\"><claim-text>9. The microprocessor as recited in claim <b>8</b>, further comprising an intermediate storage buffer coupled to said basic block sequence buffer and configured to store basic blocks that are part of the predicted basic block sequence and that are highly speculative.</claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00010\" num=\"10\"><claim-text>10. The microprocessor as recited in claim <b>9</b>, wherein said basic block cache is single-ported for writes.</claim-text></claim>"}, {"num": 11, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00011\" num=\"11\"><claim-text>11. The microprocessor as recited in claim <b>1</b>, further comprising decoding logic coupled to said basic block cache, wherein said decoding logic is configured to truncate basic blocks longer than a predetermined number of bytes by replacing any instructions that extend the length of basic block beyond the predetermined number of bytes with NULL instructions, wherein the decoding logic is configured to include the replaced instruction in the next basic block to be formed.</claim-text></claim>"}, {"num": 12, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00012\" num=\"12\"><claim-text>12. The microprocessor as recited in claim <b>1</b>, wherein each block sequence entry comprises two basic block pointers and a prediction bit, wherein one pointer points to the basic block that follows said particular basic if the branch instruction at the end of said particular basic block is taken, wherein one pointer points to the basic block that follows said particular basic block if the branch instruction at the end of said particular basic block is not taken, and wherein said prediction bit indicates whether the branch instruction at the end of said particular basic block is predicted taken or not taken.</claim-text></claim>"}, {"num": 13, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00013\" num=\"13\"><claim-text>13. The microprocessor as recited in claim <b>12</b>, wherein said prediction bits determine which pointers are conveyed to said basic block cache.</claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00014\" num=\"14\"><claim-text>14. The microprocessor as recited in claim <b>13</b>, wherein each block sequence entry further comprises additional branch pointers and prediction bits.</claim-text></claim>"}, {"num": 15, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00015\" num=\"15\"><claim-text>15. The microprocessor as recited in claim <b>14</b>, wherein each block sequence entry further comprises line status and line replacement information.</claim-text></claim>"}, {"num": 16, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00016\" num=\"16\"><claim-text>16. The microprocessor as recited in claim <b>14</b>, wherein said basic blocks comprise aligned and decoded instructions.</claim-text></claim>"}, {"num": 17, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00017\" num=\"17\"><claim-text>17. A method of operating a cache within a microprocessor comprising:</claim-text><claim-text>receiving instruction bytes corresponding to a fetch address; </claim-text><claim-text>decoding said instruction bytes into instructions; </claim-text><claim-text>forming basic blocks of instructions by: </claim-text><claim-text>grouping the instructions into basic blocks that end with branch instructions; </claim-text><claim-text>padding the basic blocks with NULL instructions if the basic blocks have less than a predetermined number of instructions; </claim-text><claim-text>dividing the basic blocks into two or more basic blocks if the basic blocks have more than the predetermined number of instructions; </claim-text><claim-text>storing the basic blocks into a basic block cache; and </claim-text><claim-text>storing pointers to the basic blocks into a basic block sequence buffer. </claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00018\" num=\"18\"><claim-text>18. The method of operating a cache as recited in claim <b>17</b>, further comprising:</claim-text><claim-text>storing predicted sequences of basic block pointers in the basic block sequence buffer; </claim-text><claim-text>accessing the basic block sequence buffer with a fetch address; and </claim-text><claim-text>outputting a predicted sequence of basic block pointers corresponding to the fetch address. </claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00019\" num=\"19\"><claim-text>19. The method as recited in claim <b>18</b>, further comprising:</claim-text><claim-text>recovering from a fetch address miss in the basic block sequence buffer by: </claim-text><claim-text>accessing a level one cache with the fetch address; </claim-text><claim-text>reading a plurality of instruction bytes corresponding to the fetch address from the level one cache; </claim-text><claim-text>decoding the plurality of instruction bytes into instructions; </claim-text><claim-text>forwarding the decoded instructions to a reorder buffer in preparation for execution; </claim-text><claim-text>forming basic blocks from the decoded instructions; </claim-text><claim-text>storing the basic blocks into a basic block cache; and </claim-text><claim-text>storing pointers to the basic blocks into a basic block sequence buffer. </claim-text></claim>"}, {"num": 20, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00020\" num=\"20\"><claim-text>20. The method as recited in claim <b>18</b>, further comprising:</claim-text><claim-text>recovering from a fetch address miss in the basic block cache by: </claim-text><claim-text>accessing a level one cache with the pointer from the basic block sequence buffer that missed in the basic block cache; </claim-text><claim-text>reading a plurality of instruction bytes from a level one cache, wherein the plurality of instruction bytes correspond to the pointer that missed in the basic block cache; </claim-text><claim-text>decoding the plurality of instruction bytes into instructions; </claim-text><claim-text>forwarding the decoded instructions to a reorder buffer in preparation for execution; </claim-text><claim-text>forming basic blocks from the decoded instructions; and </claim-text><claim-text>storing the basic blocks into a basic block cache. </claim-text></claim>"}, {"num": 21, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00021\" num=\"21\"><claim-text>21. The method as recited in claim <b>18</b>, further comprising:</claim-text><claim-text>recovering from a mispredicted branch instruction by: </claim-text><claim-text>determining the correct target address and branch direction using a functional unit; and </claim-text><claim-text>updating the basic block sequence buffer by: </claim-text><claim-text>replacing the old pointers and old prediction information with the newly determined correct target address and branch direction. </claim-text></claim>"}, {"num": 22, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00022\" num=\"22\"><claim-text>22. The method as recited in claim <b>21</b>, further comprising maintaining data coherency between the level one instruction cache and the basic block cache when a write to the level one instruction cache is performed by invalidating all basic blocks within the basic block cache that overlap the range of addresses overwritten in the level one instruction cache.</claim-text></claim>"}, {"num": 23, "parent": 22, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00023\" num=\"23\"><claim-text>23. The method as recited in claim <b>22</b>, further comprising re-synchronizing the level one instruction cache and the basic block cache after a write to the level one instruction cache by:</claim-text><claim-text>reading the instruction bytes corresponding to the invalidated basic blocks in the basic block buffer; </claim-text><claim-text>decoding the corresponding instruction bytes into instructions; </claim-text><claim-text>forming the instructions into new basic blocks; </claim-text><claim-text>saving the invalidated basic blocks in the basic block cache with the new basic blocks; and </claim-text><claim-text>validating the new basic blocks in the basic block cache. </claim-text></claim>"}, {"num": 24, "parent": 23, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00024\" num=\"24\"><claim-text>24. The method as recited in claim <b>23</b>, further comprising:</claim-text><claim-text>storing copies of the tags stored within the basic block sequence buffer; </claim-text><claim-text>comparing the stored copies of the tags with the addresses output by the basic block cache to determine if basic block overlapping has occurred; and </claim-text><claim-text>re-synching the pointers stored in the basic block sequence buffer and the overlapping basic blocks stored in the basic block cache. </claim-text></claim>"}, {"num": 25, "parent": 24, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00025\" num=\"25\"><claim-text>25. The method as recited in claim <b>24</b>, further comprising:</claim-text><claim-text>determining whether the fetch address hits in the basic block cache when the fetch address misses in the basic block sequence buffer; </claim-text><claim-text>outputting the matching basic blocks from the basic block cache; and </claim-text><claim-text>allocating a corresponding entry in the basic block sequence buffer. </claim-text></claim>"}, {"num": 26, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00026\" num=\"26\"><claim-text>26. A computer system configured to cache basic blocks of instructions comprising:</claim-text><claim-text>a microprocessor having </claim-text><claim-text>a basic block cache configured to store basic blocks of instructions, wherein each basic block comprises a variable number of instructions and ends with a branch instruction, wherein said basic block cache is configured to divide basic blocks greater a predetermined maximum length into two or more separate smaller basic blocks, and </claim-text><claim-text>a basic block sequence buffer comprising a plurality of storage locations, wherein each storage location is configured to store a block sequence entry comprising an address tag and one or more basic block pointers, wherein said address tag corresponds to the fetch address of a particular basic block, and wherein said pointers point to basic blocks that follow said particular basic block in program order; </claim-text><claim-text>a CPU bus coupled to said microprocessor; and </claim-text><claim-text>a modem coupled to said CPU bus via a bus bridge. </claim-text></claim>"}, {"num": 27, "parent": 26, "type": "dependent", "paragraph_markup": "<claim id=\"US-6185675-B1-CLM-00027\" num=\"27\"><claim-text>27. The computer system as recited in claim <b>26</b>, further comprising a second microprocessor coupled to said CPU bus.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54491688\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><p>This application claims the benefit of Provisional Application 60/062,794, filed on Oct. 24, 1997.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>This invention relates to caching instructions in microprocessors, and more particularly to caching instructions using basic blocks.</p><p>2. Description of the Relevant Art</p><p>In their continuing effort to improve the performance of microprocessors, designers have increased operating frequencies while also increasing the number of instructions executed per clock cycle. As used herein, the term \u201cclock cycle\u201d refers to an interval of time during which each pipeline stage of a microprocessor performs its intended functions. At the end of each clock cycle, the resulting values are moved to the next pipeline stage. These higher frequencies and increases in concurrently executed instructions have caused designers to seek methods for simplifying the tasks performed during each pipeline stage. One way designers have achieved the desired simplification is to limit the number and variation of instructions the microprocessor must execute. These microprocessors are referred to as Reduced Instruction Set Computer (RISC) processors.</p><p>Despite the apparent advantages of RISC architectures, the widespread acceptance of the x86 family of microprocessors has forced manufacturers to continue to develop higher operating frequency, multiple-issue microprocessors capable of executing the more complex x86 instruction set. Designers have had reasonable success in increasing the performance of x86 compatible microprocessors by aggressively implementing features such as pipelining, out-of-order execution, branch prediction, and issuing multiple instructions for concurrent execution. Such \u201csuperscalar\u201d microprocessors achieve relatively high performance characteristics while advantageously maintaining backwards compatibility with the vast amount of existing software developed for previous microprocessor generations such as the 8086, 80286, 80386, and 80486.</p><p>As previously noted, the x86 instruction set is relatively complex and is characterized by a plurality of variable length instructions. A generic format illustrative of the x86 instruction set is shown in FIG. <b>1</b>. As the figure illustrates, an x86 instruction consists of from one to five optional prefix bytes <b>202</b>, followed by an operation code (opcode) field <b>204</b>, an optional addressing mode (Mod R/M) byte <b>206</b>, an optional scale-index-base (SIB) byte <b>208</b>, an optional displacement field <b>210</b>, and an optional immediate data field <b>212</b>.</p><p>The opcode field <b>204</b> defines the basic operation for a particular instruction. The default operation of a particular opcode may be modified by one or more prefix bytes. For example, a prefix byte may be used to change the address or operand size for an instruction, to override the default segment used in memory addressing, or to instruct the processor to repeat the operation a number of times. The opcode field <b>204</b> follows the prefix bytes <b>202</b>, if any, and may be one or two bytes in length. The addressing mode (Mod R/M) byte <b>206</b> specifies the registers used as well as memory addressing modes. The scale-index-base (SIB) byte <b>208</b> is used only in 32-bit base-relative addressing using scale and index factors. A base field of the SIB byte specifies which register contains the base value for the address calculation, and an index field specifies which register contains the index value. A scale field specifies the power of two by which the index value will be multiplied before being added, along with any displacement, to the base value. The next instruction field is the optional displacement field <b>210</b>, which may be from one to four bytes in length. The displacement field <b>210</b> contains a constant used in address calculations. The optional immediate field <b>212</b>, which may also be from one to four bytes in length, contains a constant used as an instruction operand. The shortest x86 instructions are only one byte long and comprise a single opcode byte. The 80286 sets a maximum length for an instruction at 10 bytes, while the 80386 and 80486 both allow instruction lengths of up to 15 bytes.</p><p>The complexity of the x86 instruction set poses many difficulties in implementing high performance x86-compatible superscalar microprocessors. One particular difficulty arising from the variable-length nature of the x86 instruction set is fetching instructions from an instruction cache. The term \u201cfetching\u201d refers to reading an instruction from a cache (or if it is not in the cache, then from main memory) and routing the instruction to the appropriate decode and or functional unit within the microprocessor for decoding and execution. Caches are low-latency, high-bandwidth memories either on the same monolithic chip as the microprocessor or on a separate chip mounted in close proximity to the microprocessor. Caches are typically structured as an array of storage locations, wherein each storage location is configured to store a predetermined number of instruction bytes. For example, a typical instruction cache may store 32 kilobytes and may be configured with individual storage locations each capable of storing 32 bytes. Each storage location is typically referred to as a \u201ccache line\u201d.</p><p>Caches may be configured in a number of different ways. For example, many caches are set-associative, meaning that a particular line of instruction bytes may be stored in a number of different locations within the array. In a set-associative structure, the cache is configured into two parts, a data array and a tag array. Both arrays are two-dimensional and are organized into rows and columns. The column is typically referred to as the \u201cway.\u201d Thus a four-way set-associative cache would be configured with four columns. A set-associative cache is accessed by specifying a row in the data array and then examining the tags in the corresponding row of the tag array. For example, when a prefetch unit searches its instruction cache for instructions residing at a particular address, a number of bits from the address are used as an \u201cindex\u201d into the cache. The index selects a particular row within the data array and a corresponding row within the tag array. The number of address bits required for the index are thus determined by the number of rows configured into the cache. The tags addresses within the selected row are examined to determine if any match the requested address. If a match is found, the access is said to be a \u201chit\u201d and the data cache provides the associated instruction bytes from the data array. If a match is not found, the access is said to be a \u201cmiss.\u201d When a miss is detected, the prefetch unit causes the requested instruction bytes to be transferred from the memory system into the data array. The address associated with the instruction bytes is then stored in the tag array.</p><p>Instruction bytes are read from main memory and then stored in the instruction cache until they are needed. In some embodiments, microprocessors may \u201cpredecode\u201d the instruction bytes before they are stored in the instruction cache. Predecoding typically involves identifying the boundaries between consecutive instructions and possibly identifying the opcode bytes within the instruction. This predecode information is typically stored with the instruction bytes in the instruction cache. When instructions are fetched from the instruction cache, the predecode information is used to speed the alignment and decoding of the instructions.</p><p>After a requested instruction address is output to main memory, a predetermined number of sequential instruction bytes beginning at the requested address are read from main memory, predecoded, and then conveyed to the instruction cache for storage. The instruction bytes are stored into storage locations (\u201ccache lines\u201d) according to their address, typically without regard to what types of instructions are contained within the sequence of instruction bytes.</p><p>One drawback, however, of traditional caches is that they suffer from inefficiencies because branch instructions and branch targets do not naturally occur at cache line boundaries. This may deleteriously affect performance because taken branch instructions residing in the middle of a cache line may cause the end portion of the cache line to be discarded when it is fetched. Furthermore, branch targets that are not located at the start of a cache line may similarly cause the beginning portion of the cache line to be discarded. For example, upon receiving a fetch address, the typical instruction cache reads the entire corresponding cache line, and then selection logic (either internal or external to the instruction cache) selects the desired instructions and discards instruction bytes before the target address and or after a branch instruction.</p><p>In addition to discarding fetched instruction bytes, an additional performance penalty results from the alignment required before the instruction bytes can be properly decoded. While the cache-related problems highlighted above may occur in both RISC and x86 instruction sets, the problems are typically aggravated by the variable-length nature of x86 instructions.</p><p>Thus, a method and apparatus for more easily accessing instruction bytes stored in a cache is desired. In addition, a method that would improve the cache performance of both RISC microprocessors and x86 compatible microprocessors would be particularly desirable.</p><h4>SUMMARY OF THE INVENTION</h4><p>The problems outlined above are in large part solved by a cache memory configured to access stored instructions according to basic blocks. Instruction streams have natural divisions that are determined by branches. These divisions are referred to herein as \u201cbasic blocks\u201d, with the start of a basic block being the target of a branch, and the end being another (taken) branch instruction. Thus, a method for caching instructions in a block oriented manner rather than the conventional power-of-2 memory blocks is contemplated.</p><p>In one embodiment, the method comprises receiving instruction bytes corresponding to a fetch address and decoding the instruction bytes into instructions. Next, basic blocks of instructions are formed by grouping the instructions into basic blocks ending with branch instructions. The basic blocks may be padded with NULL instructions if the basic blocks have less than a predetermined number of instructions. Conversely, the basic blocks may be divided into two or more basic blocks if the basic blocks have more than the predetermined number of instructions. Once formed, the basic blocks are stored into a basic block cache. Pointers corresponding to the basic blocks are stored into a basic block sequence buffer. The pointers are stored with branch prediction information to form predicted sequences of basic blocks which are output by the sequence buffer when it receives a corresponding fetch address. Multiple basic block pointers may be output and fetched from the basic block cache in a particular clock cycle.</p><p>A microprocessor configured to cache basic blocks of instructions is also contemplated. In one embodiment, the microprocessor comprises a basic block cache and a basic block sequence buffer. The basic block cache is configured to store basic blocks, wherein each basic block may comprise a number of instructions and may end with a branch instruction. The basic block sequence buffer comprises a plurality of storage locations, each configured to store a block sequence entry. The block sequence entry has an address tag and one or more basic block pointers. The address tag corresponds to the fetch address of a particular basic block, and the pointers point to basic blocks that follow that particular basic block in a predicted order. Each block sequence entry may contain multiple basic block pointers and branch prediction information to select the basic block that is predicted to follow the block corresponding to the address tag.</p><p>A computer system configured to utilize a basic block oriented cache is also disclosed. In one embodiment, the system comprises a microprocessor having a basic block cache and a basic block sequence buffer. The basic block cache and basic block sequence buffer may be configured as described above. A CPU bus may be coupled to the microprocessor, and a modem may be coupled to the CPU bus via a bus bridge.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Other objects and advantages of the invention will become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p><p>FIG. 1 is a diagram illustrating the generic format of the x86 instruction set.</p><p>FIG. 2 is a block diagram of one embodiment of a microprocessor configured to employ basic block oriented instruction caching.</p><p>FIG. 3 is block diagram illustrating details of one embodiment of the basic block sequence buffer (BBSB) and basic block cache (BBC) from FIG. <b>2</b>.</p><p>FIG. 4 is block diagram illustrating details of another embodiment of the BBSB and BBC from FIG. <b>2</b>.</p><p>FIG. 5 is a diagram illustrating one embodiment of potential pipeline stages within the microprocessor of FIG. <b>2</b>.</p><p>FIG. 6 is a diagram illustrating one embodiment of a basic block tree showing the possible paths from a single basic block.</p><p>FIG. 7 is a table illustrating one embodiment of a sequence of accesses or basic blocks.</p><p>FIG. 8 is a diagram depicting an exemplary address scheme for the BBSB of FIG. <b>2</b>.</p><p>FIG. 9 is an illustration of one embodiment of a sequence of basic blocks.</p><p>FIG. 10A is a table of sample addresses from a basic block sequence.</p><p>FIG. 10B is a diagram illustrating one possible method for storing information about basic blocks.</p><p>FIG. 11 illustrates one embodiment of an exemplary storage line within the BBSB of FIG. <b>2</b>.</p><p>FIG. 12 illustrates another embodiment of the BBSB from FIG. <b>2</b>.</p><p>FIG. 13 illustrates one possible configuration of the functional units from FIG. <b>2</b>.</p><p>FIG. 14 is a diagram detailing one embodiment of a cache line within one embodiment of the BBC from FIG. <b>2</b>.</p><p>FIG. 15 is a diagram of an exemplary sequence of instructions.</p><p>FIG. 16 is a diagram of the operational pipeline of another embodiment of the microprocessor from FIG. <b>2</b>.</p><p>FIG. 17 is a diagram illustrating an exemplary latency of instructions propagating through one embodiment of the microprocessor from FIG. <b>2</b>.</p><p>FIG. 18 is a diagram illustrating relative basic block positions.</p><p>FIG. 19 is a diagram illustrating one exemplary division of INV_ADR.</p><p>FIG. 20 is a diagram showing one possible method for generating INV_ADR_LOW.</p><p>FIG. 21 is a diagram illustrating one example of exceeding maximum basic block length.</p><p>FIG. 22 is a diagram illustrating improper invalidation of a basic block.</p><p>FIG. 23 is a diagram illustrating changes to basic blocks as the result of self-modifying code.</p><p>FIG. 24 is a diagram illustrating a situation which may result in the loading of an improper basic block.</p><p>FIG. 25 is a diagram illustrating pointers to basic blocks.</p><p>FIG. 26 is a diagram illustrating code with multiple jump targets.</p><p>FIG. 27 is a diagram illustrating one possible method for storing instructions within one embodiment of the BBC from FIG. <b>2</b>.</p><p>FIG. 28 is a diagram illustrating another possible method for storing instructions within one embodiment of the BBC from FIG. <b>2</b>.</p><p>FIG. 29 is a diagram illustrating one possible overlapping scenario for basic blocks.</p><p>FIG. 30 is a diagram illustrates a \u201cworst case\u201d scenario for the basic block overlapping from FIG. <b>29</b>.</p><p>FIG. 31 is a diagram illustrating multiple entry lookup within one embodiment of the BBC from FIG. <b>2</b>.</p><p>FIG. 32 is a diagram illustrating instruction sequences with different basic block footprints.</p><p>FIG. 33 is a diagram illustrating an example of sequence entries within one embodiment of the BBSB from FIG. <b>2</b>.</p><p>FIG. 34 is a block diagram of one embodiment of a computer system configured to utilize the microprocessor of FIG. <b>2</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><p>While the present invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><h4>DETAILED DESCRIPTION OF SEVERAL EMBODIMENTS</h4><p>First, a general description of one embodiment of a superscalar microprocessor configured to store instructions in a \u201cbasic block oriented\u201d instruction cache will be given. After the general description, more details of the operation of the instruction cache and basic block oriented nature of microprocessor <b>10</b> will be discussed.</p><p>Exemplary Embodiment of a Microprocessor</p><p>Turning now to FIG. 2, a block diagram of one embodiment of a microprocessor <b>10</b> is shown. Microprocessor <b>10</b> includes a prefetch/predecode unit <b>12</b>, a branch prediction unit <b>14</b>, an instruction cache <b>16</b>, an instruction alignment unit <b>18</b>, a decode unit <b>20</b>, a plurality of reservation stations <b>22</b>A-<b>22</b>N, a plurality of functional units <b>24</b>A-<b>24</b>N, a load/store unit <b>26</b>, a data cache <b>28</b>, a register file <b>30</b>, a reorder buffer <b>32</b>, an MROM unit <b>34</b>, a floating point unit (FPU) <b>36</b>, a multiplexer <b>40</b>, a basic block sequence buffer (BBSB) <b>42</b>, a basic block cache (BBC) <b>44</b>, and fetch logic <b>46</b>. Elements referred to herein with a particular reference number followed by a letter may be collectively referred to by the reference number alone. For example, functional units <b>24</b>A-<b>24</b>N may be collectively referred to as functional units <b>24</b>.</p><p>Prefetch/predecode unit <b>12</b> is coupled to receive instructions from a main memory subsystem (not shown), and is further coupled to level one instruction cache <b>16</b> and branch prediction unit <b>14</b>. Branch prediction unit <b>14</b> is coupled to instruction cache <b>16</b> and functional units <b>24</b>A-N. Instruction cache <b>16</b> is further coupled to instruction alignment unit <b>18</b> and MROM unit <b>34</b>. Instruction alignment unit <b>18</b> is in turn coupled to decode unit <b>20</b>. Decode unit <b>20</b> and MROM unit <b>34</b> are coupled to each other and to BBSB <b>42</b>, BBC <b>44</b>, and multiplexer <b>40</b>. Fetch logic <b>46</b>, BBSB <b>42</b>, and BBC <b>44</b> are each coupled together, while the output from BBC <b>44</b> is coupled to multiplexer <b>40</b>, which is coupled to reorder buffer <b>32</b>. Reorder buffer <b>32</b> is in turn coupled to register file <b>30</b>, FPU <b>36</b>, and reservation stations <b>22</b>A-<b>22</b>N. Reservation stations <b>22</b>A-<b>22</b>N are coupled to respective functional units <b>24</b>A-<b>24</b>N, branch prediction unit <b>14</b>, and a result bus <b>38</b>. The result bus is also coupled to load/store unit <b>26</b>, data cache <b>28</b>, and register file <b>30</b>. Data cache <b>28</b> is coupled to load/store unit <b>26</b> and to the main memory subsystem.</p><p>Level one instruction cache <b>16</b> is a high speed cache memory configured to store instruction bytes as they are received from main memory via prefetch/predecode unit <b>12</b>. Instructions may be \u201cprefetched\u201d prior to the request thereof from instruction cache <b>16</b> in accordance with a prefetch scheme. A variety of prefetch schemes may be employed by prefetch/predecode unit <b>12</b> to store instruction bytes within instruction cache <b>16</b> before they are actually needed.</p><p>Prefetch/predecode unit <b>12</b> may also perform other task. For example, in one embodiment of prefetch/predecode unit <b>12</b>, as instructions are transferred from main memory to instruction cache <b>16</b>, prefetch/predecode unit <b>12</b> may be configured to generate three predecode bits for each byte of the instructions: a start bit, an end bit, and a functional bit. An asserted start bit corresponds to the first byte of an instruction. An asserted end bit corresponds to the lasts byte of an instruction. An asserted functional bit corresponds to an opcode byte within an instruction. These predecode bits may be stored in instruction <b>16</b> along with their corresponding instruction bytes. The predecode bits collectively form predecode \u201ctags\u201d indicative of the boundaries of each instruction. These predecode tags may be used by alignment unit <b>18</b>, decode unit <b>20</b>, and MROM unit <b>34</b> to speed the alignment and decoding process.</p><p>Instruction cache <b>16</b> may be configured to store up to 256 kilobytes of instructions in a 4-way set-associative structure having 32-byte cache lines. Alternatively, other associative or non-associative configurations and sizes may be employed for instruction cache <b>16</b>, e.g., fully associative, 2-way associative, or direct mapped configurations having sizes of 128 or 64 kilobytes. If instruction cache <b>16</b> is configured in a set-associative manner, \u201cway predictions\u201d may be employed in order to speed access times. Instead of accessing tags identifying each line of instructions and then comparing the tags to the fetch address to select a way, way prediction entails predicting which way will be accessed. In this manner, the way is selected prior to accessing the instruction storage location. Thus, the access time of instruction cache <b>16</b> may be shorter, e.g., similar to a direct-mapped cache. A tag comparison is performed after the predicted way is output, and if the way prediction is incorrect, the correct instructions are fetched and the incorrect instructions are discarded.</p><p>As instructions are fetched from instruction cache <b>16</b>, the corresponding predecode data is scanned to provide information to instruction alignment unit <b>18</b> (and to MROM unit <b>34</b>) regarding the instructions being fetched. Instruction alignment unit <b>18</b> utilizes the scanning data to align instructions for decode unit <b>20</b>. In one embodiment, decode unit <b>20</b> comprises three independent instruction decoders, each capable of decoding one instruction per clock cycle. In this embodiment, instruction alignment unit <b>18</b> may be configured to align instructions from three sets of eight instruction bytes to the three parallel instruction decoders within decode unit <b>20</b>. Instructions are selected independently from each set of eight instruction bytes into preliminary issue positions. The preliminary issue positions are then merged to a set of aligned issue positions corresponding to each decoder within decode unit <b>20</b>, such that the aligned issue positions contain the three instructions which are prior to other instructions within the preliminary issue positions in program order. In this embodiment, the first decoder within decode unit <b>20</b> receives an instruction which is prior to (in program order) instructions concurrently received by the second and third instruction decoders within decode unit <b>20</b>. Similarly, the second decoder within decode unit <b>20</b> receives an instruction which is prior to (in program order) the instruction concurrently received by the third decoder within decode unit <b>20</b>. As previously noted, predecode information generated by predecode unit <b>12</b> and stored in instruction cache <b>16</b> may be used to speed the alignment process.</p><p>MROM unit <b>34</b> monitors the instructions, and when it detects an instruction that is too complex for decode unit <b>20</b>, it replaces the instruction with a series of microcode instructions. The less complex instructions are decoded within decode unit <b>20</b>. Decode unit <b>20</b> identifies the different fields within the instruction and expands the instruction into a predetermined internal format that is more convenient for functional units <b>24</b>A-<b>24</b>N than the standard instruction format. Note that if microprocessor <b>10</b> is configured to execute only RISC instructions, alignment unit <b>18</b>, MROM unit <b>34</b>, and decode unit <b>20</b> may be greatly simplified or eliminated.</p><p>Decode unit <b>20</b> is configured to decode instructions received from instruction alignment unit <b>18</b>. Register operand information is also detected and decoded. This information is routed to register file <b>30</b> and reorder buffer <b>32</b> via multiplexer <b>40</b>. Additionally, if the instructions require one or more memory operations to be performed, decode units <b>20</b> dispatch the memory operations to load/store unit <b>26</b>. Each instruction is decoded into a set of control values for functional units <b>24</b>, and these control values are dispatched to reservation stations <b>22</b> along with operand address information and displacement or immediate data which may be included with the instruction. If decode units <b>20</b> detect a floating point instruction, the instruction is dispatched to FPU/MMX unit <b>36</b>.</p><p>When decode unit <b>20</b> outputs the decoded instructions, this may be referred to as \u201cdispatching\u201d the instructions. When instructions are dispatched to reorder buffer <b>32</b>, they are also copied in parallel into BBC <b>44</b>. BBC <b>44</b> stores the decoded instructions with an address tag comprising all or part of the fetch address that fetched the decoded instructions. In one embodiment, BBC <b>44</b> is fully associative and uses the entire fetch address as the tag. In another embodiment, BBC <b>44</b> is set associative (e.g., 4-way) and uses portions of the fetch address as the tag. This involves splitting the address into the following three portions: (1) the index, (2) higher tag bits, and (3) lower TAG bits. The index bits (used to index into BBC <b>44</b>) are bits selected from the middle of the fetch address. A tag comparison is performed for the higher and lower tag bits (offset). Thus, each basic block of instructions stored within BBC <b>44</b> has its own unique starting address and may be easily accessed.</p><p>BBSB <b>42</b> may be configured to have the same structure (e.g., addressing scheme) as BBC <b>44</b> and to receive the same fetch address information that BBC <b>44</b> receives from decode unit <b>20</b>. However, instead of storing basic blocks, BBSB <b>42</b> is configured to store information about the corresponding basic blocks in BBC <b>44</b>. For example, BBSB <b>42</b> may store predicted sequences of basic blocks and the addresses of all possible following basic blocks. It may also contain prediction information indicative of whether the corresponding branch instructions (that define the end of each basic block) will be taken or not taken. This prediction information may be used to select which basic block will be executed next.</p><p>In one embodiment, microprocessor <b>10</b> may employ branch prediction in order to speculatively fetch and or prefetch instructions subsequent to conditional branch instructions. Branch prediction unit <b>14</b> is included to perform such branch prediction operations. In one embodiment, branch prediction unit <b>14</b> is configured to store up to two branch target addresses for each 16 byte portion of each cache line in instruction cache <b>16</b>. Prefetch/predecode unit <b>12</b> determines initial branch targets when a particular line is predecoded. Subsequent updates to the branch targets corresponding to a cache line may occur due to the execution of instructions within the cache line. Instruction cache <b>16</b> provides an indication of the instruction address being fetched, so that branch prediction unit <b>14</b> may determine which branch target addresses to select for forming a branch prediction. Decode units <b>20</b> and functional units <b>24</b> provide update information to branch prediction unit <b>14</b>. Because branch prediction unit <b>14</b> stores only two targets per 16 byte portion of the cache line, predictions for some branch instructions within the line may not be stored in branch prediction unit <b>14</b>. Decode units <b>20</b> detect branch instructions which were not predicted by branch prediction unit <b>14</b>. Functional units <b>24</b> execute the branch instructions and determine if the predicted branch direction is incorrect. The branch direction may be \u201ctaken\u201d, in which subsequent instructions are fetched from the target address of the branch instruction. Conversely, the branch direction may be \u201cnot taken\u201d, in which subsequent instructions are fetched from memory locations consecutive to the branch instruction. When a mispredicted branch instruction is detected, instructions subsequent to the mispredicted branch are discarded from the various units of microprocessor <b>10</b>. A variety of suitable branch prediction algorithms may be employed by branch prediction unit <b>14</b>.</p><p>Microprocessor <b>10</b> supports out of order execution, and thus employs reorder buffer <b>32</b> to keep track of the original program sequence for register read and write operations, to implement register renaming, to allow for speculative instruction execution and branch misprediction recovery, and to facilitate precise exceptions. A temporary storage location within reorder buffer <b>32</b> is reserved upon decode of an instruction that involves the update of a register to thereby store speculative register states. If a branch prediction is incorrect, the results of speculatively executed instructions along the mispredicted path can be invalidated in the buffer before they are written to register file <b>30</b>. Similarly, if a particular instruction causes an exception, instructions subsequent to the particular instruction may be discarded. In this manner, exceptions are \u201cprecise\u201d (i.e., instructions subsequent to the particular instruction causing the exception are not completed prior to the exception). It is noted that a particular instruction is speculatively executed if it is executed prior to instructions which precede the particular instruction in program order. Preceding instructions may be a branch instruction or an exception-causing instruction, in which case the speculative results may be discarded by reorder buffer <b>32</b>.</p><p>The instruction control values and immediate or displacement data provided at the outputs of decode units <b>20</b> are routed directly to respective reservation stations <b>22</b>. In one embodiment, each reservation station <b>22</b> is capable of holding instruction information (i.e., instruction control values as well as operand values, operand tags and/or immediate data) for up to three pending instructions awaiting issue to the corresponding functional unit. It is noted that for the embodiment of FIG. 2, each reservation station <b>22</b> is associated with a dedicated functional unit <b>24</b>. Accordingly, three dedicated \u201cissue positions\u201d are formed by reservation stations <b>22</b> and functional units <b>24</b>. In other words, issue position <b>0</b> is formed by reservation station <b>22</b>A and functional unit <b>24</b>A. Instructions aligned and dispatched to reservation station <b>22</b>A are executed by functional unit <b>24</b>A. Similarly, issue position <b>1</b> is formed by reservation station <b>22</b>B and functional unit <b>24</b>B; and issue position <b>2</b> is formed by reservation station <b>22</b>C and functional unit <b>24</b>C.</p><p>Upon decode of a particular instruction, if a required operand is a register location, register address information is routed to reorder buffer <b>32</b> and register file <b>30</b> simultaneously. Those of skill in the art will appreciate that the x86 register file includes eight 32 bit real registers (i.e., typically referred to as EAX, EBX, ECX, EDX, EBP, ESI, EDI and ESP). In embodiments of microprocessor <b>10</b> which employ the x86 microprocessor architecture, register file <b>30</b> comprises storage locations for each of the 32 bit real registers. Additional storage locations may be included within register file <b>30</b> for use by MROM unit <b>34</b>. Reorder buffer <b>32</b> contains temporary storage locations for results which change the contents of these registers to thereby allow out of order execution. A temporary storage location of reorder buffer <b>32</b> is reserved for each instruction which, upon decode, is determined to modify the contents of one of the real registers. Therefore, at various points during execution of a particular program, reorder buffer <b>32</b> may have one or more locations which contain the speculatively executed contents of a given register. If following decode of a given instruction it is determined that reorder buffer <b>32</b> has a previous location or locations assigned to a register used as an operand in the given instruction, the reorder buffer <b>32</b> forwards to the corresponding reservation station either: 1) the value in the most recently assigned location, or 2) a tag for the most recently assigned location if the value has not yet been produced by the functional unit that will eventually execute the previous instruction. If reorder buffer <b>32</b> has a location reserved for a given register, the operand value (or reorder buffer tag) is provided from reorder buffer <b>32</b> rather than from register file <b>30</b>. If there is no location reserved for a required register in reorder buffer <b>32</b>, the value is taken directly from register file <b>30</b>. If the operand corresponds to a memory location, the operand value is provided to the reservation station through load/store unit <b>26</b>.</p><p>In one particular embodiment, reorder buffer <b>32</b> is configured to store and manipulate concurrently decoded instructions as a unit. This configuration will be referred to herein as \u201cline-oriented\u201d. By manipulating several instructions together, the hardware employed within reorder buffer <b>32</b> may be simplified. For example, a line-oriented reorder buffer included in the present embodiment allocates storage sufficient for instruction information pertaining to three instructions (one from each decode unit <b>20</b>) whenever one or more instructions are dispatched by decode units <b>20</b>. By contrast, a variable amount of storage is allocated in conventional reorder buffers, dependent upon the number of instructions actually dispatched. A comparatively larger number of logic gates may be required to allocate the variable amount of storage. When each of the concurrently decoded instructions has executed, the instruction results are stored into register file <b>30</b> simultaneously. The storage is then free for allocation to another set of concurrently decoded instructions. Additionally, the amount of control logic circuitry employed per instruction may be reduced because the control logic is amortized over several concurrently decoded instructions. A reorder buffer tag identifying a particular instruction may be divided into two fields: a line tag and an offset tag. The line tag identifies the set of concurrently decoded instructions including the particular instruction, and the offset tag identifies which instruction within the set corresponds to the particular instruction. It is noted that storing instruction results into register file <b>30</b> and freeing the corresponding storage is referred to as \u201cretiring\u201d the instructions. It is further noted that any reorder buffer configuration may be employed in various embodiments of microprocessor <b>10</b>.</p><p>As noted earlier, reservation stations <b>22</b> store instructions until the instructions are executed by the corresponding functional unit <b>24</b>. An instruction is selected for execution if: (i) the operands of the instruction have been provided; and (ii) the operands have not yet been provided for instructions which are within the same reservation station <b>22</b>A-<b>22</b>C and which are prior to the instruction in program order. It is noted that when an instruction is executed by one of the functional units <b>24</b>, the result of that instruction is passed directly to any reservation stations <b>22</b> that are waiting for that result at the same time the result is passed to update reorder buffer <b>32</b> (this technique is commonly referred to as \u201cresult forwarding\u201d). An instruction may be selected for execution and passed to a functional unit <b>24</b>A-<b>24</b>C during the clock cycle that the associated result is forwarded. Reservation stations <b>22</b> route the forwarded result to the functional unit <b>24</b> in this case.</p><p>In one embodiment, each of the functional units <b>24</b> is configured to perform integer arithmetic operations of addition and subtraction, as well as shifts, rotates, logical operations, and branch operations. The operations are performed in response to the control values decoded for a particular instruction by decode units <b>20</b>. It is noted that FPU/MMX unit <b>36</b> may also be employed to accommodate floating point and multimedia operations. The floating point unit may be operated as a coprocessor, receiving instructions from MROM unit <b>34</b> and subsequently communicating with reorder buffer <b>32</b> to complete the instructions. Additionally, functional units <b>24</b> may be configured to perform address generation for load and store memory operations performed by load/store unit <b>26</b>.</p><p>Each of the functional units <b>24</b> also provides information regarding the execution of conditional branch instructions to the branch prediction unit <b>14</b>. If a branch prediction was incorrect, branch prediction unit <b>14</b> flushes instructions subsequent to the mispredicted branch that have entered the instruction processing pipeline, and causes fetch of the required instructions from instruction cache <b>16</b> or main memory. It is noted that in such situations, results of instructions in the original program sequence which occur after the mispredicted branch instruction are discarded, including those which were speculatively executed and temporarily stored in load/store unit <b>26</b> and reorder buffer <b>32</b>.</p><p>Results produced by functional units <b>24</b> are sent to reorder buffer <b>32</b> if a register value is being updated, and to load/store unit <b>26</b> if the contents of a memory location are changed. If the result is to be stored in a register, reorder buffer <b>32</b> stores the result in the location reserved for the value of the register when the instruction was decoded. A plurality of result buses <b>38</b> are included for forwarding of results from functional units <b>24</b> and load/store unit <b>26</b>. Result buses <b>38</b> convey the result generated, as well as the reorder buffer tag identifying the instruction being executed.</p><p>Load/store unit <b>26</b> provides an interface between functional units <b>24</b> and data cache <b>28</b>. In one embodiment, load/store unit <b>26</b> is configured with a load/store buffer having eight storage locations for data and address information for pending loads or stores. Decode units <b>20</b> arbitrate for access to the load/store unit <b>26</b>. When the buffer is full, a decode unit must wait until load/store unit <b>26</b> has room for the pending load or store request information. Load/store unit <b>26</b> also performs dependency checking for load memory operations against pending store memory operations to ensure that data coherency is maintained. A memory operation is a transfer of data between microprocessor <b>10</b> and the main memory subsystem. Memory operations may be the result of an instruction which utilizes an operand stored in memory, or may be the result of a load/store instruction which causes the data transfer but no other operation. Additionally, load/store unit <b>26</b> may include a special register storage for special registers such as the segment registers and other registers related to the address translation mechanism defined by the x86 microprocessor architecture.</p><p>In one embodiment, load/store unit <b>26</b> is configured to perform load memory operations speculatively. Store memory operations are performed in program order, but may be speculatively stored into the predicted way. If the predicted way is incorrect, the data prior to the store memory operation is subsequently restored to the predicted way and the store memory operation is performed to the correct way. In another embodiment, stores may be executed speculatively as well. Speculatively executed stores are placed into a store buffer, along with a copy of the cache line prior to the update. If the speculatively executed store is later discarded due to branch misprediction or exception, the cache line may be restored to the value stored in the buffer. It is noted that load/store unit <b>26</b> may be configured to perform any amount of speculative execution, including no speculative execution.</p><p>Data cache <b>28</b> is a high speed cache memory provided to temporarily store data being transferred between load/store unit <b>26</b> and the main memory subsystem. In one embodiment, data cache <b>28</b> has a capacity of storing up to sixteen kilobytes of data in an eight way set-associative structure. Similar to instruction cache <b>16</b>, data cache <b>28</b> may employ a way prediction mechanism. It is understood that data cache <b>28</b> may be implemented in a variety of specific memory configurations.</p><p>In one particular embodiment of microprocessor <b>10</b> employing the x86 microprocessor architecture, instruction cache <b>16</b> and data cache <b>28</b> are linearly addressed. The linear address is formed from the offset specified by the instruction and the base address specified by the segment portion of the x86 address translation mechanism. Linear addresses may optionally be translated to physical addresses for accessing a main memory. The linear to physical translation is specified by the paging portion of the x86 address translation mechanism. It is noted that a linear addressed cache stores linear address tags. A set of physical tags (not shown) may be employed for mapping the linear addresses to physical addresses and for detecting translation aliases. Additionally, the physical tag block may perform linear to physical address translation.</p><p>Basic Block Sequence Buffer (BBSB) and Basic Block Cache (BBC)</p><p>Turning now to FIG. 3, more details regarding the organization of one embodiment of BBSB <b>42</b> and BBC <b>44</b> are shown. BBSB <b>42</b> comprises a plurality of storage lines <b>52</b>, each configured to store an address tag <b>52</b><i>a</i>, and two or more basic block pointers <b>52</b><i>b-c</i>. Similarly, BBC <b>44</b> also comprises a plurality of cache blocks <b>54</b><i>a-n</i>, each configured to store a basic block <b>56</b> and an address tag <b>58</b>. Both BBSB <b>42</b> and BBC <b>44</b> are pipelined, with BBSB <b>42</b> being accessed in the pipeline stage before BBC <b>44</b> is accessed.</p><p>Generally, on an initial lookup a fetch address may be used to fetch the target basic block from BBC <b>44</b>, as well as retrieve a set of pointers from BBSB <b>42</b>. The pointers point to a number of subsequent predicted basic blocks that may be used for the next lookup in BBC <b>44</b>. Each subsequent BBSB index is derived from information in the previous BBSB entry, as is each subsequent set of BBC indexes.</p><p>Fetch addresses are routed in parallel to both BBSB <b>42</b> and BBC <b>44</b>. BBSB <b>42</b> uses the fetch address (or a portion thereof) to access a particular storage line by performing comparisons on tags <b>52</b><i>a</i>. Upon finding a match, BBSB outputs two pointers. The first pointer <b>52</b><i>c </i>(represented as BBn+1) corresponds to the basic block that is predicted to directly follow the fetch address (BBn). Pointer <b>52</b><i>c </i>is routed to BBC <b>42</b>'s second read port (port <b>2</b>). The second pointer <b>52</b><i>b </i>(BBn+2) corresponds to the basic block that is predicted to follow pointer <b>52</b><i>c</i>. Pointer <b>52</b><i>b </i>is routed to multiplexer <b>50</b>, which selects it as the new predicted fetch address for the next fetch cycle.</p><p>The next clock cycle, BBC <b>44</b> receives the fetch address in its first read port and pointer <b>52</b><i>c </i>in its second read port. During this clock cycle, BBC <b>44</b> outputs the corresponding basic blocks (e.g., BBn and BBn+1) based upon comparisons of tags <b>58</b>. Thus, after one clock the output of BBC <b>44</b> may appear as indicated in Table 1.</p><p><tables id=\"TABLE-US-00001\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"91PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"63PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"63PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\">TABLE 1</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Port 1</entry><entry morerows=\"0\" valign=\"top\">Port 2</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></thead><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"4\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"35PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"56PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"63PT\"></colspec><colspec align=\"left\" colname=\"3\" colwidth=\"63PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Cycle 1</entry><entry morerows=\"0\" valign=\"top\">(none)</entry><entry morerows=\"0\" valign=\"top\">(none)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Cycle 2</entry><entry morerows=\"0\" valign=\"top\">BBn</entry><entry morerows=\"0\" valign=\"top\">BBn + 1</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Cycle 3</entry><entry morerows=\"0\" valign=\"top\">BBn + 2</entry><entry morerows=\"0\" valign=\"top\">BBn + 3</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"3\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>Note, for simplicity the figure and the examples above show only two blocks fetched at a time. However, BBSB <b>42</b> and BBC <b>44</b> may be extended to accommodate additional blocks. Further note that the clock cycles indicated above may vary across different implementations.</p><p>Indexes for BBC <b>44</b> are derived directly from either the fetch address (e.g., on a branch misprediction), or from the BBSB prediction pointers. BBSB indexing can be performed in the same way, or the index may be modified, e.g., by a hashing algorithm. Using normal memory addresses to access BBC <b>44</b> allows cache coherency to be maintained. Details on index formation, BBSB entries, and coherency will be discussed in greater detail below.</p><p>The form of the pointers provided by BBSB <b>42</b> and the method of detecting hits in BBC <b>44</b> is subject to the same tradeoff as conventional branch prediction, where there are two basic approaches. The first approach is for the predictor to provide a full target address for the cache lookup. A normal cache tag comparison is then performed to determine if the block is in the cache. An alternate approach is to store only cache block addresses in the predictor array (i.e., a particular cache block is predicted to contain the proper target instructions), thereby greatly reducing the array size. The full address is formed from the block index and the cache tag, and then sent to functional units <b>24</b>A-N for verification. Note that verification may be performed for either scheme. With this method a cache miss may not be detected until the branch in question is executed, hence there may be a tradeoff of less real estate versus greater delay in detecting cache misses.</p><p>Turning now to FIG. 4, another embodiment of BBC <b>44</b> is shown. In this embodiment, BBSB <b>42</b> and BBC <b>44</b> are accessed in parallel with the fetch address during the same clock cycle. Since BBC <b>44</b> may output the corresponding basic block as soon as it is available (versus waiting a clock cycle as in the previous embodiment), the first basic block (BBn) may be available one clock cycle sooner. This is reflected in the output as indicated in Table 2.</p><p><tables id=\"TABLE-US-00002\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"91PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"63PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"63PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\">TABLE 2</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Port 1</entry><entry morerows=\"0\" valign=\"top\">Port 2</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></thead><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"4\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"35PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"56PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"63PT\"></colspec><colspec align=\"left\" colname=\"3\" colwidth=\"63PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Cycle 1</entry><entry morerows=\"0\" valign=\"top\">BBn</entry><entry morerows=\"0\" valign=\"top\">(null)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Cycle 2</entry><entry morerows=\"0\" valign=\"top\">BBn + 1</entry><entry morerows=\"0\" valign=\"top\">BBn + 2</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Cycle 3</entry><entry morerows=\"0\" valign=\"top\">BBn + 3</entry><entry morerows=\"0\" valign=\"top\">BBn + 4</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"3\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>One potential advantage of this embodiment is that the target block (and only that block) of a misprediction recovery may be available one cycle earlier than in the previous embodiment. However, this improved misprediction recovery may come at the expense of greater address loading (and hence tighter timing) in the first access cycle. Differences in how BBSB <b>42</b> and BBC <b>44</b> are indexed may affect various details of operation.</p><p>Other potential advantages may be evident in different embodiments. For example, the pipeline for this embodiment of microprocessor <b>10</b> may possibly be smaller than that of other microprocessors. In one embodiment, the main instruction fetch pipeline of microprocessor <b>10</b> may comprise the following stages: (1) BBSB <b>42</b> lookup; (2) BBC <b>44</b> lookup; (3) reorder buffer lookup and allocation; (4) execution; (5) write back of results upon result bus <b>38</b>; and (6) retiring instructions. Since the instructions may already be aligned and decoded when they are read from BBC <b>44</b>, additional stages that are typically present in other microprocessors may not be necessary. Furthermore, in the case of a branch misprediction (which may be discovered during the execution stage), the penalty may be reduced if the correct branch target is stored in BBC <b>44</b>.</p><p>Another potential advantage may occur when a branch instruction between two basic blocks is correctly predicted. In this case, the current basic block and the predicted basic block may be fetched in a single clock cycle. As previously noted, in other embodiments, even more basic blocks might be dispatched in parallel (e.g., three or four blocks). The number of dispatched basic blocks may be limited by the size of BBSB <b>42</b> or the number BBC <b>44</b>'s read ports. Another limitation may be the branch prediction accuracy. Beyond a certain limit, the basic blocks at the end of the sequence may have a high likelihood of being mispredicted. Note, however, that the potential advantages discussed herein may depend to a large extent upon the exact implementation of microprocessor <b>10</b>.</p><p>Another embodiment of microprocessor <b>10</b> may be configured to improve the speed of reorder buffer <b>32</b> look ups (i.e., dependency checking). Within each basic block, all register data dependencies are likely to remain constant. Thus, once the dependencies for a particular basic block are determined (e.g., during the initial decoding phase), this information may be stored together with the basic block in BBC <b>44</b>. Upon subsequent fetches of the basic block from BBC <b>44</b>, that dependency information may be used to speedup reorder buffer lookup and allocation.</p><p>Pipeline Stages</p><p>Turning now to FIG. 5, a high-level block diagram of the pipeline stages within one embodiment of microprocessor <b>10</b> (e.g., the embodiment from FIG. 3) is shown. In this embodiment, the first pipeline stage <b>60</b> comprises the operation of BBSB <b>42</b> and basic block fetch logic <b>46</b>. The second pipeline stage comprises the accessing of BBC <b>44</b>. The third pipeline stage comprises the operation of reorder buffer logic <b>32</b>, while the fourth pipeline stage <b>66</b> comprises the operation of functional units <b>24</b>A-N. The upper stages of the pipeline, which are used to load BBSB <b>42</b> and BBC <b>44</b> in case of a cache miss, are not shown in the figure. They will be discussed in a separate section below.</p><p>Turning now to FIG. 6, a basic block tree showing the possible paths from a single basic block are shown. In the figure, blocks BBx are basic blocks and blocks BRx represent their corresponding branches. As can be seen in the figure, BB<b>1</b>-<b>0</b> is the basic block addressed by the lookup tags stored in BBSB <b>42</b>. Thus, the BBSB entry that corresponds to BB<b>1</b>_<b>0</b> contains pointers to all of the basic blocks shown in the figure (i.e., BB<b>2</b>_<b>0</b>, BB<b>2</b>_<b>1</b>, BB<b>3</b>_<b>0</b>, BB<b>3</b>_<b>1</b>, BB<b>3</b>_<b>2</b>, and BB<b>3</b>_<b>3</b>). Either BB<b>2</b>_<b>0</b> or BB<b>2</b>_<b>1</b> follows BB<b>1</b>_<b>0</b>. While BB<b>3</b>_<b>0</b> to BB<b>3</b>_<b>3</b> are not required for the current lookup, they are required to lookup the next set of basic blocks. For each branch instruction a predictor is also required in addition to the two possible basic block addresses. The predictor serves to determine the most likely path. Therefore, three predictors are required for the basic block tree in the figure, i.e., one to predict each of BR<b>1</b>_O, BR<b>2</b>_<b>0</b>, and BR<b>2</b>_<b>1</b>.</p><p>BB<b>1</b>_<b>0</b> is the first basic block in the current set of basic blocks. Its address can be sent immediately to BBC <b>44</b> for lookup. The selected BBSB entry then delivers the address of BB<b>2</b>_x, which is then asserted to BBC <b>44</b> for the lookup of the second basic block. The address of BB<b>3</b>_x is used as the lookup address in the next cycle for the next set of basic blocks, like BB<b>1</b>-<b>0</b> in the current cycle. So the lookup for a set of basic blocks uses the basic block address of the starting basic block.</p><p>Turning now to FIG. 7, a table is shown that illustrates one embodiment of the sequence of accesses for basic blocks. In the table, the address \u201cn\u201d represents the start of a new basic block sequence. Column <b>70</b> represents the current clock cycle. Column <b>72</b> represents the input address to BBSB <b>42</b>. Column <b>74</b> represents the address output by BBSB <b>42</b> in a given clock cycle. Column <b>76</b> represents the addresses received at the input ports of BBC <b>44</b>, and column <b>78</b> represents the basic block output by BBC <b>44</b> during the given clock cycle. Note that the timing herein is meant to be exemplary only, and that the timing may change according to specific implementations and the access times of BBSB <b>42</b> and BBC <b>44</b>.</p><p>Basic Block Addressing</p><p>This section describes one possible method for addressing a line within BBSB <b>42</b>. Depending upon the implementation, BBSB <b>42</b> may be addressed with either physical or linear addresses. In some embodiments, the addressing of BBSB <b>42</b> may differ from that of a regular cache. For example, a regular cache may have a fixed line size, e.g., 32 bytes. Thus, the index into the cache addresses 32 byte multiples. Accordingly, the index used to access the cache is the upper portion of the address less the offset. In this way, two sequential lines may each start with an offset of zero, but with an index which differs by one.</p><p>In contrast, each line within BBSB <b>42</b> corresponds to a basic block stored in BBC <b>44</b>. Each line in BBSB <b>42</b> is accessed using the address of the corresponding basic block. However, since each basic block may have a different length (due to the variable-length nature of the x86 instruction set), there is no constant granularity or spacing between the starting addresses of sequential basic blocks. Thus, two factors may complicate the addressing of lines within BBSB <b>42</b>. First, each basic block may not have a fixed byte size. The byte size of a basic block varies with the number of instructions and the length of the instructions. Second, each basic block may not necessarily start with an offset of zero, i.e., a basic block may start at any address.</p><p>Turning now to FIG. 8, an exemplary address scheme for BBSB <b>42</b> is shown. Index field <b>82</b> is used to index into BBSB <b>42</b>. An UP_TAG field <b>80</b> stores a tag comprising the bits between the upper tag limit (UTL) and bit <b>31</b>. A LOW_TAG field <b>84</b> stores a lower tag which is used to compare the offset of a basic block within a given range. The lower tag starts at bit <b>0</b> and continues up to the index limit (IL) bit minus one (i.e., bit IL\u22121). The size of LOW_TAG field <b>84</b> and the associativity within BBSB <b>42</b> depend on the variations of the length of an individual basic block. This is discussed in greater detail below.</p><p>Turning now to FIG. 9, an illustration of a sequence of basic blocks is shown. As the figure illustrates, two sequential basic blocks in physical address space (e.g., BB<b>1</b> and BB<b>3</b>) may each need separate entries within BBSB <b>42</b>. Thus, the minimum distance between two entries in BBSB <b>42</b> is one basic block length. Each basic block's length is determined by the number of instructions in the basic block and their particular length.</p><p>Assuming that the average instruction length is two to three bytes and that the average length of a basic block is three to four instructions, then the average length of a basic block is six to twelve bytes. In order to use a different entry for sequential basic blocks (as shown in the figure), the index may increment the address in eight byte units. Thus, one exemplary size for LOW_TAG field <b>84</b> is three bits.</p><p>Some basic blocks may be longer than the assumed average of eight bytes. This may cause some basic blocks to extend across indexes. Other basic blocks may be shorter than the assumed average of eight bytes. This may result in some basic blocks to have the same index. A number of different configurations of BBSB <b>42</b> may be used to resolve this issue.</p><p>In one embodiment of BBSB <b>42</b>, LOW_TAG field <b>84</b> may be decreased in size (e.g., two bits), thereby providing greater index resolution. However, this may result in inefficiencies as a greater number of indexes may be unused.</p><p>Another embodiment of BBSB <b>42</b> may be configured to be set associative. Thus, in the event of short basic blocks (e.g., shorter than the eight byte length in the example above) with two or more basic blocks sharing the same index, BBSB <b>42</b> may be configured to allocate two different columns (or \u201cways\u201d) within the same set for the two basic blocks. Thus, the two entries will share the same UP_TAG and index, but they will have different LOW_TAG's (which may be used to select the correct column or way). For basic blocks having lengths greater than or equal to the average length, additional \u201csets\u201d may be used for entries with different UP_TAG's. A set (also referred to as a row) comprises all the storage locations into which data with a particular address tag may be stored. For example, in a four-way set associative configuration, data corresponding to a particular tag may be stored in one of four different locations (i.e., one location in each way matches the particular tag). These four locations make up the set or row.</p><p>Turning now to FIG. 10A, a table of sample addresses of a basic block sequence is shown. The sample addresses illustrate a number of short (i.e., two byte) sequential basic blocks. One possible method for storing information about these basic blocks is illustrated in FIG. <b>10</b>B. The figure shows the basic block information stored in a four-way set associative embodiment of BBSB <b>42</b>. As the figure illustrates, the first basic block in the sequence is stored in way <b>0</b>, the second basic block in way <b>1</b>, the third basic block in way <b>2</b>, and the fourth basic block in way <b>3</b>. The set is selected by the index portion of the address of each basic block, in this case <b>00</b>.</p><p>Thus, for both cases a set associative configuration for BBSB <b>43</b> may provide the desired functionality. The number of ways may be determined by the ratio of the average basic block length versus the assumed minimum basic block length. This formula may be represented as follows: (Number of ways)=(Average basic block length)/(Minimum basic block length). In the examples above, an associatively of four ways was used. This would yield a minimum length for basic blocks of two bytes. Any basic blocks shorter than two bytes may result in undesired replacements (discussed in greater detail below). Larger associativity may yield more flexibility, but may also require more hardware. Note that this formula is merely meant to be exemplary and that other ratios for determining the set associativity of BBSB <b>42</b> may be used. Furthermore, as previously noted BBSB <b>42</b> may also function properly in a non-set associative configuration. In addition, while a LOW_TAG size of three bits and four way set-associatively are used throughout the examples herein, other configurations are also contemplated.</p><p>Basic Block Sequence Buffer (BBSB) Line Structure</p><p>Each line of BBSB <b>42</b> may contain information about a particular basic block sequence. As previously discussed, a sequence of two basic blocks may result in the storage of six basic block addresses and prediction information for three branches. In one embodiment, BBSB <b>42</b> stores full 32-bit addresses fore each of the basic blocks. Thus an exemplary storage line within BBSB <b>42</b> may contain the fields illustrated in FIG. <b>11</b>.</p><p>In the embodiment shown, fields BB<b>2</b>_ADR and BB<b>3</b>_ADR (<b>90</b>) store the two possible addresses for the second basic block (i.e., a taken address and a not-taken address). Fields BB<b>2</b>_<b>1</b>_ADR through BB<b>3</b>_<b>2</b>_ADR (<b>92</b>) store the four possible addresses for the first basic block of the next clock. Fields PBR<b>1</b> through PBR<b>3</b> (<b>94</b>) store the predictors for the 3 branches. A status field (<b>96</b>) stores line validation information and a number of bits (e.g., one for each of the six basic block address fields) to determine which stored basic block addresses are valid. A replacement field (<b>98</b>) stores least-recently used (\u201cLRU\u201d) or pseudo LRU bits to determine which way within a line should be replaced first. Note that other replacement algorithms may also be used. In addition, note that the fields listed are for explanatory purposes and that other combinations of fields may be used. For example, the number of stored basic block addresses may be increased.</p><p>Operation of BBSB</p><p>The index bits of the fetch address are used to index into BBSB <b>42</b>. Of the set matching the fetch address' index field, a storage location (or line) within the set will be a valid match only if both the upper and lower tag bits match the address. If the desired line is not stored in BBSB <b>42</b>, then the requested instructions may be fetched from instruction cache <b>16</b> and decoded by decode unit <b>20</b>. The fetch address may also be immediately forwarded to BBC <b>44</b>, because the fetch address may be used to access the first basic block (as previously described in FIGS. <b>3</b> and <b>4</b>).</p><p>If BBSB <b>42</b> contains the desired entry, then the entry provides the information illustrated in FIG. <b>11</b>. The three predictors (field <b>94</b>) may now be used to select the addresses of the two following basic blocks (BB<b>2</b>-BB<b>3</b> and BB<b>2</b>_<b>1</b>-BB<b>3</b>_<b>2</b>). The address of the first basic block (BB<b>2</b> or BB<b>3</b>) is sent to BBC <b>44</b> to fetch the second basic block. The address of the third basic block (either BB<b>2</b>-<b>1</b>, BB<b>2</b>-<b>2</b>, BB<b>3</b>-<b>1</b>, or BB<b>3</b>-<b>2</b>) is used to fetch a new basic block in the next cycle. Thus, the output for the third basic block address is used as the next block fetch address in the next cycle. Replacement information (e.g., field <b>98</b>) may also be updated during that process.</p><p>One embodiment of BBSB <b>42</b> configured to perform in this manner is shown in FIG. <b>12</b>. In this embodiment, BBSB <b>42</b> comprises selection logic <b>100</b> and two multiplexers <b>102</b>-<b>104</b>. Selection logic <b>100</b> is configured to cause multiplexer <b>102</b> to select one of the predicted basic block addresses from field <b>92</b> based upon the prediction information stored in field <b>94</b>. This address may be routed back to one of BBSB <b>42</b>'s inputs for use as a fetch address during the next cycle. Selection logic <b>100</b> is also configured to cause multiplexer <b>104</b> to select one of the basic block addresses from field <b>90</b>.</p><p>In the next clock cycle, the cycle is repeated using the basic block address from multiplexer <b>104</b> as the fetch address. This sequence continues until one of the following conditions is met: (1) a miss occurs in BBSB <b>42</b>, (2) a basic block branch misprediction occurs, or (3) a miss occurs in BBC <b>44</b>. Potential responses for each of these cases are discussed further below.</p><p>Basic Branch Prediction</p><p>The prediction mechanism used by BBSB <b>42</b> is not limited to any particular algorithm and may be implementation dependent. However, in some embodiments the basic branch prediction used in BBSB <b>42</b> is a \u201cglobal\u201d prediction method. This means that the same branch might occur in different sequences or \u201cruns\u201d within different entries in BBSB <b>42</b>. However, the predictor is only updated for the corresponding basic block sequence. Thus multiple predictor values may exist for a particular branch instruction. Another embodiment of BBSB <b>42</b> that is capable of \u201clocal\u201d prediction will be described further below.</p><p>Basic Block Cache</p><p>In one embodiment, BBC <b>44</b> contains instructions which are already aligned and decoded for functional units <b>24</b> A-N. As previously noted, in one embodiment BBC <b>44</b> is configured to store basic blocks that contain up to four instructions each. BBC may be configured to have cache lines that have lengths equal to the maximum basic block length. As noted in connection with FIGS. 3 and 4, BBC <b>44</b> may be configured with multiple read ports to allow multiple basic blocks to be looked up in parallel. The number of instructions output per clock cycle may vary according to the number of instructions per basic block and the number of read ports. For example, in one embodiment BBC <b>44</b> may be configured to output two basic blocks per clock cycle with each basic block having up to four instructions. Thus, in this embodiment BBC <b>44</b> may dispatch up to eight instructions per clock cycle.</p><p>Each of the eight instructions may be assigned to a specific functional unit <b>24</b>A-N. In one embodiment of microprocessor <b>10</b>, there are eight symmetrical functional units, thus no additional multiplexing may be needed. An example of this configuration is shown in FIG. <b>13</b>. As the figure illustrates, in this embodiment the first instruction in the basic block output by BBC <b>44</b>'s first read port is always routed directly to functional unit <b>24</b>A.</p><p>In some embodiments, the organization (and addressing scheme) of BBC <b>44</b> may be identical to the organization of BBSB <b>42</b>. This may be advantageous because both BBSB <b>42</b> and BBC <b>44</b> work with basic blocks as entities. Therefore, the same design optimizations may benefit both structures. For example, both BBSB <b>42</b> and BBC <b>44</b> may be configured as four-way set associative structures. In another embodiment, both BBSB <b>42</b> and BBC <b>44</b> may be organized as fully associative caches.</p><p>Structure of Basic Blocks</p><p>In one embodiment, the basic blocks stored within BBC <b>44</b> are limited to no more than four decoded instructions. The basic blocks may be formed by decode unit <b>20</b> from decoded instructions that are sent to multiplexer <b>40</b>. If a basic block is less than four instructions long, it may be padded with \u201cNULL\u201d instructions until it is four instructions long. NULL instructions are instructions which cause no operations to be performed. Thus, NULL instructions are similar to standard NOP (no operation) instructions, which may, however, increment the PC (program counter). Basic blocks that are longer than four instructions may be broken into two or more basic blocks, each having a length of four decoded instructions. The first basic block in the sequence is simply linked to the next basic block in the sequence through the corresponding prediction information stored in BBSB <b>42</b>. This is described in greater detail below.</p><p>BBC Line Structure</p><p>Depending upon the implementation, each cache line within BBC <b>44</b> may have a number of different configurations. In one embodiment, each cache line may contain slots for four instructions. Space may also be allocated for other information, e.g., valid bits for each instruction and a replacement field for storing replacement information.</p><p>Turning now to FIG. 14, a table detailing one embodiment of a cache line within BBC <b>44</b> is shown. Fields INS<b>1</b>-INS<b>4</b> (<b>110</b>) each store one aligned instruction. The maximum width of the instructions may vary across different implementations. INS fields <b>110</b> may contain normal instructions, partially decoded or predecoded instructions, or even fully decoded instructions. Additional information, e.g., dependency information, may be stored to speed up the operation of reorder buffer <b>32</b>.</p><p>I_Valid field <b>112</b> may store an indication as the whether the associated instruction is valid or invalid. If the instruction is valid, reorder buffer <b>32</b> dispatches the instruction. Otherwise a NULL instruction is generated for that slot. If all I_Valid bits are zero, then the line is not valid. Replacement field <b>114</b> may store LRU or Pseudo LRU (other replacement algorithms may also be used) information to determine which way within a line should be replaced first.</p><p>As previously noted, BBC <b>44</b> may have multiple read ports. Thus, each instruction may have more than one possible destination functional unit, depending on which port the instruction is output through. Note, in other embodiments functional units <b>24</b>A-N may have reservations stations <b>22</b>A-N configured to store multiple instructions pending execution. In still other embodiments, a \u201ccentral window\u201d might be used for storing the instructions pending execution. Using a central window the instructions may be routed to specialized functional units that may not be symmetrical. In still other embodiments, reorder buffer <b>32</b> may route instructions according their functional requirements, e.g., load and store instructions to load/store unit <b>26</b>.</p><p>BBC Operation</p><p>Every clock cycle, two parallel lookups (assuming BBC <b>44</b> has dual read ports) are executed for two different basic blocks. For each port, the lookup process may be the same. When BBC <b>44</b> is accessed, the index bits from the fetch address are used to index into BBC <b>44</b>. A line within the set selected by the index is only considered a valid match if both the upper and lower tag bits match the fetch address. If the requested line is not in BBC <b>44</b>, then the requested instructions must be fetched from instruction cache <b>16</b> and decoded.</p><p>If BBC <b>44</b> contains the requested line, then the basic block is sent to reorder buffer <b>32</b>, along with the line's associated valid bits (field <b>112</b>). Reorder buffer <b>32</b> may be configured to ignore invalid instructions from BBC <b>44</b>. Instead, the invalid instructions may be converted into NULL instructions. This may occur, for example, if a basic block has fewer than the predetermined maximum number of instructions per basic block. This case is discussed in further detail below.</p><p>Short and Long Basic Blocks</p><p>There are two special cases for basic storage. The first is when a BBC entry is only partially used (i.e., a \u201cshort basic block\u201d) and another, where the basic block uses multiple BBC entries (i.e., a \u201clong basic block\u201d).</p><p>As mentioned above, each instruction from BBC <b>44</b> is marked either as valid or invalid. This marking may serve as an indicator for short basic blocks. A short basic block (three instructions in length) is shown below:</p><p>1 INS<b>1</b></p><p>2 INS<b>2</b></p><p>3 JMP xxx</p><p>Turning now to FIG. 15, an exemplary sequence of instructions from which the above short block was taken is illustrated. The third instruction is a jump instruction and therefore marks the end of the basic block. Since there is not a valid fourth instruction, the fourth instruction storage location associated with the basic block is marked as invalid. As noted above, if reorder buffer <b>32</b> is configured to perform fixed assignment of instructions to functional units <b>24</b>A-N (see FIG. <b>13</b>), then one or more of functional units <b>24</b>A-N may execute NULL instructions (i.e., effectively sit idle). Thus, each time the following equation is true, the functional units may not be completely utilized: (number of instructions in basic block) mod 4 !=0. Thus, there may be a tradeoff between efficiency and faster design (i.e., higher clock rates versus less pipeline stages) due to the fixed assignment.</p><p>In the example embodiments described herein, basic blocks are considered to be \u201clong\u201d when they have more than four instructions and therefore span more than one entry. Long basic blocks may be broken into a series of smaller component basic blocks. The component basic blocks are linked by pointers stored in BBSB <b>42</b>.</p><p>Functional Examples</p><p>This section includes examples of several methods for operating BBSB <b>42</b> and BBC <b>44</b> in response to different events. The list of possible scenarios includes: (1) normal operation (i.e., both BBSB <b>42</b> and BBC <b>44</b> hit, with correct branch predictions); (2) a BBSB <b>42</b> miss occurs; (3) a BBC <b>44</b> miss occurs; or (4) a branch misprediction occurs.</p><p>Turning now to FIG. 16, a diagram of the operational pipeline of another embodiment of microprocessor <b>10</b> is shown. As the figure illustrates, after a fetch (block <b>108</b>) instruction cache <b>16</b> conveys instructions to decode unit <b>20</b>. Note multiple clock cycles may be required to complete decoding (block <b>110</b>). The decoded instructions (possibly in basic block form) are then conveyed to BBC <b>44</b> and multiplexer <b>40</b>. Multiplexer <b>40</b> routes the basic blocks to reorder buffer <b>32</b>, which may be configured to perform dependency checking. Reorder buffer <b>32</b> dispatches the instructions to functional units <b>24</b>A-N (or reservation stations <b>22</b>A-N) which execute the instructions and write back the results (block <b>112</b>) to reorder buffer <b>32</b>. Finally, reorder buffer <b>32</b> retires the instructions by writing the results (block <b>114</b>) to register file <b>30</b>.</p><p>At the end of the decode stages (block <b>110</b>), the basic blocks have been identified and properly aligned by decode unit <b>20</b>. Thus, the first basic block starts at instruction position one, and the second basic block starts at instruction position four (assuming fixed basic block lengths of four instructions). From there, the basic blocks go to the reorder buffer <b>32</b> and may also be written into BBC <b>44</b>. Multiplexer <b>40</b> selects the output from either BBC <b>44</b> or decode unit <b>20</b> as an input for reorder buffer <b>32</b>.</p><p>Normal Operation</p><p>During normal operation, the required basic blocks are already stored in BBC <b>44</b>, and the proper sequence information is already stored in the BBSB <b>42</b>. Thus, accesses to BBC <b>44</b> and BBSB <b>42</b> both result in hits. During normal operation the basic block predictions are correct. Due to the pipelined nature of microprocessor <b>10</b>, the operational stages may perform their tasks in parallel. Each stage is outlined below.</p><p>Stage 1\u2014BBSB Lookup</p><p>The speculative program counter (PC) points to the next basic block, thus the PC is directly forwarded to Port <b>1</b> of BBC <b>44</b>. BBSB <b>42</b> is looked up with the PC value and in this case hits in BBSB <b>42</b>. Thus, in the next clock cycle BBSB <b>42</b> may output the addresses of the second and third basic blocks. The second basic block address is conveyed to the second read port of BBC <b>44</b>, while the third basic block address is used as the address of the next (speculative) PC. At the end of the clock, the replacement information for BBSB <b>44</b> may also be updated.</p><p>Stage 2\u2014BBC Lookup</p><p>BBC <b>44</b> accesses and attempts to output cache lines corresponding to the two addresses from both ports. Assuming both addresses hit in BBC <b>44</b>, the following information is provided to reorder buffer <b>32</b>: basic block <b>1</b> (instructions 1-4), basic block <b>2</b> (instructions 1-4), the valid bits for each instruction in each basic block, and possibly additional predecode and reorder buffer (i.e., dependency) information. At the end of the clock cycle, the replacement information within BBC <b>44</b> may also be updated.</p><p>In parallel with the BBC accesses above, the functions in stage 1 execute. Thus, BBSB <b>42</b> is looked up with the predicted address of the third basic block from above.</p><p>Stage 3\u2014Reorder Buffer (ROB) Operation</p><p>In this stage, reorder buffer <b>32</b> receives and processes the eight instructions from basic blocks <b>1</b> and <b>2</b>. In one embodiment, all reorder buffer operations (e.g., allocating an entry, dependency checking, etc.) may be performed in that clock cycle. As previously noted, the valid bits mark which instructions are valid. Valid instructions are processed normally, while others are converted to NULL instructions.</p><p>Stage 4\u2014Execute</p><p>In this stage, a set of instructions may be dispatched from reorder buffer <b>32</b> to functional units <b>24</b>A-C. In one embodiment, each reorder buffer slot may have a corresponding functional unit (see FIG. <b>13</b>). Thus, no multiplexing or busses may be needed. If reservation stations <b>22</b>A-N are empty, then the instructions may begin (and possibly complete) execution in that stage. In this stage, branch information may be routed back to BBSB <b>44</b> and branch prediction unit <b>14</b> to update the corresponding prediction information.</p><p>Stage 5\u2014Writeback</p><p>In this stage, the results are put on result bus <b>38</b> (see FIG. 1) so that reorder buffer <b>32</b> may store them.</p><p>Stage 6\u2014Retire</p><p>In this stage, reorder buffer <b>32</b> retires instructions by copying their results into register file <b>30</b>. This updates the architectural (non-speculative) state of microprocessor <b>10</b>. Note, the stages outlined above illustrate only one of many possible methods of operation for BBC <b>44</b>. Further note the above sequence assumes that all cache accesses hit and that BBSB <b>42</b> correctly predicted the next basic block sequence.</p><p>BBSB Miss</p><p>If instead an access to BBSB <b>42</b> misses, a recovery process may be used. One embodiment of such a recovery process is described below.</p><p>Initially, the speculative program counter (PC) points to the next basic block. The PC is directly forwarded to Port <b>1</b> of BBC <b>44</b>. Next, BBSB <b>42</b> is indexed with the PC value. In one embodiment, BBSB <b>42</b> is loaded with the required information as outlined in the following paragraphs:</p><p>(a) Level one instruction cache <b>16</b> is indexed using the PC. Assuming instruction cache <b>16</b> generates a hit, the instructions may be fed directly to decode unit <b>20</b>.</p><p>(b) As previously noted, BBSB <b>42</b> and BBC <b>44</b> may be configured to handle two basic blocks at once. In this embodiment, up to eight instructions may be decoded at once. It is unclear at the fetch stage whether the two basic blocks actually span eight instructions or less. Furthermore, it is also unclear whether the second basic block is sequential to the first basic block (due to the absence of an accurate predictor). Therefore, it is assumed that the two basic blocks are sequential. In parallel with the fetch, the instructions step through the align and decode stages (in decode unit <b>20</b>).</p><p>(c) At the end of the decoder pipe, the basic blocks have been identified and properly aligned. Thus, the first basic block starts at the first instruction position and the second basic block starts at the fifth instruction position. Decode unit <b>20</b> may also provide information indicating whether or not the basic blocks are in fact sequential. Note, the second basic block may be highly speculative in nature. Once aligned and decoded, the two basic blocks may be handled similar to the normal operation case. For example, the blocks may be conveyed to reorder buffer <b>32</b>. From there, the blocks may step through the final parts of the pipeline, i.e., execution in functional units <b>24</b>A-N, write back <b>112</b>, and then retirement <b>114</b>. In addition, the two basic blocks may also be stored into BBC <b>44</b>. The instruction valid bits and replacement information may also be stored/updated.</p><p>(d) In parallel, an entry in BBSB <b>42</b> is generated. The entry is addressed with the address of the first basic block. Other information is speculatively filled in for the second basic block and following basic block or blocks, depending upon the implementation. Once again, the replacement info may also be updated.</p><p>(e) In one embodiment, fetch logic <b>46</b>, coupled to decode unit <b>20</b>, BBSB <b>42</b>, and BBC <b>44</b>, may determine whether the first and second basic blocks are sequential (by monitoring decode unit <b>20</b>'s output). If they are sequential, then fetch logic <b>46</b> may predict the third basic block and look up the third basic block in BBSB <b>42</b>. Once again, it is assumed that a hit occurs and then the sequence starts with the regular operation sequence.</p><p>Alternatively, if the two basic blocks are not sequential, then fetch logic <b>46</b> may wait for the outcome of the first branch. It may then take the resulting address and access BBC <b>44</b> with it. If the basic block is in BBC <b>44</b>, its four instructions are sent to reorder buffer <b>32</b>. If the basic block is not in BBC <b>44</b>, instruction cache <b>16</b> is once again accessed. This time, however, the access is only for four instructions, which are decoded and handled as described above. Once decoded, the instructions are sent to reorder buffer <b>32</b> and are also written into BBC <b>44</b>. Thus, for both cases the entries in BBSB <b>42</b> are updated to reflect the outcome of the branch. Note, however, that any mispredicted instructions may also need to be cleared from reorder buffer <b>32</b>.</p><p>(f) The predictor for the third basic block is now used to access BBSB <b>42</b> again. Here it is once again assumed that a hit occurs and that the sequence starts with the normal operation sequence.</p><p>BBC Miss</p><p>In the event that an access to BBC <b>44</b> results in a miss, the access to BBSB <b>42</b> may still hit (if not, the situation described above may apply). One possible method for recovering from a BBC miss is described in detail in the following paragraphs, which assume that a miss occurs for at least one lookup address in BBC <b>44</b>.</p><p>When a miss occurs, instruction cache <b>16</b> may be looked up with the missed address from BBSB <b>42</b>. The missed address may be either from the first basic block or the second basic block. If both basic blocks miss, then the first basic block may be fetched first. Assuming the missing address hits in instruction cache <b>16</b>, the instructions may be directly fed to decode unit <b>20</b>.</p><p>If the first basic block was a hit and the second basic block address missed, then the first basic block may be immediately conveyed from BBC <b>44</b> to reorder buffer <b>32</b>. From there, the first basic block steps through the final parts of the pipeline (i.e., execution, write back, and retire). The second basic block may follow one pipeline stage behind the first basic block through the execution pipeline.</p><p>Regardless of which basic block missed, the decoded basic block from decode unit <b>20</b> is immediately conveyed to reorder buffer <b>32</b>. From there, the instructions step through the final parts of the pipeline (i.e., execution, write back, and retire). In addition, the missing basic block(s) may also be stored into BBC <b>44</b>. This may include the instruction valid bits. In addition, the replacement information may also be updated.</p><p>In one embodiment, self-modifying code may be prohibited to allow different handling of BBSB and BBC misses. If self-modifying code is allowed, another embodiment of microprocessor <b>10</b> is configured to generate a BBSB miss whenever a BBC miss occurs, thereby re-synchronizing basic block boundaries after the code modification. This is discussed in greater detail further below.</p><p>Basic Block Mispredictions</p><p>Basic block mispredictions may be detected by functional units <b>24</b>A-N and branch prediction unit <b>14</b> during the execution stage. This is done by comparing the calculated branch address with the predicted branch address and examining the branch direction. If the two addresses are not identical, there may be two reasons. First, the branch direction might have been mispredicted (i.e., the predictor stored in BBSB <b>42</b> predicted the wrong branch direction). Second, the predicted branch target address might have been wrong. This could occur as a result of a change in the address of a taken branch. In this case the branch direction prediction was correct, but the taken branch address changed.</p><p>Thus, there are three different causes for basic block mispredictions: (1) a branch is mispredicted as taken, (2) a branch is mispredicted as not taken, and (3) the branch was correctly predicted as taken but the branch target address was wrong. Possible methods to recover from each of these basic block mispredictions are highlighted below. Since branch prediction unit <b>12</b> and functional units <b>24</b>A-N may detect that the branch was mispredicted during the execution stage, the recovery steps may begin in the next clock. Note that for explanatory purposes the following examples assume that all accesses to BBSB <b>42</b> and BBC <b>44</b> hit. Otherwise, the recovery actions discussed above may be performed in addition to the basic block misprediction recovery.</p><p>Misprediction Recovery</p><p>Functional units <b>24</b>A-C send the correct address together with the correct branch direction to BBSB <b>42</b>. BBSB <b>42</b>'s fetch logic <b>46</b> may updates BBSB <b>42</b>'s entries as follow First, the predictor field <b>94</b> may be updated with the branch direction information. Second, the basic block address field <b>90</b> (i.e., the taken or not taken branch address field) is updated with the branch target address.</p><p>This may be performed for all cases of wrong target addresses, as well as the initial address load when the basic block address field is empty in the BBSB entry, which may occur after a BBSB entry is first generated. However, this may not need to be performed if only the branch direction is mispredicted (but the target address is correct). However, in some embodiments BBSB <b>42</b> may be configured to perform the write because it would merely over write the previous (correct) address with a copy of itself.</p><p>Next, a determination may be made as to whether the mispredicted basic block was the second basic block in a BBSB entry or the starting basic block of the next BBSB entry. If it was the second basic block, then the BBC is looked up with the new address of the basic block. Assuming a hit occurs, at the end of the clock cycle the basic block will be available to reorder buffer <b>32</b>. From there, the instructions step through the final parts of the pipeline. Note that in this case only four instructions (1 basic block) are dispatched. The other instructions are NULL instructions. Next, the predictor for the third basic block is used to access BBSB <b>44</b> again. Instead, if it was the starting basic block of the next BBSB entry, then BBSB <b>42</b> is looked up using the new address.</p><p>Advantageously, in this embodiment the misprediction latency may be small. If the accesses hit in BBSB <b>42</b> and BBC <b>44</b>, then an exemplary latency may be four clock cycles as shown in the table in FIG. <b>17</b>. In the figure, BBSB<b>1</b> creates the misprediction, and BBSBC is the correct BBSB entry. Thus, the upper pipeline stages do not take part in the misprediction recovery if all accesses hit in BBSB <b>42</b> and BBC <b>44</b>.</p><p>Prediction Information</p><p>In one embodiment, BBSB <b>42</b> may be configured to send the following information about the basic block sequence to branch prediction unit <b>14</b>: (1) the predicted address of next basic block, (2) the pointer to the BBSB entry, and (3) an identifier of the branch path with the basic block tree (e.g., BB<b>1</b>_<b>0</b>, BB<b>2</b>_<b>0</b>, BB<b>2</b>_<b>1</b>).</p><p>Branch prediction unit <b>14</b> may use the predicted address of the next basic block during execution to determine whether the branch was correctly predicted or mispredicted. If the branch was correctly predicted, then branch unit <b>14</b> may send that information to BBSB <b>42</b> together with the other information listed above (i.e., items 2 and 3). If, on the other hand, the branch was mispredicted, then branch prediction unit <b>14</b> may send that information to BBSB <b>42</b> together with the other information (i.e., items 2 and 3) and the new branch target address. BBSB <b>42</b> may be configured to take that information and use it to update the stored information of the indicated branch in the indicated BBSB entry. For example, for BR<b>1</b>_<b>0</b> it may send the address of predicted BB<b>2</b>_x, and for BR<b>2</b>_x the address of predicted BB<b>3</b>_x. It may also send the BBSB entry number for BB<b>1</b>_<b>0</b>.</p><p>Coherency</p><p>In one embodiment, microprocessor <b>10</b>, BBSB <b>42</b> and BBC <b>44</b> are fully coherent. This means that changes made to instruction cache <b>16</b> are also reflected in BBSB <b>42</b> and BBC <b>44</b>. While normal code does not alter the contents of the instruction memory space (stored in instruction cache <b>16</b>), self-modifying code does. Self-modifying code is rare, and thus some embodiments of microprocessor <b>10</b> may not be configured to support it and or may not be optimized for it.</p><p>Advantageously, instruction cache <b>16</b> may play a significant role in reducing the invalidation overhead incurred by BBSB <b>42</b> and BBC <b>44</b>. In one embodiment, BBC <b>44</b> and instruction cache <b>16</b> may provide \u201cinclusion\u201d, meaning that every instruction contained in BBC <b>44</b> also is in instruction cache <b>16</b>. Given inclusion, instruction cache <b>16</b> may then filter all relevant invalidation requests to BBC <b>44</b>. Only invalid requests which hit in instruction cache <b>16</b> are sent to BBC <b>44</b>. This may be important in some embodiments because otherwise any invalid request, whether it is contained in BBC <b>44</b> or not, goes to BBC <b>44</b> and BBSB <b>42</b>. The same is true for data space invalidation, because it is not initially known whether the request will hit in BBC <b>44</b>. Thus, if instruction cache <b>16</b> were omitted, there may be a performance penalty due to all the invalidation requests directed to BBC <b>44</b> and BBSB <b>42</b> even though they do not hit in BBC <b>44</b>. Therefore, instruction cache <b>16</b> may be advantageous for performance in certain embodiments.</p><p>Coherency handling may be split into 2 parts. The first part is the invalidation process, while the second part is the resynchronization process. Each part is described in detail below.</p><p>Invalidation Process</p><p>In one embodiment, coherency is achieved by invalidation. Advantageously, using invalidation instead of updating may reduce the need for complex decoding and resynchronization (due to the unpredictable lengths of instructions). Furthermore, invalidation may in some embodiments be limited to BBC entries (i.e., not invalidating the BBSB entries).</p><p>For example, in one embodiment, if a basic block is the second basic block in a sequence, then the basic block address may not be easily looked up in BBSB <b>42</b>. Only the start address of the first basic block may be looked up. In addition, multiple BBSB entries might point to the same BBC entry for the second basic block. Therefore, it may be difficult to ensure that all BBSB entries are invalidated for a given basic block address.</p><p>Thus, in one embodiment, only the BBC entry is invalidated.</p><p>To reduce the amount of hardware used by this embodiment, a conservative approach may be used that invalidates more instructions than may be necessary. One such conservative embodiment, is described below.</p><p>Each time a write to instruction cache <b>16</b> occurs, the associated address is passed to the fetch logic <b>46</b> of BBSB <b>42</b> and BBC <b>44</b> for use as an invalidation address (INV_ADR). However, since the basic block starting addresses may not coincide with the invalidation address, an invalidation window may be used instead of just the single invalidation address. The invalidation window may have the same size as individual lines within instruction cache <b>16</b>. For example, the line size may be 32 bytes long.</p><p>Using the invalidation window, each line in BBC <b>44</b> is invalidated if it is selected with the index and the UP_TAG matches the INV_ADR. The LOW_TAG field is not considered. Thus, for invalidation the basic blocks start at an address where the lowest three address bits (A<b>2</b>-A<b>0</b>) are zero. For example, for a particular INV_ADR all instructions in the range from INV_ADR up to INV_ADR+31 may be invalidated.</p><p>In one embodiment, invalidation may cover two cases. The first is when the INV_ADR is less than or equal to the basic block starting address. In this case the invalidation window covers the basic block. The second case is when the INV_ADR is greater than the basic block starting address. In this case, the basic block runs into the invalidation window.</p><p>Turning now to FIG. 18, a graphical representation of these two cases is shown. In the figure, basic block BB<b>1</b> illustrates case 1, while basic block BB<b>2</b> illustrates case 2. Each case is discussed separately below.</p><p>Case 1: INV_ADR&lt;=Basic Block Starting Address</p><p>Assuming the invalidation window spans 32 bytes, and BBC <b>44</b> has a step size of eight bytes, then four BBC lines (with indexes 0-3) may be covered by the invalidation window. To invalidate all possible combinations of four lines, the INV_ADR is divided into sections.</p><p>An example of this division is illustrated in FIG. <b>19</b>. As the figure illustrates, bits <b>31</b>-<b>5</b> may be taken from the INV_ADR, while bits <b>4</b>-<b>3</b> may represent the index which is incremented from zero to three, and bits <b>2</b>-<b>0</b> may be zero. Using these sections, fetch logic <b>46</b> may execute four different invalidations. Bit <b>31</b>-<b>5</b> and <b>2</b>-<b>0</b> of the invalidation addresses may remain constant while the index is incremented from zero to three. Thus, a line is invalidated if the index selects the line and the UP_TAG matches part of UP_INV_ADR. As mentioned before, the LOW_TAG field need not be used for the comparison. This allows invalidation of basic blocks having LOW_TAG's not equal to zero.</p><p>This process also provides for invalidation for short basic blocks (i.e., those less than eight bytes long). If a basic block is shorter than eight bytes, then the next sequential basic block may possibly have the same index and UP_TAG and a different LOW_TAG (i.e., sharing the same set/index but in a different way/column). Advantageously, the process described above may be used to invalidate all four ways with a given index and UP_TAG, thus addressing the concern of short basic blocks.</p><p>Case 2: INV-ADR&gt;Basic Block Starting Address</p><p>In the event that the basic block starts at an address below the INV_ADR, it may extend into the invalidation window. To address this possibility, these instructions may also be invalidated. In one embodiment this is accomplished by subtracting the maximum basic block length from the INV_ADR as indicated by the following equation: (INV_ADR_LOW)=(INV-ADR)\u2212(Maximum Basic Block Length). The generation of INV_ADR_LOW is illustrated in FIG. <b>20</b>. INV_ADR_LOW may be used instead of INV_ADR to invalidate the BBC entries. This process may be performed in a similar fashion to that of case one above, except for using INV_ADR_LOW instead of INV_ADR.</p><p>Assuming for explanatory purposes that the maximum basic block length 60 bytes long (i.e., 4\u00d7maximum instruction length=4\u00d715 byte=60 bytes), using an index resolution of eight bytes would result in eight invalidation cycles. This may, however, be reduced if the maximum basic block size is limited to a smaller size, e.g., 32 bytes. The process of limiting the maximum basic block length is described below. Assuming an embodiment of microprocessor <b>10</b> has limited the maximum basic block size length to 32 bytes, then the invalidation process may be reduced to only 4 invalidation cycles (index 0-3).</p><p>Limiting the Maximum Basic Block Length</p><p>As previously noted, in some embodiment, microprocessor <b>10</b> may limit the size of basic blocks. This limiting may be performed when BBC <b>44</b> is filled, or during the instruction decode and basic block formation process. For example, decode unit <b>20</b> may be configured to determine the size of the basic block during decoding. If a particular instruction would exceed the maximum basic block length (e.g., 32 bytes), then it is put into the next sequential basic block and the first block is marked to be followed by a sequential block. The empty instruction slot in the first basic block may be filled with a NULL instruction as shown in FIG. <b>21</b>.</p><p>As the figure illustrates, INS<b>4</b> would have exceeded the maximum basic block length of 32 bytes. Therefore, it was placed into BB<b>2</b> and the I<b>4</b> slot was filled with a NULL instruction. This mechanism may be used to limit the maximum basic block length to any particular length. The performance may be somewhat reduced because one instruction slot is not filled with a useful instruction. However, the performance impact may be minimal because a basic block length of more than 32 bytes may be quite rare, depending upon the instruction set and program characteristics.</p><p>Invalidation Summary</p><p>As the above embodiments indicate, invalidations may deleteriously affect performance if they occur too frequently. Some embodiments may, however, elect to invalidate more instructions than necessary in order to simply the implementation. This is true for instructions within a basic block that begins in the invalidation window but extends outside the invalidation window. In this case the end instruction may be unnecessarily invalidated. In some cases, entire basic blocks may be invalidated unnecessarily. This may be the case with a basic block that is smaller than the maximum basic block size. This is illustrated in FIG. <b>22</b>. If the block starts after INV_ADR_LOW but ends before INV_ADR, then it may be unnecessarily invalidated.</p><p>However, even with unnecessary invalidations, cache coherency is still maintained. Thus, no false instructions are processed. The only potential negative effect may be on performance because additional unnecessary BBC misses may occur. In one embodiment, microprocessor <b>10</b> may be configured to detect and avoid these unnecessary invalidations. For example, microprocessor <b>10</b> may be configured with invalidation hardware that uses the basic block length information to generate the ending addresses of the basic blocks. Then bounds checking hardware may be used to determine whether the basic block needs to be invalidated or not. However, this more complicated hardware may not be necessary given the relatively low frequency in which such unnecessary invalidations may occur in many implementations.</p><p>Resynchronization Process</p><p>The resynchronization process may take place after BBC entries have been invalidated and an invalidated BBC entry is requested. In some embodiments, this request may occur if a BBSB miss is executed or a valid BBSB still points to the invalidated BBC entry. As noted above, the BBSB entries may not be invalidated. A BBSB miss may occur if information in BBSB <b>42</b> has been replaced after the invalidation but before the request. In that case, a BBSB miss sequence is executed, which correctly refills, aligns, and decodes all instructions into BBSB <b>42</b> and BBC <b>44</b>.</p><p>The second case may occur when one or more BBC entries (basic blocks) are invalidated, but one or more BBSB entries still contain pointers to these entries. Turning now to FIG. 23, an example of what may occur during the code modifications in instruction cache <b>16</b> is shown. The figure illustrates one worst case scenario that involves changing the number of instructions for a particular address window.</p><p>The example shows a JMP instruction that has been modified into two other instructions. Thus, INS<b>21</b> is now sequential to INS<b>15</b>, whereas before the modification the JMP instruction branched to a different location. This also affects the block boundaries of BB<b>2</b> because BB<b>1</b> may only hold four instructions in this embodiment. In a different example, the opposite result may occur, wherein INS<b>14</b> and INS<b>15</b> are transformed into a JMP instruction. Thus, that after a BBC entry has been modified, the following basic blocks may need to be rebuilt.</p><p>To address this, BBC <b>44</b> may be configured so that BBC <b>44</b> misses result in a BBSB miss sequence to rebuild all the of the basic blocks. If only one BBC entry is a miss, this will result in the BBSB miss sequence previously described. The miss sequence may be repeated for multiple basic blocks depending upon the code changes. It may stop when the second basic blocks in a BBSB entry are once again valid in BBC <b>44</b>. Thus, the sequence is identical to starting from scratch until valid BBC entries are reached. After that, the BBC entries will have been correctly rebuilt.</p><p>After the above BBSB miss sequence has been executed, there may still be one additional case to consider. Assuming that all the BBC entries have been filled with valid basic block information, other BBSB entries may point to the changed BBC entries as their second or third basic block. If an access to the entry that points to a changed entry results in a BBC miss, it will trigger the BBSB miss process described above that will re-synchronize the changed entries.</p><p>However, in some cases the second basic block may have changed length. If the second basic block is marked as having a sequential following basic block and no branch is contained in the new basic block, then the pointer to the third basic block may still point to the old third basic block position. This may result in the wrong basic block being loaded. This situation is illustrated in FIG. <b>24</b>.</p><p>To avoid this potential problem, the next predicted sequential basic block address (\u201cpred_next_basic_block\u201d) may be compared with the calculated next basic block address (\u201ccalc_next_basic_block\u201d), which is defined as follows: (calc next_basic block)=(start_act_basic block)+(basic_block_length). If the predicted next basic block address is not equal to the calculated next basic block address, then the mispredicted sequence must be executed. In one embodiment, this comparison may be performed in functional units <b>24</b>A-N or branch prediction unit <b>14</b> in a similar manner to the comparisons performed for checking branch predictions.</p><p>Note that in some cases this may affect reorder buffer <b>32</b>. For example, the instructions from the old BB<b>3</b> may be speculatively executed. The comparison may show that the target address for the sequential feich was wrong. Thus, the instructions from BB<b>3</b> in reorder buffer <b>32</b> may need to be invalidated. In the example above, however, the final instructions from BB<b>2</b> may have higher addresses than the first instruction from the old BB<b>3</b>.</p><p>Note, that in the examples above, BBSB entries do not depend on the contents of BBC <b>44</b>. If a basic block contains different instructions or a different number of instructions (but not more than four instructions), then the BBSB entries are not changed until the BBC changes create different start or end addresses for the basic blocks. Thus, BBSB <b>42</b> simply provides the pointers to the basic blocks. In this case, the changes to the contents of the basic blocks in BBC <b>44</b> do not affect the pointers within BBSB <b>42</b>. Thus, BBSB may need to be updated only if the pointers need to be changed. This relationship is illustrated in FIG. <b>25</b>.</p><p>Overlapping Detection for Long Basic Blocks</p><p>In some embodiments of microprocessor <b>10</b>, overlapping detection for long basic blocks may be implemented to improve performance in certain cases. As previously noted, a long basic block is a long series of sequential instructions. For example, if fifty instructions are sequentially executed without any interceding control instructions (e.g., JMPs), then the sequence will be stored as a long basic block comprising a number of individual basic blocks linked by pointers stored in BBSB <b>42</b>.</p><p>Difficulties may arise, however, if individual basic blocks within the long basic block are used by different runs through the basic block sequence. An example of this is illustrated in the code sequence of FIG. <b>26</b>. In this code sequence, the JMP instructions jumps into different parts of the basic block depending upon the variable. The sequence always ends at the end of the basic block. This mechanism is sometimes used in graphics applications to avoid loop control code. In such code, the three instructions of each label (i.e., L<b>1</b>, L<b>2</b>, L<b>3</b>, L<b>4</b>) are identical and represent the loop code. The starting point is then determined by how often the loop should execute. For example, if the loop should execute twice, then the JMP instruction would target L<b>3</b>. There are also other applications for this type of code.</p><p>The code in the figure may create problems for the embodiments of microprocessor <b>10</b> as described above. In particular, assuming that the first run through the code would start at L<b>1</b>, then all instructions from INS<b>1</b> up to INS<b>43</b> will be stored in BBSB <b>42</b> and BBC <b>44</b> as described above. In one embodiment, the instructions will be stored in BBC <b>44</b> as illustrated in FIG. <b>27</b>. In BBSB <b>42</b>, the basic blocks X through X+2 are marked as sequential basic blocks.</p><p>However, the next time the processor runs through the long basic block it could possibly start at L<b>2</b>. So the address of L<b>2</b> is looked up in BBSB <b>42</b>. No hit occurs because L<b>2</b> is not a starting address of a basic block and because INS<b>21</b> is in slot <b>14</b> of the basic block X and therefore cannot be looked up. This generates a BBSB miss sequence and BBSB <b>42</b> and BBC <b>44</b> are filled from instruction cache <b>16</b> as previously described. Thus, in addition to the previous entries, the basic blocks are once again stored in BBC <b>44</b> and BBSB <b>42</b> with a different alignment. In one embodiment, the instructions may be put in BBC <b>44</b> as illustrated in FIG. <b>28</b>.</p><p>As a result, the same instructions (I<b>21</b>-I<b>43</b>) are stored twice in BBC <b>44</b> and BBSB <b>42</b> with different basic block alignments. This may be advantageous because it allows BBC <b>44</b> to send the instructions straight to the functional execution slots. This so called \u201coverlapping\u201d may continue until a basic block of the old sequence starts at the same address as a basic block of the new sequence. Thus, when multiple different paths through the same long basic block are executed, BBSB <b>44</b> and BBC <b>42</b> may become polluted with different basic block alignments of the same instructions. This may reduce the efficiency of BBSB <b>44</b> and BBC <b>42</b>.</p><p>Note, however, that the behavior described above may rarely occur in normal code. In normal code, branches occur quite frequently (e.g., every 4-6 instructions). The branches mark the ends of each basic block (after which the next basic block starts). Thus, the frequent branch instructions provide a re-synchronization function for the basic block alignment. So even if some code jumps directly into the middle of another basic block, a new basic block will most likely be fetched after a couple of instructions. Then the second run will access a basic block which was generated by the previous run. So in these cases the overlap may only be for a small number of instructions and will probably only result in two different entry points. Thus, the impact of overlapping is small. However, for long runs (e.g., 30 instructions or more) with multiple entry points the scenario is different.</p><p>One embodiment of microprocessor <b>10</b> configured to address this potential concern is disclosed below. Note, however, that for the reasons explained above, the following mechanism is optimized for execution speed rather than for maximum storage efficiency. Other embodiments are also contemplated.</p><p>To assist in the understanding of the operation of this embodiment, a particular \u201cworst case\u201d overlapping scenario is illustrated in FIG. <b>29</b>. Assuming worst case start index and instruction lengths, then the basic blocks may appear as in FIG. <b>30</b>. In this example, the basic blocks start with an offset of one instruction. As a worst case, that may translate into two basic block having BBC indexes that differ by one. Thus, if instruction I<b>12</b> is looked up an index of one is used, whereas instruction I<b>11</b> translates to an index of zero. This may continue for the next basic blocks in the sequence because (as the example shows) the basic blocks may exactly span two index sizes. This may be a worst case type of scenario, but it may be possible and may deleteriously affect the performance of microprocessor <b>10</b>.</p><p>This problem is highlighted when instruction I<b>12</b> is looked up for the second run, but basic block BB<b>11</b> is not at the same index. Thus, BBC <b>44</b>'s fetch logic <b>46</b> does not detect the presence of basic block BB<b>11</b> because it is accessed with an index of zero, not one. In one embodiment, microprocessor <b>10</b> may avoid this problem by increasing the size of the BBC lookup performed for a given index value. For example, the lookup process may return as many entries as are necessary to fill a basic block having a maximum length. For this example, the maximum length is assumed to 32 bytes. Thus, the index distance (i.e., the minimum byte length between two sequential indexes) is eight bytes. Therefore, four sequential entries (i.e., those with indexes zero through three) are returned.</p><p>Restated, each BBC <b>44</b> lookup returns four entries having indexes from zero through three. From these four entries, the requested BBC entry may then be selected using a multiplexer. The other BBC entries may be used to perform comparisons to detect overlapping (described in detail below). In this embodiment, BBC has four ways so a total of sixteen BBC lines are returned.</p><p>Turning now to FIG. 31, one embodiment of BBC <b>44</b> capable of outputting multiple lines per index is illustrated. As per the figure, BBC <b>44</b> may be addressed with address bits Ax-A<b>5</b> (whereas x is dependent upon the size of BBC <b>44</b>). After receiving these address bits, BBC <b>44</b> may then output the four corresponding entries, starting with index zero. A<b>4</b> and A<b>3</b> address bits are \u201cdon't care\u201d bits for the BBC lookup. Instead, they are used in multiplexer <b>190</b> to select the desired entry. Comparators <b>182</b>A-D perform comparisons to detect any overlaps.</p><p>Detecting Overlaps at Basic Block Boundaries</p><p>As previously described, overlapping can occur if different runs through the same basic blocks have different footprints. The previous section described a detection method for basic blocks having different alignments at the instruction level. This section describes another case, i.e., where overlapping occurs at the basic block boundary level. This may occurs if a case or switch instruction within a loop skips a basic block. An example of this is shown in FIG. <b>32</b>.</p><p>Turning now to FIG. 33, an example of the sequence entries within BBSB <b>42</b> generated by these different footprints is shown. As the figure illustrates, in the case illustrated above four different BBSB entries are allocated for the four basic blocks. This may be advantageous because only then will the basic blocks be properly aligned so that two basic blocks may be dispatched in parallel. It may also allow the global branch prediction scheme to work more efficiently.</p><p>However, there may be cases where BBSB <b>42</b> becomes overloaded with different footprints of the same basic blocks. In such cases it may be desirable to detect if different footprints are present in BBSB <b>42</b>. The potential difficulty may be with the second basic block. The first basic block may not create a problem because in this embodiment there is only one entry for each basic block.</p><p>To detect the address of the second basic block, a special overlap tag (OTAG) array may be used. When BBSB <b>42</b> outputs the second basic block address, it may be used in the next clock cycle to perform a lookup in the OTAG array. The OTAG array may be configured to store the first basic block tags from BBSB <b>42</b>. This OTAG lookup may indicate whether the second basic block address is also stored as a tag for a first basic block in BBSB <b>42</b>. If this is the case, then overlapping exists.</p><p>In the example in the figure, in cycle <b>3</b> basic block <b>0</b> is the second basic block. The lookup of the OTAG in cycle <b>4</b> would indicate that basic block <b>0</b> already exists in entry <b>0</b> of BBSB <b>42</b>. In one embodiment, a counter may be included to track how often overlaps occur. Using the counter's information, a determination may be made as to whether a re-synchronization should be performed to the basic block already stored in BBSB <b>42</b> or not. This may result in a tradeoff between performance and storage efficiency because a re-synchronization may reduce performance. BBSB <b>42</b> may be configured to ensure that the tags stored in the OTAG array are identical to the tags stored in BBSB <b>42</b>. Advantageously, this method may provide control over basic block overlapping that occurs at the basic block boundary level.</p><p>Still other embodiments are contemplated. For example, one embodiment of microprocessor <b>10</b> may be configured to determine whether an access hits in BBC <b>44</b> even though the access misses in BBSB <b>42</b>. If the access only misses in BBSB <b>42</b>, then recovery time may be reduced by using the instructions within BBC <b>44</b>. Instead, a new entry in BBSB <b>42</b> is allocated. In yet another embodiment, the second more speculative basic block may be stored in an immediate buffer during BBC <b>44</b> fills. This may prevent use of BBC <b>44</b> in the case of a misprediction.</p><p>In other embodiments, the hardware required for BBC <b>44</b> and BBSB <b>42</b> may be reduced in quantity and complexity. For example, BBC <b>44</b> may be configured with a single write port in lieu of dual write ports. While this may affect the number of clock cycles need to store two basic blocks, it may not necessarily impact overall performance to any large extent if a buffer is used to store the second block to be written.</p><p>Example Computer System</p><p>Turning now to FIG. 34, a block diagram of a computer system <b>400</b> including microprocessor <b>10</b> coupled to a variety of system components through a bus bridge <b>402</b> is shown. In the depicted system, a main memory <b>404</b> is coupled to bus bridge <b>402</b> through a memory bus <b>406</b>, and a graphics controller <b>408</b> is coupled to bus bridge <b>402</b> through an AGP bus <b>410</b>. Finally, a plurality of PCI devices <b>412</b>A-<b>412</b>B are coupled to bus bridge <b>402</b> through a PCI bus <b>414</b>. A secondary bus bridge <b>416</b> may further be provided to accommodate an electrical interface to one or more EISA or ISA devices <b>418</b> through an EISA/ISA bus <b>420</b>. Microprocessor <b>10</b> is coupled to bus bridge <b>402</b> through a CPU bus <b>424</b>.</p><p>In addition to providing an interface to an ISA/EISA bus, secondary bus bridge <b>416</b> may further incorporate additional functionality, as desired. For example, in one embodiment, secondary bus bridge <b>416</b> includes a master PCI arbiter (not shown) for arbitrating ownership of PCI bus <b>414</b>. An input/output controller (not shown), either external from or integrated with secondary bus bridge <b>416</b>, may also be included within computer system <b>400</b> to provide operational support for a keyboard and mouse <b>422</b> and for various serial and parallel ports, as desired. An external cache unit (not shown) may further be coupled to CPU bus <b>424</b> between microprocessor <b>10</b> and bus bridge <b>402</b> in other embodiments. Alternatively, the external cache may be coupled to bus bridge <b>402</b> and cache control logic for the external cache may be integrated.</p><p>Main memory <b>404</b> is a memory in which application programs are stored and from which microprocessor <b>10</b> primarily executes. A suitable main memory <b>404</b> comprises DRAM (Dynamic Random Access Memory), and preferably a plurality of banks of SDRAM (Synchronous DRAM).</p><p>PCI devices <b>412</b>A-<b>412</b>B are illustrative of a variety of peripheral devices such as, for example, network interface cards, video accelerators, audio cards, hard or floppy disk drives or drive controllers, SCSI (Small Computer Systems Interface) adapters and telephony cards. Similarly, ISA device <b>418</b> is illustrative of various types of peripheral devices, such as a modem.</p><p>Graphics controller <b>408</b> is provided to control the rendering of text and images on a display <b>426</b>. Graphics controller <b>408</b> may embody a typical graphics accelerator generally known in the art to render three-dimensional data structures which can be effectively shifted into and from main memory <b>404</b>. Graphics controller <b>408</b> may therefore be a master of AGP bus <b>410</b> in that it can request and receive access to a target interface within bridge logic unit <b>402</b> to thereby obtain access to main memory <b>404</b>. A dedicated graphics bus accommodates rapid retrieval of data from main memory <b>404</b>. For certain operations, graphics controller <b>408</b> may further be configured to generate PCI protocol transactions on AGP bus <b>410</b>. The AGP interface of bus bridge <b>402</b> may thus include functionality to support both AGP protocol transactions as well as PCI protocol target and initiator transactions. Display <b>426</b> is any electronic display upon which an image or text can be presented. A suitable display <b>426</b> includes a cathode ray tube (\u201cCRT\u201d), a liquid crystal display (\u201cLCD\u201d), etc. It is noted that, while the AGP, PCI, and ISA or EISA buses have been used as examples in the above description, any bus architectures may be substituted as desired.</p><p>It is still further noted that the present discussion may refer to the assertion of various signals. As used herein, a signal is \u201casserted\u201d if it conveys a value indicative of a particular condition. Conversely, a signal is \u201cdeasserted\u201d if it conveys a value indicative of a lack of a particular condition. A signal may be defined to be asserted when it conveys a logical zero value or, conversely, when it conveys a logical one value. Additionally, various values have been described as being discarded in the above discussion. A value may be discarded in a number of manners, but generally involves modifying the value such that it is ignored by logic circuitry which receives the value. For example, if the value comprises a bit, the logic state of the value may be inverted to discard the value. If the value is an n-bit value, one of the n-bit encodings may indicate that the value is invalid. Setting the value to the invalid encoding causes the value to be discarded. Additionally, an n-bit value may include a valid bit indicative, when set, that the n-bit value is valid. Resetting the valid bit may comprise discarding the value. Other methods of discarding a value may be used as well.</p><p>A microprocessor and computer system capable of caching basic blocks of instructions has been disclosed. A method for operating a basic block oriented data cache has also been disclosed. Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated, particularly in light of the number of different embodiments disclosed. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Uwe", "last_name": "Kranich", "name": ""}, {"first_name": "David S.", "last_name": "Christie", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "ADVANCED MICRO DEVICES, INC."}, {"first_name": "", "last_name": "GLOBALFOUNDRIES U.S. INC.", "name": ""}, {"first_name": "", "last_name": "GLOBALFOUNDRIES INC.", "name": ""}, {"first_name": "", "last_name": "AMD TECHNOLOGIES HOLDINGS, INC.", "name": ""}, {"first_name": "", "last_name": "ADVANCED MICRO DEVICES, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F   9/38"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/38        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "712238"}, {"primary": false, "label": "711125"}, {"primary": false, "label": "712E09056"}], "ecla_classes": [{"label": "G06F   9/38B2"}], "cpc_classes": [{"label": "G06F   9/3804"}, {"label": "G06F   9/3806"}, {"label": "G06F   9/3836"}, {"label": "G06F   9/3806"}, {"label": "G06F   9/3804"}, {"label": "G06F   9/3857"}, {"label": "G06F   9/3836"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1997-10-24", "application_date": "1998-08-21", "family_members": [{"ucid": "US-6185675-B1", "titles": [{"lang": "EN", "text": "Basic block oriented trace cache utilizing a basic block sequence buffer to indicate program order of cached basic blocks"}]}]}