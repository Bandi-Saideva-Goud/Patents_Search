{"patent_number": "US-6606686-B1", "publication_id": 73500242, "family_id": 27670489, "publication_date": "2003-08-12", "titles": [{"lang": "EN", "text": "Unified memory system architecture including cache and directly addressable static random access memory"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11551666\" source=\"national office\"><p>A data processing apparatus includes a central processing unit and a memory configurable as cache memory and directly addressable memory. The memory is selectively configurable as cache memory and directly addressable memory by configuring a selected number of ways as directly addressable memory and configuring remaining ways as cache memory. Control logic inhibits indication that tag bits matches address bits and that a cache entry is the least recently used for cache eviction if the corresponding way is configured as directly addressable memory. In an alternative embodiment, the memory is selectively configurable as cache memory and directly addressable memory by configuring a selected number of sets equal to 2M, where M is an integer, as cache memory and configuring remaining sets as directly addressable memory.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50534690\"><p>A data processing apparatus includes a central processing unit and a memory configurable as cache memory and directly addressable memory. The memory is selectively configurable as cache memory and directly addressable memory by configuring a selected number of ways as directly addressable memory and configuring remaining ways as cache memory. Control logic inhibits indication that tag bits matches address bits and that a cache entry is the least recently used for cache eviction if the corresponding way is configured as directly addressable memory. In an alternative embodiment, the memory is selectively configurable as cache memory and directly addressable memory by configuring a selected number of sets equal to 2<sup>M</sup>, where M is an integer, as cache memory and configuring remaining sets as directly addressable memory.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00001\" num=\"1\"><claim-text>1. A data processing apparatus comprising:</claim-text><claim-text>a central processing unit for executing instructions manipulating data; and </claim-text><claim-text>a memory connected to said central processing unit, configured as a predetermined number of cache entries organized in a predetermined number of 2<sup>N </sup>sets serving as proxy for N address bits and a predetermined number of ways for each set and selectively configurable as cache memory and directly addressable memory by configuring a selected number of ways as directly addressable memory and configuring remaining ways as cache memory, said memory including </claim-text><claim-text>tag bits for each cache entry, </claim-text><claim-text>an address comparator for each cache entry having a first input connected to said corresponding tag bits and a second input receiving an address to which access is sought, said address comparator indicating whether said tag bits matches a predetermined number of most significant bits of said address, and </claim-text><claim-text>control logic coupled to said address comparator for inhibiting indication that said tag bits matches said predetermined number of address bits if said corresponding way is configured as directly addressable memory. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00002\" num=\"2\"><claim-text>2. The data processing apparatus of <claim-ref idref=\"US-6606686-B1-CLM-00001\">claim 1</claim-ref>, wherein:</claim-text><claim-text>said control logic includes an AND gate having a first input receiving said indication of whether said tag bits matches said predetermined number of address bits and a second input receiving a signal indicating whether said corresponding way is configured as directly addressable memory for inhibiting generation of a match signal if said corresponding way is configured as directly addressable memory and an output. </claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00003\" num=\"3\"><claim-text>3. The data processing apparatus of <claim-ref idref=\"US-6606686-B1-CLM-00001\">claim 1</claim-ref>, further comprising:</claim-text><claim-text>control bits corresponding to each cache entry storing an indication of a least recently used state of said cache entry; </claim-text><claim-text>a least recently used detector for each set connected to said control bits for each cache entry within each set determining the least recently used cache entry within said set for cache eviction; and </claim-text><claim-text>said control logic inhibiting indication that a cache entry is the least recently used for cache eviction if said corresponding way is configured as directly addressable memory. </claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00004\" num=\"4\"><claim-text>4. The data processing apparatus of <claim-ref idref=\"US-6606686-B1-CLM-00001\">claim 1</claim-ref>, wherein:</claim-text><claim-text>said control logic includes a bank select circuit enabling one cache way matching an address to be accessed dependent upon a predetermined at least one address bit and the particular ways configured as directly addressable memory. </claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00005\" num=\"5\"><claim-text>5. The data processing apparatus of <claim-ref idref=\"US-6606686-B1-CLM-00001\">claim 1</claim-ref>, wherein:</claim-text><claim-text>said control logic loads said tag bits corresponding to cache entries configured as directly addressable memory with an address equal to a predetermined address assigned to directly addressable memory for said cache entry. </claim-text></claim>"}, {"num": 6, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00006\" num=\"6\"><claim-text>6. A data processing apparatus comprising:</claim-text><claim-text>a central processing unit for executing instructions manipulating data; and </claim-text><claim-text>a memory connected to said central processing unit, configured as a predetermined number of cache entries organized in a predetermined number of 2<sup>N </sup>sets serving as proxy for N address bits and a predetermined number of ways for each set and selectively configurable as cache memory and directly addressable memory by configuring a selected number of sets equal to 2<sup>M</sup>, where M is an integer, as cache memory and configuring remaining sets as directly addressable memory. </claim-text></claim>"}, {"num": 7, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00007\" num=\"7\"><claim-text>7. A memory subsystem comprising:</claim-text><claim-text>a data array including a plurality of cache entries for storing data, said data array configured as a predetermined number of cache entries organized in a predetermined number of 2<sup>N </sup>sets serving as proxy for N address bits and a predetermined number of ways for each set; </claim-text><claim-text>a tag array storing a plurality of bits for each of said cache entries; </claim-text><claim-text>an address comparator for each cache entry having a first input connected to said corresponding tag bits and a second input receiving an address to which access is sought, said address comparator indicating whether said tag bits matches a predetermined number of most significant bits of said address, and </claim-text><claim-text>control logic coupled to said data array and said tag array for selectively configuring at least part of memory as directly addressable memory, and coupled to said address comparator for inhibiting indication that said tag bits matches said predetermined number of address bits if said corresponding way is configured as directly addressable memory. </claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00008\" num=\"8\"><claim-text>8. The memory subsystem of <claim-ref idref=\"US-6606686-B1-CLM-00007\">claim 7</claim-ref>, wherein:</claim-text><claim-text>said control logic includes an AND gate having a first input receiving said indication of whether said tag bits matches said predetermined number of address bits and a second input receiving a signal indicating whether said corresponding way is configured as directly addressable memory for inhibiting generation of a match signal if said corresponding way is configured as directly addressable memory and an output. </claim-text></claim>"}, {"num": 9, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00009\" num=\"9\"><claim-text>9. The memory subsystem of <claim-ref idref=\"US-6606686-B1-CLM-00007\">claim 7</claim-ref>, further comprising:</claim-text><claim-text>control bits corresponding to each cache entry storing an indication of a least recently used state of said cache entry; </claim-text><claim-text>a least recently used detector for each set connected to said control bits for each cache entry within each set determining the least recently used cache entry within said set for cache eviction; and </claim-text><claim-text>said control logic inhibiting indication that a cache entry is the least recently used for cache eviction if said corresponding way is configured as directly addressable memory. </claim-text></claim>"}, {"num": 10, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00010\" num=\"10\"><claim-text>10. The memory subsystem of <claim-ref idref=\"US-6606686-B1-CLM-00007\">claim 7</claim-ref>, wherein:</claim-text><claim-text>said control logic includes a bank select circuit enabling one cache way matching an address to be accessed dependent upon a predetermined at least one address bit and the particular ways configured as directly addressable memory. </claim-text></claim>"}, {"num": 11, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00011\" num=\"11\"><claim-text>11. The memory subsystem of <claim-ref idref=\"US-6606686-B1-CLM-00007\">claim 7</claim-ref>, wherein:</claim-text><claim-text>said control logic loads said tag bits corresponding to cache entries configured as directly addressable memory with an address equal to a predetermined address assigned to directly addressable memory for said cache entry. </claim-text></claim>"}, {"num": 12, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00012\" num=\"12\"><claim-text>12. A memory subsystem comprising:</claim-text><claim-text>a data array including a plurality of cache entries for storing data, said data array configured as a predetermined number of cache entries organized in a predetermined number of 2<sup>N </sup>sets serving as proxy for N address bits and a predetermined number of ways for each set; </claim-text><claim-text>a tag array storing a plurality of bits for each of said cache entries; and </claim-text><claim-text>control logic coupled to said data array and said tag array for selectively configuring at least part of memory as directly addressable memory by configuring a selected number of sets equal to 2<sup>M</sup>, where M is an integer, as cache memory and configuring remaining sets as directly addressable memory. </claim-text></claim>"}, {"num": 13, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00013\" num=\"13\"><claim-text>13. The data processing apparatus of <claim-ref idref=\"US-6606686-B1-CLM-00001\">claim 1</claim-ref>, wherein:</claim-text><claim-text>said cache ways configured as directly addressable memory are accessed at predetermined addresses. </claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00014\" num=\"14\"><claim-text>14. The data processing apparatus of <claim-ref idref=\"US-6606686-B1-CLM-00013\">claim 13</claim-ref>, wherein:</claim-text><claim-text>said predetermined addresses of said cache ways when configured as directly addressable memory are contiguous; and </claim-text><claim-text>said cache ways are configured as directly addressable memory in an order such that addresses of all cache ways currently configured as directly addressable memory are contiguous. </claim-text></claim>"}, {"num": 15, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00015\" num=\"15\"><claim-text>15. The data processing apparatus of <claim-ref idref=\"US-6606686-B1-CLM-00014\">claim 14</claim-ref>, wherein:</claim-text><claim-text>said predetermined addresses of said cache way having the lowest predetermined addresses when configured as directly addressable memory include a lowest possible memory address. </claim-text></claim>"}, {"num": 16, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00016\" num=\"16\"><claim-text>16. The data processing apparatus of <claim-ref idref=\"US-6606686-B1-CLM-00013\">claim 13</claim-ref>, wherein:</claim-text><claim-text>attempted access to said predetermined addresses of a cache way configured as cache memory are invalid. </claim-text></claim>"}, {"num": 17, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00017\" num=\"17\"><claim-text>17. The memory subsystem of <claim-ref idref=\"US-6606686-B1-CLM-00007\">claim 7</claim-ref>, wherein:</claim-text><claim-text>said cache ways configured as directly addressable memory are accessed at predetermined addresses. </claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00018\" num=\"18\"><claim-text>18. The memory subsystem of <claim-ref idref=\"US-6606686-B1-CLM-00017\">claim 17</claim-ref>, wherein:</claim-text><claim-text>said predetermined addresses of said cache ways when configured as directly addressable memory are contiguous; and </claim-text><claim-text>said cache ways are configured as directly addressable memory in an order such that addresses of all cache ways currently configured as directly addressable memory are contiguous. </claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00019\" num=\"19\"><claim-text>19. The memory subsystem of <claim-ref idref=\"US-6606686-B1-CLM-00018\">claim 18</claim-ref>, wherein:</claim-text><claim-text>said predetermined addresses of said cache way having the lowest predetermined addresses when configured as directly addressable memory includes a lowest possible memory address. </claim-text></claim>"}, {"num": 20, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6606686-B1-CLM-00020\" num=\"20\"><claim-text>20. The memory subsystem of <claim-ref idref=\"US-6606686-B1-CLM-00017\">claim 17</claim-ref>, wherein:</claim-text><claim-text>attempted access to said predetermined addresses of a cache way configured as cache memory are invalid. </claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53969648\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><p>This application claims priority under 35 USC \u00a7119(e) (1) of Provisional Application No. 60/144,550, filed Jul. 15, 1999 and Provisional Application No. 60/166,534, filed Nov. 18, 1999.</p><h4>CROSS REFERENCE TO RELATED APPLICATIONS</h4><p>This application is related to the following co-pending applications:</p><p>U.S. Pat. No. 6,484,237 entitled UNIFIED MULTILEVEL MEMORY SYSTEM ARCHITECTURE WHICH SUPPORTS BOTH CACHE AND ADDRESSABLE SRAM filed Jun. 26, 2000, which claims priority from U.S. Provisional Application No. 60/166,538 filed Nov. 18, 1999 and U.S. Provisional Application No. 60/144,550 filed Jul. 15, 1999;</p><p>U.S. Pat. No. 6,446,241 entitled AN AUTOMATED METHOD FOR TESTING CACHE issued September 3, 2002;</p><p>U.S. patent application Ser. No. 09/603,057 entitled METHOD AND APPARATUS FOR OPERATING ONE OR MORE CACHES IN CONJUNCTION WITH DIRECT MEMORY ACCESS CONTROLLER filed Jun. 26, 2000, which claims priority from U.S. Provisional Application No. 60/144,569 filed Jul. 15, 1999;</p><p>U.S. Pat. No. 6,535,958 entitled MULTILEVEL CACHE SYSTEM COHERENCE WITH MEMORY SELECTIVELY CONFIGURED AS CACHE OR DIRECT ACCESS MEMORY AND DIRECT MEMORY ACCESS filed Jun. 26, 2000, which claims priority from U.S. Provisional Application No. 60/166,527 filed Nov. 18, 1999 and U.S. Provisional Application No. 60/144,550 filed Jul. 15, 1999;</p><p>U.S. patent application Ser. No. 09/603,333 entitled PROGRAMMER INITIATED CACHE BLOCK OPERATIONS filed Jun. 26, 2000, which claims priority from U.S. Provisional Application No. 60/166,535 filed Nov. 18, 1999 and U.S. Provisional Application No. 60/144,550 filed Jul. 15, 1999; and</p><p>U.S. Pat. No. 6,408,345 entitled SUPERSCALAR MEMORY TRANSFER CONTROLLER IN MULTILEVEL MEMORY ORGANIZATION issued Jun. 18, 2002.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>TECHNICAL FIELD OF THE INVENTION</h4><p>The technical field of this invention is data processing systems and particularly data processing systems with combined cache memory and random memory access memory.</p><h4>BACKGROUND OF THE INVENTION</h4><p>Data processing systems typically employ data caches or instruction caches to improve performance. A small amount of high speed memory is used as the cache. This cache memory is filled from main memory on an as needed basis. When the data processor requires data or an instruction, this is first sought from the cache memory. If the data or instruction sought is already stored in the cache memory, it is recalled faster than it could have been recalled from main memory. If the data or instruction sought is not stored in the cache memory, it is recalled from main memory for use and also stored in the corresponding cache. A performance improvement is achieved using cache memory based upon the principle of locality of reference. It is likely that the data or the instruction just sought by the data processor will be needed again in the near future. Use of cache memories speeds the accesses needed to service these future needs. A typical high performance data processor will include instruction cache, data cache or both on the same integrated circuit as the data processor core.</p><p>Cache memories are widely used in general purpose microprocessors employed in desktop personal computers and workstations. Cache memories are frequently used in microprocessors employed in embedded applications in which the programmable nature of the microprocessor controller is invisible to the user. Caching provides a hardware managed, programmer transparent access to a large memory space via a physically small static random access memory (SRAM) with an average memory access time approaching the access time of the SRAM. The hardware managed and programmer transparent aspect of cache systems enables better performance while freeing the programmer from explicit memory management.</p><p>Cache memories are typically not used with digital signal processors. Digital signal processors are generally used in applications with real time constraints. Such real time constraints typically do not operate well with cache memories. When employing cache memories the access time for a particular instruction or data cannot be predetermined. If the sought item is stored in the cache, then the access time is a known short time. However, if the item sought is not stored in the cache, then the access time will be very much longer. Additionally, other demands for main memory access will make the access time from main memory vary greatly. This variation in memory access time makes planning for real time applications extremely difficult or impossible.</p><p>Digital signal processors will more typically include some directly addressable SRAM on the same integrated circuit as the data processor core. The programmer must manage transfer of critically needed instructions and data to the on-chip SRAM. Often this memory management employs a direct memory access unit. A direct memory access unit typically controls data moves between memories or between a memory and a peripheral ordered by the data processor core. Once begun on a particular data transfer the direct memory access unit operates autonomously from the data processor core. Once stored in the on-chip SRAM, these items are available to the data processor core at a greatly lowered access time. Thus these items will be available to service the real time constraints of the application. Note that both the data processor core and the direct memory access unit may access the on-chip SRAM. The memory management task is difficult to program. The programmer must anticipate the needs of the application for instructions and data and assure that these items are loaded into the on-chip SRAM ahead of their need. Additionally, the programmer must juggle conflicting needs for the typically limited space of the on-chip SRAM. While this is a difficult programming task, it is generally preferable to the unknown memory latencies of cache systems in real time applications.</p><p>Digital signal processor architectures are becoming more complex. The complexity of new applications have increased and their real time constraints have become more stringent. These advances have made the programming problem of real time memory management using on-chip SRAM increasingly difficult. This has slowed applications development. With variety in the size of on-chip SRAM and the variations in external memory latency, these programs have increasingly been limited to specific product configurations. Thus it has not been possible to employ the same set of instructions to solve a similar memory management problem in a similar product. This need for custom algorithms for each product prevents re-use of instruction blocks and further slows product development. The increasing architectural capabilities of processors also require bigger on-chip memories (either cache or SRAM) to prevent processor stalls. Processor frequencies are increasing. This increasing memory size and processor frequency works against easy scaling of the on-chip memory with increasing data processing requirements.</p><p>There is a need in the art for a memory management solution that includes the best of both cache systems and programmer managed on-chip SRAM. This new memory management solution would preferably enable programmer selection of more cache system benefits or more on-chip SRAM benefits.</p><h4>SUMMARY OF THE INVENTION</h4><p>A data processing apparatus includes a central processing unit and a memory configurable as cache memory and directly addressable memory. The memory is configured as a predetermined number of cache entries organized in a predetermined number of sets and a predetermined number of ways. In the preferred embodiment the memory is selectively configurable as cache memory and directly addressable memory by configuring a selected number of ways as directly addressable memory and configuring remaining ways as cache memory. Control logic inhibits indication that tag bits matches a predetermined number of address bits if said corresponding way is configured as directly addressable memory. Control logic inhibits indication that a cache entry is the least recently used for cache eviction if the corresponding way is configured as directly addressable memory. A bank select circuit enables one cache entry within a set matching an address to be accessed dependent upon at least one address bit and the ways configured as directly addressable memory.</p><p>In an alternative embodiment, the memory is selectively configurable as cache memory and directly addressable memory by configuring a selected number of sets equal to 2<sup>M</sup>, where M is an integer, as cache memory and configuring remaining sets as directly addressable memory.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>These and other aspects of this invention are illustrated in the drawings, in which:</p><p>FIG. 1 illustrates the organization of a typical digital signal processor to which this invention is applicable;</p><p>FIG. 2 illustrates the data paths to and from the level two unified cache illustrated in FIG. 1;</p><p>FIG. 3 illustrates the organization of a typical cache memory;</p><p>FIG. 4 illustrates a first manner of selectively dividing a memory between cache memory and directly addressable random access memory;</p><p>FIG. 5 illustrates a second manner of selectively dividing a memory between cache memory and directly addressable random access memory;</p><p>FIG. 6 illustrates the manner of selectively dividing the memory between cache and directly addressable memory in accordance with the preferred embodiment of this invention;</p><p>FIG. 7 illustrates the addresses assigned to the directly addressable memory portion of the memory in the preferred embodiment of this invention;</p><p>FIG. 8 illustrates details of a very long instruction word digital signal processor core suitable for use in FIG. 1; and</p><p>FIGS. 9A and 9B together illustrate additional details of the digital signal processor of FIG. <b>8</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS</h4><p>FIG. 1 illustrates the organization of a typical digital signal processor system <b>100</b> to which this invention is applicable. Digital signal processor system <b>100</b> includes central processing unit core <b>110</b>. Central processing unit core <b>110</b> includes the data processing portion of digital signal processor system <b>100</b>. Central processing unit core <b>110</b> could be constructed as known in the art and would typically includes a register file, an integer arithmetic logic unit, an integer multiplier and program flow control units. An example of an appropriate central processing unit core is described below in conjunction with FIGS. 8, <b>9</b>A and <b>9</b>B.</p><p>Digital signal processor system <b>100</b> includes a number of cache memories. FIG. 1 illustrates a pair of first level caches. Level one instruction cache (L<b>1</b>I) <b>121</b> stores instructions used by central processing unit core <b>110</b>. Central processing unit core <b>110</b> first attempts to access any instruction from level one instruction cache <b>121</b>. Level one data cache (L<b>1</b>D) <b>123</b> stores data used by central processing unit core <b>110</b>. Central processing unit core <b>110</b> first attempts to access any required data from level one data cache <b>123</b>. The two level one caches are backed by a level two unified cache (L<b>2</b>) <b>130</b>. In the event of a cache miss to level one instruction cache <b>121</b> or to level one data cache <b>123</b>, the requested instruction or data is sought from level two unified cache <b>130</b>. If the requested instruction or data is stored in level two unified cache <b>130</b>, then it is supplied to the requesting level one cache for supply to central processing unit core <b>110</b>. As is known in the art, the requested instruction or data may be simultaneously supplied to both the requesting cache and central processing unit core <b>110</b> to speed use.</p><p>Level two unified cache <b>130</b> is further coupled to higher level memory systems. Digital signal processor system <b>100</b> may be a part of a multiprocessor system. The other processors of the multiprocessor system are coupled to level two unified cache <b>130</b> via a transfer request bus <b>141</b> and a data transfer bus <b>143</b>. A direct memory access unit <b>150</b> provides the connection of digital signal processor system <b>100</b> to external memory <b>161</b> and external peripherals <b>169</b>.</p><p>In accordance with the preferred embodiment of this invention, level two unified cache <b>130</b> may be configured to include variable amounts of static random access memory (SRAM) instead of cache memory. In accordance with this invention some or all of level two unified cache <b>130</b> may be configured as normal read/write memory which operates under program control. If some of level two unified cache <b>130</b> is configured as SRAM, then this memory space may be either a source or a destination of a direct memory access. This will be more fully described below.</p><p>The complex interrelation of parts of digital signal processor system <b>100</b> permits numerous data movements. These are illustrated schematically in FIG. <b>1</b> and will be listed here. First, level one instruction cache <b>121</b> may receive instructions recalled from level two unified cache <b>130</b> (1) for a cache miss fill. In this example, there is no hardware support for self-modifying code so that instructions stored in level one instruction cache <b>121</b> are not altered. There are two possible data movements between level one data cache <b>123</b> and level two unified cache <b>130</b>. The first of these data movements is a cache miss fill from level two unified cache <b>130</b> to level one data cache <b>123</b> (2). Data may also pass from level one data cache <b>123</b> to level two unified cache <b>130</b> (3). This data movement takes place upon; a write miss to level one data cache <b>123</b> which must be serviced by level two unified cache <b>130</b>; a victim eviction from level one data cache <b>123</b> to level two unified cache <b>130</b>; and a snoop response from level one data cache <b>123</b> to level two unified cache <b>130</b>. Data can be moved between level two unified cache <b>130</b> and external memory <b>160</b>. This can take place upon: a cache miss to level two unified cache <b>130</b> service from external memory (4) or a direct memory access <b>150</b> data movement from external memory <b>161</b> and level two unified cache <b>130</b> configured as SRAM; a victim eviction from level two unified cache <b>130</b> to external memory <b>161</b> (5) or a direct memory access <b>150</b> data movement from a portion of level two unified cache <b>130</b> configured as SRAM to external memory <b>161</b>. Finally, data can move between level two unified cache <b>130</b> and peripherals <b>169</b>. These movements take place upon: or a direct memory access <b>150</b> data movement from peripheral <b>169</b> and level two unified cache <b>130</b> configured as SRAM; or a direct memory access <b>150</b> data movement from a portion of level two unified cache <b>130</b> configured as SRAM to peripherals <b>169</b>. All data movement between level two unified cache <b>130</b> and external memory <b>161</b> and between level two unified cache <b>130</b> and peripherals <b>169</b> employ data transfer bus <b>143</b> and are controlled by direct memory access unit <b>150</b>. These direct memory access data movements may take place as result of a command from central processing unit core <b>110</b> or a command from another digital signal processor system received via transfer request bus <b>141</b>.</p><p>FIG. 2 illustrates the data connections among parts of digital signal processing system <b>100</b> illustrated in FIG. <b>1</b>. FIG. 2 illustrates the data path widths between the various parts. The level one instruction cache interface includes a 256-bit data path from level two unified cache <b>130</b> to level one instruction cache <b>121</b>. This data path size corresponds to one half of the 64 byte cache line size within level one instruction cache <b>121</b> and equals one instruction fetch packet. In the preferred embodiment, the 256-bits are 64 bits from each of the four banks of level two unified cache <b>130</b>. Thus level two unified cache <b>130</b> can source this amount of data in a single cycle. This occurs regardless of the amount of level two unified cache <b>130</b> configured as cache. The cache/SRAM partitioning within level two unified cache <b>130</b> is across the data banks rather than within the data banks. Thus level two unified cache <b>130</b> can always supply 256 bits to level one instruction cache <b>121</b> if any part is partitioned as cache. Level one instruction cache <b>121</b> may also receive data directly from data transfer bus <b>143</b>, for example upon fetching code from non-cacheable memory addresses. Data transfer bus <b>143</b> supplies only 64 bits per cycle, thus at least four cycles are needed to accumulate the 256 bits. The data source for transfers to level one instruction cache <b>121</b> is selected by multiplexer <b>131</b>. FIG. 1 illustrates supply of 32 address bits from level one instruction cache <b>121</b> to level two unified cache <b>130</b>. Because level one instruction cache <b>121</b> operates on 256 bit boundaries, the 8 least significant bits are always zero and may be omitted from the address. Note that writes to level one instruction cache <b>121</b> are not permitted, therefore level one instruction cache <b>121</b> never supplies data to level two unified cache <b>130</b>.</p><p>The level one data cache interface includes a 128-bit data path from level two unified cache <b>130</b> to level one data cache <b>123</b>. In the preferred embodiment, the 128 bits are 64 bits from each of two banks of level two unified cache <b>130</b>. This assumes no bank conflicts with other data transfers. Level two unified cache <b>130</b> only services one cache fill data transfer to level one data cache 123 per cycle. Thus if two load/store units in central processing unit <b>110</b> each request data and produce a read cache miss within level one data cache <b>123</b>, the two read miss requests to level two unified cache <b>130</b> are serviced in sequence. As noted above, the cache/SRAM partitioning of level two unified cache <b>130</b> is across the memory banks. Thus level two unified cache <b>130</b> can supply data to level one data cache <b>123</b> from two banks so long as level two unified cache <b>130</b> is partitioned to include some cache. Level one data cache <b>123</b> may also receive data directly from data transfer bus <b>143</b>, for example upon fetching data from non-cacheable memory addresses. Data transfer bus <b>143</b> supplies only 64 bits per cycle, however accesses to non-cacheable memory addresses are at most 32 bits. In this case, the 32 bits are transferred in a single data transfer cycle. The data source for transfers to level one data cache <b>123</b> is selected by multiplexer <b>133</b>. FIG. 1 illustrates supply of two sets of 32 address bits from level one data cache <b>123</b> to level two unified cache <b>130</b>. Because level one data cache <b>123</b> operates on 64 bit boundaries, the 6 least significant bits are always zero and may be omitted from the address.</p><p>Level one data cache <b>123</b> may supply data to level two unified cache <b>130</b>. This occurs on a write miss, a cache entry eviction and a response to a snoop hit to data in the modified state within level one data cache <b>123</b>. It is possible that each of the load/store units within central processing unit <b>110</b> would require data transfer from level one data cache <b>123</b> to level two unified cache <b>130</b> in the same cycle. Upon a write miss within level one data cache <b>123</b>, only the 32 bits of the write data is supplied from level one data cache <b>123</b> to level 2 unified cache <b>130</b>. For either a cache eviction or a snoop data response, level one data cache <b>121</b> supplies 128 bits to level two unified cache <b>130</b>, the same data width as opposite transfers. Data from level one data cache <b>123</b> may also be supplied to data transfer bus <b>143</b> as selected by multiplexer <b>137</b>. This could occur as a result of a write to a non-cacheable address.</p><p>The interface between level two unified cache <b>130</b> and data transfer bus <b>143</b> includes two 64-bit data busses. A first of these data busses supplies data from data transfer bus <b>143</b> to level two unified cache <b>130</b>. This data may be stored in level two unified cache <b>130</b> via a single 64-bit write port as selected by multiplexer <b>135</b>. The second bus is a 64-bit bus supplying data from level two unified cache <b>130</b> or level one data cache <b>123</b> as selected by multiplexer <b>137</b>. All transfers using data transfer bus <b>143</b> employ direct memory access unit <b>150</b> responsive to commands via transfer request bus <b>141</b>.</p><p>FIG. 3 illustrates the organization of a typical cache memory. Level two unified cache <b>130</b> includes a plurality of cache entries <b>201</b>. These cache entries are sometimes called cache lines. Each cache entry <b>201</b> stores a data of a fixed size. The fixed size is a design choice. This fixed size of a cache entry <b>201</b> typically includes more than the minimum addressable data size of the data processing system. This factor of the minimum addressable data size is an integral power of 2 for reasons that will be explained below. It is known in the art to select a cache entry data size equal to the external data bus size, which may differ from and is typically greater than the minimum addressable data size. The cache entry data size may also be selected some size larger than the external data bus size due to the capability of providing wider data busses internal to an integrated circuit embodying the data processing system than is feasible for external connections. As an example only, it is typical for a data processing system to address individual bytes (8 bits) within memory while cache entries may each include 8 to 128 bytes.</p><p>The cache entries <b>201</b> are organized in a plurality of sets <b>203</b> and a plurality of ways <b>205</b> in a manner known in the art. It is known in the art to construct a cache memory in a manner that not all cache entries may store data from all memory addresses. Plural memory locations alias into each set. The particular set within sets <b>203</b> serves as a proxy for some bits of the address. Thus the number of sets must be an integral power of 2. The address bits selected are typically the next most significant bits beyond the cache entry size. These bits are selected to distribute adjacent or nearby and presumably locally referenced data sets across the sets <b>203</b>. FIG. 3 illustrates eight sets <b>203</b>. This is an example for illustrative purposes only. Level two unified cache <b>130</b> preferable includes <b>128</b> such sets. Known cache systems may have more or fewer sets. The data position within each cache entry <b>201</b> serves as a proxy for the least significant bits of the address. For example, in a cache entry having data sixty-four times the minimum addressable data size, the cache entry location serves as proxy for the six least significant address bits. Thus the number of minimum data size blocks within each cache entry <b>201</b> must be an integral power of 2.</p><p>It is known in the art to provide more than one cache entry for each set. The number of such cache entries is known as the ways of the cache. The number of ways is also known as the set associativity of the cache. FIG. 3 illustrates a cache memory organized into 4 ways <b>205</b>. Those skilled in the art would realize that this is merely a convenient example. Known cache systems may include more or fewer ways. The size of the cache entries and the number of sets of the cache determines the number of tag bits that must be provided for each cache entry. The position within each cache entry serves as a proxy for a number of least significant bits of the address. The particular set serves as proxy for a number of address bits next more significant than the position within the cache entry. A set of tag bits must be stored in a read/write memory for each cache entry to completely specify the memory address cached. If the total number of address bits is M, the number of sets is 2<sup>s </sup>and each cache entry holds 2<sup>c </sup>blocks of the minimum data size, then the number of tag bits for each cache entry is M-S-C. In the preferred embodiment the address has 32 bits which point to individual data bytes. Level two unified cache <b>130</b> has cache entries of 128 bytes, thus C is 7. Level two unified cache <b>130</b> includes 128 sets, thus S is also 7. Accordingly, each cache entry must have 18 tag bits. Note that the minimum addressable data size and the minimum data transfer size are not necessarily the same. It is known to address memory in bytes but transfer data in words of 32 bits (4 bytes).</p><p>Provision of plural ways within the cache permits caching more than one memory address that aliases into the same set. This is considered advantageous up to a point. It is generally believed that a cache with 8 ways operates nearly as well as a cache where any memory address may be cached within any cache entry. The use of cache ways reduces the problem of tracking the least recently used cache entry for replacement. Data located at a particular main memory address may be stored only in the corresponding cache set. Upon a cache miss requiring victim eviction, the victim must be from the same set as the set which may store the missed address. Thus the least recently used information need only be kept relative to the other ways within each set. Since practical caches rarely have more than 4 or 8 ways, no more than two or three bits are required per cache entry to track the least recently used cache entry. The use of sets as a proxy for next more significant address bits greatly reduces the number of address comparisons with variable data needed to determine a cache hit and identify where the sought data is stored upon a cache hit.</p><p>FIG. 4 illustrates a manner of selectively configuring level two unified cache <b>130</b> as cache or as directly addressable SRAM. As shown in FIG. 4, each way of cache system <b>200</b> may be selected as cache memory or as directly addressable static random access memory (SRAM). Configuring a way of cache system <b>200</b> as SRAM correspondingly reduces the ways of the cache memory. If the number of ways of the cache system is N, then the cache system may be configured with any integer from 0 to N SRAM partitions. If the number of SRAM partitions is M, with 0\u2266M\u2266N, then the configured cache system has N-M ways. In the example illustrated in FIG. 4, the cache memory system has 4 ways. The cache memory system may be partitioned with 0, 1, 2, 3 or 4 of the ways as SRAM. The effective ways of associativity of the cache is reduced for each partition selected as SRAM. Table 1 shows the available options for the 4-way set-associative example illustrated in FIG. <b>4</b>.</p><p><tables id=\"TABLE-US-00001\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"77pt\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"56pt\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"84pt\"></colspec><thead><row><entry nameend=\"3\" namest=\"1\" rowsep=\"1\">TABLE 1</entry></row><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry>SRAM Partitions</entry><entry>Cache Partitions</entry><entry>Cache Associativity</entry></row><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row></thead><tbody valign=\"top\"><row><entry>0</entry><entry>4</entry><entry>4-way</entry></row><row><entry>1</entry><entry>3</entry><entry>3-way</entry></row><row><entry>2</entry><entry>2</entry><entry>2-way</entry></row><row><entry>3</entry><entry>1</entry><entry>direct mapped</entry></row><row><entry>4</entry><entry>0</entry><entry>\u2014</entry></row><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row></tbody></tgroup></table></tables></p><p>Thus: if 0 partitions are configured as SRAM, then there are 4 cache ways <b>205</b>; if 1 partition <b>211</b> is configured as SRAM, then there are three cache ways <b>221</b>; if 2 partitions <b>213</b> are configured as SRAM, then there are two cache ways <b>223</b>; if three partitions <b>215</b> are configured as SRAM, then there is one way <b>225</b> (also known as direct mapped because each memory address may be stored in only one cache entry); and if 4 partitions <b>217</b> are configured as SRAM, then there is no cache memory. In the preferred embodiment, the ways may only be configured as SRAM only in a predetermined order enabling the SRAM memory addresses to be contiguous whatever number of partitions are configured as SRAM. Note that the location within a cache entry still serves as proxy for the least significant memory address bits. The portion of level two unified cache <b>130</b> configured as SRAM thus becomes a directly addressable memory having a greater data width than the minimum addressable data size.</p><p>FIG. 5 illustrates an alternative manner of selectively configuring level two unified cache <b>130</b> as cache or as directly addressable SRAM. As shown in FIG. 5, the number of sets devoted to cache varies with the amount of SRAM allocated. Because the number of sets must be an integral power of 2, this technique implies a different set of relative sizes than the prior embodiment. If the number of sets is S, then the permitted cache sizes are S/2<sup>N</sup>, where N goes from 0 to S. In the example illustrated in FIG. 5 there are 8 sets. Table 2 shows the available options in this example.</p><p><tables id=\"TABLE-US-00002\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"91pt\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"49pt\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"77pt\"></colspec><thead><row><entry nameend=\"3\" namest=\"1\" rowsep=\"1\">TABLE 2</entry></row><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry>SRAM Relative</entry><entry>Cache Relative</entry><entry>Cache</entry></row><row><entry>Size</entry><entry>Size</entry><entry>Associativity</entry></row><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row></thead><tbody valign=\"top\"><row><entry>0</entry><entry>1</entry><entry>4-way</entry></row><row><entry>1/2</entry><entry>1/2</entry><entry>4-way</entry></row><row><entry>3/4</entry><entry>1/4</entry><entry>4-way</entry></row><row><entry>7/8</entry><entry>1/8</entry><entry>4-way</entry></row><row><entry>1</entry><entry>0</entry><entry>\u2014</entry></row><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row></tbody></tgroup></table></tables></p><p>Thus: if 0 sets are configured as SRAM, then there are 8 sets of cache <b>203</b>; if 4 sets <b>231</b> are configured as SRAM, then there are four sets of cache <b>221</b>; if 6 sets <b>233</b> are configured as SRAM, then there are two sets of cache <b>243</b>; if seven sets <b>235</b> are configured as SRAM, then there is one set of cache <b>245</b>; and if 8 sets <b>237</b> are configured as SRAM, then there is no cache memory. In the preferred embodiment, the sets may only be configured as SRAM only in a predetermined order enabling the SRAM memory addresses to be contiguous whatever number of sets are configured as SRAM. This technique does not yield the same amount of partition flexibility as the first technique. However, the number of ways of the cache is unchanged. Thus the cache performance will vary only with the amount of memory allocated to cache and not with the number of ways.</p><p>FIG. 6 illustrates the manner of controlling the selection of cache or directly addressable memory in the preferred embodiment of this invention. FIG. 6 illustrates some parts of one set of level two unified cache <b>130</b>. In the preferred embodiment level two unified cache <b>130</b> is a 64 Kbyte memory having 128 sets of 4 ways each and a cache entry size of 128 bytes. Thus the circuits illustrated in FIG. 6 are replicated 128 times within level two unified cache <b>130</b>.</p><p>Each set, such as set S<sub>i </sub><b>314</b> illustrated in FIG. 6, includes four cache entries <b>326</b>, <b>336</b>, <b>346</b> and <b>356</b>. Each cache entry has a corresponding set of address tag bits and control bits. In FIG. <b>6</b>: address tag bits <b>320</b> and control bits <b>323</b> correspond to cache entry <b>326</b>; address tag bits <b>330</b> and control bits <b>333</b> correspond to cache entry <b>336</b>; address tag bits <b>340</b> and control bits <b>343</b> correspond to cache entry <b>346</b>; and address tag bits <b>350</b> and control bits <b>353</b> correspond to cache entry <b>356</b>.</p><p>FIG. 6 illustrates a 32 bit address <b>310</b> divided into three parts, least significant bits <b>311</b>, middle bits <b>312</b> and most significant bits <b>323</b>. In accordance with one convention known in the art, each address uniquely specifies a byte (8 bits) within the memory space. In the preferred embodiment there are 128 bytes within each cache entry <b>326</b>, <b>336</b>, <b>346</b> and <b>356</b>, thus least significant bits <b>311</b> (6:0) specify one byte of <b>128</b> positioned with each cache entry. In the preferred embodiment there are 128 sets <b>314</b>, thus middle bits <b>312</b> (13:7) specify the particular set within the <b>128</b> sets. When level two unified cache <b>130</b> is configured as all cache, address <b>310</b> is parsed as follows. Middle bits <b>312</b> are decoded to select one of the <b>128</b> sets such as set S<sub>i </sub><b>314</b> illustrated in FIG. <b>6</b>. This selection is typically made via a 1-of-128 decoder (not shown). Thus middle bits <b>312</b> point to a particular set such as set S<sub>i </sub><b>314</b> illustrated in FIG. <b>6</b>. Each of the address tag bits <b>320</b>, <b>330</b>, <b>340</b> and <b>350</b> have a corresponding address compare circuit <b>321</b>, <b>331</b>, <b>341</b> and <b>351</b>. The address compare circuits <b>321</b>, <b>331</b>, <b>341</b> and <b>351</b> compare the most significant bits <b>313</b> of address <b>310</b> with the corresponding address tag bits <b>320</b>, <b>330</b>, <b>340</b> and <b>350</b>. As known in the art, address tag bits <b>320</b>, <b>330</b>, <b>340</b> and <b>350</b> are loaded with the most significant bits of the address of the data cached in the corresponding cache entries <b>326</b>, <b>336</b>, <b>346</b> and <b>356</b>. If one of the address compare circuits <b>321</b>, <b>331</b>, <b>341</b> or <b>351</b> finds a match, this indicates a cache hit. Note that when configured as all cache, SRAM select lines <b>324</b>, <b>334</b>, <b>344</b> and <b>354</b> are all 0. Supply of this 0 to the inverting input causes respective AND gates <b>322</b>, <b>332</b>, <b>342</b> and <b>352</b> to pass the match signal and indicate the cache hit. The data corresponding to the address to be accessed is stored in the corresponding cache entry <b>326</b>, <b>336</b>, <b>346</b> or <b>356</b>. Level two unified cache <b>130</b> then enables access to data stored in the corresponding cache entry <b>326</b>, <b>336</b>, <b>346</b> or <b>356</b>. The data position within the cache entry <b>326</b>, <b>336</b>, <b>346</b> or <b>356</b> corresponds uniquely to the least significant bits of the address. Thus central processing unit <b>110</b> can access the date for read or write without requiring data transfer to or from the main memory. If no address compare circuit <b>321</b>, <b>331</b>, <b>341</b> or <b>351</b> finds a match, this indicates a cache miss. This cache miss is serviced in a</p><p>In the preferred embodiment, level two unified cache <b>130</b> may be configured as directly addressable memory on the basis of cache ways. Consider the example of one cache way of four configured as directly addressable memory. The SRAM select line <b>324</b> is 1 and SRAM select lines <b>334</b>, <b>344</b> and <b>354</b> are 0. Thus AND gates <b>332</b>, <b>342</b> and <b>352</b> are enabled to pass a match signal indicating a cache hit from respective address compare circuits <b>331</b>, <b>341</b> and <b>351</b>. In contrast, AND gate <b>322</b> is blocked from passing the match signal from address compare circuit <b>320</b> indicating a cache hit due to the 1 on its inverting input. Accordingly, cache entry <b>326</b> is never accessed as cache because the corresponding address tag bits <b>320</b> can never generate a cache hit signal.</p><p>At the same time zero detect circuit <b>315</b> and bank select circuit <b>316</b> may enable selection of cache entry <b>326</b>. Assuming middle bits <b>312</b> select set Si <b>314</b> illustrated in FIG. 6, then bits <b>14</b> and <b>15</b> enable access to cache entry <b>326</b> if they are 00 and bits <b>16</b> to <b>31</b> are all 0. Zero detect circuit <b>315</b> receives address bits <b>16</b> to <b>31</b> and indicates whether these bits are all 0. Note that the predetermined addresses assigned to the SRAM configured portion of level two unified cache <b>130</b> has these bits all 0. If this is the case, bank select circuit <b>316</b> is enabled. In this example bank select circuit <b>316</b> enables cache entry <b>326</b>. Then least significant bits <b>311</b> point to one byte of the 128 bytes within cache entry <b>326</b>. Thus the address selects a physical location within level two cache <b>130</b> corresponding to the received address.</p><p>In the event that two or more ways are configured as cache, bank select circuit <b>316</b> controls selection of one of cache entries <b>326</b>, <b>336</b>, <b>346</b> or <b>356</b> corresponding to the state of address bits <b>14</b> and <b>15</b>. Table 3 shows the operation of bank select circuit <b>316</b> for all cases.</p><p><tables id=\"TABLE-US-00003\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"6\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"21pt\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"56pt\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"42pt\"></colspec><colspec align=\"center\" colname=\"4\" colwidth=\"28pt\"></colspec><colspec align=\"center\" colname=\"5\" colwidth=\"42pt\"></colspec><colspec align=\"center\" colname=\"6\" colwidth=\"28pt\"></colspec><thead><row><entry nameend=\"6\" namest=\"1\" rowsep=\"1\">TABLE 3</entry></row><row><entry align=\"center\" nameend=\"6\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry></entry><entry>Number of Ways</entry><entry>Cache</entry><entry>Cache</entry><entry>Cache</entry><entry>Cache</entry></row><row><entry>Bits</entry><entry>Configured</entry><entry>Entry</entry><entry>Entry</entry><entry>Entry</entry><entry>Entry</entry></row><row><entry>14:15</entry><entry>as SRAM</entry><entry>326</entry><entry>335</entry><entry>346</entry><entry>356</entry></row><row><entry align=\"center\" nameend=\"6\" namest=\"1\" rowsep=\"1\"></entry></row></thead><tbody valign=\"top\"><row><entry>XX</entry><entry>0</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>00</entry><entry>1</entry><entry>Select</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>01</entry><entry>1</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>10</entry><entry>1</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>11</entry><entry>1</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>00</entry><entry>2</entry><entry>Select</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>01</entry><entry>2</entry><entry>\u2014</entry><entry>Select</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>10</entry><entry>2</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>11</entry><entry>2</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>00</entry><entry>3</entry><entry>Select</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>01</entry><entry>3</entry><entry>\u2014</entry><entry>Select</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>10</entry><entry>3</entry><entry>\u2014</entry><entry>\u2014</entry><entry>Select</entry><entry>\u2014</entry></row><row><entry>11</entry><entry>3</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>00</entry><entry>4</entry><entry>Select</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>01</entry><entry>4</entry><entry>\u2014</entry><entry>Select</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>10</entry><entry>4</entry><entry>\u2014</entry><entry>\u2014</entry><entry>Select</entry><entry>\u2014</entry></row><row><entry>11</entry><entry>4</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry><entry>Select</entry></row><row><entry align=\"center\" nameend=\"6\" namest=\"1\" rowsep=\"1\"></entry></row></tbody></tgroup></table></tables></p><p>If no directly addressable memory is selected, then bank select circuit <b>316</b> never enables any cache entry. Thus bits <b>14</b> and <b>15</b> are a don't care condition XX. If one cache way is configured as directly addressable memory, then cache entry <b>326</b> is enabled only if address bits <b>14</b> and <b>15</b> are 00 as described above. If address bits <b>14</b> and <b>15</b> are not both 0, then the address is beyond the range of the SRAM configured portion of level two unified cache <b>130</b>. If two cache ways are configured as directly addressable memory, then cache entry <b>326</b> is enabled if address bits <b>14</b> and <b>15</b> are 00, cache entry <b>336</b> is enabled if address bits <b>14</b> and <b>15</b> are 01, otherwise no cache entry is enabled. With three cache ways configured as directly addressable memory, cache entry <b>326</b> is enabled if address bits <b>14</b> and <b>15</b> are 00, cache entry <b>336</b> is enabled if address bits <b>14</b> and <b>15</b> are 01, cache entry <b>346</b> is enabled if address bits <b>14</b> and <b>15</b> are 10 and no cache entry is enabled if address bits <b>14</b> and <b>15</b> are 11. Lastly, if the whole of level two unified cache <b>130</b> is configured as directly addressable memory, then cache entry <b>326</b> is enabled if address bits <b>14</b> and <b>15</b> are 00, cache entry <b>336</b> is enabled if address bits <b>14</b> and <b>15</b> are 01, cache entry <b>346</b> is enabled if address bits <b>14</b> and <b>15</b> are 10 and cache entry <b>356</b> is enabled if address bits <b>14</b> and <b>15</b> are 11. Thus bank select circuit <b>316</b> selects the cache entry <b>326</b>, <b>336</b>, <b>346</b> or <b>356</b> or no cache entry depending on the amount of directly addressable memory specified and the address. The result is that the address specifies a physical location within the directly addressable memory configured portion of level two unified cache <b>130</b>.</p><p>As shown in FIG. 7, the portions of level two unified cache <b>130</b> partitioned as directly addressable memory have predetermined addresses. Digital signal processor system <b>100</b> preferably employs a 32 bit address. FIG. 7 shows the addresses assigned to the directly addressable memory configured portions of level two unified cache <b>130</b> in hexadecimal. The first quarter starts at Hexadecimal 00000000. When one quarter of level two unified cache <b>130</b> is configured as SRAM, this memory occupies addresses between Hex 00000000 and Hex 00003FFF. The second quarter starts at Hexadecimal 000040000. When half of level two unified cache <b>130</b> is configured as SPAM, this memory occupies addressed between Hex 00000000 and Hex 00007FFF. The third quarter starts at Hexadecimal 000080000. When three quarters of level two unified cache <b>130</b> is configured as SRAM, this memory occupies addresses between Hex 00000000 and Hex 0000BFFF. The final quarter starts at Hexadecimal 0000C0000. When all of level two unified cache <b>130</b> is configured as SRAM, this memory occupies addresses between Hex 00000000 and Hex 0000FFFF. Read accesses to addresses within these ranges when configured as cache will return invalid data. Write accesses to addresses within these ranges when configured as cache will be discarded and not change the data stored in level two unified cache <b>130</b>.</p><p>FIG. 6 further illustrates a manner to prevent eviction of a cache entry that is configured as directly addressable memory. Each cache entry <b>326</b>, <b>336</b>, <b>346</b> and <b>356</b> has a corresponding set of control bits <b>323</b>, <b>333</b>, <b>343</b> and <b>353</b>. These control bits are known in the art. They typically include an indication of whether the cache entry is valid or invalid, whether the cache entry is clean (not modified) or dirty (modified) and some indication of the least recently used entry. It is typical in the art to replace the least recently used cache entry within the way when replacement is necessary. Upon a cache miss requiring victim eviction, the victim must be from the same set as the set which may store the missed address. It is typical to retain information regarding recent accesses to the cache entries and evict the least recently used cache entry. The preferred embodiment of this invention prevents eviction of a cache entry configured as directly addressable memory. Consider cache entry <b>326</b> when at least some of level two unified memory <b>130</b> is configured as directly addressable memory. In that event SRAM select line <b>324</b> is 1 as previously described. Multiplexer circuit <b>325</b> receives least recently used (LRU) bits from control bits <b>323</b> indicative of the least recently used state of cache entry <b>326</b> at one input. The other input receives a constant 00, which does not indicate a least recently used condition. With SRAM select line <b>324</b> as 1, multiplexer selects the constant input. Even if cache entry <b>326</b> was the least recently used among the cache entries <b>326</b>, <b>336</b>, <b>346</b> and <b>356</b> of the set <b>316</b>, the substitution of the constant at multiplexer <b>325</b> prevents least recently used detect circuit <b>319</b> from indicating this. Instead least recently used circuit indicates another of the cache entries <b>336</b>, <b>346</b> or <b>356</b> is selected as the least recently used entry. Thus another cache entry is replaced upon a cache miss to this set <b>314</b>.</p><p>FIG. 6 illustrates a similar multiplexer <b>335</b>, <b>345</b> and <b>355</b> for cache entries <b>336</b>, <b>346</b> and <b>356</b>, respectively. When the corresponding SRAM select line <b>334</b>, <b>344</b> or <b>354</b> is 1, indicating that the corresponding cache entry <b>336</b>, <b>346</b> or <b>356</b> is configured as directly addressed memory, the corresponding multiplexer <b>335</b>, <b>345</b> and <b>355</b> selects the constant input. Consequently least recently used detector <b>317</b> cannot indicate that cache entry was the least recently used.</p><p>The result of the operation of the circuits illustrated in FIG. 6 is as follows. No cache entry configured as directly addressable memory can cause a cache hit or be detected as the least recently used cache entry of the set for cache entry eviction. If the memory address is within the range of addresses of the directly addressable memory, then the circuits enable bank selection of the addressed cache entry. Those skilled in the art would realize that other circuits could achieve these results. For example, rather than block or enable a bit signal produced by an address compare circuit, it is feasible to disable the address compare circuit. This would also achieve the desired result of preventing generation of a hit signal corresponding to a cache entry configured as directly addressable memory. As a further alternative, zero detect circuit <b>315</b>, bank select circuit <b>316</b> and AND gates <b>322</b>, <b>332</b>, <b>342</b> and <b>352</b> could be eliminated. Instead, tag bits <b>320</b>, <b>330</b>, <b>340</b> and <b>350</b> would be loaded with bits corresponding to the directly addressable memory address. If one way were configured as cache, then tag bits <b>320</b> for each set would be loaded with all 0. When the address is within the address range Hex 00000000 and Hex 00003FFF assigned to directly addressable memory in this case, then the normal cache mechanism would generate a cache hit to cache entry <b>326</b> of the corresponding set <b>314</b>. If two ways were configured as directly addressable memory, then tag bits <b>320</b> would be set as all 0 and tag bits <b>330</b> would be set with bit <b>14</b> as 1 and bits <b>15</b> to <b>31</b> as zero. An address within the address range Hex 00000000 and Hex 00003FFF would generate a cache hit to cache entry <b>326</b> of the corresponding set <b>314</b> and an address within the range Hex 000040000 to Hex 00007FFF would generate a cache hit to entry <b>336</b> of the corresponding set <b>314</b>. Similar tag bits would enable other combinations of configuration as shown in Table 4.</p><p><tables id=\"TABLE-US-00004\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"5\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"56pt\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"42pt\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"35pt\"></colspec><colspec align=\"center\" colname=\"4\" colwidth=\"49pt\"></colspec><colspec align=\"center\" colname=\"5\" colwidth=\"35pt\"></colspec><thead><row><entry nameend=\"5\" namest=\"1\" rowsep=\"1\">TABLE 4</entry></row><row><entry align=\"center\" nameend=\"5\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry>Number of Ways</entry><entry>Tag Bits</entry><entry>Tag Bits</entry><entry>Tag Bits</entry><entry>Tag Bits</entry></row><row><entry>Configured</entry><entry>320</entry><entry>330</entry><entry>340</entry><entry>350</entry></row><row><entry>as SRAM</entry><entry>(15:14)</entry><entry>(15:14)</entry><entry>(15:14)</entry><entry>(15:14)</entry></row><row><entry align=\"center\" nameend=\"5\" namest=\"1\" rowsep=\"1\"></entry></row></thead><tbody valign=\"top\"><row><entry>0</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>1</entry><entry>00</entry><entry>\u2014</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>2</entry><entry>00</entry><entry>01</entry><entry>\u2014</entry><entry>\u2014</entry></row><row><entry>3</entry><entry>00</entry><entry>01</entry><entry>10</entry><entry>\u2014</entry></row><row><entry>4</entry><entry>00</entry><entry>01</entry><entry>10</entry><entry>11</entry></row><row><entry align=\"center\" nameend=\"5\" namest=\"1\" rowsep=\"1\"></entry></row></tbody></tgroup></table></tables></p><p>A mechanism to prevent eviction of a cache entry configured as directly addressable memory would still be required. This could take the form of the least recently used multiplexers <b>325</b>, <b>335</b>, <b>345</b> and <b>355</b> as described above and illustrated in FIG. <b>6</b>.</p><p>FIG. 6 illustrates circuits for the preferred embodiment where memory is configured as cache or directly addressable memory on the basis of cache ways. Those skilled in the art would realize that similar circuits could be used to embody the alternate in which memory is configured on the basis of sets. Bank select circuit <b>316</b> would need to be responsive to address bits within middle bits <b>312</b> to select the set configured as directly addressable memory and make the appropriate bank selection. The AND gates <b>322</b>, <b>332</b>, <b>342</b> and <b>352</b> would be controlled on the basis of sets rather than on the basis of ways as illustrated in FIG. <b>6</b>. Thus only a single SRAM select line would be required for each set. This SRAM select line would block indication of a cache hit based upon the sets configured as directly addressable memory. A similar mechanism would block determination that the corresponding cache entry was the least recently used. Other portions of FIG. 6 would be unchanged.</p><p>FIG. 8 is a block diagram illustrating details of a digital signal processor core suitable for this invention. The digital signal processor core of FIG. 8 is a 32-bit eight-way VLIW pipelined processor. The digital signal processor includes central processing unit <b>1</b>, shown in the right center portion of FIG. <b>8</b>. The digital signal processor includes program memory <b>2</b> which may optionally be used as a program cache. The digital signal processor may also have varying sizes and types of data memory <b>3</b>. The digital signal processor also includes peripherals <b>4</b> to <b>9</b>. These peripherals preferably include an external memory interface (EMIF) <b>4</b> and a direct memory access (DMA) controller <b>5</b>. External memory interface (EMIF) <b>4</b> preferably supports access to supports synchronous and a synchronous SRAM and synchronous DRAM. Direct memory access (DMA) controller <b>5</b> preferably provides 2-channel auto-boot loading direct memory access. These peripherals include power-down logic <b>6</b>. Power-down logic <b>6</b> preferably can halt central processing unit activity, peripheral activity, and phase lock loop (PLL) clock synchronization activity to reduce power consumption. These peripherals also include host ports <b>7</b>, serial ports <b>8</b> and programmable timers <b>9</b>.</p><p>The digital signal processor core has a 32-bit, byte addressable address space. Internal memory on the same integrated circuit is preferably organized in a data space including data memory <b>3</b> and a program space including program memory <b>2</b>. When off-chip memory is used, preferably these two spaces are unified into a single memory space via the external memory interface (EMIF) <b>4</b>.</p><p>Program memory <b>3</b> may be internally accessed by central processing unit <b>1</b> via two internal ports <b>3</b><i>a </i>and <b>3</b><i>b</i>. Each internal port <b>3</b><i>a </i>and <b>3</b><i>b </i>preferably has 32 bits of data and a 32-bit byte address reach. Program memory <b>2</b> may be internally accessed by central processing unit <b>1</b> via a single port <b>2</b><i>a</i>. Port <b>2</b><i>a </i>of program memory <b>2</b> preferably has an instruction-fetch width of 256 bits and a 30-bit word (four bytes) address, equivalent to a 32-bit byte address. Central processing unit <b>1</b> includes program fetch unit <b>10</b>, instruction dispatch unit <b>11</b>, instruction decode unit <b>12</b> and two data paths <b>20</b> and <b>30</b>. First data path <b>20</b> includes four functional units designated L<b>1</b> unit <b>22</b>, S<b>1</b> unit <b>23</b>, M<b>1</b> unit <b>24</b> and D<b>1</b> unit <b>25</b> and <b>16</b> 32-bit registers forming register file <b>21</b>. Second data path <b>30</b> likewise includes four functional units designated L<b>2</b> unit <b>32</b>, S<b>2</b> unit <b>33</b>, M<b>2</b> unit <b>34</b> and D<b>2</b> unit <b>35</b> and <b>16</b> 32-bit registers forming register file <b>31</b>. Central processing unit <b>1</b> includes control registers <b>13</b>, control logic <b>14</b>, and test logic <b>15</b>, emulation logic <b>16</b> and interrupt logic <b>17</b>.</p><p>Program fetch unit <b>10</b>, instruction dispatch unit <b>11</b> and instruction decode <b>12</b> unit recall instructions from program memory <b>2</b> and deliver up to eight 32-bit instructions to the functional units every instruction cycle. Processing occurs in each of the two data paths <b>20</b> and <b>30</b>. As previously described above each data path has four corresponding functional units (L, S, M and D) and a corresponding register file containing 16 32-bit registers. Each functional unit is controlled by a 32-bit instruction. The data paths are further described below. A control register file <b>13</b> provides the means to configure and control various processor operations.</p><p>FIGS. 9A and 9B together illustrate the data paths of central processing unit <b>1</b>. There are two general purpose register files <b>21</b> and <b>31</b>. Each of general purpose register files <b>21</b> and <b>31</b> include 16 32-bit registers. These registers are designated registers A<b>0</b> to A<b>15</b> for register file <b>21</b> and registers B<b>0</b> to B<b>15</b> for register file <b>31</b>. These general purpose registers can be used for data, data address pointers or as condition registers.</p><p>There are eight functional units L<b>1</b> unit <b>22</b>, L<b>2</b> unit <b>32</b>, S<b>1</b> unit <b>23</b>, S<b>2</b> unit <b>33</b>, M<b>1</b> unit <b>24</b>, M<b>2</b> unit <b>34</b>, D<b>1</b> unit <b>25</b> and D<b>2</b> unit <b>35</b>. These eight functional units can be divided into two virtually identical groups of 4 (<b>22</b> to <b>25</b> and <b>32</b> to <b>35</b>) coupled to a corresponding register file. There are four types of functional units designated L, S, M and D. Table 5 lists the functional capabilities of these four types of functional units.</p><p><tables id=\"TABLE-US-00005\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"2\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"70pt\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"147pt\"></colspec><thead><row><entry nameend=\"2\" namest=\"1\" rowsep=\"1\">TABLE 5</entry></row><row><entry align=\"center\" nameend=\"2\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry>Functional Unit</entry><entry>Description</entry></row><row><entry align=\"center\" nameend=\"2\" namest=\"1\" rowsep=\"1\"></entry></row></thead><tbody valign=\"top\"><row><entry>L Unit</entry><entry>32/40-bit arithmetic and compare operations</entry></row><row><entry>(L1, L2)</entry><entry>Left most 1, 0, bit counting for 32 bits</entry></row><row><entry></entry><entry>Normalization count for 32 and 40 bits</entry></row><row><entry></entry><entry>32 bit logical operations</entry></row><row><entry>S Unit</entry><entry>32-bit arithmetic and bit-field operations</entry></row><row><entry>(S1, S2)</entry><entry>32/40 bit shifts</entry></row><row><entry></entry><entry>32 bit logical operations</entry></row><row><entry></entry><entry>Branching</entry></row><row><entry></entry><entry>Constant generation</entry></row><row><entry></entry><entry>Register transfers to/from control register file</entry></row><row><entry>M Unit</entry><entry>16 \u00d7 16 bit multiplies</entry></row><row><entry>(M1, M2)</entry></row><row><entry>D Unit</entry><entry>32-bit add, subtract, linear and circular</entry></row><row><entry>(D1, D2)</entry><entry>address calculation</entry></row><row><entry align=\"center\" nameend=\"2\" namest=\"1\" rowsep=\"1\"></entry></row></tbody></tgroup></table></tables></p><p>Most data lines within central processing unit <b>1</b> support 32-bit operands. Some data lines support long (40-bit) operands. Each functional unit has its own 32-bit write port into the corresponding general-purpose register file. Functional units L<b>1</b> unit <b>22</b>, Si unit <b>23</b>, Ml unit <b>24</b> and D<b>1</b> unit <b>25</b> write to register file <b>21</b>. Functional units L<b>2</b> unit <b>32</b>, S<b>2</b> unit <b>33</b>, M<b>2</b> unit <b>34</b> and D<b>2</b> unit <b>35</b> write to register file <b>31</b>. As depicted in FIG. 9, each functional unit has two 32-bit read ports for respective source operands src<b>1</b> and src<b>2</b> from the corresponding register file. The four functional units L<b>1</b> unit <b>22</b>, L<b>2</b> unit <b>32</b>, Si unit <b>23</b> and S<b>2</b> unit <b>33</b> have an extra 8-bit wide write port for 40-bit long writes as well as an extra 8-bit wide read port for 40-bit long reads. Because each functional unit has its own 32-bit write port, all eight functional units can be used in parallel every cycle.</p><p>FIG. 9A and 9B together illustrate cross register paths <b>1</b>X and <b>2</b>X. Function units L<b>1</b> unit <b>22</b>, S<b>1</b> unit <b>23</b> and M<b>1</b> unit <b>24</b> may receive one operand from register file <b>31</b> via cross register path <b>1</b>x. Function units L<b>2</b> unit <b>32</b>, S<b>2</b> unit <b>33</b> and M<b>2</b> unit <b>34</b> may receive one operand from register file <b>21</b> via cross register path <b>2</b>X. These paths allow the S, M and L units from each data path to access operands from either register file <b>21</b> or <b>31</b>. Four functional units, M<b>1</b> unit <b>24</b>, M<b>2</b> unit <b>34</b>, S<b>1</b> unit <b>23</b> and S<b>2</b> unit <b>33</b>, have one 32-bit input multiplexer which may select either the same side register file or the opposite file via the respective cross path <b>1</b>X or <b>2</b>X. Multiplexer <b>26</b> supplies an operand from either register file <b>21</b> or register file <b>31</b> to the second source input src<b>2</b> of M unit <b>24</b>. Multiplexer <b>36</b> supplies an operand from either register file <b>21</b> or register file <b>31</b> to the second source input src<b>2</b> of M unit <b>34</b>. Multiplexer <b>27</b> supplies an operand from either register file <b>21</b> or register file <b>31</b> to the second source input src<b>2</b> of S unit <b>23</b>. Multiplexer <b>37</b> supplies an operand from either register file <b>21</b> or register file <b>31</b> to the second source input src<b>2</b> of S unit <b>33</b>. Both the 32-bit inputs of function units L<b>1</b> unit <b>22</b> and L<b>2</b> unit <b>32</b> include multiplexers which may select either the corresponding register file or the corresponding cross path. Multiplexer <b>28</b> supplies the first source input src<b>1</b> of L unit <b>22</b> and multiplexer <b>29</b> supplies the second source input src<b>2</b>. Multiplexer <b>38</b> supplies the first source input src<b>1</b> of L unit <b>32</b> and multiplexer <b>39</b> supplies the second source input src<b>2</b>.</p><p>There are two 32-bit paths for loading data from memory to the register file. Data path LD<b>1</b> enables loading register file A and data path LD<b>2</b> enables loading register file B. There are also two 32-bit paths for storing register values to memory from the register file. Data path ST<b>1</b> enables storing data from register file A to memory and data path ST<b>2</b> enables storing data from register file B to memory. These store paths ST<b>1</b> and ST<b>2</b> are shared with the L unit and S unit long read paths.</p><p>FIGS. 9A and 9B together illustrate two data address paths (DA<b>1</b> and DA<b>2</b>) coming from respective D units <b>25</b> and <b>35</b>. These data address paths allow supply of data addresses generated by the D units to specify memory address. D unit <b>25</b> and D unit <b>35</b> each supply one input to address multiplexers <b>41</b> and <b>42</b>. Address multiplexers <b>41</b> and <b>42</b> permit D unit <b>25</b> to support loads from memory to either register file <b>21</b> or register file <b>31</b> and to support stores from either register file <b>21</b> or register file <b>31</b> to memory. Address multiplexers <b>41</b> and <b>42</b> likewise permit D unit <b>35</b> to support loads and stores involving either register file <b>21</b> or register file <b>31</b>.</p><p>FIG. 9B illustrates data paths enabling S<b>2</b> unit <b>33</b> to read from and to write to the control register file <b>13</b>.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Sanjive", "last_name": "Agarwala", "name": ""}, {"first_name": "Charles L.", "last_name": "Fuoco", "name": ""}, {"first_name": "David A.", "last_name": "Comisky", "name": ""}, {"first_name": "Timothy D.", "last_name": "Anderson", "name": ""}, {"first_name": "Christopher L.", "last_name": "Mobley", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "TEXAS INSTRUMENTS INCORPORATED"}, {"first_name": "", "last_name": "TEXAS INSTRUMENTS INCORPORATED", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/12        20060101A I20051008RMEP"}, {"label": "G06F  12/08        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711129"}, {"primary": false, "label": "711146"}, {"primary": false, "label": "711173"}, {"primary": false, "label": "711E12075"}, {"primary": false, "label": "711E12045"}, {"primary": false, "label": "711122"}, {"primary": false, "label": "711170"}], "ecla_classes": [{"label": "S06F212:601"}, {"label": "S06F212:2515"}, {"label": "G06F  12/08B6M"}, {"label": "G06F  12/12B6"}, {"label": "S06F12:08B10"}], "cpc_classes": [{"label": "G06F2212/601"}, {"label": "G06F2212/2515"}, {"label": "G06F  12/0864"}, {"label": "G06F2212/601"}, {"label": "G06F  12/0864"}, {"label": "G06F  12/0846"}, {"label": "G06F  12/126"}, {"label": "G06F2212/2515"}, {"label": "G06F  12/126"}, {"label": "G06F  12/0846"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1999-07-15", "application_date": "2000-06-26", "family_members": [{"ucid": "US-6606686-B1", "titles": [{"lang": "EN", "text": "Unified memory system architecture including cache and directly addressable static random access memory"}]}]}