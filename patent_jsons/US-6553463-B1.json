{"patent_number": "US-6553463-B1", "publication_id": 73376330, "family_id": 23734489, "publication_date": "2003-04-22", "titles": [{"lang": "EN", "text": "Method and system for high speed access to a banked cache memory"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50480323\"><p>A method and system for high speed data access of a banked cache memory. In accordance with the method and system of the present invention, during a first cycle, in response to receipt of a request address at an access controller, the request address is speculatively transmitted to a banked cache memory, where the speculative transmission has at least one cycle of latency. Concurrently, the request address is snooped in a directory associated with the banked cache memory. Thereafter, during a second cycle the speculatively transmitted request address is distributed to each of multiple banks of memory within the banked cache memory. In addition, the banked cache memory is provided with a bank indication indicating which bank of memory among the multiple banks of memory contains the request address, in response to a bank hit from snooping the directory. Thereafter, data associated with the request address is output from the banked cache memory, in response to the bank indication, such that access time to a high latency remote banked cache memory is minimized.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00001\" num=\"1\"><claim-text>1. A method for high speed data access to a banked cache memory, said method comprising:</claim-text><claim-text>during a first cycle, in response to receipt of a request address at an access controller: </claim-text><claim-text>speculatively transmitting said request address to a banked cache memory, wherein </claim-text><claim-text>said speculative transmission has at least one cycle of latency; and </claim-text><claim-text>concurrently snooping said request address in a directory associated with said banked cache memory; </claim-text><claim-text>thereafter, during a second cycle: </claim-text><claim-text>distributing said speculatively transmitted request address to each of a plurality of banks of memory within said banked cache memory; </claim-text><claim-text>in response to a bank hit from snooping said directory, providing said banked cache memory with a bank indication indicating which bank of memory among said plurality of banks of memory contains a storage location identified by said request address; and </claim-text><claim-text>outputting data associated with said request address from said banked cache memory, in response to said bank indication, such that access time to a high latency remote banked cache memory is minimized. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00002\" num=\"2\"><claim-text>2. The method for high speed access to a banked cache memory according to <claim-ref idref=\"US-6553463-B1-CLM-00001\">claim 1</claim-ref>, wherein said step of speculatively transmitting said request address further comprises speculatively transmitting an n-bit request address and a valid address signal.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00003\" num=\"3\"><claim-text>3. The method for high speed access to a banked cache memory according to <claim-ref idref=\"US-6553463-B1-CLM-00001\">claim 1</claim-ref>, wherein said step of speculatively transmitting said request address further comprises:</claim-text><claim-text>utilizing a driver to drive said request address from a first latch within said access controller to a receiver within said banked cache memory; and </claim-text><claim-text>utilizing said receiver to drive said request address to a second latch within said banked cache memory. </claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00004\" num=\"4\"><claim-text>4. The method for high speed access to a banked cache memory according to <claim-ref idref=\"US-6553463-B1-CLM-00001\">claim 1</claim-ref>, wherein said step of speculatively transmitting said request address further comprises speculatively transmitting said request address to a banked DRAM cache.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00005\" num=\"5\"><claim-text>5. The method for high speed access to a banked cache memory according to <claim-ref idref=\"US-6553463-B1-CLM-00001\">claim 1</claim-ref>, wherein said step of speculatively transmitting said request address further comprises speculatively transmitting said request address to an off-chip banked embedded DRAM cache.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00006\" num=\"6\"><claim-text>6. The method for high speed access to a banked cache memory according to <claim-ref idref=\"US-6553463-B1-CLM-00001\">claim 1</claim-ref>, wherein said step of concurrently snooping said request address in a directory further comprises snooping a directory coupled to said access controller, wherein said directory contains an address array for said banked cache memory.</claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00007\" num=\"7\"><claim-text>7. The method for high speed access to a banked cache memory according to <claim-ref idref=\"US-6553463-B1-CLM-00006\">claim 6</claim-ref>, wherein said step of snooping a directory further comprises snooping a directory embedded within an integrated circuit.</claim-text></claim>"}, {"num": 8, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00008\" num=\"8\"><claim-text>8. The method for high speed access to a banked cache memory according to <claim-ref idref=\"US-6553463-B1-CLM-00001\">claim 1</claim-ref>, wherein said step of distributing said speculatively transmitted request address further comprises placing said speculatively transmitted request address on a dedicated high-level circuit path which directs said request address to each of said plurality of banks of memory.</claim-text></claim>"}, {"num": 9, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00009\" num=\"9\"><claim-text>9. The method for high speed access to a banked cache memory according to <claim-ref idref=\"US-6553463-B1-CLM-00001\">claim 1</claim-ref>, wherein said step of providing said banked cache with a bank indication further comprises:</claim-text><claim-text>driving said bank indication from said directory to a latch within said access controller; and </claim-text><claim-text>driving said bank indication with a driver from said latch within said access controller to a receiver within said banked cache memory, wherein said receiver drives said bank indication to each of said plurality of banks of memory to indicate which bank of said plurality of banks of memory contains a storage location identified by said request address. </claim-text></claim>"}, {"num": 10, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00010\" num=\"10\"><claim-text>10. The method for high speed access to a banked cache memory according to <claim-ref idref=\"US-6553463-B1-CLM-00001\">claim 1</claim-ref>, wherein said step of outputting data associated with said requested address from said banked cache memory further comprises:</claim-text><claim-text>accessing said bank of memory indicated by said bank indication with said request address only if a queue associated with said indicated bank of memory is empty; and </claim-text><claim-text>storing said request address in said queue for access in a subsequent cycle, in response to providing a bank indication for a bank of memory which does not have an empty queue. </claim-text></claim>"}, {"num": 11, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00011\" num=\"11\"><claim-text>11. The method for high speed access to a banked cache memory according to <claim-ref idref=\"US-6553463-B1-CLM-00001\">claim 1</claim-ref>, wherein said step of outputting data associated with said requested address from said banked cache memory further comprises accessing said bank of memory indicated by said bank indication with said request address only when said bank of memory is power refreshed.</claim-text></claim>"}, {"num": 12, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00012\" num=\"12\"><claim-text>12. A system, said system comprising:</claim-text><claim-text>a banked cache memory, wherein said banked cache memory [comprises] includes a plurality of memory banks; </claim-text><claim-text>a directory associated with said banked cache memory; </claim-text><claim-text>an access controller, wherein during a first cycle said access controller speculatively transmits any request received at said processor system to said banked cache memory while concurrently snooping said request in said directory, and wherein during a second cycle said access controller provides said banked cache memory with a bank indication indicating which memory bank among said plurality of memory banks contains a storage location identified by said request address; and </claim-text><claim-text>a banked cache controller for controlling said banked cache memory, wherein during said second cycle said banked cache controller distributes said speculatively transmitted request address to each of a plurality of memory banks within said banked cache memory along a low latency path. </claim-text></claim>"}, {"num": 13, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00013\" num=\"13\"><claim-text>13. The system according to <claim-ref idref=\"US-6553463-B1-CLM-00012\">claim 12</claim-ref>, said access controller further comprising:</claim-text><claim-text>control means for speculatively transmitting an n-bit request address and a valid address signal. </claim-text></claim>"}, {"num": 14, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00014\" num=\"14\"><claim-text>14. The system according to <claim-ref idref=\"US-6553463-B1-CLM-00012\">claim 12</claim-ref>, said system further comprising:</claim-text><claim-text>a driver within said access controller to drive said request address from a first latch within said access controller to said banked cache controller; and </claim-text><claim-text>a receiver coupled to said driver for driving said request address to a second latch and said low latency path within said banked cache controller. </claim-text></claim>"}, {"num": 15, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00015\" num=\"15\"><claim-text>15. The system according to <claim-ref idref=\"US-6553463-B1-CLM-00012\">claim 12</claim-ref>, wherein said banked cache memory comprises a banked DRAM cache.</claim-text></claim>"}, {"num": 16, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00016\" num=\"16\"><claim-text>16. The system according to <claim-ref idref=\"US-6553463-B1-CLM-00012\">claim 12</claim-ref>, wherein said system comprises a first integrated circuit containing said access controller and a processor core and a second integrated circuit containing said banked cache memory.</claim-text></claim>"}, {"num": 17, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00017\" num=\"17\"><claim-text>17. The system according to <claim-ref idref=\"US-6553463-B1-CLM-00012\">claim 12</claim-ref>, wherein said directory further comprises an address array for said banked cache memory.</claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00018\" num=\"18\"><claim-text>18. The system according to <claim-ref idref=\"US-6553463-B1-CLM-00017\">claim 17</claim-ref>, wherein said system comprises an integrated circuit, separate from said banked cache memory, that includes a processor core and said directory.</claim-text></claim>"}, {"num": 19, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00019\" num=\"19\"><claim-text>19. The system according to <claim-ref idref=\"US-6553463-B1-CLM-00012\">claim 12</claim-ref>, wherein said low latency path comprises a dedicated high-level circuit path which directs said request address to each of said plurality of banks of memory.</claim-text></claim>"}, {"num": 20, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00020\" num=\"20\"><claim-text>20. The system according to <claim-ref idref=\"US-6553463-B1-CLM-00012\">claim 12</claim-ref>, wherein said banked cache controller further comprises:</claim-text><claim-text>first control means within said banked cache controller for permitting access to said bank of memory indicated by said bank indication with said request address only if a queue associated with said indicated bank is empty; and </claim-text><claim-text>second control means within said banked cache controller for storing said request address in said queue for access in a subsequent cycle if said queue associated with said indicated bank of memory is not empty. </claim-text></claim>"}, {"num": 21, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6553463-B1-CLM-00021\" num=\"21\"><claim-text>21. The system according to <claim-ref idref=\"US-6553463-B1-CLM-00012\">claim 12</claim-ref>, wherein said banked cache controller further comprises:</claim-text><claim-text>control means for permitting access to said bank of memory indicated by said bank indication only when said bank of memory is power refreshed.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53910540\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Technical Field</p><p>The present invention relates in general to an improved method and system for accessing cache memory within a data processing system, and in particular, to an improved method and system for high speed access to a banked cache memory. Still more particularly, the present invention relates to an improved method and system for high speed access to a high latency remote banked cache memory wherein the access time to the cache memory is minimized.</p><p>2. Description of the Related Art</p><p>A digital computer system utilizes a central processing unit and a computer main memory to perform a wide range of functions. Such circuitry permits repetitive functions to be carried out at rates much higher than if the corresponding functions could be performed manually. Memory locations provide storage from which data can be read or to which data can be written.</p><p>As technology develops, multiple processors and multiple levels of memory and cache have been added to digital computer systems. In addition, the utilization of on-chip and off-chip cache continues to increase processing capabilities. In general, requests come from a system bus to a processor system that includes a processor core and multiple levels of on-chip and off-chip cache. The processor system then performs a snoop to detect whether the requested data is available in the on-chip cache. In addition, the processor system typically snoops an off-chip cache associated therewith. From the snoop return, the request may be directed to a particular cache in order to access the requested data.</p><p>Several types of memory have been developed which may be utilized as on-chip cache, off-chip cache and/or main memory. These random access memories (RAM) are preferably semi-conductor based memory that can be read from and written to by the central processing unit and other hardware devices. The storage locations within RAM can be accessed in any order. For example, one type of RAM which is well known in the art is a dynamic RAM (DRAM). Dynamic RAM is typically utilized for storing large increments of data. In particular, DRAMs store information in integrated circuits containing capacitors. Because capacitors lose their charge over time, DRAM circuits typically include logic to refresh the DRAM chips continuously. While a DRAM chip is being refreshed, the chip cannot be read by the processor, which leads to wait states while the DRAM chips is being refreshed. Another type of RAM which is well known in the art is static RAM (SRAM). SRAMs store information in logic circuits known as flip-flops, which retain information as long as there is enough power to run the device. SRAMs do not have the delay states inherent in DRAMs, however SRAM circuitry is more complex than DRAM circuitry and is typically utilized in smaller increments.</p><p>In general, memory devices such as SRAM and DRAM are formed in memory locations which form memory arrays. The memory locations of the memory arrays are identified by memory addresses. When memory locations of a memory array are to be accessed, the addresses of the memory locations are provided to decoder circuitry of the memory device, as is well known in the art. The decoder circuitry decodes the address signals applied thereto to permit access to the memory locations identified by the address signals. Typically, multiple banks of SRAM or DRAM may be placed together whereby a controller controls access to each bank of memory and routes addresses to the proper bank of memory within the banked cache memory.</p><p>In a recent configuration of processor/memory devices, a processor accesses an on-chip level-one (L1) cache which comprises small, fast SRAM, an on-chip level-two (L2) cache which comprises banked SRAM and an off-chip level-three (L3) cache which comprises banked DRAM cache. In addition, the processor may access a main memory which is shared among multiple devices. There is a greater latency inherent in accessing data from off-chip memories than from on-chip memories. However, off-chip memories are typically larger than on-chip memories and thus can provide large amounts of data for a single access. Among the off-chip memories, a processor can access an L3 cache much more quickly than a main memory, particularly when the main memory is shared by multiple processors.</p><p>Several methods have been developed to reduce the latency inherent in accessing a remote L3 cache and in accessing banked DRAM cache to determine which bank of memory to access next. According to one method known as bank parking, for each request from a system bus, a speculative access to the bank of memory that was previously accessed is made. By this method, speculative access is only beneficial if the bank of memory that was previously accessed is requested again. According to another method known as redundant access, each bank of memory is a clone of all other banks of memory. Therefore, for each request, the same address is passed to each bank of memory whereby any available banks of memory can respond to the request. In order for redundant access to be beneficial, a small number of banks of memory and small amount of cache in each bank of memory are utilized. However, it is preferable to utilize a large number of banks and a large amount of cache in each bank for an L3 cache.</p><p>In view of the foregoing, it is therefore desirable to provide a method of accessing a high latency banked cache memory and in particular accessing off-chip banked DRAM cache from a processor, whereby fast access to the cache is provided.</p><h4>SUMMARY OF THE INVENTION</h4><p>In view of the foregoing, it is therefore an object of the present invention to provide an improved method and system for accessing cache memory within a data processing system.</p><p>It is another object of the present invention to provide an improved method and system for high speed access to a banked cache memory.</p><p>It is yet another object of the present invention to provide an improved method and system for high speed access to a high latency remote banked cache memory wherein the access time to the cache is minimized.</p><p>In accordance with the method and system of the present invention, during a first cycle, in response to receipt of a request address at an access controller, the request address is speculatively transmitted to a banked cache memory, where the speculative transmission has at least one cycle of latency. Concurrently, the request address is snooped in a directory associated with the banked cache memory. Thereafter, during a second cycle the speculatively transmitted request address is distributed to each of multiple banks of memory within the banked cache memory. In addition, the banked cache memory is provided with a bank indication indicating which bank of memory among the multiple banks of memory contains the request address, in response to a bank hit from snooping the directory. Thereafter, data associated with the request address is output from the banked cache memory, in response to the bank indication, such that access time to a high latency remote banked cache memory is minimized.</p><p>All objects, features, and advantages of the present invention will become apparent in the following detailed written description.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>DESCRIPTION OF THE DRAWINGS</h4><p>The invention itself, as well as a preferred mode of use, further objects, and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein:</p><p>FIG. 1 depicts a high-level block diagram of a generic data processing system having multiple levels of accessible cache;</p><p>FIG. 2 illustrates a detailed block diagram of the processor and multi-level cache depicted in FIG. 1; and</p><p>FIG. 3 depicts a timing diagram for accessing an address request in accordance with the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF A PREFERRED EMBODIMENT</h4><p>With reference now to the figures and in particular with reference now to FIG. 1, there is depicted a high-level block diagram of a generic data processing system having multiple levels of accessible cache. A data processing system <b>10</b> is illustrated including a processor core <b>12</b> with access to data stored in a level-one (L1) cache <b>14</b> within processor core <b>12</b> and a level-two (L2) cache <b>16</b>. L1 cache is preferably a small memory cache built into processor core <b>12</b> which provides for low latency memory access. L2 cache is preferably a memory cache which is larger than L1 cache and may consist of static random access memory (SRAM).</p><p>An access controller <b>18</b> directs address requests and data within said data processing system <b>10</b>. A bus interface <b>20</b> interfaces with a system interconnect <b>28</b> to control the flow of data and addresses between access controller <b>18</b> and system interconnect <b>28</b>. System interconnect <b>28</b> provides multiple buses for transporting data and addresses between components within a data processing system. While not depicted, multiple processors with associated levels of cache may be connected along system interconnect <b>28</b> within data processing system <b>10</b>.</p><p>Access controller <b>18</b> also directs address requests and data to an L3 controller <b>26</b> within level-three (L3) banked cache <b>24</b> by an L3 interconnect <b>25</b>. L3 controller <b>26</b> controls access to each of banks of memory <b>28</b><i>a</i>-<b>28</b><i>h</i>. L3 interconnect <b>25</b> preferably includes multiple buses by which addresses and data may be transmitted between access controller <b>18</b> and L3 controller <b>26</b>. In particular, the present invention depicts a banked dynamic random access memory (DRAM) cache as banked cache <b>24</b>, however in alternate embodiments, other forms of banked cache may be utilized. Banked cache <b>24</b> includes multiple banks <b>28</b><i>a</i>-<b>28</b><i>h</i>, wherein the total cache memory provided by banked cache <b>24</b> is substantially larger than that provided by L2 cache <b>16</b>. For example, L2 cache <b>16</b> may comprise 2 MB of SRAM while banked cache <b>24</b> comprises 16 MB of DRAM. However, access to L2 cache <b>16</b> is typically lower in latency than access to banked cache <b>24</b> since L2 cache <b>16</b> typically resides on-chip.</p><p>A snoop path from access controller <b>18</b> to each of the caches is provided whereby for an address request received at access controller <b>18</b>, a snoop of each of the caches is performed to determine if the address request will hit in any of the caches. In particular, in snooping the cache, access controller <b>18</b> snoops an L3 directory <b>22</b> and concurrently speculatively transmits the address request to banked cache <b>24</b>. Once the address request is received at L3 controller <b>26</b>, the address request is passed to each of banks <b>28</b><i>a</i>-<b>28</b><i>h</i>. If a bank hit is returned from snooping L3 directory <b>22</b>, access controller <b>18</b> transmits a bank indicator to L3 controller <b>26</b> indicating which bank contains said address request, whereby the address request may be accessed.</p><p>If there is not a hit in the cache returned from the snoop, access controller <b>18</b> may send the address request to a main memory (not shown), wherein the address request is received by a main memory controller (not shown). Typically, accesses to a main memory are slow because the main memory is shared by multiple devices. However, by utilizing an L3 cache, such as L3 banked cache <b>24</b>, large amounts of data can be accessed for each cycle, thus encouraging a large L3 banked cache memory.</p><p>Referring now to FIG. 2, there is illustrated a detailed block diagram of the processor and multi-level cache depicted in FIG. <b>1</b>. As previously described, when access controller <b>18</b> receives an address request, address controller <b>18</b> speculatively transmits the address request to L3 controller <b>26</b> of banked cache <b>24</b>. In particular, a driver <b>40</b> drives the address request signal from a latch interface <b>42</b> within access controller <b>18</b> to a receiver <b>44</b> which drives the address request signal to latch interface <b>46</b> within L3 controller <b>26</b>.</p><p>Inherently, transmitting the address request from latch interface <b>42</b> to latch interface <b>46</b> requires at least one clock cycle. Therefore, for example, at a first clock cycle an address request is latched in latch interface <b>42</b> and at a second clock cycle an address request is latched in latch interface <b>46</b>. The address request in latch interface <b>46</b> is distributed to multiple control logic <b>70</b><i>a</i>-<b>70</b><i>h </i>for each of banks <b>28</b><i>a</i>-<b>28</b><i>h </i>over a fast path high-level metal interconnect <b>48</b> within L3 controller <b>26</b> during the second clock cycle. In particular, high-level metal interconnect <b>48</b> is a high-level interconnection within banked cache <b>24</b> and provides a low latency path by which the address request is distributed.</p><p>With reference now to FIG. 3, there is depicted a timing diagram for accessing an address request in accordance with the present invention. A clock cycle is depicted at reference numeral <b>92</b> for the timing diagram whereby clock cycles are determined. In the example, during a first clock cycle, an address request is received at access controller <b>18</b> as illustrated at reference numeral <b>80</b>. During a second clock cycle, the address and data valid signal are speculatively transmitted, as depicted at reference numeral <b>82</b>. Thereafter, during a third clock cycle, the address and data valid signal are distributed along metal interconnect fast path <b>48</b> to each of the banks, as depicted at reference numeral <b>86</b>.</p><p>Referring again to FIG. 2, as previously described, during a first clock cycle, access controller <b>18</b> snoops the L3 directory with an address request. When access controller <b>18</b> receives a bank hit from snooping the L3 directory, bank indication data is sent to L3 controller <b>26</b>, whereby the bank containing the address request may be accessed. In particular, a driver <b>50</b> drives the address request from a latch interface <b>52</b> within access controller <b>18</b> to a receiver <b>54</b> which drives the address request to control logic <b>70</b><i>a</i>-<b>70</b><i>h </i>for each of banks <b>28</b><i>a</i>-<b>28</b><i>h </i>during a second clock cycle. Thereby, both the address request signal and the bank indicator signal arrive at control logic <b>70</b><i>a</i>-<b>70</b><i>h </i>for each of banks <b>28</b><i>a</i>-<b>28</b><i>h </i>during a single clock cycle.</p><p>In particular, for eight banks, an 8-bit bank indicator signal is driven, 1-bit per bank, to each of banks <b>28</b><i>a</i>-<b>28</b><i>h</i>. The 8-bit indicator signal specifies in which bank a hit may have occurred. Within each control logic <b>70</b><i>a</i>-<b>70</b><i>h </i>a logic gate <b>60</b> ANDs the corresponding bit of the 8-bit bank indicator with a read queue empty signal. Each control logic <b>70</b><i>a</i>-<b>70</b><i>h </i>includes a read queue <b>66</b>, or other data structure for storing address requests which are waiting to be accessed. If read queue <b>66</b> is empty, then the read queue empty signal is high. Therefore a fast path read is indicated only when read queue <b>66</b> is empty.</p><p>With reference again to FIG. 3, during a second clock cycle, the address request is snooped in the caches, including the L3 directory, as depicted at reference numeral <b>84</b>. During a third clock cycle, the snoop returns a bank indication signal that is sent to access controller <b>18</b>, as illustrated at reference numeral <b>88</b>. Thereby, the address request is available at each bank and the bank indicator signal determines which bank can access the address request. Preferably, during a fourth cycle, the indicated bank is accessed with the address request and data is read, as depicted at reference numeral <b>90</b>.</p><p>Referring again to FIG. 2, L3 controller <b>26</b> utilizes control logic <b>70</b><i>a</i>-<b>70</b><i>h </i>to control access to each of banks <b>28</b><i>a</i>-<b>28</b><i>h</i>. A pass-gate multiplexer <b>62</b><i>a </i>selects between address requests for input to bank <b>28</b><i>a </i>as a read access. In particular, pass-gate multiplexer <b>62</b><i>a </i>selects an address request from high-level metal interconnect <b>48</b> when a control input is \u201c1\u201d and an address request from read queue <b>66</b><i>a </i>when a control input is \u201c0\u201d. In the present embodiment, if the ANDing of the lowest bit of the bank indicator signal and the read queue empty signals is high, then a pass-gate multiplexer <b>62</b><i>a </i>receives a control input of \u201c1\u201d. Alternatively, in the present embodiment, if the ANDing of the lowest bit of the bank indicator signal and the read queue empty signal is low, then pass-gate multiplexer <b>62</b><i>a </i>receives a control input of \u201c0\u201d.</p><p>A logic gate <b>64</b><i>a </i>ANDs the inverse of the read queue empty signal, the lowest bit of the bank indicator signal and the address request signal from high-level metal interconnect <b>48</b>. Thereby, when read queue <b>66</b><i>a </i>is not empty and the lowest bit of the bank indicator signal indicates a hit, the address request signal is transferred to read queue <b>66</b><i>a</i>. Thereby, each of the queued address request signals may be sent as a read access when the corresponding bank is available. In particular, because the power applied to DRAM caches must be refreshed periodically, each bank of banked cache <b>24</b> may not be available when an address request is received with a bank indicator indicating a hit in that bank. Therefore, read queue <b>66</b><i>a </i>queues address requests when a bank is not available due to power refresh.</p><p>As has been described, the present invention provides an improved method and system for high speed data access to a remote banked DRAM cache from a processor. However, in alternate embodiments of the present invention, other high speed access to other types of banked cache may be provided. In addition, in alternate embodiments, the banked cache may be on-chip with the processor core. Further, while one type of control logic is depicted for controlling access to each bank of the banked DRAM cache, in alternate embodiments, other types of control logic may be utilized, as will be apparent to one skilled in the art.</p><p>While the invention has been particularly shown and described with reference to a preferred embodiment, it will be understood by those skilled in the art that various changes in from and detail may be made therein without departing from the spirit and scope of the invention.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Lakshminarayana Baba", "last_name": "Arimilli", "name": ""}, {"first_name": "Ravi Kumar", "last_name": "Arimilli", "name": ""}, {"first_name": "James Stephen", "last_name": "Fields, Jr.", "name": ""}, {"first_name": "Sanjeev", "last_name": "Ghai", "name": ""}, {"first_name": "Praveen S.", "last_name": "Reddy", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTERNATIONAL BUSINESS MACHINES CORPORATION"}, {"first_name": "", "last_name": "INTERNATIONAL BUSINESS MACHINES CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711146"}, {"primary": false, "label": "711E12018"}, {"primary": false, "label": "711137"}, {"primary": false, "label": "711167"}, {"primary": false, "label": "711124"}, {"primary": false, "label": "711005"}, {"primary": false, "label": "711105"}, {"primary": false, "label": "711E12045"}], "ecla_classes": [{"label": "S06F212:6082"}, {"label": "G06F  12/08B10"}, {"label": "G06F  12/08B6M"}], "cpc_classes": [{"label": "G06F  12/0864"}, {"label": "G06F2212/6082"}, {"label": "G06F  12/0846"}, {"label": "G06F2212/6082"}, {"label": "G06F  12/0846"}, {"label": "G06F  12/0864"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1999-11-09", "application_date": "1999-11-09", "family_members": [{"ucid": "US-6553463-B1", "titles": [{"lang": "EN", "text": "Method and system for high speed access to a banked cache memory"}]}]}