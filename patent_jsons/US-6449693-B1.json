{"patent_number": "US-6449693-B1", "publication_id": 73129657, "family_id": 23099823, "publication_date": "2002-09-10", "titles": [{"lang": "EN", "text": "Method and apparatus for improving caching within a processor system"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50375855\"><p>A processor system is provided that comprises a plurality of L0 caches, a processor having a plurality of execution units, and an L1 cache for caching any data and instructions used by the processor. A portion of the execution units provided are configured so that each execution unit within the portion accesses one of the L0 caches. Each of the L0 caches is accessible by only one of the portion of the execution units, and each L0 cache caches a subset of any data used by the processor which is not cacheable by any of the other L0 caches. The processor system preferably comprises an instruction dispatcher that dispatches instructions executable by the processor and that selectively designates data as cacheable by only one of the L0 caches, preferably at dispatch time.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00001\" num=\"1\"><claim-text>1. A processor system comprising:</claim-text><claim-text>an L1 cache; </claim-text><claim-text>a plurality of L0 caches; </claim-text><claim-text>a processor comprising a plurality of execution units, a portion of the execution units each for accessing one of the L0 caches; </claim-text><claim-text>the L1 cache for caching any data and instructions to be used by the processor; </claim-text><claim-text>each of the L0 caches accessible by only one of the portion of execution units, the L0 caches each for caching a subset of said any data to be used by the processor which is not cacheable by any others of the L0 caches; and </claim-text><claim-text>an additional L0 cache and a requestor for accessing the additional L0 cache, the additional L0 cache capable of caching a subset of said any data to be used by the processor which is not cacheable by any others of the L0 caches and which is accessible in the additional L0 cache only by said requestor. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00002\" num=\"2\"><claim-text>2. The system of <claim-ref idref=\"US-6449693-B1-CLM-00001\">claim 1</claim-ref> further comprising an instruction dispatcher for dispatching instructions executable by the processor and for selectively designating data as cacheable by only one of the L0 caches.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00003\" num=\"3\"><claim-text>3. The system of <claim-ref idref=\"US-6449693-B1-CLM-00002\">claim 2</claim-ref> wherein the instruction dispatcher includes means for selectively designating the data at dispatch time.</claim-text></claim>"}, {"num": 4, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00004\" num=\"4\"><claim-text>4. The system of <claim-ref idref=\"US-6449693-B1-CLM-00002\">claim 2</claim-ref> wherein the dispatcher receives a linear (effective) address of data and includes means for selectively designating the data based on the linear address.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00005\" num=\"5\"><claim-text>5. The system of <claim-ref idref=\"US-6449693-B1-CLM-00001\">claim 1</claim-ref> wherein the portion of execution units comprise load/store units.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00006\" num=\"6\"><claim-text>6. The system of <claim-ref idref=\"US-6449693-B1-CLM-00001\">claim 1</claim-ref> wherein the L1 cache comprises a 64 k, 4-2ay set associative cache having 32-byte lines.</claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00007\" num=\"7\"><claim-text>7. The system of <claim-ref idref=\"US-6449693-B1-CLM-00006\">claim 6</claim-ref> wherein each L0 cache comprises a 4 k, 2-way set associative cache having 32-byte lines.</claim-text></claim>"}, {"num": 8, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00008\" num=\"8\"><claim-text>8. A processor system comprising:</claim-text><claim-text>an L1 cache; </claim-text><claim-text>a plurality of L0 caches; </claim-text><claim-text>a processor comprising a plurality of execution units, a portion of the execution units each for accessing one of the L0 caches; </claim-text><claim-text>the L1 cache for caching any data and instructions to be used by the processor; </claim-text><claim-text>each of the L0 caches accessible by only one of the portion of execution units, the L0 caches each for caching a subset of said any data to be used by the processor which is not cacheable by any others of the L0 caches; and </claim-text><claim-text>an instruction dispatcher comprises an instruction dispatch circuit for designating data as cacheable by only one of the L0 caches based on a portion of a linear address for the data. </claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00009\" num=\"9\"><claim-text>9. The system of <claim-ref idref=\"US-6449693-B1-CLM-00008\">claim 8</claim-ref> wherein the instruction dispatch circuit comprises a mechanism for designating data as cacheable by only one of the L0 caches based on a portion of cache line bits of the linear address for the data.</claim-text></claim>"}, {"num": 10, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00010\" num=\"10\"><claim-text>10. The system of <claim-ref idref=\"US-6449693-B1-CLM-00008\">claim 8</claim-ref> wherein the instruction dispatch circuit comprises a programmable dispatch select register for selecting which portion of the linear address is employed to designate data as cacheable by only one of the L0 caches.</claim-text></claim>"}, {"num": 11, "parent": 10, "type": "dependent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00011\" num=\"11\"><claim-text>11. The system of <claim-ref idref=\"US-6449693-B1-CLM-00010\">claim 10</claim-ref> wherein the instruction dispatch circuit comprises:</claim-text><claim-text>an AND gate having a first input for receiving a portion of the linear address, a second input coupled to the programmable dispatch select register for receiving selection bits therefrom, and an output; and </claim-text><claim-text>a zero detect circuit having an input coupled to the output of the AND gate, the zero detect circuit for outputting a first logic state if the output of the AND gate is a low voltage and for outputting a second logic state if the output of the AND gate is a high voltage; </claim-text><claim-text>wherein the instruction dispatcher designates data as cacheable by a first L0 cache in response to the first logic state and designates data as cacheable by a second L0 cache in response to the second logic state. </claim-text></claim>"}, {"num": 12, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00012\" num=\"12\"><claim-text>12. The system of <claim-ref idref=\"US-6449693-B1-CLM-00008\">claim 8</claim-ref> further comprising a mask and merge circuit for removing at least one bit from the linear address used to designate data as cacheable by an L0 cache before the L0 cache is accessed via the linear address.</claim-text></claim>"}, {"num": 13, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00013\" num=\"13\"><claim-text>13. A method of reducing the average access time to memory operands in a processor system having an L1 cache for caching data and instructions and a plurality of execution units, the method comprising:</claim-text><claim-text>providing a plurality of L0 caches; </claim-text><claim-text>coupling each L0 cache to a different one of the execution units; </claim-text><claim-text>providing an instruction dispatcher for dispatching instructions to the execution units and for selectively designating data as cacheable by only one of the L0 caches; and </claim-text><claim-text>employing the instruction dispatcher to dispatch instructions to the execution units and to selectively designate data as cacheable by only one of the L0 caches based on a linear address of the data. </claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6449693-B1-CLM-00014\" num=\"14\"><claim-text>14. The method of <claim-ref idref=\"US-6449693-B1-CLM-00013\">claim 13</claim-ref> further comprising removing at least one bit from the linear address used to designate data as cacheable by an L0 cache before the L0 cache is accessed via the linear address.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53630808\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>FIELD OF THE INVENTION</h4><p>The present invention relates to processor systems and more specifically to a method and apparatus for improving caching within a processor system.</p><h4>BACKGROUND OF THE INVENTION</h4><p>Typical processor designs include an on-chip, \u201clevel-1\u201d cache (\u201cL1 cache\u201d) for fast access to the contents (e.g., data or instructions, hereinafter \u201cinformation\u201d) of the most recently used memory locations. Many processors can access and use L1 cache contents in a single central processing unit (CPU) cycle (hereinafter \u201ccycle\u201d) rather than in the two or more cycles required for accessing an off-chip, \u201clevel-2\u201d cache (\u201cL2 cache\u201d). Access to the contents of system memory requires even more cycles.</p><p>Recent advances in semiconductor manufacturing technologies and processor design techniques have produced highly complex CPU microarchitectures coupled with large L1 caches that improve many aspects of CPU performance (e.g., processor speed). However, increased L1 cache size has rendered single-cycle L1 cache access difficult. For example, as a cache's size is increased, additional address bits from the address are required to directly access the information stored within the cache, and a larger decoder is required to decode the additional address bits. A larger decoder is inherently slower than a smaller decoder due to additional gate delays in the decode path of the larger decoder, and due to additional loading of each address line that drives an input of the larger decoder. Thus, a larger L1 cache has a longer decode time than a smaller L1 cache.</p><p>One technique for reducing the increased decode delay of a larger L1 cache is to increase the cache's associativity (e.g., the number of lines per cache row). For example, a 64 kilobyte (\u201cK\u201d), eight-way set associative cache with 32-byte lines stores eight 32-byte lines per cache row (e.g., in eight different \u201carray cells\u201d) for a total of 256 bytes per cache row, and 256 cache rows per cache. Therefore, only an 8-bit address decoder (e.g., 2<sup>8</sup>=256) is required to access the 256 cache rows instead of an 11-bit address decoder if only one 32-byte line per cache row was employed (e.g., a \u201csingle-set\u201d associative cache). Decode delay thereby is reduced.</p><p>While increasing cache associativity decreases decoder size, each decoder output must drive additional array cells (e.g., eight arrays cells per cache row for an 8-way set associative cache). Buffering may mitigate loading effects, but buffer circuitry itself creates additional delays. Further, once a cache row is identified via a decode operation, the cache must determine whether the identified cache row actually contains the desired information within one of the cache row's array cells, and if so, in which array cell the information resides (e.g., via tag compare and select operations). These determinations may cause additional cache access delays.</p><p>In addition to decode delays, tag compare delays and select delays, the increased physical dimensions of a large L1 cache contribute to cache access delay by increasing the cache's internal wiring lengths (e.g., increasing signal propagation times). High-performance CPUs which have large L1 caches typically employ additional, and often more complex requesters such as execution units, instruction fetch units and the like. The increased size and number of requestors that must interface a large L1 cache makes placement of the requesters near cache input and output ports difficult, increases external wiring lengths and thus further increases cache access time. Cache arbitration among multiple requesters accessing the larger L1 cache also increases cache access time.</p><p>The delays associated with larger decoders, tag compare and select operations, increased wiring lengths and cache arbitration, as well as other delays, combine to make cache access the timing bottleneck for most processor designs employing large L1 caches. Accordingly, a need exists for a method and apparatus for improving caching within a processor system by reducing the pressure on cache access time.</p><h4>SUMMARY OF THE INVENTION</h4><p>To overcome the needs of the prior art, an inventive processor system is provided. The inventive processor system comprises a plurality of level-0 (L0) caches, a processor having a plurality of execution units, and an L1 cache for caching any data and instructions used by the processor. The L1 cache and the L0 caches preferably are internal to the processor, although external caches may be employed. A portion of the execution units provided are configured so that each execution unit within the portion accesses one of the L0 caches. Each of the L0 caches is accessible by only one of the portion of the execution units, and each L0 cache caches a subset of any data used by the processor which is not cacheable by any of the other L0 caches.</p><p>The processor system preferably comprises an instruction dispatcher that dispatches instructions executable by the processor and that selectively designates data as cacheable by only one of the L0 caches. The designation of data as cacheable by only one of the L0 caches preferably occurs at the time instructions are dispatched by the instruction dispatcher (i.e., at dispatch time). For example, an instruction dispatch circuit may be provided that designates data as cacheable by only one of the L0 caches based on a portion of a linear address for the data.</p><p>A significant advantage of the inventive processor system is that each L0 cache is associated with (e.g., is \u201ctightly coupled\u201d to) only one execution unit so that L0 cache design is greatly simplified. For example, because each L0 cache is accessed by only one execution unit, arbitration for L0 cache access is not required (e.g., cache arbitration circuitry within each L0 cache is unnecessary), and cache access occurs at the fastest possible speeds (e.g., is not limited by arbitration delays). Further, because memory locations are not shared between L0 caches, L0 cache resources are maximized (e.g., all L0 cached data is non-duplicative data). The addresses assigned to the L0 caches may be assigned without regard for the current thread or task so that assigning and managing task algorithms are not required; and the small size of the L0 caches allows the L0 caches to be located near its associated execution unit (e.g., reducing wiring lengths and thus signal propagation delays).</p><p>Other objects, features and advantages of the present invention will become more fully apparent from the following detailed description of the preferred embodiments, the appended claims and the accompanying drawings.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The present invention is described with reference to the accompanying drawings. In the drawings, like reference numbers indicate identical or functionally similar elements. Additionally, the left-most digit of a reference number identifies the drawing in which the reference number first appears.</p><p>FIG. 1 is a block diagram of an inventive processor system configured in accordance with the present invention;</p><p>FIG. 2 is a pipeline timing diagram for a single-cycle load instruction within the inventive processor system of FIG. 1; and</p><p>FIG. 3 is a schematic diagram of an instruction dispatch circuit for dispatching load/store instructions within the inventive processor system of FIG. <b>1</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>FIG. 1 is a block diagram of an inventive processor system <b>101</b> configured in accordance with the present invention. The inventive processor system <b>101</b> comprises a processor <b>103</b> coupled to a system bus <b>105</b> comprising a 64-bit data bus <b>107</b><i>a </i>and a 32-bit address bus <b>107</b><i>b. </i>The system bus <b>105</b> couples the processor <b>103</b> to a variety of other components such as a memory controller, an L2 cache, input/output devices and the like (not shown), and allows the processor <b>103</b> to read information from and write information to these components.</p><p>The processor <b>103</b> comprises a bus interface unit <b>109</b> coupled to the system bus <b>105</b>, an L1 cache <b>111</b> coupled to the bus interface unit <b>109</b> and a first level-0 (L0) cache <b>113</b>, a second L0 cache <b>115</b> and an instruction translation look-aside buffer (TLB) <b>117</b> coupled to the L1 cache <b>111</b>. The L1 cache <b>111</b> stores both instructions and data and is accessed via \u201cphysical\u201d addresses (described below). Access to the L1 cache <b>111</b> is arbitrated by an arbiter <b>111</b><i>a </i>internal to the L1 cache <b>111</b>.</p><p>The processor <b>103</b> further comprises an instruction fetch unit <b>119</b> coupled to the L1 cache <b>111</b> and to the instruction TLB <b>117</b>, an instruction decoder <b>121</b> coupled to the instruction fetch unit <b>119</b>, an address generator <b>123</b> coupled to the instruction decoder <b>121</b> and an instruction dispatcher <b>125</b> coupled to the address generator <b>123</b>. A floating point unit <b>127</b>, an integer execution unit <b>129</b>, a first load/store unit <b>131</b> and a second load/store unit <b>133</b> also are provided, and each is coupled to the instruction dispatcher <b>125</b>. The first load/store unit <b>131</b> is coupled to the first L0 cache <b>113</b> and to a data TLB <b>135</b>, and the second load/store unit <b>133</b> is coupled to the second L0 cache <b>115</b> and to the data TLB <b>135</b> such that the first L0 cache <b>113</b> is accessible only by the first load/store unit <b>131</b> and the second L0 cache <b>115</b> is accessible only by the second load/store unit <b>133</b>.</p><p>The processor <b>103</b> further comprises floating point registers <b>137</b> coupled to the floating point unit <b>127</b>, to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b>, and general purpose registers <b>139</b> coupled to the address generator <b>123</b>, to the integer execution unit <b>129</b>, to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b>.</p><p>In operation, the instruction fetch unit <b>119</b> requests an instruction from the L1 cache <b>111</b> by sending a virtual or \u201clinear\u201d address (e.g., an address into the total possible memory space of the inventive processor system <b>101</b>) to the instruction TLB <b>117</b>. In the inventive processor system <b>101</b> of FIG. 1, the virtual address is 32 bits for a total possible memory space of about four gibabytes. The instruction TLB <b>117</b> translates the virtual address into a physical address (e.g., an address into the physically available memory space of the inventive processor system <b>101</b>) and sends the physical address and a fetch request to the L1 cache <b>111</b>. The arbiter <b>111</b>a arbitrates the fetch request with any requests from the bus interface unit <b>109</b>, the first L0 cache <b>113</b> and the second L0 cache <b>115</b>.</p><p>Assuming the requested instruction resides within the L1 cache <b>111</b>, the L1 cache <b>111</b> sends the requested instruction to the instruction fetch unit <b>119</b>, and the instruction fetch unit <b>119</b> passes the instruction to instruction decoder <b>121</b>. In response thereto, the instruction decoder <b>121</b> identifies the type of instruction, the locations of operands required for the instruction (e.g., in memory, in a register, etc.) and the location to which to return results of executing the instruction. In the embodiment of FIG. 1, the instruction set of the processor <b>103</b> defines instructions for accessing memory (e.g., load/store instructions) that are unique from instructions for operating on data (e.g., non-memory instructions such as add, jump, etc.). It will be understood that a more complex instruction set that operates directly on memory operands may be employed with the processor <b>103</b> (e.g., by first decomposing complex instructions into load/store instructions and non-memory instructions).</p><p>If the instruction identified by the instruction decoder <b>121</b> is a load/store instruction, the instruction decoder <b>121</b> forwards the instruction to the address generator <b>123</b>; otherwise, the instruction decoder <b>121</b> bypasses the address generator <b>123</b> and forwards the instruction to the instruction dispatcher <b>125</b>. The address generator <b>123</b> calculates a virtual data address for any data operand associated with a load/store instruction from immediate (e.g., hard-coded) or displacement (e.g., offset) fields within the encoded instruction and/or from values held in the general purpose registers <b>139</b></p><p>The address generator <b>123</b> forwards the load/store instruction and its associated virtual data address to the instruction dispatcher <b>125</b>. Thereafter, the instruction dispatcher <b>12</b>S assigns the instruction either to the first load/store unit <b>131</b> or to the second load/store unit <b>133</b> and forwards the instruction thereto. Specifically, the instruction dispatcher <b>125</b> uses a bit from the virtual data address accompanying the instruction to determine which of the load/store units <b>131</b>, <b>133</b> is to receive the instruction. Preferably, the bit employed to select one of the load/store units <b>131</b>, <b>133</b> is programmably selectable as described below with reference to FIG. <b>3</b>.</p><p>Once the instruction is forwarded to one of the load/store units <b>131</b>, <b>133</b>, the load/store unit receiving the instruction performs the load/store operation designated by the instruction. For example, if the instruction dispatcher <b>125</b> transfers a load instruction to the second load/store unit <b>133</b>, the second load/store unit <b>133</b> executes the load instruction by first determining whether the second L0 cache <b>115</b> contains the data required for the load instruction. More specifically, the second load/store unit <b>133</b> sends the virtual data address of the data to be loaded to the data TLB <b>135</b>, and the data TLB <b>135</b> translates the \u201cpage\u201d portion of the virtual address into a \u201cphysical\u201d page. The physical address then is forwarded to the second L0 cache <b>115</b> for cache row decoding and for use in indexing the bytes of one of the 32-byte lines via offset bits of the physical address. The tag bit portion of the physical address is used to perform a tag compare operation on the array cells of the identified cache row and, if a tag of one of the array cells matches the tag bits, the offset bits are used to identify the appropriate data byte from the 32-byte line within the array cell. Thereafter, the data byte is transferred from the second L0 cache <b>115</b> to the second load/store unit <b>133</b>. The data byte then may be forwarded directly to the floating point registers <b>137</b> or to the general purpose registers <b>139</b>.</p><p>If the tag compare operation fails and the second L0 cache <b>115</b> does not contain the data to be loaded (i.e., a \u201cmiss\u201d), the second L0 cache <b>115</b> sends a request for the data to the L1 cache <b>111</b>. If the L1 cache ill contains the data, the data is transferred from the L1 cache <b>111</b> to the second L0 cache <b>115</b> and from the second L0 cache <b>115</b> to the target location. However, if the L1 cache <b>111</b> does not contain the data, the L1 cache <b>111</b> sends a request for the data to the next memory level (e.g., an L2 cache, system memory, etc.). This process is repeated until the data is found.</p><p>If the instruction dispatcher <b>125</b> transfers a store instruction to one of the load/store units <b>131</b>, <b>133</b>, a similar operation is performed. For example, if the instruction dispatcher <b>125</b> transfers a store instruction to the second load/store unit <b>133</b>, the second load/store unit <b>133</b> executes the store instruction. The contents of the second L0 cache <b>115</b> are examined to ensure that the appropriate cache row is present within the second L0 cache <b>115</b>, and if not, the cache row is retrieved from another memory location (e.g., the L1 cache <b>111</b>, an L2 cache, system memory, etc.). Thereafter, data is transferred from either the floating point registers <b>137</b> or the general purpose registers <b>139</b> to the second load/store unit <b>133</b>, and from the second load/store unit <b>133</b> to the second L0 cache <b>115</b>.</p><p>If the instruction identified by the instruction decoder <b>121</b> is a non-memory instruction, the instruction dispatcher <b>125</b> assigns the instruction to either the floating point unit <b>127</b> or the integer execution unit <b>129</b>, depending on the instruction type. If more than one floating point unit or integer unit is present (not shown), the instruction dispatcher <b>125</b> may employ more sophisticated algorithms for assigning non-memory instructions to a particular floating point or integer unit, as are known in the art.</p><p>Instructions assigned to the floating point unit <b>127</b> read operands from the floating point registers <b>137</b>, perform data operations on the operands and write operation results back to the floating point registers <b>137</b>. L1 kewise, instructions assigned to the integer execution unit <b>129</b> read operands from the general purpose registers <b>139</b>, perform data operations on the operands and write operation results back to the general purpose registers <b>139</b>. The first load/store unit <b>131</b> and the second load/store unit <b>133</b> both have access to the floating point registers <b>137</b> and to the general purpose registers <b>139</b> to allow data transfer between the floating point registers <b>137</b>, the general purpose registers <b>139</b> and the first L0 cache <b>113</b>, and between the floating point registers <b>137</b>, the general purpose registers <b>139</b> and the second L0 cache <b>115</b>.</p><p>In the preferred embodiment, the L1 cache <b>111</b> is a 64K, four-way set associative cache with 32-byte lines, and each L0 cache <b>113</b>, <b>115</b> is a 4K, two-way set associative cache with 32-byte lines. Other cache types (e.g., different sizes, different ways, etc.) may be employed. However, the preferred cache types allow the data TLB <b>135</b> and the first L0 cache <b>113</b> or the second L0 cache <b>115</b> to be accessed within the same CPU cycle (e.g., because no tag address bits are required to identify the desired cache row and the desired byte within each 32-byte line). The 64K cache organization for the L1 cache <b>111</b> (e.g., the size thereof) requires the use of two physical address bits to identify the desired cache row of the L1 cache <b>111</b> so that address translation by the instruction TLB <b>117</b> must occur before L1 cache access. L1 cache access thereby requires two CPU cycles. However, because the physical address must be ready prior to L1 cache access by the instruction TLB <b>117</b>, fewer 32-byte lines per cache row are required (e.g., longer decode delays may be tolerated) and the L1 tag compare is greatly simplified.</p><p>A significant advantage of the inventive processor system <b>101</b> is that the first L0 cache <b>113</b> and the second L0 cache <b>115</b> are \u201ctightly coupled\u201d to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b>, respectively. That is, because the instruction dispatcher <b>125</b> dispatches instructions to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b> based on address bits, no memory address contents are simultaneously held in more than one of the first L0 cache <b>113</b> and the second L0 cache <b>115</b>. Accordingly, the first L0 cache <b>113</b> is accessed only by the first load/store unit <b>131</b>, the second L0 cache <b>115</b> is accessed only by the second load/store unit <b>133</b> and L0 cache design is greatly simplified. For example, because each L0 cache <b>113</b>, <b>115</b> is accessed by only one load/store unit, arbitration for cache access is not required (e.g., rendering cache arbitration circuitry within each L0 cache unnecessary), and cache access occurs at the fastest possible speed (e.g., cache access is not limited by arbitration delays). Coherency between the L0 caches <b>113</b>, <b>115</b> is maintained by virtue of the dispatcher <b>125</b> and without the use of complex coherency circuitry.</p><p>Another advantage of the inventive processor system <b>101</b> is that the tightly coupled nature of the L0 caches <b>113</b>, <b>115</b> yields the largest \u201clogical size\u201d for the L0 cache \u201cpool\u201d (e.g., the amount of non-duplicative cache memory). Assuming each L0 cache <b>113</b>, <b>115</b> is a 4K cache (as preferred), because no memory address contents are simultaneously held in more than one of the first L0 cache <b>113</b> and the second L0 cache <b>115</b> (requiring coherency control), the logical size of the L0 cache pool is always 8K. If memory address contents could be simultaneously held in more than one of the first L0 cache <b>113</b> and the second L0 cache <b>115</b>, the logical size of the L0 cache pool would vary between 4K and 8K, making inefficient use of L0 cache resources. Thus, the inventive processor system <b>101</b> allows maximum utilization of the L0 cache resources. Note that addresses are assigned to the first L0 cache <b>113</b> and to the second L0 cache <b>115</b> without regard for the current thread or task that is running so that algorithms for assigning and managing tasks are not required.</p><p>The use of small L0 caches allows the first L0 cache <b>113</b> to be located near the first load/store unit <b>131</b>, and the second L0 cache <b>115</b> to be located near the second load/store unit <b>133</b>. Wiring lengths and signal propagation times between components thereby are reduced. Accordingly, with short wiring lengths, small cache sizes and no cache arbitration, single cycle access to the first L0 cache <b>113</b> and to the second L0 cache <b>115</b> is easily achieved.</p><p>Because the first L0 cache <b>113</b> and the second L0 cache <b>115</b> are smaller than the L1 cache <b>111</b>, the L0 cache pool holds less data than the L1 cache <b>111</b> and the miss rate of the L0 cache pool is higher than the miss rate of the L1 cache <b>111</b>. However, due to the fast access times (e.g., higher operating frequency) of the first L0 cache <b>113</b> and the second L0 cache <b>115</b>, the average access time for obtaining memory operands within the inventive processor system <b>101</b> is significantly reduced over the average access time for a conventional processor system employing only a large L1 cache. For instance, assume a conventional processor system has a 128K L1 cache with single-cycle access, a one percent L1 cache miss rate and a maximum single-cycle cache access operating frequency of 250 MHZ. If an L1 cache miss requires four CPU cycles to service (e.g., to obtain the data from another memory location such as from an L2 cache, system memory, etc.), the conventional processor system has an average memory access of 1.03 CPU cycles.</p><p>Assume further the inventive processor system <b>101</b> is employed and the L1 cache <b>111</b> is a 64K cache with two-cycle access and a three percent miss rate, and the first L0 cache <b>113</b> and the second L0 cache <b>115</b> are 4K caches each with single-cycle access and a ten percent miss rate. If an L1 cache miss requires four CPU cycles to service, the inventive processor system <b>101</b> has an average memory access of 1.16 CPU cycles. However, because the access time of the L1 cache <b>111</b> has been relaxed to two cycles and because of the design of the first L0 cache <b>113</b> and the second L0 cache <b>115</b> (as previously described), the maximum single-cycle cache access operating frequency of the inventive processor system <b>101</b> may be raised to 300 MHZ. The average memory access time for the inventive processor system <b>101</b> thereby is six percent faster than the conventional processor system's average memory access time due to the higher operating frequency of the inventive processor system <b>101</b> (despite the inventive processor system <b>101</b>'s higher miss rate).</p><p>FIG. 2 is a pipeline timing diagram <b>201</b> for a single-cycle load instruction within the inventive processor system <b>101</b>. With reference to FIG. 2, during CPU cycle 1, the instruction fetch unit <b>119</b> fetches an instruction from the L1 cache <b>111</b> (e.g., via the instruction TLB <b>117</b> as previously described) and passes the instruction to the instruction decoder <b>121</b>. Thereafter, during CPU cycle 2, the instruction decoder <b>121</b> identifies the instruction as a load instruction, identifies the location of data required for the load instruction (e.g., the memory address containing the data to be loaded) and identifies the location to which to return results of executing the load instruction (e.g., a register within the floating point registers <b>137</b> or within the general purpose registers <b>139</b>). Because the instruction is a load instruction, the instruction decoder <b>121</b> forwards the instruction to the address generator <b>123</b>.</p><p>During the first half of the CPU cycle 3, the address generator <b>123</b> calculates a virtual data address for the data associated with the load instruction (as described) and forwards the load instruction and the virtual data address to the instruction dispatcher <b>125</b>. In response thereto, during the second half of the CPU cycle 3, the instruction dispatcher <b>125</b> assigns the load instruction either to the first load/store unit <b>131</b> or to the second load/store unit <b>133</b>. The simplicity of the dispatch algorithm (described further below with reference to FIG. 3) allows the virtual data address calculation and instruction dispatch to occur in one CPU cycle.</p><p>Thereafter, during the first half of the CPU cycle 4, the load/store unit to which the load instruction is dispatched receives the load instruction from the instruction dispatcher <b>125</b> and begins execution of the load instruction. For example, if the second load/store unit <b>133</b> receives the load instruction, the second load/store unit <b>133</b> sends the virtual data address of the data to be loaded to the data TLB <b>135</b>, the data TLB <b>135</b> translates the page portion of the virtual address into a physical page (i.e., the TLB lookup) and a cache row and its associated tags are identified via the remainder of the physical address (i.e., the L0 tag lookup). During the second half of the CPU cycle 4, the tag compare operation (i.e., the L0 tag compare) is performed between the tags associated with the identified cache row and the physical page to identify if the desired data is within the second L0 cache <b>115</b>, hit or miss information is returned, and, if the data is present within the second L0 cache <b>115</b>, the data is returned. Because of the small size and lack of arbitration required to access the L0 caches <b>113</b>, <b>115</b>, the data TLB lookup, the L0 tag lookup, the L0 tag compare and the hit/miss and data return may be performed within one CPU cycle. In CPU cycle 5, the load instruction is completed by writing the identified data to either the floating point registers <b>137</b> or to the general purpose registers <b>139</b>.</p><p>FIG. 3 is a schematic diagram of an instruction dispatch circuit <b>301</b> for dispatching load/store instructions from the instruction dispatcher <b>125</b> to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b>. The instruction dispatch circuit <b>301</b> comprises a linear address result register <b>303</b> coupled to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b> (not shown), and a mask &amp; merge circuit <b>305</b> having a data input coupled to the linear address result register <b>303</b> and a data output coupled to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b>. The instruction dispatch circuit <b>301</b> further comprises an AND gate <b>307</b> having a first input coupled to the linear address result register <b>303</b>, a zero detect circuit <b>309</b> having an input coupled to an output of the AND gate <b>307</b>, and a programmable dispatch select register <b>311</b> coupled to a second input of the AND gate <b>307</b>. A negate circuit <b>313</b> is coupled to the programmable dispatch select register <b>311</b>, and a negate register <b>315</b> is coupled between the mask &amp; merge circuit <b>305</b> and the negate circuit <b>313</b>.</p><p>In operation, the linear address result register <b>303</b> stores each virtual or \u201clinear\u201d data address generated by the address generator <b>123</b>. In the instruction dispatch circuit <b>301</b> of FIG. 3, the virtual data address comprises a 5-bit offset field <b>317</b> for accessing the bytes within a particular 32-byte line of a cache row, a 7-bit cache row field <b>319</b> for selecting one of <b>128</b> cache rows and a 20-bit virtual tag <b>321</b> (e.g., a virtual page number) which is translated by the data TLB <b>135</b> into a \u201cphysical\u201d tag (e.g., a physical page number). The physical tag is compared to the L0 cache's tags during a tag compare operation.</p><p>For the inventive processor system <b>101</b> of FIG. 1, the instruction dispatch circuit <b>301</b> employs one of the bits within the 7-bit cache row field <b>319</b> to select which of the first load/store unit <b>131</b> and the second load/store unit <b>133</b> is to receive an instruction. However, for proper instruction dispatch, if four L0 caches are employed, 2 bits of the 7-bit cache row field <b>319</b> are required and if eight L0 caches are employed, 3 bits of the 7-bit cache row field <b>319</b> are required.</p><p>The particular bit of the 7-bit cache row field <b>319</b> that selects which L0 load/store unit <b>131</b>, <b>133</b> receives an instruction is set by the programmable dispatch select register <b>311</b>. To designate a particular bit of the 7-bit cache row field <b>319</b> as a \u201cselect bit\u201d, the desired bit is set to a logical one within the programmable dispatch select register <b>311</b> and all other register bits are set to a logical zero. Thus, the programmable dispatch select register <b>311</b> may be used to \u201ctune\u201d instruction dispatching by the instruction dispatch circuit <b>301</b> as necessary for improved cache performance.</p><p>The contents of the programmable dispatch select register <b>311</b> are ANDed with the 7-bit cache row field <b>319</b> via the AND gate <b>307</b> (e.g., each bit within the cache row field <b>319</b> is ANDed with its corresponding bit within the programmable dispatch select register <b>301</b> to produce seven AND results), and the results of the AND operations are input to the zero detect circuit <b>309</b>. If the zero detect circuit <b>309</b> detects all zeros, the instruction dispatcher <b>125</b> transfers the instruction to the first load/store unit <b>131</b>, and if the zero detect circuit <b>309</b> detects a one, the instruction dispatcher <b>125</b> transfers the instruction to the second load/store unit <b>133</b> (or vice-versa). In this manner, the two L0 caches <b>113</b>, <b>115</b> never contain the same data, and the largest possible L0 cache pool is maintained.</p><p>As stated, the first and the second L0 caches <b>113</b>, <b>115</b> preferably are 4K, two-way set associative caches with 32-byte lines per cache row. Thus, each cache requires 12 bits to access the data within the cache (e.g., 2<sup>12</sup>=4096). Five offset bits are required to access the bytes within each 32-byte line of a cache row and six bits are required to access one of the 64 cache rows within the cache. Thus, because the L0 caches <b>113</b>, <b>115</b> are 4K, two-way set associative caches with 32 byte-sets, only six of the seven cache row bits of the 7-bit cache row field <b>319</b> are employed to access each cache row. The 7<sup>th </sup>bit in this example is used to select one of the L0 caches <b>113</b>, <b>115</b> during instruction dispatch.</p><p>The six bits used for cache row access must be separated from the original seven bits of the 7-bit cache row field <b>319</b> because the select bit is a programmed bit. To separate the six bits, the contents of the programmable dispatch select register <b>311</b> are negated by the negate circuit <b>313</b> and the results are stored in the negate register <b>315</b>. The contents of the negate register <b>315</b> then are supplied to the mask and merge circuit <b>305</b> which masks the seven bits of the 7-bit cache row field <b>319</b> with the contents of the negate register <b>315</b> (e.g., effectively zeroing the select bit), and which merges the seven bits into six bits (e.g., the six \u201cnon-select\u201d bits). The six bits are supplied to the first and the second L0 caches <b>113</b>, <b>115</b> and serve as the cache row bits for the L0 caches <b>113</b>, <b>115</b>.</p><p>The foregoing description discloses only the preferred embodiments of the invention, modifications of the above disclosed apparatus and method which fall within the scope of the invention will be readily apparent to those of ordinary skill in the art. For instance, the specific type of logic gates described herein are merely preferred and any functionally equivalent logic gates may be similarly employed.</p><p>Accordingly, while the present invention has been disclosed in connection with the preferred embodiments thereof, it should be understood that other embodiments may fall within the spirit and scope of the invention, as defined by the following claims.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "John W.", "last_name": "Goetz", "name": ""}, {"first_name": "Paul T.", "last_name": "Gutwin", "name": ""}, {"first_name": "Stephen W.", "last_name": "Mahin", "name": ""}, {"first_name": "Wilbur D.", "last_name": "Pricer", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTERNATIONAL BUSINESS MACHINES CORPORATION"}, {"first_name": "", "last_name": "INTERNATIONAL BUSINESS MACHINES CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/38        20060101A I20051008RMEP"}, {"label": "G06F  12/08        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711122"}, {"primary": false, "label": "712E09046"}, {"primary": false, "label": "711E12046"}, {"primary": false, "label": "711128"}, {"primary": false, "label": "711123"}, {"primary": false, "label": "711E12043"}], "ecla_classes": [{"label": "G06F  12/08B6M2"}, {"label": "G06F   9/38D"}, {"label": "G06F  12/08B22L"}], "cpc_classes": [{"label": "G06F  12/0848"}, {"label": "G06F   9/3824"}, {"label": "G06F  12/0897"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1999-04-05", "application_date": "1999-04-05", "family_members": [{"ucid": "US-6449693-B1", "titles": [{"lang": "EN", "text": "Method and apparatus for improving caching within a processor system"}]}]}