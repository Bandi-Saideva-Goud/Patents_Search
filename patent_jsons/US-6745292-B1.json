{"patent_number": "US-6745292-B1", "publication_id": 73834552, "family_id": 32326839, "publication_date": "2004-06-01", "titles": [{"lang": "EN", "text": "Apparatus and method for selectively allocating cache lines in a partitioned cache shared by multiprocessors"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50673374\"><p>A computer system includes a cache memory which is shared by multiple processors. The cache memory is divided into a plurality of regions. Each of the processor is exclusively associated with one or more of the regions. All the processors have access to all regions on hits. However, on misses, a processor can cause memory allocation only within its associated region or regions. This means that a processor can cause memory allocation only over data it had fetched. By such arrangement, the \u201ccross-thrash\u201d problem is avoided.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6745292-B1-CLM-00001\" num=\"1\"><claim-text>1. In a computer system including a plurality of processors, a main memory and a cache memory, a method for managing the cache memory comprising the steps of:</claim-text><claim-text>(a) dividing said cache memory into a plurality of regions; </claim-text><claim-text>(b) associating each of said processors with a different one of said regions; </claim-text><claim-text>(c) generating an access address to said main memory that contains data desired by one of said processors; </claim-text><claim-text>(d) determining if a copy of said data resides in said cache memory; </claim-text><claim-text>(e) providing access to said copy of said data residing in said cache memory if said copy of said data resides in any region within said cache memory; and </claim-text><claim-text>(f) copying said data from said main memory into the region of said cache memory associated with said one of said processors if a copy of said data does not reside in any region within said cache memory. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6745292-B1-CLM-00002\" num=\"2\"><claim-text>2. The method of <claim-ref idref=\"US-6745292-B1-CLM-00001\">claim 1</claim-ref>, wherein said step of copying said data further comprising the step of:</claim-text><claim-text>mapping a block of said main memory containing said access address into the region of said cache memory associated with said one of said processors. </claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6745292-B1-CLM-00003\" num=\"3\"><claim-text>3. The method of <claim-ref idref=\"US-6745292-B1-CLM-00002\">claim 2</claim-ref>, said region of said cache memory associated with said one of said processors contains a plurality of slots, wherein said step of mapping maps said block into at least one slot of said region of said cache memory associated with said one of said processors.</claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6745292-B1-CLM-00004\" num=\"4\"><claim-text>4. The method of <claim-ref idref=\"US-6745292-B1-CLM-00003\">claim 3</claim-ref>, wherein said mapping step uses associative mapping.</claim-text></claim>"}, {"num": 5, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6745292-B1-CLM-00005\" num=\"5\"><claim-text>5. The method of <claim-ref idref=\"US-6745292-B1-CLM-00003\">claim 3</claim-ref>, wherein said mapping step uses set-associative mapping.</claim-text></claim>"}, {"num": 6, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6745292-B1-CLM-00006\" num=\"6\"><claim-text>6. An apparatus for accelerating the access speed of a main memory, comprising:</claim-text><claim-text>(a) a cache memory including a plurality of regions, said cache memory is shared by a plurality of processors, each of said processors is associated with one of said regions; </claim-text><claim-text>(b) means for generating an access address that contains data desired by one of said processors; and </claim-text><claim-text>(c) means for determining if a copy of said data resides in said cache memory; </claim-text><claim-text>(e) means for providing access to said copy of said data residing in said cache memory if said copy of said data resides in any region within said cache memory; and </claim-text><claim-text>(f) means for copying said data from said main memory into the region of said cache memory associated with said one of said processors if a copy of said data does not reside in any region within said cache memory. </claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6745292-B1-CLM-00007\" num=\"7\"><claim-text>7. The apparatus of <claim-ref idref=\"US-6745292-B1-CLM-00006\">claim 6</claim-ref>, further comprising:</claim-text><claim-text>means for mapping a block of said main memory containing said access address into said region of said cache memory associated with said one of said processors. </claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6745292-B1-CLM-00008\" num=\"8\"><claim-text>8. The apparatus of <claim-ref idref=\"US-6745292-B1-CLM-00007\">claim 7</claim-ref>, said region of said cache memory associated with said one of said processors containing a plurality of slots, wherein said mapping means maps said block into at least one slot of said region of said cache memory associated with said one of said processors.</claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6745292-B1-CLM-00009\" num=\"9\"><claim-text>9. The apparatus of <claim-ref idref=\"US-6745292-B1-CLM-00008\">claim 8</claim-ref>, wherein said mapping means maps said block into said at least one slot of said region of said cache memory associated with said one of said processors by using associative mapping.</claim-text></claim>"}, {"num": 10, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6745292-B1-CLM-00010\" num=\"10\"><claim-text>10. The apparatus of <claim-ref idref=\"US-6745292-B1-CLM-00008\">claim 8</claim-ref>, wherein said mapping means maps said block into said at least one slot of said region of said cache memory associated with said one of said processors by using set-associative mapping.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54187776\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><p>This is a continuation of application Ser. No. 08/569,335, filed on Dec. 8, 1995 now abandoned.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>The present invention relates to computer systems, and more specifically to a computer system where multiple processors share a cache memory.</p><p>A typical computer system includes at least a processor and a main memory. In performing an instruction, the processor needs to get access to the main memory, to read a word or several words from, or possibly to write a word or several words to, the main memory. A word can be the instruction itself, an operand or a piece of data.</p><p>To obtain the fastest memory speed available and at the same time have a large memory size without imposing undue cost, a cache memory is provided between the processor and the main memory. Usually, the cache memory is faster in speed and smaller in size than the main memory.</p><p>Because the cache memory has a smaller size than that of the main memory, it contains only a copy of portions of the main memory. When the processor attempts to get access to an address in the main memory, a check is made to determine whether the main memory address has been allocated in the cache memory. If so, a desired operation (read or write operation) will be performed on the allocated address in the cache memory.</p><p>If the main memory address has not been allocated in the cache memory, a procedure will be invoked to allocate a space of the cache memory for the main memory address.</p><p>In getting access to an main memory address, if the main memory address has been allocated in the cache memory, it is a hit; if the main memory address has not been allocated in the cache memory, it is a miss. The performance of a cache memory can be measured by hit ratio.</p><p>When multiple processors share a single large cache memory, they can all take advantage of the large cache size to increase hit ratio and may effectively share programs and data already fetched by any one of the processors.</p><p>One problem to this scheme is that the access to the single large cache by the multiple processors may \u201ccross-thrash,\u201d that is, an allocation in the cache memory may replace an entry that had been fetched (may be recently fetched) by some other processors.</p><p>Thus, there has been a need to provide an improved cache memory management, and a need to overcome the \u201ccross-thrash\u201d problem, in an environment where multiple processors share a single cache. The present invention provides the method and apparatus meeting these two needs.</p><h4>SUMMARY OF THE INVENTION</h4><p>In principle, the present invention divides a cache memory, which is shared by multiple processors, into a plurality of regions. Each of the processor is exclusively associated with one or more of the regions. All the processors have access to all regions on hits. However, on misses, a processor can cause memory allocation only within its associated region or regions. This means that a processor can cause memory allocation only over data it had fetched. By such arrangement, the \u201ccross-thrash\u201d problem is eliminated.</p><p>In one aspect, the present invention provides a novel method in use with a computer system including a plurality of processors, a main memory and a cache memory. The method comprises the steps of:</p><p>(a) dividing said cache memory into a plurality of regions;</p><p>(b) associating each of said processors with a respective one of said regions;</p><p>(c) generating an access address that contains content desired by one of said processors; and</p><p>(d) if said access address has not been allocated in said cache memory, causing an allocation within a respective region associated with said one of said processors.</p><p>In another aspect, the present invention provides a novel apparatus for accelerating the access speed of a main memory. The apparatus comprises:</p><p>(a) a cache memory including a plurality of regions, said cache memory is shared by a plurality of processors, each of said processors is associated with a respective one of said regions;</p><p>(b) means for generating an access address that contains content desired by one of said processors; and</p><p>(c) means, if said access address has not been allocated in said cache memory, for causing an allocation within a respective region associated with said one of said processors.</p><p>Accordingly, it is an objective of the present invention to provide an improved cache memory management in an environment where multiple processors share a single cache.</p><p>It is another objective of the present invention to overcome the \u201ccross-thrash\u201d problem in an environment where multiple processors share a single cache.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The present invention will be readily understood to those skilled in the art from the following preferred embodiments and the appended claims, in conjunction with the accompanying drawing, in which:</p><p>FIG. 1 is a block diagram of a computer system in which multiple processors share a cache memory, in accordance with the present invention;</p><p>FIG. 2 depicts one structural scheme of the cache memory shown in FIG. 1;</p><p>FIG. 3A depicts another structural scheme of the cache memory shown in FIG. 1;</p><p>FIG. 3B depicts a specific structural scheme of the cache memory sets shown in FIG. 3A;</p><p>FIG. 4 depicts a flow chart showing cache memory operation by a processor, in accordance with the present invention;</p><p>FIG. 5A depicts the address format which is adaptable to associative mapping method;</p><p>FIG. 5B depicts the address format which is adaptable to set-associative mapping method;</p><p>FIG. 6A depicts a flow chart showing the mapping process using associative mapping method, in accordance with the present invention; and</p><p>FIG. 6B depicts a flow chart showing the mapping process using set-associative mapping method, in accordance with the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>Referring now to FIG. 1, there is shown computer system <b>100</b> in accordance with the present invention. Computer system <b>100</b> includes four processors (PROC <b>0</b>, PROC <b>1</b>, PROC <b>2</b> and PROC <b>3</b>), main memory <b>102</b>, main memory bus <b>104</b>, cache memory <b>106</b>, cache memory bus <b>108</b>, cache memory control logic <b>109</b>, main bus <b>110</b>, and CPU buses <b>112</b>, <b>114</b>, <b>116</b>, and <b>118</b>.</p><p>Cache memory <b>106</b> is divided into a plurality of regions. Each of the four processors is exclusively associated with one or more regions. The structure of the cache memory will be shown in FIG. 2, and FIGS. 3A-3B in greater detail.</p><p>Main memory <b>102</b> has a plurality of blocks (BLOCK <b>0</b>, BLOCK <b>1</b>, . . . , and BLOCK n). Each of the blocks has a plurality of memory units and is associated with a block tag field.</p><p>Main memory bus <b>104</b> is connected between main memory <b>102</b> and main bus <b>110</b>. Cache memory bus <b>108</b> is connected between cache memory <b>106</b> and main bus <b>110</b>. PROC <b>0</b>, PROC <b>1</b>, PROC <b>2</b> and PROC <b>3</b> are connected to main bus <b>110</b> via CPU buses <b>112</b>, <b>114</b>, <b>116</b> and <b>118</b>, respectively.</p><p>Through the interconnections of main memory bus <b>104</b>, cache memory bus <b>108</b>, main bus <b>110</b>, and CPU buses <b>112</b>, <b>114</b>, <b>116</b>, and <b>118</b>, each of the four processors is connected to both main memory <b>102</b> and cache memory <b>106</b>.</p><p>Cache memory control logic <b>109</b> is connected between cache memory <b>106</b> and main bus <b>110</b>, via connection bus <b>103</b> and control line <b>111</b> respectively.</p><p>When PROC i (i=0, 1, 2, 3) attempts to get access to an address in main memory <b>102</b>, if it is a hit, the processor performs a desired operation on the allocated address in cache memory <b>106</b>. If it is a miss, a procedure is invoked to carry out the steps as to be shown in FIGS. <b>4</b> and <b>6</b>A-<b>6</b>B. The procedure invoked can be stored either in main memory <b>102</b> or in an internal memory of PROC i.</p><p>When it is a hit, PROC i can perform an operation on an allocated address in cache memory <b>106</b> via main bus <b>110</b> and cache memory bus <b>108</b>.</p><p>When it is a miss, PROC i can perform an operation on an access address in main memory <b>102</b> via main bus <b>110</b> and main memory bus <b>104</b>. The contents in the block of main memory <b>102</b> containing the access address can also be moved into allocated slot in cache memory <b>106</b> via main memory bus <b>104</b>, main bus <b>110</b> and cache memory bus <b>108</b>.</p><p>FIG. 2 depicts one structural scheme of cache memory <b>106</b> of FIG. 1, which is divided into four regions (REG <b>0</b>, REG <b>1</b>, REG <b>2</b> and REG <b>3</b>). PROC <b>0</b>, PROC <b>1</b>, PROC <b>2</b>, and PROC <b>3</b> are associated with REG <b>0</b>, REG <b>1</b>, REG <b>2</b> and REG <b>3</b> respectively. Each of the four regions has a plurality of memory slots. The four processors have access to the four regions of cache memory <b>106</b> on hits. However on misses, a specific processor PROC i (i=0, 1, 2, and 3) can only cause allocation to REG i (i=0, 1, 2, and 3).</p><p>FIG. 3A depicts another structural scheme of cache memory <b>106</b> of FIG. 1, which has n sets shown as SET <b>0</b>, SET <b>1</b>, . . . , SET m. Each of the sets has a plurality of memory slots.</p><p>FIG. 3B depicts a specific structural scheme for SET j (j=0, 1, . . . , n) of FIG. 3A, with each SET j having <b>16</b> memory slots. In the embodiment shown in FIG. 3B, SET j is used as a 16-way set and divided into four regions (REG <b>0</b>, REG <b>1</b>, REG <b>2</b> and REG <b>3</b>). For a SET j (j=0, 1, . . . , n), PROC <b>0</b> PROC <b>1</b>, PROC <b>2</b> and PROC <b>3</b> are associated with the four regions (REG <b>0</b>, REG <b>1</b>, REG <b>2</b>, and REG <b>3</b>) respectively. The four processors have access to the four regions of SET j (j=0, 1, . . . , n) on hits. However on misses, a PROC i (i=0, 1, 2, 3) can only cause allocation to REG i in SET j (j=0, 1, . . . , n).</p><p>In FIGS. 2, <b>3</b>A and <b>3</b>B, each of the memory slots has a plurality of memory units and is associated with a slot tag field for storing the information to indicate corresponding memory block in main memory <b>102</b> shown in FIG. <b>1</b>.</p><p>FIG. 4 depicts a flow chart showing cache memory operation initiated by PROC i, in accordance with the present invention.</p><p>In step <b>404</b>, PROC i generates an access address to main memory <b>102</b> shown in FIG. <b>1</b> and sends the access address to cache memory control logic <b>109</b>. The access address should comply with the format shown in FIG. 5A or <b>5</b>B.</p><p>In step <b>406</b>, detection is made to detect whether the main memory block containing the access address has been allocated in cache memory <b>106</b> shown in FIG. <b>1</b>.</p><p>If the main memory block has been allocated in cache memory, in step <b>412</b> PROC i performs a desired operation on the cache memory address allocated.</p><p>If the main memory block containing the access address has not been allocated in the cache memory, in step <b>409</b> PROC i will get access to the access address contained the main memory block.</p><p>In step <b>410</b>, PROC i will cause allocation of a cache memory slot within an associated region for the main memory block containing the access address.</p><p>After allocating the cache memory slot at step <b>410</b>, in step <b>412</b> PROC i performs desired operation on the cache memory address allocated.</p><p>In reading operation, step <b>412</b> writes the content stored in the main memory block into the cache memory slot allocated. In writing operation, however, step <b>412</b> write the desired content into both the main memory block containing the access address and the cache memory slot allocated.</p><p>The cache memory slot allocation shown in step <b>410</b> of FIG. 4 includes a mapping process, by which the main memory block is mapped into a cache memory slot using a specific mapping method.</p><p>One mapping method, which can be used by the present invention, is associative mapping method. FIG. 5A depicts the address format of a memory unit within a main memory block, which is adaptable to associative mapping method. The address consists of a block tag field and a word field.</p><p>FIG. 6A depicts a flow chart showing the mapping process using associative mapping method, in accordance with the present invention. In detecting whether the main memory block containing the access address has been allocated in cache memory, in step <b>602</b>, a comparison is made between the main memory block tag field and all cache memory slot tag fields.</p><p>Step <b>604</b> determines whether a match is detected. The detection of a match indicates that the main memory block has been allocated in the cache memory. In step <b>606</b>, the word field is then used to select one of the memory units in the matched slot.</p><p>If no match is detected, the main memory block has not been allocated in the cache memory. In step <b>608</b>, the word field is used to map the main memory block into a slot within an associated region shown in FIG. <b>2</b>. Under associative method, a main memory block can be mapped into any slot within the associated region. Thus, to increase the speed of match detection, cache memory contro logic <b>109</b> in FIG. 1 is able to simultaneously examine all slot tag fields of the cache memory shown in FIG. <b>2</b>.</p><p>Another mapping method, which can be used by the present invention, is set-associative mapping method. FIG. 5B depicts the address format of a memory unit within a main memory block, which is adaptable to set-associative mapping method. The address consists of a block tag field, a set field, and a word field. The set number field is used to index the associated main memory block to a SET j (j=0, 1, . . . , n) shown in FIG. 3A or <b>3</b>B.</p><p>FIG. 6B depicts a flow chart showing the mapping process using associative mapping method, in accordance with the present invention. Like associative mapping method, in step <b>612</b>, set-associative mapping method also compares the main memory block tag field with all cache memory slot tag fields.</p><p>Step <b>614</b> determines whether a match is detected. The detection of a match indicates that the main memory block has been allocated in the cache memory. In step <b>616</b>, the word field is used to select one of the memory units in the matched slot.</p><p>If no match is detected, in step <b>615</b>, the set field is used to index the main memory block into a SET j (j=0, 1, . . . , n) as shown in FIG. <b>3</b>B. And in step <b>618</b>, the word field is used to map the main memory block into one of the slots in the associated region REG i (i=0, 1, 3, 4) shown in FIG. <b>3</b>B. Under set-associative method, after SET j is selected, a main memory block can be mapped into any slot of the associated region within SET j. Thus, to increase the speed of match detection, cache memory control logic <b>109</b> in FIG. 1 is able to simultaneously examine all slot tag fields of all memory sets in the cache memory shown in FIGS. 3A and 3B.</p><p>As reflected in FIGS. 4, <b>6</b>A and <b>6</b>B, one of the features of the present invention is that PROC i (i=0, 1, 2, 3) can cause allocations only over the data it had fetched when a miss occurs.</p><p>The present invention creatively uses associative mapping method and set-associative mapping method to implement the novel cache memory allocation process. In the description above, applicant has sufficiently disclosed how to use these two mapping methods to enable those skilled in the art to implement the cache memory allocation process. Applicant, however, will not explain them in every detail because these two mapping methods themselves are well known to those skilled in the art.</p><p>It should be also noted that any suitable mapping methods can be used to implement the cache memory allocation process, in accordance the spirit of the present invention.</p><p>Although the present invention has been described with particular reference to certain preferred embodiments, variations and modifications of the present invention can be effected within the spirit and scope of the following claims.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Roy M.", "last_name": "Stevens", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "NCR CORPORATION"}, {"first_name": "", "last_name": "TERADATA US, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20060722RMEP"}, {"label": "G06F  12/12        20060101A I20060722RMEP"}, {"label": "G06F  12/00        20060101A I20051110RMEP"}], "national_classes": [{"primary": true, "label": "711129"}, {"primary": false, "label": "711128"}, {"primary": false, "label": "711154"}, {"primary": false, "label": "711E12077"}, {"primary": false, "label": "711E12038"}, {"primary": false, "label": "711118"}], "ecla_classes": [{"label": "S06F212:6042"}, {"label": "S06F12:08B6M2"}, {"label": "G06F  12/12B8"}, {"label": "G06F  12/08B4S"}], "cpc_classes": [{"label": "G06F2212/6042"}, {"label": "G06F  12/084"}, {"label": "G06F  12/0848"}, {"label": "G06F  12/128"}, {"label": "G06F  12/128"}, {"label": "G06F2212/6042"}, {"label": "G06F  12/0848"}, {"label": "G06F  12/084"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1995-12-08", "application_date": "1997-06-05", "family_members": [{"ucid": "US-6745292-B1", "titles": [{"lang": "EN", "text": "Apparatus and method for selectively allocating cache lines in a partitioned cache shared by multiprocessors"}]}]}