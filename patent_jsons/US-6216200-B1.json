{"patent_number": "US-6216200-B1", "publication_id": 72628326, "family_id": 23262218, "publication_date": "2001-04-10", "titles": [{"lang": "EN", "text": "Address queue"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11165022\" source=\"national office\"><p>An address queue in a processor having the capability to track memory-dependencies of memory-access instructions is disclosed. The queue includes a first matrix of RAM cells that tracks a first dependency relationship between a plurality of instructions based upon matching virtual addresses (that identify a common cache set) and the order of instructions in the queue. To facilitate out-of-order instruction execution, dependencies may be tracked before virtual addresses are actually calculated based upon a presumption of dependency. Such dependency is dynamically corrected as addresses become available. The same comparison mechanism used to determine matching virtual addresses for the dependency relationship may also be used to read status bits of a cache set being accessed. The queue also includes a second matrix of RAM cells that tracks a second dependency relationship between a plurality of instructions based upon matching virtual addresses (that identify a common cache set, common doubleword and overlapping byte), the order of instructions in the queue and instruction type. Also disclosed is a method for processing memory instructions that uses a single comparison step between first and second virtual addresses (calculated from instructions) to indicate a dependency relationship between the instructions and to read memory status bits. The status bits are read to determine accessibility of a way within an addressed cache set.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72537860\"><p>An address queue in a processor having the capability to track memory-dependencies of memory-access instructions is disclosed. The queue includes a first matrix of RAM cells that tracks a first dependency relationship between a plurality of instructions based upon matching virtual addresses (that identify a common cache set) and the order of instructions in the queue. To facilitate out-of-order instruction execution, dependencies may be tracked before virtual addresses are actually calculated based upon a presumption of dependency. Such dependency is dynamically corrected as addresses become available. The same comparison mechanism used to determine matching virtual addresses for the dependency relationship may also be used to read status bits of a cache set being accessed. The queue also includes a second matrix of RAM cells that tracks a second dependency relationship between a plurality of instructions based upon matching virtual addresses (that identify a common cache set, common doubleword and overlapping byte), the order of instructions in the queue and instruction type. Also disclosed is a method for processing memory instructions that uses a single comparison step between first and second virtual addresses (calculated from instructions) to indicate a dependency relationship between the instructions and to read memory status bits. The status bits are read to determine accessibility of a way within an addressed cache set.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00001\" num=\"1\"><claim-text>1. An address queue comprising:</claim-text><claim-text>a plurality of entries; </claim-text><claim-text>a comparator circuit, coupled to said plurality of entries, </claim-text><claim-text>operable to compare information stored in a first entry derived from a first instruction with at least a portion of a virtual address derived from a second instruction, said comparator circuit generating a match signal when said information represents at least a portion of a stored address that matches said portion of said virtual address or said information represents an uncomputed address unavailable for comparison, and said comparator circuit generating a non-match signal when said information represents at least a portion of a stored address that does not match said portion of said virtual address; </claim-text><claim-text>a first fixed matrix of RAM cells coupled to said comparator circuit for identifying a first dependency relationship between said first and second instructions, said first matrix comprising a first row of RAM cells and a second row of RAM cells; </claim-text><claim-text>a match line coupling said comparator circuit to said first row of RAM cells, said match line being operable to carry said match signal and said non-match signal; </claim-text><claim-text>matching logic circuitry coupled to said match line, said logic circuitry using said match signal to set a first RAM cell to indicate a dependency when said first memory instruction precedes said second memory instruction in said address queue; and </claim-text><claim-text>a resetting switch coupled to said match line and using said non-match signal to enable the resetting of a second RAM cell to indicate no dependency by said first instruction on said second instruction. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00002\" num=\"2\"><claim-text>2. The address queue of claim <b>1</b> further comprising:</claim-text><claim-text>an array of status cells coupled to said first matrix of RAM cells, said array containing a plurality of rows of status cells, wherein a first row of status cells is coupled to said first row of RAM cells and a second row of status cells is coupled to said second row of RAM cells, said first row of status cells containing a first plurality of status bits for a first cache set; and </claim-text><claim-text>wherein said match line couples said comparator circuit to said first row of status cells and enables reading of said first row of status cells when carrying said match signal. </claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00003\" num=\"3\"><claim-text>3. The address queue of claim <b>1</b> further comprising a second fixed matrix of RAM cells coupled to said comparator circuit for identifying a second dependency relationship between said first and second instructions based in part upon instruction type.</claim-text></claim>"}, {"num": 4, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00004\" num=\"4\"><claim-text>4. The address queue of claim <b>2</b> further comprising a multiplexer disposed between said first matrix of RAM cells and said array of status cells, said multiplexer having an input coupled to said match line and an output coupled to said first plurality of status cells to read said plurality of bits.</claim-text></claim>"}, {"num": 5, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00005\" num=\"5\"><claim-text>5. The address queue of claim <b>3</b> wherein said first fixed matrix of RAM cells and said second fixed matrix of RAM cells are configured in a single array.</claim-text></claim>"}, {"num": 6, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00006\" num=\"6\"><claim-text>6. The address queue of claim <b>4</b> wherein said match line is selected by said multiplexer when a real address created from said virtual address is compared with a cache tag to identify a particular way in said first cache set at approximately the same time that said information is compared with said virtual address.</claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00007\" num=\"7\"><claim-text>7. The address queue of claim <b>6</b> wherein said first row and said second row of RAM cells identify a first set of instructions which said first instruction is dependent upon and a second set of instructions which said second instruction is dependent upon, respectively, said first matrix further comprising:</claim-text><claim-text>a third row of RAM cells identifying a third set of instructions which a third instruction is dependent upon; </claim-text><claim-text>a first column of RAM cells identifying a fourth set of instructions which are dependent upon said third instruction; </claim-text><claim-text>a fourth row of RAM cells identifying a fifth set of instructions which a fourth instruction is dependent upon; </claim-text><claim-text>a second column of RAM cells identifying a sixth set of instructions which are dependent upon said fourth instruction; </claim-text><claim-text>a read line coupled to each of said RAM cells in said third row of cells and said first column of cells enabling the output of bit values held in each RAM cell in said third row and each RAM cell in said first column; and </claim-text><claim-text>a logic OR gate having a first input coupled to a first RAM cell in said third row and said second column, a second input coupled to a first RAM cell in said fourth row and said first column, and an output coupled to said multiplexer, said logic gate generating a logic signal based upon a certain dependency relationship by said third instruction upon a fourth instruction and by said fourth instruction upon said third instruction to indicate matching cache sets, said logic signal being used to read a fourth row of status cells residing in said array of status cells, said fourth row containing status bits for a second cache set. </claim-text></claim>"}, {"num": 8, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00008\" num=\"8\"><claim-text>8. A microprocessor comprising:</claim-text><claim-text>an n-way, set associative cache; </claim-text><claim-text>an adder coupled to said cache for calculating a plurality of virtual addresses from a plurality of memory instructions; and </claim-text><claim-text>an address queue holding said plurality of memory instructions, said address queue comprising: </claim-text><claim-text>a set of comparators for comparing pairs of said plurality of virtual addresses in parallel and generating a plurality of match signals to indicate matching portions of said virtual addresses that identify a particular cache set; </claim-text><claim-text>a first fixed matrix of RAM cells, coupled to said set of comparators for indicating a first dependency relationship among said plurality of memory instructions, said first matrix including a plurality of rows of RAM cells, each row including: </claim-text><claim-text>a plurality of RAM cells that are operable to identify memory instructions which one of said plurality of memory instructions is dependent upon; </claim-text><claim-text>a match line operable to transmit one of said plurality of match signals that indicates matching address portions between said one of said plurality of memory instructions and another memory instruction, said match signal operable to set at least one RAM cell of said matrix of RAM cells; and </claim-text><claim-text>an array of memory status cells containing a plurality of rows of status cells representing status of each way in one or more sets of said cache, each row of status cells being coupled to a particular match line of one of said plurality of rows of RAM cells and capable of being read in parallel when said particular match line transmits said one of said plurality of match signals. </claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00009\" num=\"9\"><claim-text>9. The microprocessor of claim <b>8</b> further comprising a second fixed matrix of RAM cells indicating a second dependency relationship among said plurality of memory instructions based upon portions of said plurality of virtual addresses, instruction order in said address queue, and memory instruction type.</claim-text></claim>"}, {"num": 10, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00010\" num=\"10\"><claim-text>10. The microprocessor of claim <b>8</b> wherein said array of memory status cells contains a plurality of columns of status cells, each of said plurality of columns being individually coupled in a logical OR circuit to generate a single set of status values based upon each row of cells being read, which represent a combined status of each way of said particular cache set.</claim-text></claim>"}, {"num": 11, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00011\" num=\"11\"><claim-text>11. The address queue of claim <b>8</b> wherein said match signal is generated before said one of said plurality of memory instructions and said another memory instruction are executed.</claim-text></claim>"}, {"num": 12, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00012\" num=\"12\"><claim-text>12. The microprocessor of claim <b>9</b> wherein said first dependency relationship is a cache-set dependency.</claim-text></claim>"}, {"num": 13, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00013\" num=\"13\"><claim-text>13. The microprocessor of claim <b>9</b> wherein said first fixed matrix of RAM cells and said second fixed matrix of RAM cells are configured in a single array.</claim-text></claim>"}, {"num": 14, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00014\" num=\"14\"><claim-text>14. The microprocessor of claim <b>12</b> wherein said second dependency relationship is a store-to-load dependency.</claim-text></claim>"}, {"num": 15, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00015\" num=\"15\"><claim-text>15. The microprocessor of claim <b>14</b> wherein each row of status cells in said array of memory status cells contains a pair of lock and use bits that record the status of each way in a cache set.</claim-text></claim>"}, {"num": 16, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00016\" num=\"16\"><claim-text>16. In a microprocessor, a method for processing memory instructions comprising the steps of:</claim-text><claim-text>calculating a first virtual address from a first memory instruction and a second virtual address from a second memory instruction; </claim-text><claim-text>generating a match signal when said first virtual address matches said second virtual address, said first and second virtual addresses identifying a common cache set; </claim-text><claim-text>setting a first RAM cell in a first fixed matrix of RAM cells to indicate a first dependency relationship between said first memory instruction and said second memory instruction, said first dependency relationship being based upon said match signal and a relative order of said first and second memory instructions; </claim-text><claim-text>determining a first real address for identifying a desired way of said common cache set based upon said first virtual address; and </claim-text><claim-text>reading status bits of said common cache set using said match signal to determine accessibility of said desired way. </claim-text></claim>"}, {"num": 17, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00017\" num=\"17\"><claim-text>17. The method of claim <b>16</b> further comprising the step of setting a second RAM cell in said first matrix of RAM cells to indicate a preliminary first dependency relationship between said first memory instruction and an anticipated third memory instruction, said anticipated third memory instruction having an uncomputed third virtual address and previous in order to said first memory instruction.</claim-text></claim>"}, {"num": 18, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00018\" num=\"18\"><claim-text>18. The method of claim <b>16</b> wherein said status bits include a lock bit and a use bit.</claim-text></claim>"}, {"num": 19, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00019\" num=\"19\"><claim-text>19. The method of claim <b>17</b> further comprising the step of resetting said second RAM cell after computing said third virtual address and determining no dependency exists.</claim-text></claim>"}, {"num": 20, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00020\" num=\"20\"><claim-text>20. The method of claim <b>18</b> further comprising the step of setting a RAM cell in a second matrix of RAM cells to indicate a second dependency relationship between said first memory instruction and said second memory instruction, said second dependency relationship being based upon at least said match signal, a relative order of said first and second memory instructions and instruction type.</claim-text></claim>"}, {"num": 21, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00021\" num=\"21\"><claim-text>21. An address queue comprising:</claim-text><claim-text>a plurality of entries; </claim-text><claim-text>a comparator circuit, coupled to said plurality of entries, operable to compare first information stored in a first entry derived from a first instruction with second information derived from a second instruction, said comparator circuit generating a match signal when said first information matches said second information or said first information represents an uncomputed address, and said comparator circuit generating a non-match signal when said first information otherwise does not match said second information; </claim-text><claim-text>a first fixed matrix of RAM cells coupled to said comparator circuit for identifying a first dependency relationship between said first and second instructions, said first matrix comprising a first row of RAM cells; </claim-text><claim-text>a match line coupling said comparator circuit to said first row of RAM cells, said match line being operable to carry said match signal and said non-match signal; </claim-text><claim-text>matching logic circuitry coupled to said match line, said logic circuitry using said match signal to set a first RAM cell to indicate a dependency when said first instruction precedes said second instruction in said address queue; and </claim-text><claim-text>a resetting switch coupled to said match line and using said non-match signal to enable the resetting of a second RAM cell to indicate no dependency by said first instruction on said second instruction. </claim-text></claim>"}, {"num": 22, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6216200-B1-CLM-00022\" num=\"22\"><claim-text>22. An address queue holding a plurality of instructions, said address queue comprising:</claim-text><claim-text>a set of comparators for comparing addresses in parallel and generating a plurality of match signals based upon results of said comparing, each address being associated with one of said plurality of instructions; </claim-text><claim-text>a first fixed matrix of RAM cells, coupled to said set of comparators, for indicating a first dependency relationship among said plurality of instructions, said first matrix including a plurality of rows of RAM cells, each row including: </claim-text><claim-text>a plurality of RAM cells that are operable to identify at least one instruction which one of said plurality of instructions is dependent upon; </claim-text><claim-text>a match line operable to transmit one of said plurality of match signals that indicates matching address information between said one of said plurality of instructions and another instruction, said match signal operable to set at least one RAM cell of said matrix of RAM cells; and </claim-text><claim-text>an array of memory status cells containing a plurality of rows of status cells representing status of each way in one or more sets of a memory, each row of status cells being coupled to a particular match line of one of said plurality of rows of RAM cells and capable of being read in parallel when said particular match line transmits said one of said plurality of match signals. </claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54524832\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><p>This is a continuation-in-part application of application Ser. No. 08/324,129, filed Oct. 14, 1994, and entitled ADDRESS QUEUE, now abandoned.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><p>A preferred embodiment of the present invention is incorporated in a superscalar processor identified as \u201cR10000,\u201d which was developed by Silicon Graphics, Inc., of Mountain View, Calif. Copies of Chapters 11, 12 and 13 of the design notes describing the R10000 are included as an appendix to this application and are hereby incorporated by reference in their entirety for all purposes.</p><h4>BACKGROUND OF THE INVENTION</h4><p>This invention relates in general to computers capable of executing instructions out of order and, in particular, to a computer capable of tracking dependencies between out-of-order instructions that are used to access memory.</p><p>From the perspective of a programmer, instructions in a conventional processor are executed sequentially. When an instruction loads a new value into its destination register, that new value is immediately available for use by subsequent instructions. This is not true, however, for pipelined computer hardware because some results are not available for many clock cycles. Sequencing becomes more complicated in a superscalar processor, which has multiple execution pipelines running in parallel. But the hardware must behave as if each instruction were completed sequentially.</p><p>Each instruction depends on previous instructions which produced its operands, because it cannot begin execution until those operands become valid. These dependencies determine the order in which instructions can be executed. The actual execution order depends on the organization of the processor. In a typical pipelined processor, instructions are executed only in program order. The next sequential instruction may begin execution during the next cycle provided all its operands are valid. Otherwise, the pipeline stalls until the operands become valid. Because instructions execute in order, stalls usually delay all subsequent instructions. A sophisticated compiler can improve performance by re-arranging instructions to reduce the frequency of these stall cycles.</p><p>In an in-order superscalar processor, several consecutive instructions may begin execution simultaneously, if all their operands are valid, but the processor stalls at any instruction whose operands are still busy. In an out-of-order superscalar processor, each instruction is eligible to begin execution as soon as its operands become valid, independently of the original instruction sequence. In effect, the hardware re-arranges instructions to keep its execution units busy. This process is called \u201cdynamic issuing.\u201d</p><p>Dynamic issue and execution of pipelined instructions creates a special need to monitor and resolve data dependencies between instructions. A newly-issued instruction is dependent on a previous instruction if, for example, the newly-issued instruction must use an output of the previous instruction as an operand. Such dependency inserts a restriction on the order of instruction execution.</p><p>Similarly, when out-of-order instructions are used in memory-access operations, the execution order of such instructions is restricted, at least in part, by memory dependency (i.e., two instructions accessing and altering the same memory location). Accordingly, there is a need for tracking the memory-dependency of memory-access instructions which may be executed out of order to maintain data integrity.</p><h4>SUMMARY OF THE INVENTION</h4><p>The present invention offers a highly efficient apparatus for tracking memory dependencies of memory-access instructions that may be executed out of order. This apparatus also provides for special identification of portions of a memory cache set to prevent unnecessary cache thrashing.</p><p>In one embodiment, the present invention provides an address queue for holding a plurality of entries used to access a set-associative data cache. This queue includes a comparator circuit, first matrix of RAM cells and second matrix of RAM cells. The comparator circuit compares a newly calculated partial address derived from a new queue entry with a previously calculated partial address derived from one of a number of previous entries. The first matrix of RAM cells tracks all of the previous entries in the queue that use a cache set that is also used by the new queue entry, The second matrix of RAM cells tracks queue entries that are store instructions which store a portion of data in the data cache which is accessed by a subsequent load instruction. The address queue may also assign status bits to certain blocks stored in the cache to identify the type of access allowed; i.e., random or sequential.</p><p>A better understanding of the nature and advantages of the present invention may be had with reference to the detailed description and the drawings below.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>FIG. 1 discloses a functional block diagram of a superscalar processor;</p><p>FIG. 2 discloses rows in the address queue disclosed herein;</p><p>FIG. 3 discloses physical layout of address queue;</p><p>FIG. 4 illustrates format of improved branch instruction formats;</p><p>FIGS. 5A and 5B (collectively referred to as \u201cFIG. <b>5</b>\u201d) illustrate instruction set format;</p><p>FIG. 6 illustrates new instruction formats in Mips-4 ISA;</p><p>FIGS. 7A and 7B (collectively referred to as \u201cFIG. <b>7</b>\u201d) disclose connection of the address queue;</p><p>FIG. 8 discloses instruction fields sent to the address queue and address stack;</p><p>FIG. 9 discloses address queue and address stack contents;</p><p>FIGS. 10A and 10B (collectively referred to as \u201cFIG. <b>10</b>\u201d) define the 7-bit operation codes stored in the address queue;</p><p>FIG. 11 is an address calculation timing diagram;</p><p>FIG. 12 illustrates priority protocol for using the data cache;</p><p>FIGS. 13A and 13B (collectively referred to as \u201cFIG. <b>13</b>\u201d) disclose active-signal logic of the address queue;</p><p>FIG. 14 discloses generating an active mask;</p><p>FIGS. 15A and 15B (collectively referred to as \u201cFIG. <b>15</b>\u201d) disclose priority logic of the address queue;</p><p>FIG. 16 is an example of retry access priority;</p><p>FIGS. 17A and 17B (collectively referred to as \u201cFIG. <b>17</b>\u201d) disclose retry access priority logic of the address queue;</p><p>FIGS. 18A and 18B (collectively referred to as \u201cFIG. <b>18</b>\u201d) disclose synchronize mask logic of the address queue;</p><p>FIG. 19 is an example of a synchronize mask;</p><p>FIG. 20 illustrates a high group within the synchronize mask;</p><p>FIG. 21 discloses access request logic of the address queue;</p><p>FIG. 22 discloses dependency comparators of the address queue;</p><p>FIG. 23 is an address queue timing diagram;</p><p>FIG. 24 discloses dependency matrixes in the address queue;</p><p>FIGS. 25<i>a </i>and <b>25</b><i>b </i>disclose dependency matrix operation in the address queue;</p><p>FIG. 26 discloses dependency checks during tag check cycles;</p><p>FIG. 27 discloses dependency comparator logic of the address queue;</p><p>FIG. 28 discloses dependency comparator circuits of the address queue;</p><p>FIG. 29 discloses byte overlap circuit of the address queue;</p><p>FIGS. 30<i>a </i>and <b>30</b><i>b </i>disclose dependency matrix logic cells of the matrixes of FIG. 24;</p><p>FIG. 30<i>c </i>shows a portion of dependency matrix <b>2400</b>;</p><p>FIG. 31 discloses an alternative embodiment of dependency matrixes in the address queue;</p><p>FIGS. 32<i>a </i>and <b>32</b><i>b </i>disclose dependency matrix logic cells of the matrixes of FIG. 31;</p><p>FIGS. 33A and 33B (collectively referred to as \u201cFIG. <b>33</b>\u201d) disclose dependency matrix logic cells where the two matrixes of FIG. 31 are laid out in a single array; and</p><p>FIG. 34 is a dependency timing diagram.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF THE PREFERRED EMBODIMENT</h4><h4>Contents</h4><p>I. Superscalar Processor Architecture</p><p>A. Superscalar Processor Overview</p><p>B. Operation</p><p>C. Instruction Queues</p><p>D. Coprocessors</p><p>E. Instruction Formats</p><p>II. Address Queue</p><p>A. Contents</p><p>B. Address Calculation Sequence</p><p>C. Issuing Instructions to the Data Cache</p><p>D. Retry Accesses</p><p>E. State Changes Within Address Queue</p><p>III. Address Stack</p><p>IV. Memory Dependency</p><p>A. Memory Dependency Checks</p><p>B. Dependency Logic</p><p>C. Dependency Logic\u2014Alternative Embodiment</p><p>D. Uncached Memory Dependency</p><h4>I. SUPERSCALAR PROCESSOR ARCHITECTURE</h4><p>In the following discussion, the same signal may be identified with and without a letter suffix (i.e., \u201cAQvStore[j]\u201d and \u201cAQvStore\u201d can refer to the same signal). The suffix expressly identifies one of many signals bearing the same name. The same signal name without a letter suffix implicitly identifies one of many signals bearing the same name.</p><p>FIG. 1 discloses a functional block diagram of a superscalar processor <b>100</b> which incorporates an address queue built and operating in accordance with the present invention. As discussed below, this address queue enables, among other things, the tracking of memory dependencies in a processor that provides for out-of-order execution of instructions. Processor <b>100</b>, which generally represents the R10000 Super-Scalar Processor developed by Silicon Graphics, Inc., of Mountain View, Calif., provides only one example of an application for the address queue of the present invention. This processor is described in J. Heinrich, <i>MIPS R</i>10000 <i>Microprocessor User's Manual</i>, MIPS Technologies, Inc., (1994), which is hereby incorporated by reference in its entirety for all purposes.</p><p>A. Superscalar Processor Overview</p><p>A superscalar processor can fetch and execute more than one instruction in parallel. Processor <b>100</b> fetches and decodes four instructions per cycle. Each decoded instruction is appended to one of three instruction queues. These queues can issue one new instruction per cycle to each of five execution pipelines.</p><p>The block diagram of FIG. 1 is arranged to show the stages of an instruction pipeline and illustrates functional interconnectivity between various processor elements. Generally, instruction fetch and decode are carried out in stages 1 and 2; instructions are issued from various queues in stage 3; and instruction execution is performed in stages 4-7.</p><p>Referring to FIG. 1, a primary instruction cache <b>102</b> reads four consecutive instructions per cycle, beginning on any word boundary within a cache block. A branch target cache <b>104</b>, instruction register <b>106</b> and instruction decode and dependency logic <b>200</b>, convey portions of issued instructions to floating point mapping table <b>204</b> (32 word by 6 bit RAM) or integer mapping table <b>206</b> (33 word by 6 bit RAM). These tables carry out a \u201cregister renaming\u201d operation, which renames logical registers identified in an instruction with a physical register location for holding values during instruction execution. A redundant mapping mechanism is built into these tables to facilitate efficient recovery from branch mispredictions. The architecture and operation of mapping tables <b>204</b>, <b>206</b> and the associated redundant mapping mechanism is described in detail in commonly-owned, co-pending patent application Ser. No. 08/324,127, which is hereby incorporated by reference in its entirety for all purposes.</p><p>Mapping tables <b>204</b> and <b>206</b> also receive input from a floating point free list <b>208</b> (32 word by 6 bit RAM) and an integer free list <b>210</b> (32 word by 6 bit RAM), respectively. Output of both mapping tables is fed to active list <b>212</b> which, in turn, feeds the inputs of free lists <b>208</b> and <b>210</b>.</p><p>A branch unit <b>214</b> also receives information from instruction register <b>106</b>, as shown in FIG. <b>1</b>. This unit processes no more than one branch per cycle. The branch unit includes a branch stack <b>216</b> which contains one entry for each conditional branch. Processor <b>100</b> can execute a conditional branch speculatively by predicting the most likely path and decoding instructions along that path. The prediction is verified when the condition becomes known. If the correct path was taken, processing continues along that path. Otherwise, the decision must be reversed, all speculatively decoded instructions must be aborted, and the program counter and mapping hardware must be restored.</p><p>Referring again to FIG. 1, mapping tables <b>204</b> and <b>206</b> support three general pipelines, which incorporate five execution units. A floating-point pipeline is coupled to floating-point mapping table <b>204</b>. The floating-point pipeline includes a sixteen-entry instruction queue <b>300</b> which communicates with a sixty-four-location floating point register file <b>302</b>. Register file <b>302</b> and instruction queue <b>300</b> feed parallel multiply unit <b>400</b> and adder <b>404</b> (which performs, among other things, comparison operations to confirm floating-point branch predictions). Multiply unit <b>400</b> also provides input to a divide unit <b>408</b> and square root unit <b>410</b>.</p><p>Second, an integer pipeline is coupled to integer mapping table <b>206</b>. The integer pipeline includes a sixteen-entry integer instruction queue <b>304</b> which communicates with a sixty-four-location integer register file <b>306</b>. Register file <b>306</b> and instruction queue <b>304</b> feed two arithmetic logic units (\u201cALU\u201d); ALU#1 <b>412</b> (which contains an ALU, shifter and integer branch comparator) and ALU#2 <b>414</b> (which contains an ALU, integer multiplier and divider).</p><p>Third, a load/store pipeline (or load/store unit) <b>416</b> is coupled to integer mapping table <b>206</b>. This pipeline includes a sixteen-entry address queue <b>308</b> which communicates with register file <b>306</b>. Address queue <b>308</b> is built and operates in accordance with the present invention.</p><p>Register file <b>306</b> and address queue <b>308</b> feed integer address calculate unit <b>418</b> which, in turn, provides virtual-address index entries for address stack <b>420</b>. These virtual addresses are converted to physical addresses in translation lookaside buffer (TLB) <b>422</b>, and used to access a data cache <b>424</b> that holds data <b>425</b> and tags <b>426</b>. These physical addresses are also stored in address stack <b>420</b>. The architecture of TLB <b>422</b> is described in detail in commonly-owned, co-pending patent application, Ser. No. 08/324,128, now abandoned, which is hereby incorporated by reference in its entirety for all purposes.</p><p>Data input to and output from data cache <b>424</b> pass through store aligner <b>430</b> and load aligner <b>428</b>, respectively. Data cache <b>424</b> and surrounding architecture is described in detail in commonly-owned, co-pending patent application, Ser. No. 08/324,124, now abandoned which is hereby incorporated by reference in its entirety for all purposes.</p><p>Address stack <b>420</b> and data cache <b>424</b> also communicate with secondary cache controller and external interface <b>434</b>. Further, data cache <b>424</b> and controller-interface <b>434</b> communicate with secondary cache <b>432</b>. External interface <b>434</b> sends a 4-bit command (DCmd[<b>3</b>:<b>0</b>]) to data cache <b>424</b> and address queue <b>308</b> (connection not shown) to indicate what operation the cache will perform for it. Address queue <b>308</b> derives signals which control reading and writing from data cache <b>424</b>.</p><p>B. Operation</p><p>Processor <b>100</b> uses multiple execution pipelines to overlap instruction execution in five functional units. As described above, these units include the two integer ALUs <b>412</b>, <b>414</b>, load/store unit <b>416</b>, floating-point adder <b>404</b> and floating-point multiplier <b>400</b>. Each associated pipeline includes stages for issuing instructions, reading register operands, executing instructions, and storing results. There are also three \u201citerative\u201d units (i.e., ALU#2 <b>414</b>, floating-point divide unit <b>408</b> and floating-point square root unit <b>410</b>) which compute more complex results.</p><p>Register files <b>302</b> and <b>306</b> must have multiple read and write ports to keep the functional units of processor <b>100</b> busy. Integer register file <b>306</b> has seven read and three write ports; floating-point register file <b>302</b> has five read and three write ports. The integer and floating-point execution units each use two dedicated operand ports and one dedicated result port in the appropriate register file. Load/store unit <b>416</b> uses two dedicated integer operand ports for address calculation. It must also load or store either integer or floating-point values, sharing a result port and a read port in both register files. These shared ports are also used to move data between the integer and floating-point register files, and for \u201cJump and Link\u201d and \u201cJump Register\u201d instructions.</p><p>In a pipeline, the execution of each instruction is divided into a sequence of simpler operations. Each operation is performed by a separate hardware section called a stage. Each stage passes its result to the next stage. Usually, each instruction requires only a single cycle in each stage, and each stage can begin a new instruction while previous instructions are being completed by later stages. Thus, a new instruction can often begin during every cycle.</p><p>Pipelines greatly improve the rate at which instructions can be executed. However, the efficient use of a pipeline requires that several instructions be executed in parallel. The result of each instruction is not available for several cycles after that instruction entered the pipeline. Thus, new instructions must not depend on the results of instructions which are still in the pipeline.</p><p>Processor <b>100</b> fetches and decodes instructions in their original program order, but may execute and complete these instructions out of order. Once completed, instructions are \u201cgraduated\u201d in their original program order. Instruction fetching is carried out by reading instructions from instruction cache <b>102</b>, shown in FIG. <b>1</b>. Instruction decode operation includes dependency checks and register renaming, performed by instruction decode and dependency logic <b>200</b> and mapping tables <b>204</b> or <b>206</b>, respectively. The execution units identified above compute an arithmetic result from the operands of an instruction. Execution is complete when a result has been computed and stored in a temporary register identified by register file <b>302</b> or <b>306</b>. Finally, graduation commits this temporary result as a new permanent value.</p><p>An instruction can graduate only after it and all previous instructions have been successfully completed. Until an instruction has graduated, it can be aborted, and all previous register and memory values can be restored to a precise state following any exception. This state is restored by \u201cunnaming\u201d the temporary physical registers assigned to subsequent instructions. Registers are unnamed by writing an old destination register into the associated mapping table and returning a new destination register to the free list. Renaming is done in reverse program order, in the event a logical register was used more than once. After renaming, register files <b>302</b> and <b>306</b> contain only the permanent values which were created by instructions prior to the exception. Once an instruction has graduated, however, all previous values are lost.</p><p>Active list <b>212</b> is a list of \u201cactive\u201d instructions in program order. It records status, such as which instructions have been completed or have detected exceptions. Instructions are appended to its bottom when they are decoded. Completed instructions are removed from its top when they graduate.</p><p>C. Instruction Queues</p><p>Processor <b>100</b> keeps decoded instructions in three instruction queues. These queues dynamically issue instructions to the execution units. Referring to FIG. 2, the entries in each queue are logically arranged in four rows (i.e., rows <b>220</b>-<b>226</b>) of four entries <b>218</b>, as shown in FIG. <b>2</b>. (This \u201crow\u201d and \u201ccolumn\u201d terminology is figurative only; the physical layout has one column of sixteen entries <b>350</b>, as shown in FIG. 3.) While an instruction queue has four write ports <b>236</b>-<b>242</b>, each queue entry <b>218</b> has only a single write port <b>219</b>. Entries within each row share the same queue write port, but each row has a separate port. These inputs are fed from four four-to-one multiplexers <b>228</b>-<b>234</b> (MUXes) which can select any of the four instructions currently being decoded. These MUXes align new instructions with an empty entry. A new instruction can be written into each row if it has at least one empty entry.</p><p>1. Integer Queue</p><p>Integer queue <b>304</b>, as shown in FIG. 1, issues instructions to two integer arithmetic units: ALU#1 <b>412</b> and ALU#2 <b>414</b>. This queue contains <b>16</b> instruction entries. Newly decoded integer instructions are written into empty entries without any particular order. Up to four instructions may be written during each cycle. Instructions remain in this queue only until they have been issued to an ALU. Branch and shift instructions can be issued only to ALU#1 <b>412</b>. Integer multiply and divide instructions can be issued only to ALU#2 <b>414</b>. Other integer instructions can be issued to either ALU.</p><p>The Integer Queue controls six dedicated ports to integer register file <b>306</b>. These include two operand read ports and a destination write port for each ALU.</p><p>2. Floating Point Queue</p><p>Floating-point queue <b>300</b>, as shown in FIG. 1, issues instructions to floating-point multiplier <b>400</b> and floating-point adder <b>404</b>. This queue contains <b>16</b> instruction entries. Newly decoded floating-point instructions are written into empty entries without any particular order. Up to four instructions may be written during each cycle. Instructions remain in this queue only until they have been issued to a floating-point execution unit.</p><p>The Floating-point queue controls six dedicated ports to floating-point register file <b>302</b>. These include two operand read ports and a destination port for each execution unit. Queue <b>300</b> uses the issue port of multiplier <b>400</b> to issue instructions to square-root unit <b>410</b> and divide unit <b>408</b>. These instructions also share the register ports of multiplier <b>400</b>.</p><p>Further, Floating-Point queue <b>300</b> contains simple sequencing logic for multi-pass instructions, such as Multiply-Add. These instructions require one pass through multiplier <b>400</b> and then one pass through the adder <b>404</b>.</p><p>3. Address Queue</p><p>Address queue <b>308</b>, as shown in FIG. 1, issues instructions within load/store unit <b>416</b>. It contains <b>16</b> instruction entries. Unlike the other two queues, address queue <b>308</b> is organized as a circular First-In-First-Out (\u201cFIFO\u201d) buffer. Newly decoded load/store instructions are written into the next sequential empty entries. Up to four instructions may be written during each cycle. The FIFO order maintains the program's original instruction sequence so that memory address dependencies may be computed easily. Instructions remain in this queue until they have graduated. They cannot be deleted immediately after being issued, because load/store unit <b>416</b> may not be able to immediately complete the operation.</p><p>Address queue <b>308</b> contains more complex control logic than the other queues. An issued instruction may fail to complete, because of a memory dependency, a cache miss, or a resource conflict. In these cases, the queue must re-issue the instruction until it has graduated.</p><p>Address queue <b>308</b> has three issue ports; issue, access and store. The first two are dynamic (i.e., may issue instructions out of order) while the third issues instructions in order. First, address queue <b>308</b> issues each instruction once to address calculation unit <b>418</b>. This unit uses a 2-stage pipeline to compute the memory address of an instruction and translate it in Translation Look-aside Buffer <b>422</b> (\u201cTLB\u201d). Addresses are stored in address stack <b>420</b> and in the dependency logic of the queue, as discussed below. This port controls two dedicated read ports to integer register file <b>306</b>. This logic is similar to the other queues. Issue port may use tag check circuitry (discussed below) if it is not used by the access port.</p><p>Second, address queue <b>308</b> can issue \u201caccesses\u201d to data cache <b>424</b>. The queue allocates usage of four sections of the cache, which consist of tag and data sections of the two cache banks. Load and store instructions begin with a tag check cycle, which checks if the desired address is already in cache. Load instructions also read and align a doubleword value from the data array. This access may be concurrent or later than the tag check. If the data is present and no dependencies exist, the instruction is marked \u201cdone\u201d in the queue.</p><p>Third, address queue <b>308</b> can issue \u201cstore\u201d instructions to the data cache. A store instruction may not modify the data cache until it graduates. Only one store can graduate per cycle, but it may be anywhere within the four oldest instructions, if all previous instructions are already \u201cdone\u201d.</p><p>The \u201cAccess\u201d and \u201cStore\u201d ports share two integer and two floating-point register file ports. These \u201cshared\u201d ports are also used for \u201cJump and Link\u201d and \u201cJump Register\u201d instructions and for move instructions between the integer and register files.</p><p>D. Coprocessors</p><p>Processor <b>100</b> can operate with up to four tightly-coupled coprocessors (designated CP0 through CP3). Coprocessor unit number zero (CP0) supports the virtual memory system together with exception handling. Coprocessor unit number one CP1 (and unit three (CP3) in Mips-4 Instruction Set Architecture, discussed below) is reserve for floating-point operations.</p><p>E. Instruction Formats</p><p>Processor <b>100</b> implements the Mips-4 Instruction Set Architecture (\u201cISA\u201d) and is compatible with earlier Mips-1, Mips-2 and Mips-3 ISAs. The formats of these instructions are summarized in FIGS. 4-6. FIG. 4 shows jump and branch instruction formats. The \u201cBranch on Floating-Point Condition\u201d (CP1) instruction was enhanced in Mips-4 ISA to include eight condition code bits, instead of the single bit in the original instruction set. In Mips-3 or earlier ISA, the three-bit condition code select field (\u201cCC\u201d) must be zero. FIG. 5 shows other formats in the Mips-3 and earlier ISAs. A discussion of Mips ISAs is provided in J. Heinrich, <i>MIPS R</i>4000 <i>User's Manual</i>, PTR Prentice Hall (1993) and G. Kane et al., <i>MIPS RISC Architecture</i>, Prentice Hall (1992), both hereby incorporated by reference in their entirety for all purposes. A description of Mips-4 ISA is provided in C. Price, <i>MIPS R</i>10000<i>\u2014Mips IV ISA Manual</i>, MIPS Technologies, Inc. (1994), which is also hereby incorporated by reference in its entirety for all purposes.</p><p>The extensions for the MIPS-4 ISA are shown in FIG. <b>6</b>. These new instructions facilitate floating-point and vector programs. These include floating-point multiply-add, double-indexed load and store floating-point, and conditional move instructions. The floating-point compare instruction has been modified to set any of eight condition bits.</p><h4>II. ADDRESS QUEUE</h4><p>Address Queue <b>308</b> keeps track of all memory instructions in the pipeline. As noted above, it contains <b>16</b> entries, which are organized as a circular FIFO buffer or list <b>500</b>, as indicated in FIG. <b>7</b>. When a memory load or store instruction is decoded, it is allocated to the next sequential entry at the \u201cbottom\u201d of list <b>500</b>, which includes list segments <b>509</b>-<b>514</b>. Any or all of the four instructions (i.e., <b>501</b>-<b>504</b>) decoded during a cycle may be loads or stores, so up to four instructions may be appended to address queue <b>308</b> in one cycle.</p><p>FIG. 7 shows the loading of an instruction containing portions <b>522</b>-<b>524</b> to list <b>500</b>. Portion <b>522</b> contains mapped physical register numbers for operands A and B and destination D, which are appended to \u201cA\u201d segment <b>509</b>, \u201cB\u201d segment <b>510</b> and \u201cD\u201d segment <b>514</b>, respectively. Operand B is also appended to \u201cC\u201d segment <b>513</b> (see Table <b>1</b>). Further, instruction portion <b>523</b> contains opcode (\u201cop\u201d), function (\u201cfn\u201d) and subfunction (\u201csf\u201d) values which are decoded in decode logic <b>505</b> and appended to \u201cfunction\u201d segment <b>512</b>. Finally, instruction portion <b>522</b> contains an \u201cimmediate\u201d value (described below) which is appended to \u201cimmediate\u201d segment <b>511</b> along with a portion of the \u201cfn\u201d value from portion <b>523</b>.</p><p>Remaining portions of list <b>500</b> include \u201cdependency\u201d segment <b>515</b>, \u201cindex\u201d segment <b>516</b> and \u201cstate\u201d segment <b>517</b>. Also shown is address stack <b>420</b> which includes \u201ccontrol\u201d segment <b>518</b> and \u201cphysical address\u201d segment <b>519</b>. Blocks <b>508</b>, <b>520</b> and <b>521</b> represent issue logic which is described below.</p><p>The instruction fields sent to the address queue and address stack are summarized in FIG. <b>8</b>. Symbols and footnotes used in this figure are defined below:</p><p>\u201c1\u201d: Indexed address calculations add two 64-bit registers (base+index) to form an address. All other address calculations add a 16-bit signed offset to a 64-bit base register. Indexed calculations are used only for floating-point loads and stores.</p><p>\u201c2\u201d: In Mips-1 and Mips-2 ISA, load and store instructions access either the low (even-numbered logical register) or high half (odd) of one of <b>16</b> double-precision registers. The low bit of the logical register number is stored in AQvFltHi in the Address Queue. For load instructions, the operand word must be merged with the other half of the old destination register.</p><p>\u201cD\u201d: This bit is set unless the integer destination register is zero, which indicates no destination.</p><p>\u201cV\u201d: This bit is set if there is a valid operand register (excluding integer register #0, which has a zero value).</p><p>\u201caaaaaa\u201d: The function field (instruction bits <b>5</b>:<b>0</b>) is part of the immediate field (instruction bits <b>15</b>:<b>0</b>).</p><p>\u201ch\u201d: Prefetch instructions (LPF and PFETCH) contain a 5-bit \u201chint\u201d field.</p><p>\u201c-\u201d: These fields are not used. Their content is not specified.</p><p>Instructions are deleted from the \u201ctop\u201d of list <b>500</b> when they graduate. Up to four instructions can graduate per cycle, but only one of these can be a \u201cstore\u201d. Instructions may also be deleted from the bottom when a speculative branch is reversed and all instructions following the branch are aborted. The queue is cleared when an exception is taken. Various address queue contents are summarized in FIG. <b>9</b>.</p><p>The FIFO uses two 5-bit pointers <b>506</b> and <b>507</b>, shown in FIG. <b>7</b>. The low four bits select one of the 16 entries. The high bit detects when the bottom of the queue has \u201cwrapped\u201d back to the top. \u201cWrite\u201d pointer <b>507</b> selects the next empty entry. \u201cRead\u201d pointer <b>506</b> selects the oldest entry, which will be the next to graduate. The write pointer is copied onto branch stack <b>216</b> (FIG. 1) in one of four \u201cshadow\u201d pointers whenever a speculative branch is taken. It is restored from the corresponding shadow if the branch decision is reversed.</p><p>Because of layout restrictions, address queue <b>308</b> is physically implemented in two sections. The \u201caddress queue\u201d section (i.e., segments and blocks <b>508</b>-<b>517</b>, <b>520</b> and <b>521</b>), which is located between instruction decode <b>200</b> and integer register file <b>306</b>, contains most of the control logic. It contains the base and index register fields, and address offset field, and it issues instructions to address calculation unit <b>418</b>. It also issues load and store instructions to data cache <b>424</b> and resolves memory dependencies. The address stack section (i.e., segments <b>518</b>, <b>519</b>), which is located near TLB <b>422</b>, contains the translated physical address.</p><p>The Address Queue has three issue ports. The \u201cCalculate\u201d port issues each instruction once to address calculation unit <b>418</b>, TLB <b>422</b>, and (if available) to data cache <b>424</b>. The \u201cAccess\u201d port is used to retry instructions. The \u201cStore\u201d port is used to graduate store instructions.</p><p>A. Contents</p><p>Address queue <b>308</b> contains \u201cLoad\u201d and \u201cStore\u201d instructions, which access data cache <b>424</b> and main memory. It also contains \u201cCache\u201d instructions, which manipulate any of the caches. This queue also controls address calculation circuit <b>418</b>. For \u201cIndexed\u201d instructions, it provides two physical register operands via integer register file <b>306</b>. For other load or store instructions, it provides one register operand and a 16-bit immediate value.</p><p>The fields within each address queue entry are listed in tables 1-3. Table 1 lists instruction fields, which contain bits that are loaded into segments <b>509</b>-<b>514</b> (FIG. 7) after an instruction is decoded.</p><p><tables id=\"TABLE-US-00001\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 1</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Address Queue Instruction Fields</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"14PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"49PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"154PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Field</entry><entry morerows=\"0\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(AQv-\u2003\u2003)</entry><entry morerows=\"0\" valign=\"top\">Description</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ActiveF</entry><entry morerows=\"0\" valign=\"top\">Indicates entry is active. This signal is</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">decoded from queue pointers 506 and 507, and</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">then delayed one cycle in a register. This</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">signal enables stack retry requests. (1 bit.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Tag</entry><entry morerows=\"0\" valign=\"top\">Active List tag uniquely identifies an</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">instruction within the pipeline. (5 bits.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Func</entry><entry morerows=\"0\" valign=\"top\">Instruction opcode and function. Address queue</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">308 receives a 17-bit function code from decode</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">logic 200. It condenses this to a 7-bit code.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u20020nnnnnn</entry><entry morerows=\"0\" valign=\"top\">\u20026-bit major opcode (modified during</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">instruction predecode), or</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u200210nnnnn</entry><entry morerows=\"0\" valign=\"top\">\u20026-bit function code from COP1X opcode. (AQ</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">gets codes #00-#37 octal only.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u200211 fff cc</entry><entry morerows=\"0\" valign=\"top\">5-bit subfunction code for CACHE operations</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(3-bit function, 2-bit cache select.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Imm</entry><entry morerows=\"0\" valign=\"top\">The immediate field contains the address offset</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">from instruction bits. During decode, the high</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">10 bits are from instruction portion 524 and</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">the low 6 bits are from portion 523 (Fig. 7).</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(16 bits.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Base Register:</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OpSelA</entry><entry morerows=\"0\" valign=\"top\">Operand A, select physical register # in</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">integer register file 306. (6 bits.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OpRdyA</entry><entry morerows=\"0\" valign=\"top\">Operand A is ready for address calculation. (1</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">bit.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OpValA</entry><entry morerows=\"0\" valign=\"top\">Operand A is valid for address calculation.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(Integer register # is not zero; 1 bit.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Index Register or Integer Operand:</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OpSelB</entry><entry morerows=\"0\" valign=\"top\">Operand B, select physical register # in</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">integer register file 306. (For integer</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">stores, this value is duplicated in AQvOpSelC;</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">6 bits.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OpRdyB</entry><entry morerows=\"0\" valign=\"top\">Operand B is ready. (1 bit.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OpValB</entry><entry morerows=\"0\" valign=\"top\">Operand B is valid. (Integer register # is not</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">zero; 1 bit.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Floating-point Operand:</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OpSelC</entry><entry morerows=\"0\" valign=\"top\">Operand C, select physical register # in flt.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">pt. register file 302. (For integer stores,</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">this field contains a copy of AQvOpSelB; 6</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">bits.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OpRdyC</entry><entry morerows=\"0\" valign=\"top\">Operand C is ready. (1 bit.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OpValC</entry><entry morerows=\"0\" valign=\"top\">Operand C is Valid. (1 bit.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Dest</entry><entry morerows=\"0\" valign=\"top\">Destination, select physical register #. (6</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">bits.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">DType</entry><entry morerows=\"0\" valign=\"top\">Destination type (or hint; 2 bits):</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">00 = No destination register. (If prefetch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">instruction, hint = \u201cshared\u201d.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">01 = No destination register. (If prefetch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">instruction, hint = \u201cexclusive\u201d.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">10 = Integer destination register.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">11 = Floating-point destination register.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">UseR</entry><entry morerows=\"0\" valign=\"top\">Which ports ofthe shared register files are</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">required to execute this instruction (4 bits)?</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Bit 3: Flt.pt. Write.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Bit 2: Flt.pt. Read.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Bit 1: Integer Write.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Bit 0: Integer Read.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Store</entry><entry morerows=\"0\" valign=\"top\">This instruction is a store. (1 bit.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Flt</entry><entry morerows=\"0\" valign=\"top\">This instruction loads or stores a floating-</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">point register. (1 bit.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">FltHi</entry><entry morerows=\"0\" valign=\"top\">Load or store high half of floating-point</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">register (if FR=0; 1 bit).</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>With respect to the AQvFunc entry listed in Table 1, a \u201cpredecode\u201d operation partially decodes an instruction as it is written into instruction cache <b>102</b> during a refill operation (i.e., refilling cache <b>102</b> in the event of a miss). This step re-arranges fields within each instruction to facilitate later decoding. In particular, the register select fields are arranged for convenient mapping and dependency checking. The destination and source register fields are put into fixed locations, so they can be used directly as inputs to mapping tables <b>204</b> and <b>206</b> (FIG. <b>1</b>), without further decoding or multiplexing.</p><p>Table 2 below lists address and dependency bits, which are loaded from address calculation unit <b>418</b> and held in segments <b>515</b> and <b>516</b>, respectively, as shown in FIG. <b>7</b>. The dependency bits are updated continuously as previous instructions graduate and other instructions are calculated.</p><p><tables id=\"TABLE-US-00002\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 2</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Address Queue Dependency Bits</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"14PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"56PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"147PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Field</entry><entry morerows=\"0\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(AQv-\u2003\u2003)</entry><entry morerows=\"0\" valign=\"top\">Description</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Index</entry><entry morerows=\"0\" valign=\"top\">Primary cache index address.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">[13:3]</entry><entry morerows=\"0\" valign=\"top\">Bits [13:5] select a set within the primary data</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">cache. Set contains two 8-word blocks.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Bit [5] selects either Bank 0 (if 0) or Bank 1</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(if 1) of the primary data cache.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Bit [4:3] select a doubleword within an 8-word</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">cache block. (Bits [2:0] are decoded into an 8-</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">bit byte mask, which is stored in AQvBytes).</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Bytes</entry><entry morerows=\"0\" valign=\"top\">8-bit mask of bytes used with the addressed</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">doubleword. (Approximate) A byte mask is used</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">to determine if dependencies exist between load</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">and store instructions which access the same</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">double word in the cache. For simplicity, load</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">or store \u201cleft/right\u201d instructions are assumed to</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">use the entire word. This may cause spurious</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">dependencies in a few cases, but the only effect</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">is to delay the load instruction.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">DepC</entry><entry morerows=\"0\" valign=\"top\">16 by 16-bit dependency matrix identifies all.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">previous entries which use the same cache set.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Discussed below.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">DepS</entry><entry morerows=\"0\" valign=\"top\">16 by 16-bit dependency matrix identifies all</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">previous entries which cause a store-to-load</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">dependency (i.e., between stores and subsequent</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">loads). Discussed below.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>As discussed below, Index [<b>13</b>:<b>3</b>] and bytes are also held in address stack <b>420</b>. However, the byte mask contained in address stack <b>420</b> is more precise than that held in address queue <b>308</b>.</p><p>Table 3 lists control bits, which are determined during the course of instruction execution and held in segment <b>517</b>, shown in FIG. <b>7</b>.</p><p><tables id=\"TABLE-US-00003\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 3</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Address Queue Control Bits</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"2\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"1\" colwidth=\"56PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"161PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\">Field</entry><entry morerows=\"0\" valign=\"top\">Description</entry></row><row><entry morerows=\"0\" valign=\"top\">(AQv-\u2003\u2003)</entry><entry morerows=\"0\" valign=\"top\">(Each fieid is 1 bit)</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Calc</entry><entry morerows=\"0\" valign=\"top\">Address has been calculated in the address</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">calculation unit 418. This bit is set at the end</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">of cycle \u201cE2\u201d of the address calculation</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">sequence. The address must be calculated before</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">any other operation can be completed.</entry></row><row><entry morerows=\"0\" valign=\"top\">TagCk</entry><entry morerows=\"0\" valign=\"top\">Entry has completed a tag check cycle with data</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">cache 424. If it resulted in a \u201ccache hit\u201d, the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">operation may proceed. Otherwise, the entry will</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">wait until it can be executed sequentially.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(i.e., If a retry is necessary, it is delayed</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">until all dependencies are removed.)</entry></row><row><entry morerows=\"0\" valign=\"top\">Wait</entry><entry morerows=\"0\" valign=\"top\">This entry set its \u201chit\u201d state bit but that bit</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">was later reset by an external interface command</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">from unit 434. Unless an exception was also set,</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">this entry must retry its tag check cycle to</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">fetch a new copy of its block. However, its</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">request must wait until this entry becomes the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">oldest entry within the address queue 308. This</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">bit set if a \u201cmark invalid\u201d command matches any</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">entry, or if a \u201cmark shared\u201d command matches an</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">entry containing a store instruction. The</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">entry's request is delayed so that the cache will</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">not be continuously thrashed by speculative</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">reads.</entry></row><row><entry morerows=\"0\" valign=\"top\">Ref</entry><entry morerows=\"0\" valign=\"top\">Memory block is being refilled by external</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">interface 434 into a primary cache (i.e.,</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">instruction cache 102 or data cache 424).</entry></row><row><entry morerows=\"0\" valign=\"top\">Upg</entry><entry morerows=\"0\" valign=\"top\">Memory block is being upgraded for a store</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">instruction, so that it can be written. (This</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">block's cache state will become \u201cdirty exclusive,</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">inconsistent\u201d.)</entry></row><row><entry morerows=\"0\" valign=\"top\">Hit</entry><entry morerows=\"0\" valign=\"top\">Memory block is in cache, or is being refilled</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">into cache (if AQvRefill). More specifically,</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">the \u201chit\u201d bit is set if the queue gets a cache</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">hit or if it initiated a refill operation for the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">addressed block. \u201cHit\u201d only means that the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">entry's address matches a valid address in the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">cache tag; it can be set before the refill</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">operation is completed.</entry></row><row><entry morerows=\"0\" valign=\"top\">Way</entry><entry morerows=\"0\" valign=\"top\">Way of the primary data cache in which this block</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">is located. (When the queue gets a \u201chit\u201d or</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">initiates a cache refill, it records which way of</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">the cache was used.)</entry></row><row><entry morerows=\"0\" valign=\"top\">Unc</entry><entry morerows=\"0\" valign=\"top\">This bit is set for cache-ops and loads or stores</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">to addresses within \u201cuncached\u201d regions of memory.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">These instructions must be executed sequentially.</entry></row><row><entry morerows=\"0\" valign=\"top\">Done</entry><entry morerows=\"0\" valign=\"top\">This entry has been completed. (This bit is</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">never set for store instructions.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">This entry has an exception and will be aborted.</entry></row><row><entry morerows=\"0\" valign=\"top\">Exccal</entry><entry morerows=\"0\" valign=\"top\">\u2002ExcCal: The exception was detected during</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">address calculation or translation. Invalid</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">addresses or mis-aligned addresses are detected</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">from the calculated address. Translation</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">exceptions include TLB miss, invalid</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">translations, or write protection violations.</entry></row><row><entry morerows=\"0\" valign=\"top\">Excsoft</entry><entry morerows=\"0\" valign=\"top\">\u2002Excsoft: A soft exception is detected when</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">external interface 434 invalidates a cache block</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">used by a load instruction which is already</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u201cdone\u201d but has not yet graduated. (Soft</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">exceptions flush the pipeline to clear any use of</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">stale data, thus preserving strong memory</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">consistently, but they are not visible to the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">program.)</entry></row><row><entry morerows=\"0\" valign=\"top\">ExcBus</entry><entry morerows=\"0\" valign=\"top\">\u2002ExcBus: The exception was detected when</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">external interface 434 signalled a bus error for</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">a memory operation initiated by this entry.</entry></row><row><entry morerows=\"0\" valign=\"top\">BusyS</entry><entry morerows=\"0\" valign=\"top\">Stage \u201cC1\u201d: Entry is busy. Either:</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">1) \u2003The entry was issued to the address cal-</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u2003\u2003culation unit 418 during previous cycle, or</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">2) \u2003The entry is being retried by address queue</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u2003\u2003308.</entry></row><row><entry morerows=\"0\" valign=\"top\">BusyT</entry><entry morerows=\"0\" valign=\"top\">Stage \u201cC2\u201d: Entry is busy.</entry></row><row><entry morerows=\"0\" valign=\"top\">MatchS</entry><entry morerows=\"0\" valign=\"top\">Entry is in pipeline state \u201cC1\u201d following end of</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">refill state.</entry></row><row><entry morerows=\"0\" valign=\"top\">MatchT</entry><entry morerows=\"0\" valign=\"top\">Entry is in pipeline state \u201cC2\u201d following end of</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">refill state.</entry></row><row><entry morerows=\"0\" valign=\"top\">MatchU</entry><entry morerows=\"0\" valign=\"top\">Entry is in pipeline state \u201cC3\u201d following end of</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">refill state.</entry></row><row><entry morerows=\"0\" valign=\"top\">LoadReq</entry><entry morerows=\"0\" valign=\"top\">Entry requested a \u201cfreeload\u201d cycle within the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">last three cycles. If this request was not</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">granted, it will request a \u201cjustload\u201d cycle</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">instead.</entry></row><row><entry morerows=\"0\" valign=\"top\">LockA</entry><entry morerows=\"0\" valign=\"top\">This entry has completed a tag check cycle, and</entry></row><row><entry morerows=\"0\" valign=\"top\">LockB</entry><entry morerows=\"0\" valign=\"top\">has \u201clocked\u201d the block it needs into its cache</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">set. Blocks can be locked out of a program</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">sequence, but only one block can be locked per</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">cache set.</entry></row><row><entry morerows=\"0\" valign=\"top\">UseA</entry><entry morerows=\"0\" valign=\"top\">This entry is using a block which is not</entry></row><row><entry morerows=\"0\" valign=\"top\">UseB</entry><entry morerows=\"0\" valign=\"top\">(usually) locked in the cache. This \u201cuse\u201d is</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">permitted only for the oldest entry within each</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">cache set.</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>The \u201cC\u201d numbers referenced in Table 3 identify pipeline \u201ccycles\u201d or \u201cstages\u201d associated with cache operations. As discussed below, address calculation uses a three-stage pipeline (i.e., C0-C2, and writes the result into address stack <b>420</b> during a stage C3). Similarly, \u201cE\u201d numbers identify pipeline \u201ccycles\u201d or \u201cstages\u201d associated with execution operations. Generally, cycles E1, E2 and E3 correlate with pipeline stages 4, 5 and 6, identified in FIG. <b>1</b>. \u201cE\u201d and \u201cC\u201d numbers are essentially interchangeable.</p><p>Busy signals AQvBusyS and BusyT prevent any entry from requesting while it is still in the pipeline. Two entries can be busy simultaneously, whenever the queue calculates the address for one entry while it retries a second entry. These entries can be distinguished, because the AQvCalc bit has not yet been set within the first entry.</p><p>The \u201crefill operation\u201d referenced in Table 3 (in conjunction with match signals AQvMatchS, MatchT and MatchU) is the process of loading new data into a cache block. More specifically, if a load instruction gets a \u201cmiss\u201d from data cache <b>424</b>, it waits until the cache is refilled. During a primary cache refill, an 8-word data block is read from either secondary cache <b>432</b> or main memory and is written into data cache <b>424</b>. Each block is transferred as quadwords (4 words or 16 bytes) on two cycles.</p><p>Refill state is reset after data is refilled into data cache <b>424</b> unless there was a secondary cache miss or ECC error. (A 9-bit \u201cError Check and Correction\u201d (ECC) code is appended to each 128-bit doubleword in secondary cache <b>432</b>. This code can be used to correct any single-bit error and to detect all double-bit errors.)</p><p>The three \u201cblock address match\u201d signals are pipelined into stages \u201cC1\u201d, \u201cC2\u201d and \u201cC3.\u201d A miss or an error is detected during \u201cC2\u201d. If neither occurs, refill state ends during phase 2 of cycle \u201cC3.\u201d</p><p>The \u201cfreeload\u201d cycle referenced in Table 3 (in conjunction with signal AQvLoadReq) is when address queue <b>308</b> bypasses data to register files <b>302</b> or <b>306</b> for a \u201cload\u201d instruction, while external interface <b>434</b> is writing this same data into data cache <b>424</b> during a cache refill. This \u201cfree\u201d load avoids using an extra cache cycle for the load. (Only one register can be freeloaded per cycle.) However, if a requested freeload cycle is not granted, the entry will request a \u201cjustload\u201d cycle. This cycle reads only the data array <b>425</b> of data cache <b>424</b>\u2014tag array <b>426</b> is not used.</p><p>The AQvLoadReq bit overrides the \u201crefill\u201d state to enable the freeload and justload requests for three cycles. After this, the \u201crefill\u201d state will have been reset, unless there was a secondary cache miss or ECC error.</p><p>The last entries in Table 3 (i.e., AQvLockA, LockB, UseA, UseB) are used to prevent cache thrashing. Once an entry either \u201chits\u201d or \u201crefills\u201d a cache block, that block is kept in the cache until the associated entry graduates, unless it is invalidated by external interface <b>434</b>. This procedure depends on the 2-way set-associative organization of data cache <b>424</b>. That is, there are two blocks per cache set. The first entry to access a cache set may \u201clock\u201d its block into that set. It remains locked as long as it is addressed by any active entry in queue <b>308</b>. To prevent thrashing, the other block can be used only sequentially by the oldest entry which addresses that cache set. This is determined using AQvDepC (Table 2). This oldest entry marks its block as \u201cused.\u201d (The \u201cuse\u201d bits are required to keep the block in the cache. The \u201clock\u201d bits could be reset if the locking instructions are aborted by a reversed branch.)</p><p>1. Address Queue Function Codes</p><p>FIG. 10 defines the 7-bit operation codes stored in the address queue <b>308</b> at segment <b>512</b> (as shown in FIG. <b>7</b>). This code is condensed from 17 bits of the instruction through decode logic <b>505</b>. The top two bits of the condensed function code indicate the type of function. (The subfield which is stored in the queue is underlined in the \u201cFunction\u201d column of FIG. 8.)</p><p>\u201c0nnnnnn\u201d Bits <b>5</b>:<b>0</b> are a major opcode, from bits <b>31</b>:<b>26</b> of the original instruction. The opcode may have been modified during predecode. These opcodes include most load/store instructions.</p><p>\u201c10nnnnn\u201d Bits <b>5</b>:<b>0</b> are a COP1X function code, from bits <b>5</b>:<b>0</b> of the original instruction. These instructions use base-index addressing for floating-point registers or for prefetch.</p><p>\u201c11nnncc\u201d Bits <b>4</b>:<b>0</b> are a CACHE operation code, from bits <b>20</b>:<b>16</b> of the original instruction. The three high bits specify an operation; the low two bits select a cache.</p><p>The top section of the table shows modified opcodes (Instruction bits <b>31</b>:<b>26</b>). Several opcodes are modified during predecode to facilitate decoding. The original opcodes of these instructions are shown in parentheses, as \u201c(op=32)\u201d; their previous positions are marked by their mnemonic in small italic letters, as \u201c(ldr)\u201d. All modifications are limited to the high three opcode bits, so all changes are within a column. \u201cLWC2\u201d is mapped into the same box as \u201cSWC2\u201d; both instructions cause the same exceptions and do not load any destination register.</p><p>The COP1X opcode (octal #23), which is split and moved to two separate opcodes (octal#13 and #33) during predecode, and the CACHE opcode (octal #57) are replaced with their function and subfunction fields, respectively. Thus, these codes are not stored in the queue. (The corresponding boxes are shaded to indicate that these codes do not occur.)</p><p>The middle section of the table contains five function codes from \u201cCOP1X\u201d. These do \u201cLoad Indexed\u201d, \u201cStore Indexed\u201d, and \u201cPrefetch Indexed\u201d operations. The function code is instruction bits <b>5</b>:<b>0</b>. The two \u201cLoad\u201d instructions are moved to #33, because they have a floating-point destination register. The prefetch and the two \u201cStore\u201d instructions are moved to #13, because they have no destination register.</p><p>The bottom section contains cache operations. There are eight operations which operate on the Instruction Cache (IC), Data Cache (DC) or Secondary Cache (SC).</p><p>There are two formats of \u201cprefetch\u201d instructions: the \u201cLPF\u201d opcode (#63 changed to #73) and the \u201cCOP1X\u201d function \u201cPFETCH\u201d (#17).</p><p>2. Address Queue Operand Registers</p><p>Each address queue entry contains up to three operands. Operand A, stored in segment <b>509</b> (FIG. <b>7</b>), is the integer register which contains the base address. Operand B, stored in segment <b>510</b>, is an integer register. For integer \u201cload modify\u201d instructions (such as LWL and LWR), it contains the old destination register number. For integer store instructions, it contains the value to be stored. For indexed floating-point load/store instructions, operand B contains the index register. If \u201cRegister 0\u201d was selected for either operand A or B, the operand is not \u201cValid\u201d, and its value is zero.</p><p>Operand C, stored in segment <b>513</b>, contains a floating-point register number. For a floating-point store instruction, it is the register value to be stored. For a floating-point \u201cload word\u201d instruction in Mips-2, it contains the old destination register number.</p><p>Operands A and B each have a \u201cReady\u201d bit (see FIG. <b>8</b>), which indicates if the operand value is available yet. These bits are initialized from a Busy Bit Table (not shown), during the cycle following decode. This bit can later be set when the operand is written, using an array of associative comparators.</p><p>Operand A must be ready before issuing integer load/store instructions for address calculation. For floating-point, both operand A and B must be ready. (Both operands are used for indexed load/store; operand B defaults to \u201cready\u201d for other floating-point load/store.). Before a stack operation can be requested, operand B (for integer) or C (for floating-point) must be ready. These operands are needed for \u201cload/modify\u201d instructions.</p><p>The ready bits are not checked when graduating store instructions, because all previous instructions must have graduated before the store is eligible to graduate.</p><p>3. Address Associative Ports</p><p>The Address Queue has two associative ports which compare cache index addresses. Each port uses dynamic comparators which are clocked on the leading edge of phase 2 (i.e., \u03c62) of the processor clock.</p><p>The dependency port compares the calculated memory address (VAdr[<b>13</b>:<b>5</b>]) generated by address calculation unit <b>418</b> to each entry of segment <b>516</b> (FIG. 7) to detect accesses to the same cache set. These comparisons occur during stage C1 of each address calculation sequence.</p><p>An external port <b>525</b> determines if an external request affects any entries in the stack. It compares the cache index (VAdr[<b>13</b>:<b>5</b>]), the doubleword address (VAdr[<b>4</b>:<b>3</b>]), and the cache way. These comparisons occur during stage C0 of each external operation sequence.</p><p>B. Address Calculation Sequence</p><p>Address queue <b>308</b> issues instructions to address calculation unit <b>418</b> when its base (and index for indexed load and store instructions) is ready. This sequence is illustrated in the timing chart shown in FIG. <b>11</b>.</p><p>Address calculation requires a three-stage pipeline; i.e., stages C0 (issue), C1 (address calculation) and C2 (data cache) as shown in FIG. <b>11</b>. This calculation is performed only once for each load/store instruction.</p><p>During phase 1 of stage C0, address queue <b>308</b> issues the oldest entry with a pending request. During phase 2, the base (and index) registers of the instruction are read from integer register file <b>306</b>. At the end of cycle C0, the base register is latched in operand \u201cA\u201d and either the index register or the 16-bit immediate field is latched in operand \u201cB.\u201d Either register may be bypassed from any of the three write ports <b>526</b>-<b>528</b> of integer register file <b>306</b> (FIG. <b>7</b>).</p><p>For speed, data may be \u201cbypassed\u201d around a memory when reading a location during the same cycle that it is written. In other words, the newly written value is multiplexed directly to a read port without waiting for it to be actually written into the memory. A bypass multiplexer is selected whenever the operand register number of a current instruction equals the destination register number of a previous instruction. Bypassing is necessary to reduce instruction latency.</p><p>During phase 1 of stage C1, the address is calculated using 64-bit adder <b>529</b> disposed in address calculation unit <b>418</b> (FIG. <b>7</b>). For \u201cbase+offset\u201d format, adder <b>529</b> adds the base register on line <b>530</b> to the sign-extended 16-bit immediate field on line <b>531</b>. For \u201cbase+index\u201d format, adder <b>529</b> adds base register on line <b>530</b> and index register on line <b>532</b> together. The resulting virtual address on line <b>533</b> is sent to address queue <b>308</b> (i.e., segments <b>515</b> and <b>516</b>) and TLB <b>422</b>, as shown in FIG. <b>7</b>.</p><p>During phase 2 of C1, TLB <b>422</b> compares \u201cvirtual page\u201d address bits <b>63</b>:<b>62</b> and <b>43</b>:<b>13</b> to each of its entries. Also during phase 2, address queue <b>308</b> compares \u201cindex\u201d bits VAdr[<b>13</b>:<b>3</b>] to the index stored in each entry in segment <b>516</b>. This helps determine cache block and store-to-load dependencies, as discussed below. The newly calculated index is written into the new entry in segment <b>516</b>.</p><p>During stage C2, a physical address is output from the entry of the TLB that matched.</p><p>Stage C3 (write result) is used for writing a calculated physical address to address stack <b>420</b> (i.e., segment <b>519</b> as shown in FIG. <b>7</b>).</p><p>C. Issuing Instructions to the Data Cache</p><p>Data cache <b>424</b> is the subject of a copending application, as noted above, and therefore will not be discussed in detail. However, as background, the data cache contains 32K-bytes of memory. It is interleaved using two identical 16K-byte banks (i.e., banks #0 and #1). Each bank consists of a 256-row by 576-bit data array and 64-row by 35-bit tag array. The data array can access two 72-bit doublewords in parallel. The tag array can access two 32-bit tags in parallel.</p><p>The banks operate independently. For some operations, the tag and data arrays can operate independently. Thus, there are four arrays (two tag and two data) which are separately allocated.</p><p>The arrays are allocated among requests from four circuits. All four circuits can operate simultaneously, if they are allocated the cache array(s) they need. The highest priority request is used for external interface. The next-highest request is used for graduating store instructions if oldest in active list <b>212</b>. The next-highest request is used for all other load/store operations which are retried from the Address Queue. The next priority is for instructions whose address is being calculated. The lowest priority is for a store that is not the oldest instruction in processor <b>100</b>. (See Table 4).</p><p>Each bank is 2-way set associative. Two tags are read and compared in parallel for tag checks for the CPU and external \u201cinterrogate\u201d operations. This determines which way of the cache, if any, contains the desired data. The way is remembered and used later for graduating stores, or for external refill or writeback operations.</p><p>The data cache is addressed by virtual address bits. Address bits [<b>2</b>:<b>0</b>] select a byte within a doubleword (i.e., 64 bits). Bits [<b>4</b>:<b>3</b>] select a doubleword within a block (8-words). Bit <b>5</b> selects bank #0 or bank #1. Bits [<b>13</b>:<b>6</b>] address a set within the bank. More specifically, these 8 bits are decoded to select one of 256 \u201cword lines\u201d in the cache data array. Each word line enables eight doublewords (16 words). Thus, the word line enables two blocks (i.e., one block in each way) which represent a single set in a 2-way set-associative cache. The desired block (i.e., way) is identified through tag checking.</p><p>Doublewords within these blocks are selected using 4-to-1 multiplexers in the sense amplifiers. Bits are interlaced so that the cache can access doublewords differently for processor or external interface operations. The processor associatively accesses doublewords within two blocks. The external interface accesses two doublewords within the same block in parallel.</p><p>As noted above, data cache <b>424</b> is shared between three processor units and external interface unit <b>434</b>. The processor units perform tag checks, load instructions, and store instructions. These units compete for cache and register resources based on priorities described below. Each unit can do only one operation per cycle, but all four units may operate simultaneously, if their resource requirements do not collide. These operation are described in detail in copending application Ser. No. 08/324,124 which, as noted above, is incorporated herein by reference.</p><p>All load and store instructions are put in the address queue <b>308</b> after they are decoded. When their base and index registers are available, they are issued to address calculation unit <b>418</b> (FIG. <b>7</b>), which computes a virtual address (VAdr[<b>63</b>:<b>62</b>,<b>43</b>:<b>0</b>]), and the TLB <b>422</b> which converts this into a physical address (PAdr[<b>39</b>:<b>0</b>]). These addresses are written into address stack <b>420</b>. The \u201cindex\u201d bits of the virtual address (i.e., VAdr[<b>13</b>:<b>0</b>]) are written into address queue <b>308</b> (at segments <b>515</b> and <b>516</b>), for dependency checking. This sequence is performed by a dedicated pipeline.</p><p>Data cache <b>424</b> can be accessed in parallel with a TLB access, if the required bank's tag array is available. If the tag array generates a cache \u201chit\u201d for a load instruction, that instruction can be completed immediately. Otherwise, or if there are \u201cstore-to-load\u201d dependencies, the instruction must later be re-issued from the address queue <b>308</b>. These later accesses can do tag checks, loads, or both. A separate unit controls writing register values into the cache, as part of graduating store instructions.</p><p>1. Data Cache Usage</p><p>As noted above, data cache <b>424</b> contains two identical banks. Each bank contains tag and data arrays which can operate separately. Thus, there are four separate cache arrays which can be allocated independently. Each request contains a 4-bit code which indicates which of these arrays it needs:</p><p>...UseC[<b>3</b>]: Bank 1 Tag Array.</p><p>...UseC[<b>2</b>]: Bank 1 Data Array.</p><p>...UseC[<b>1</b>]: Bank 0 Tag Array.</p><p>...UseC(<b>0</b>]: Bank 0 Data Array.</p><p>Usage signals are generated for the external interface (ExtUseC), queue retry (AccUseC), and store (StoUseC) during the request cycle (\u201cC0\u201d). This cycle is two cycles before the array is read or written. If none of these requests occur, the arrays are available for use by the instruction whose address is being calculated.</p><p>Priority for using the Data Cache is illustrated in FIG. <b>12</b> and Table 4. Each unit can request an operation from either Data Cache bank, depending on its address bit <b>5</b>. Some operations require use of both the tag and data arrays within that bank. High priority requests are determined during the \u201cC0\u201d request cycle. Lower requests are not determined until the next cycle, because they depend on signals during \u201cC1\u201d.</p><p>External interface <b>434</b> provides a 4-bit command code (\u201cDCmd[<b>3</b>:<b>0</b>]\u201d) and a cache index (\u201cIndex[<b>13</b>:<b>4</b>]\u201d) to the processor (i.e., cycle C0), two cycles before it uses data cache <b>424</b>. The command and address bit <b>5</b> are decoded to determine which cache arrays are needed. The external interface has highest priority; its requests are always granted. If this command refills a data quadword, and address queue <b>308</b> contains a \u201cload\u201d instruction in an entry waiting for this quadword, the queue selects and issues that entry to the load unit. This operation, as described above, is called a \u201cfreeload.\u201d</p><p>Store operations are requested only if the oldest store instruction is ready to graduate, its store value is ready from a register and any previous load has been completed without an exception. (The request for a shared read port is made one cycle earlier. This request has low priority, because it is unknown if the store could graduate.) If this store instruction is the oldest instruction (not just the oldest store instruction), it is given high priority for the cache, because it is guaranteed to graduate if ready. Otherwise, other queue accesses are given priority.</p><p>Queue retry accesses can perform tag checks, loads, or both. Each entry can generate one of four requests, based on its state and bit <b>5</b> of its address. For each bank, these include a request for the tag array (and perhaps the data array) and a request for only the data array. Address queue <b>308</b> uses four priority encoders to select one entry from each group of requests. One of these four is then selected, after it determines which arrays were needed by the external interface and guaranteed stores. Tag checks are given priority over data-only requests. Priority between the banks is alternated. (This makes use of the two banks more uniform. Also, if a tag check generates a refill, that bank's tag array will be busy during the next cycle, which would abort any new operation which uses that tag array.)</p><p>Lower-priority requests are resolved during \u201cC1\u201d cycle. Active list <b>212</b> determines which instructions graduate. If a store instruction graduates at the beginning of the \u201cC1\u201d cycle, it will use its data cache bank. For instructions whose address is being calculated, the bank is selected using VAdr[<b>5</b>], which becomes valid at about the middle of phase 1. A tag check cycle is performed, if the selected tag array is available. For load instructions, a data load operation is also performed, if the data array is also available. (It is useful to complete the tag check, even if the load must be repeated. If the tag check results in a \u201cmiss\u201d or a memory dependency, the data array would not be used anyway. If it hits, the next retry from the queue will need only a data array.)</p><p><tables id=\"TABLE-US-00004\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 4</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Priority for using Data Cache</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"4\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"14PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"35PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"42PT\"></colspec><colspec align=\"left\" colname=\"3\" colwidth=\"126PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Pri-</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ority</entry><entry morerows=\"0\" valign=\"top\">Unit</entry><entry morerows=\"0\" valign=\"top\">Description</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"3\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">External</entry><entry morerows=\"0\" valign=\"top\">External interface may preempt use of</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Interface</entry><entry morerows=\"0\" valign=\"top\">either bank of the data cache 424 by</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">asserting a request on two cycles before</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">the cache access.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">2</entry><entry morerows=\"0\" valign=\"top\">Certain</entry><entry morerows=\"0\" valign=\"top\">A store instruction is given priority if</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Store</entry><entry morerows=\"0\" valign=\"top\">it is guaranteed to graduate.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">3</entry><entry morerows=\"0\" valign=\"top\">Retry</entry><entry morerows=\"0\" valign=\"top\">Instruction retrying access has priority</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Access</entry><entry morerows=\"0\" valign=\"top\">over new address calculation.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">4</entry><entry morerows=\"0\" valign=\"top\">Address</entry><entry morerows=\"0\" valign=\"top\">Until a new address is calculated, the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Calculate</entry><entry morerows=\"0\" valign=\"top\">priority circuit does not know which</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">cache bank it requires. It will access</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">the cache if the required bank is</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">available.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">5</entry><entry morerows=\"0\" valign=\"top\">Other</entry><entry morerows=\"0\" valign=\"top\">If a store is not the oldest instruction</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Store</entry><entry morerows=\"0\" valign=\"top\">in the processor, it might not graduate.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Thus, it has low priority so that it</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">will not interfere with operations which</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">are known to be needed.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"3\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>2. Shared Register Usage</p><p>Most processor requests require use of a shared register port. These ports are allocated independently of the cache, by a move unit (not shown). The move unit transfers data between register files <b>302</b>, <b>306</b> and data cache <b>424</b> and branch unit <b>214</b>. These units share control of one read and one write port to each of the integer and floating-point <b>15</b> register files. These shared ports are used for all register accesses except for the dedicated ports assigned the integer and floating-point arithmetic units. An instruction cannot be completed if it does not get the register ports it requires. However, a tag check can be completed.</p><p>Most loads require a destination register in either the integer <b>306</b> or floating-point <b>302</b> register files. If an integer load specifies \u201cregister 0,\u201d however, no destination is stored. \u201cLoad/modify\u201d instructions also require a read port to fetch the old value of the destination register. The required registers are determined in the decode unit, and are signaled to the queue as operand \u201cvalid\u201d bits.</p><p>For uncached load instructions, registers are allocated at the end of the \u201cC0\u201d cycle. These requests have absolute priority, because these instructions cannot be retried. For other load instructions, the ports are allocated during \u201cC1\u201d. These requests have low priority.</p><p>For a store instruction, the data register is located and the appropriate register file port is allocated one cycle before \u201cC0\u201d. Stores have the lowest priority for ports, because it is not known if they can actually graduate. Stores are selected by default if no other request is present.</p><p>3. \u201cActive\u201d Signals in Address Queue</p><p>Each queue entry is \u201cactive\u201d if it contains a valid instruction. For timing reasons, there are several sets of \u201cactive\u201d signals. These signals are generated using read pointer <b>506</b> and write pointer <b>507</b> (FIG. 7) of the address queue. Only the primary \u201cactive\u201d signals correspond exactly to the pointers. The other signals vary slightly logically, but have better timing for special uses. The associated logic <b>1200</b> is illustrated in FIG. <b>13</b>. Active signals are defined as follows:</p><p>AQvActive[<b>15</b>:<b>01</b>]: Active bits determined directly from the \u201cread\u201d and \u201cwrite\u201d pointers. These signals become valid late during phase 1. (\u201cPrimary\u201d signals.)</p><p>AQvActiveB[<b>15</b>:<b>0</b>]: AQvActive, but delete entries which are aborted if a branch is reversed. These signals become valid late during phase 2.</p><p>AQvActiveL[<b>15</b>:<b>0</b>]: AQvActiveB delayed in a phase-2 latch. These signals become valid at the end of phase 2, and can be read dynamically using a phase-i strobe.</p><p>AQvActiveF[<b>15</b>:<b>0</b>]: AQvActiveL delayed in a phase-1 latch. These signals switch cleanly at the beginning of each cycle. They are used to reset rows and columns of the dependency matrices discussed below. (The two latches create an edge-triggered register.)</p><p>Referring to FIG. 13, new decoded entries to address queue <b>308</b>, identified by a 4-bit \u201cload/store instruction mask,\u201d are processed by logic block <b>1201</b>. Block <b>1201</b> generates a 16-bit \u201cPutVect\u201d signal which maps one of four input instructions to each of the address queue write ports. Block <b>1201</b> also informs logic block <b>1203</b> of the number of instructions written into address queue <b>308</b> each cycle via signal \u201cInstrWr.\u201d Block <b>1203</b> calculates the next value of write pointer (\u201cWrPtr\u201d) using the InstrWr signal. This block also calculates the next value of read pointer (\u201cRdPtr\u201d) using the \u201cmask of graduating load/store instructions\u201d (i.e., a signal indicating the number of instructions graduating from address queue <b>308</b>). The RdPtr and WrPtr signals are forwarded to logic block <b>1204</b> which generates read mask <b>1205</b> and write masks <b>1206</b> and <b>1207</b>. (Mask <b>1207</b> is the same as <b>1206</b> except aborted entries may be deleted if a branch is reversed via MUX <b>1208</b>.) These signals are further processed in block <b>1204</b> to generate \u201cactive\u201d signals, discussed below. In the event of a reverse branch, branch unit <b>214</b> provides information to restore the WrPtr to its pre-branch state. Major elements of the conventional logic used to implement these blocks is illustrated in FIG. <b>13</b>.</p><p>As discussed above, RdPtr and WrPtr are 5-bit counters which configure the queue's entries as a circular FIFO. The four lower bits select one of the queue's 16 entries. The fifth bit distinguishes between an empty queue (read and write pointers are identical) and a full queue (pointers differ in high bit only). This bit also indicates if the write pointer has \u201cwrapped\u201d to zero module <b>16</b>, and the read pointer has not. In this case, the write pointer is less than or equal to the read pointer.</p><p>As shown in FIG. 13, \u201cactive\u201d bits are generated from two masks formed from the two pointers. Each mask sets all bits lower than the pointer, as shown in FIG. <b>14</b>. For example, when the read pointer equals \u20186\u2019, its mask has bits [<b>5</b>:<b>0</b>] set. The \u201cactive\u201d bits are set for entries which have been decoded (\u201cwritten\u201d WrMask) but not graduated (\u201cread\u201d \u02dcRdMask). If the write pointer has not wrapped, the masks are logically ANDed. If it has wrapped, the masks are logically ORed.</p><p>The write pointer is the counter output directly. It is incremented at the end of each cycle by the number of load/store instructions which were decoded in block <b>1203</b>. These instructions are written into the queue, and become active, at the beginning of the next cycle.</p><p>If a speculative branch was mispredicted, the write pointer will be restored using a shadow register associated with that branch from branch unit <b>214</b>. This deletes all the instructions which were speculatively decoded after the branch. The shadow register is loaded from the write pointer (plus the number of load/store instructions decoded before the branch, if any), when the branch is originally decoded. There are four shadow registers (i.e., <b>1209</b>-<b>1212</b>), because the decode unit can speculatively fetch past four nested speculative branches. The restore signal (i.e., \u201cRstrEn\u201d) is not valid until early in phase 2, so the write pointer and active mask are not updated until the next cycle.</p><p>However, the restore does delete entries from a later \u201cactive\u201d signal; i.e., AQvActiveB[<b>15</b>:<b>0</b>). These signals are logically ORed with signal \u201cNewEntry[<b>15</b>:<b>0</b>],\u201d which indicates newly decoded entries. The results are saved in a register composed of latches <b>1213</b> and <b>1214</b>. The output of latch <b>1213</b> (i.e., AQvActiveL[<b>15</b>:<b>0</b>]) becomes valid late during phase 2, and can be read dynamically during phase 1 of the next cycle. The output of latch <b>1214</b> is valid during the following cycle. These signals are used to reset rows and columns of the dependency matrices. These signals are logically equivalent to the original \u201cactive\u201d signals, except that they go inactive one cycle later after an instruction graduates.</p><p>The read pointer is the output of adder <b>1215</b> in logic block <b>1203</b>, which increments the read counter (\u201cRdPtrD\u201d) by the number of load/store instructions which graduated at the end of the previous cycle (\u201cGR1WOGradLS\u201d). These signals occur too late to be added before the clock edge.</p><p>The pointers are subtracted to determine the number of empty slots in address queue <b>308</b> in logic block <b>1202</b>. This information is sent to the decode logic <b>200</b> (FIG. <b>1</b>), so it will not decode more load/store instructions than will fit in the queue. The subtraction uses 1s-complement arithmetic to simplify decoding. The signals \u201cAQ0D0EmptySlot[<b>3</b>:<b>0</b>)\u201d are a unary mask.</p><p>4. Priority Logic within Address Queue</p><p>The Address Queue's priority logic <b>1500</b> is illustrated in FIG. <b>15</b>. In this illustration, logic is arranged according to its pipeline position. This logic corresponds to four pipeline stages. The first stage locates the oldest and next oldest store instructions. The second \u201cRequest\u201d stage allocates the cache arrays for external interface, guaranteed store, and retry accesses. The third \u201cSet-up\u201d stage allocates arrays for other stores which graduate, and for instructions in the Address Calculation unit. FIG. 15 shows the timing and relationship between major signals.</p><p>A \u201c...UseR\u201d signal contains four bits which indicate which shared register file ports of register files <b>302</b> and <b>306</b> are needed.</p><p>Bit <b>3</b>: Floating-point Register File, shared write port.</p><p>Bit <b>2</b>: Floating-point Register File, shared read port.</p><p>Bit <b>1</b>: Integer Register File, shared write port.</p><p>Bit <b>0</b>: Integer Register File, shared read port.</p><p>The External Interface Command code (\u201cEX0DCmd\u201d in FIG. 15) is decoded to determine which data cache sections are required by external interface <b>434</b> (\u201cExtUseC\u201d). This code also indicates if refill data is available.</p><p>Requests for retry accesses of the Address Queue are determined during cycle C0. There are three groups of requests which compete: \u201cfreeload\u201d (i.e., a load instruction in the queue can use data which is being refilled into the data cache <b>424</b>), \u201cretry access\u201d (i.e., an instructions already in the queue can request that its operation be retried, if there are no dependencies, the required cache section is available, and the addressed block is not waiting for a refill to be completed) and \u201caddress calculate\u201d (i.e., the cache can be accessed in parallel with address calculation and translation).</p><p>Freeload operations are given priority because they share use of the cache bank which the external interface is refilling. These operations do not have to compete for cache resources. However, they do have to compete for register ports. The operation may also fail if the secondary cache misses or has an ECC error.</p><p>Retry access uses four sets of requests which correspond to the data cache sections, as described above with respect to the \u201cUseC\u201d signal. One of these sets is selected, based on which sections are not used by external interface <b>434</b> or a store. Thus, each set of requests is enabled only if the corresponding cache section is available. This resource check only considers one section for each request. If both the data and tag arrays are needed, only the tag array is checked. The tag array is used for tag check operations. If the data array is also available, it can be used for loading data. But the tag check can be done even if the data array is not available, and the data array will be accessed independently later.</p><p>Newly calculated instructions have the lowest priority. First they tend to be newly decoded instructions, and older instructions are usually given higher priority. The older instructions are more likely to have later instructions dependent on them and are less likely to be speculative. Also, the bank which they require is not know during cycle \u201cC0\u201d; it depends on address bit VAdr[<b>5</b>], which is calculated early during cycle \u201cC1\u201d. Thus, the availability of its cache sections is not known until that cycle.</p><p>These three sets of requests are combined into a single request set at the end of cycle \u201cC0\u201d identified as \u201cAccComReq,\u201d as shown in FIG. <b>15</b>. The highest priority request in the combined set is granted by a dynamic priority encoder at the beginning of cycle \u201cC1\u201d. This encoder gives higher priority to older instructions, based on the read pointer (AQvRdPtr) of the address queue. (Generally, performance is improved by giving older instructions priority. Also, this avoids a possible deadlock case. In some cases, an entry will continually retry its operation every three cycles while it waits for a resource. This occurs, for instance, for \u201cload-modify\u201d instructions if the old value of their destination register is still busy. In rare cases, three such entries could monopolize all queue issue cycles, inhibiting lower priority instructions. If priority was not based on program order, this would create a deadlock if one of the inhibited instructions generates the old destination value.)</p><p>External interface <b>434</b> also sends the cache index address (i.e., \u201cCC0PDIndex[<b>13</b>:<b>4</b>]\u201d and \u201cCC0PDWay\u201d). If these match any queue entry which is in \u201crefill\u201d state, that entry's refill bit is reset. This allows that entry to request a load operation. For a load instruction, the refill data can be bypassed to the processor while it is being written into the cache. Because this bypass does not require an extra cache cycle, it is called a \u201cfreeload\u201d operation, as described above. The refill may be aborted during the \u201cC2\u201d cycle if there was a Secondary Cache miss or an ECC error. If aborted, each corresponding entry must have its refill bit set again (AQvRefill). Thus, the match signals are pipelined in each entry (AQvFreeS and AQvFreeT; i.e., the freeload request signal is pipelined in analogous fashion to the match signals described above). If the refill is aborted during a freeload, the abort signal inhibits a \u201cLoadDone\u201d signal of the load instruction. If the load is retried later (but was requested before aborting), the AQvRefill bit of the subject entry is inhibited.</p><p>5. Access Priority Logic</p><p>Older instructions have priority for retry accesses in the address queue <b>308</b>. The read pointer <b>506</b> of the queue (FIG. <b>7</b>), which points to the oldest instruction in the queue, selects the entry with highest priority. Subsequent entries have decreasing priority. An example is illustrated in FIG. <b>16</b>. As shown, entry 9 is the oldest. Entries 10 through 15, and then entries 0 through 8, are next oldest. Entry 8 contains the newest instruction and has the lowest priority.</p><p>Implementing logic is shown in FIG. <b>17</b>. The sixteen entries are divided into four groups of four entries. Each group is implemented with identical logic. The grant signal has two inputs. The first input (upper and-or input in drawing) is asserted by any request in the highest group of requests. This group is selected by decoding the upper two pointer bits: RdPtr[<b>3</b>:<b>2</b>]. This group may also contain several of the lowest requests, which are gated off using a bit mask generated from RdPtr[<b>1</b>:<b>0</b>].</p><p>Lower priority requests are granted using a 4-wide and-or gate which determines if any higher-priority group contains any request. The grant for the \u201chighest\u201d group enables the lowest entries, which were masked off from the group requests. This priority circuit includes the \u201chighest\u201d bits, but it will not be enabled if any of those requests are pending.</p><p>The tables in FIG. 17 (i.e., <b>1610</b>-<b>1613</b>) identify which bits within each group are enabled for a selected group. A number indicates the bit is enabled while an \u201cx\u201d indicates the bit is disabled. Accordingly, if Ptr[<b>3</b>:<b>2</b>] is \u201c00\u201d then, all table <b>1610</b> outputs (coupled to AND gates <b>1602</b>-<b>1605</b>) are high. Tables <b>1611</b>-<b>1613</b> operate in similar fashion.</p><p>Logic <b>1600</b>, shown in FIG. 17, implements \u201cGroup 0\u201d (i.e., containing bits <b>3</b>:<b>0</b>). This logic is replicated four times for sixteen bits (i.e., Groups 0 through 3). RdPtr[<b>3</b>:<b>0</b>] selects the oldest entry in the queue. Ptr[<b>3</b>:<b>2</b>] identifies the associated group and \u201cHigh[n]\u201d is active for the nth group (i.e., if Group 0 contains the oldest entry, then High[<b>0</b>] is a logic \u201c1\u201d). \u201cReq[<b>0</b>]\u201d to \u201cReq(<b>3</b>]\u201d are retry requests from entries 0 through 3, respectively, in a particular group.</p><p>Lines <b>1606</b> to <b>1609</b> (i.e., \u201cGroup0\u201d to \u201cGroup3\u201d) are a logic 1 when a request is present in the respective group (i.e., each line is coupled to an OR gate which is, in turn, coupled to a circuit identical to logic <b>1600</b> for that particular group). When a request in a group is granted, the appropriate \u201cGrant\u201d signal (i.e., Grant[<b>0</b>], [<b>1</b>], [<b>2</b>] or [<b>3</b>]) is high. Additional circuit functional description is provided in FIG. <b>17</b>.</p><p>6. Synchronize Instruction (\u201cSYNC\u201d)</p><p>A \u201cSYNC\u201d instruction (\u201cSYNC\u201d, opcode 0 with function octal \u201817\u2019) provides a memory barrier, which may be used to control sequencing of memory operations in a loosely ordered system. Architecturally, it guarantees that the entire system has completed all previous memory instructions, before any subsequent memory instruction can graduate. Write-back buffers (for implementing a write-back protocol) of external interface <b>434</b> must be empty, and the external memory system of processor <b>100</b> has no pending operations. External interface <b>434</b> asserts signal \u201cCD0SyncGradEn\u201d whenever a SYNC instruction may graduate.</p><p>SYNC instructions are implemented in a \u201clight-weight\u201d fashion on processor <b>100</b>. The processor continues to fetch and decode instructions. It is allowed to process load and store instructions speculatively and out-of-order following a \u201cSYNC.\u201d This includes refilling the cache and loading values into registers. Because processor <b>100</b> graduates instructions in order, however, no data is stored and none of these instructions can graduate until after the SYNC graduates. One of these speculative loads could use a cache block which is invalidated before it is graduated, but it will be aborted with a \u201csoft exception.\u201d</p><p>Whenever a primary data cache block is invalidated, its index is compared to all load instructions in address stack <b>420</b>. If it matches and the load has been completed, a soft exception (\u201cstrong ordering violation\u201d) is flagged for that instruction. The exception prevents the instruction from graduating. When it is ready to graduate, the entire pipeline is flushed and the state of the processor is restored to before the load was decoded. Because this exception is soft (and must not be reported to the kernel), the pipeline immediately resumes executing instructions.</p><p>Address queue <b>308</b> continues to execute load and store instructions speculatively. If an external bus operation causes the needed cache line to be invalidated, the instruction will be aborted using the \u201csoft\u201d exception mechanism and then automatically retried.</p><p>A \u201cSYNC\u201d instruction is loaded into address queue <b>308</b>, but the queue does not calculate an address or perform any operation. It is marked \u201cdone\u201d after it becomes the oldest instruction in the address queue and external interface <b>434</b> asserts \u201cCD0SyncGradEn.\u201d It can then graduate in order.</p><p>7. Synchronizing Cache-op Instructions</p><p>Cache-op instructions (\u201cCACHE\u201d, opcode octal \u201857\u2019; FIG. 10) are executed sequentially by the address queue <b>308</b>. Whenever a cache-op instruction is in the queue, the execution of all later instructions is inhibited until it graduates. Address calculation is performed, but cache access is not enabled.</p><p>Whenever one or more cache-op instructions are in the address queue <b>308</b>, they generate a mask which inhibits any subsequent instructions from completing a load instruction, or from requesting a retry access. The logic which generates this mask is illustrated in FIG. <b>18</b>. This generates a mask which inhibits all entries after a sequential instruction. The 4-bit read pointer \u201cPtr[<b>3</b>:<b>0</b>]\u201d selects the oldest instruction in the queue. The input \u201cSync[<b>15</b>:<b>0</b>]\u201d is a 16 bit vector in which a \u20181\u2019 indicates that the entry is a cache-op. This circuit sets a \u20181\u2019 in all bits following the first input bit which is set.</p><p>The sixteen entries are divided into four groups of four entries. Each group is implemented with identical logic. The \u201cSyncWait\u201d signal has two inputs. The first input (upper and-or input in drawing) is asserted by any request in the highest group of requests. This group is selected by decoding the upper two pointer bits: RdPtr[<b>3</b>:<b>2</b>]. This group may also contain several of the lowest requests, which are gated off using a bit mask generated from Rdptr[<b>1</b>:<b>0</b>].</p><p>Like FIG. 17, the logic <b>1800</b> in FIG. 18 implements \u201cGroup 0,\u201d which contains bits <b>3</b> to <b>0</b>. This logic is replicated four time for 16 bits. Only tables <b>1801</b>, <b>1802</b>, <b>1803</b> and <b>1804</b> change for each group. Ptr[<b>3</b>:<b>0</b>] selects the oldest entry. Sync[<b>3</b>:<b>0</b>] is input and SyncWait[<b>3</b>:<b>0</b>] is output. In tables <b>1801</b>-<b>1804</b>, a number indicates an enabled (i.e., high) bit while an \u201cx\u201d indicates a disabled (i.e., low) bit.</p><p>FIG. 19 provides an example of the synchronize mask and FIG. 20 shows how the low pointer bits affect bits within the high group. Specifically, the \u201chigh\u201d group contains four entries which may vary between the highest and lowest priority, depending on the low two bits of the pointer. When both pointer bits are zero, all four entries are within the highest group and each can set mask bits for any entry to its left. For other pointers, some entries are within the lowest group. These entries can set mask bits within the lowest, but they cannot set any mask bits for the highest entries. All of the lowest mask bits are set if there is any bit set within any other group.</p><p>D. Retry Accesses</p><p>1. Access Request Logic</p><p>Each entry within the address queue <b>308</b> can generate a request to retry its operation. This logic is summarized in Table 5. Signals which are associated with retry accesses have a prefix \u201cAcc...\u201d.</p><p><tables id=\"TABLE-US-00005\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 5</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Cache Access Requests (AccComReq)</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"1\" colwidth=\"42PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"49PT\"></colspec><colspec align=\"left\" colname=\"3\" colwidth=\"126PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">16-bit</entry><entry morerows=\"0\" valign=\"top\">Description (In order of decreasing</entry></row><row><entry morerows=\"0\" valign=\"top\">Request</entry><entry morerows=\"0\" valign=\"top\">Select</entry><entry morerows=\"0\" valign=\"top\">priority.)</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"3\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">UncLd-</entry><entry morerows=\"0\" valign=\"top\">RdPtrVec</entry><entry morerows=\"0\" valign=\"top\">Begin an uncached load when it becomes</entry></row><row><entry morerows=\"0\" valign=\"top\">ReqEn</entry><entry morerows=\"0\" valign=\"top\">[15:0]</entry><entry morerows=\"0\" valign=\"top\">the oldest instruction.</entry></row><row><entry morerows=\"0\" valign=\"top\">or</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Finish an uncached load when data is</entry></row><row><entry morerows=\"0\" valign=\"top\">E0Unc-</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">returned by External Interface.</entry></row><row><entry morerows=\"0\" valign=\"top\">Load</entry></row><row><entry morerows=\"0\" valign=\"top\">UncStReq</entry><entry morerows=\"0\" valign=\"top\">StSel</entry><entry morerows=\"0\" valign=\"top\">An uncached store instruction needs</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">[15:0]</entry><entry morerows=\"0\" valign=\"top\">both the tag and data sections of the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">cache. It uses the tag section to</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">send the address to the External</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Interface.</entry></row><row><entry morerows=\"0\" valign=\"top\">E0-</entry><entry morerows=\"0\" valign=\"top\">ExtFreeR</entry><entry morerows=\"0\" valign=\"top\">A \u201cfree load\u201d cycle is requested for a</entry></row><row><entry morerows=\"0\" valign=\"top\">GrantExt</entry><entry morerows=\"0\" valign=\"top\">eq</entry><entry morerows=\"0\" valign=\"top\">cacheable load instruction, while the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">[15:0]</entry><entry morerows=\"0\" valign=\"top\">cache is refilled from the External</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Interface.</entry></row><row><entry morerows=\"0\" valign=\"top\">Req-</entry><entry morerows=\"0\" valign=\"top\">AccReqMux</entry><entry morerows=\"0\" valign=\"top\">An entry requests a retry operation</entry></row><row><entry morerows=\"0\" valign=\"top\">AnyAcc</entry><entry morerows=\"0\" valign=\"top\">[15:0]</entry><entry morerows=\"0\" valign=\"top\">for a cacheable instruction. The</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">corresponding section of the data</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Cache is not being used by External</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Interface.</entry></row><row><entry morerows=\"0\" valign=\"top\">\u02dcInhi-</entry><entry morerows=\"0\" valign=\"top\">AQIssue</entry><entry morerows=\"0\" valign=\"top\">An access is requested simultaneously</entry></row><row><entry morerows=\"0\" valign=\"top\">bitACalc</entry><entry morerows=\"0\" valign=\"top\">[15:0]</entry><entry morerows=\"0\" valign=\"top\">with the initial address calculation,</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">except when it is aborted or to</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">re=calculate an address for an</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">exception</entry></row><row><entry morerows=\"0\" valign=\"top\">default</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">No requests.</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"3\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>Uncached Loads: Uncached load instructions are executed in program order. The request signal \u201cUncLdReqEn\u201d is generated when the oldest instruction in the processor is a load instruction whose address has an \u201cuncached\u201d attribute. This instruction is identified by active list <b>212</b>. This instruction is requested in stage \u201cC0\u201d and is sent to external interface <b>434</b> during the tag check cycle in stage \u201cC2\u201d. The instruction is limited to be in one stage at a time; new requests are inhibited while a previous request is still in stages \u201cC1\u201d or \u201cC2\u201d. This request is always for the oldest entry in the queue, which is selected by queue's read pointer (RdPtrVec).</p><p>Uncached Stores: Uncached store instructions send their address to external interface <b>434</b> using the cache tag section. Thus, they require the access port of the queue as well as the store port. This request is for the oldest entry which contains a store instruction (StSel). This entry must be one of the oldest four entries within the queue. It is selected by the store logic.</p><p>Freeloads: Data may be bypassed directly to the processor, while it is being refilled into the Data Cache. Each refill addresses is compared to all entries during stage \u201cC0\u201d. If this address matches any load entry which is in \u201crefill\u201d state, that entry requests a \u201cfreeload\u201d access.</p><p>Access Requests: Each entry may request an \u201caccess retry\u201d cycle, depending on its state and its dependencies on older instructions. It requests either a tag check or a data cycle. Either request is gated by the AccReqEn signal, which is summarized in Table 6.</p><p><tables id=\"TABLE-US-00006\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 6</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Enable Cache Access Requests (AccReqEn)</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"14PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"63PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"140PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Signals</entry><entry morerows=\"0\" valign=\"top\">Description</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AQvActive</entry><entry morerows=\"0\" valign=\"top\">Entry is active. (It contains an active</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">instruction.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AQvCalc</entry><entry morerows=\"0\" valign=\"top\">Entry's address has been calculated.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(Each entry is first issued to the</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Address Calculation unit to generate and</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">translate its address. Subsequently, it</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">can request a retry.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u02dcAQvBusyS &amp;</entry><entry morerows=\"0\" valign=\"top\">Entry is not in pipeline stage C1 or C2.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u02dcAQvBusyT</entry><entry morerows=\"0\" valign=\"top\">(An entry can only have one operation in</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">progress at a time.)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u02dcAQvDone &amp;</entry><entry morerows=\"0\" valign=\"top\">No further activity is allowed if an</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u02dcAQvExc</entry><entry morerows=\"0\" valign=\"top\">entry has already been completed, or if</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">any exception was detected for it.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u02dcAQvUnc</entry><entry morerows=\"0\" valign=\"top\">Cycles for uncached loads, uncached</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">stores, or CacheOp instructions use</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">separate request circuits.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u02dcAQvRef &amp;</entry><entry morerows=\"0\" valign=\"top\">No requests are made while the queue is</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u02dcAQvUpg</entry><entry morerows=\"0\" valign=\"top\">waiting for the External Interface to</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">complete a cache refill or upgrade</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">operation.</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>Requests are enabled only if the entry is active (determined from the queue pointers), and the address has been calculated. Each entry may have only one operation in the pipeline at a time. Thus, new requests are inhibited if this entry is busy in stages \u201cC1\u201d (S) or \u201cC2\u201d (T). An entry cannot request another cycle after its instruction has been completed (\u201cdone\u201d), or it has encountered an exception. Requests are delayed if there is any cache set or store dependency. (Because addresses are calculated out of order, some dependencies appear only until previous addresses have been calculated.)</p><p>Requests are inhibited for uncached instructions or Cacheop instructions. These instructions use separate requests, which are described above. Requests are delayed while the entry is waiting for external interface <b>434</b> to complete a cache refill (AQvRef) or upgrade (AQvUpg) operation.</p><p>Signal AccReqEn enables a \u201cfirst\u201d tag check. This tag check cycle sets AQvTagCk to indicate that this entry has already tried once. This bit may also be set during the initial address calculation sequence, if a tag check was completed. It is not set if the cache was busy, and the tag check could not be made. It is set if the tag check failed because of a memory dependency, however. This prevents the entry from being continuously retried when it has little chance of success.</p><p>If the latest tag check did not set the entry's \u201chit\u201d bit it can request a tag check cycle again, if it has no cache or store dependency on a previous instruction.</p><p>When an entry requests a tag check, it also may use the data section of the cache if it needs it, and if it is available.</p><p>If a load instruction sets its \u201chit\u201d bit, but it has not yet completed, it may request a data-only cycle, if it has no cache or store dependency on a previous instruction.</p><p>Address Calculate: If there are no other requests, access is granted to the entry (if any) whose address is being calculated. This provides data access simultaneously with the address translation (\u201cC2\u201d cycle). (Address Calculation logic <b>418</b> can use the tag section, if it is not used by a retry access. It does not require the access port to do this, because all the necessary signals come directly from address calculation <b>418</b>.)</p><p>Detailed access request logic <b>2100</b> is shown in FIG. <b>21</b>.</p><p>2. Retry Load Hazard</p><p>Certain external events could create timing hazards when the Address Queue retries a load instruction which already has a cache \u201chit\u201d. For example, an invalidate operation resets the \u201chit\u201d bit in the queue for any entry with a matching address. If that entry is already \u201cdone\u201d, a soft exception must be set.</p><p>3. Dependency CAN Logic</p><p>Address queue <b>308</b> contains the index address field associated with each address entry. The value held in this field is used for, among other things, dependency operations. Implementing logic <b>2200</b> is shown in FIG. <b>22</b>. This logic is discussed in greater detail below in conjunction with FIG. 27, which shows an alternative embodiment of a portion of logic <b>2200</b>.</p><p>The \u201cblock match\u201d signal (DepMatchBlk) compares address bits <b>13</b>:<b>5</b>. This signal is pipelined into stages \u201cC1\u201d, \u201cC2\u201d, and \u201cC3\u201d. If the entry becomes not active, these later signals are inhibited.</p><p>The ExtMatch signal generates a request for a free load. It is generated if the entry is ready to do a load (AccReqEt), and its block, doubleword, and way bits match the corresponding bit from external interface <b>434</b>. This signal sets a LoadReq flipflop, which is normally kept on for three cycles. This allows earlier access to the data.</p><p>E. State Changes Within Address Queue</p><p>1. Gating of Data Cache Hit signals</p><p>Address queue <b>308</b> controls when Data Cache Tag logic does a tag check for any entry in the queue. Cache hit or refill can be inhibited by the queue's dependency logic.</p><p>If either way is \u201clocked\u201d into cache, that way cannot be selected for replacement.</p><p>If either way is \u201clocked\u201d into cache, and the entry has any cache dependency, the other way cannot be used. It is reserved for the oldest instruction which references that cache set. Thus, neither way of the cache may be refilled. The other way is inhibited from generating a cache hit.</p><p>Any refill is also inhibited if the external interface intends to use the same cache bank on the next cycle. Whenever a refill is initiated, the new tag must be written into the tag RAM during the next cycle. If this cycle is not available, no refill can occur.</p><p>F. Address Oueue Timing</p><p>Address Queue timing is illustrated in FIG. <b>23</b>.</p><p>All cache operations use three pipeline stages. Stage \u201cC0\u201d (or\u201cR\u201d) requests access. Stage \u201cC1\u201d (or\u201cS\u201d) sets up the address and write data. Stage \u201cC2\u201d (or \u201cT\u201d) does reads or writes the tag and/or data arrays during phase 1. Tag checks and data alignment occur during phase 2. For load instructions, a fourth stage (\u201cU\u201d) is used to write the result into the integer or floating-point register file.</p><p>The \u201cstore\u201d sequence requires an initial extra cycle (\u201cCN\u201d) to find the oldest store instruction.</p><p>1. Address Queue Busy Mask</p><p>As mentioned above, address queue operations are pipelined in four 1-cycle steps:</p><p>\u201cR\u201d Cycle \u201cC0\u201d Request operation.</p><p>\u201cS\u201d Cycle \u201cC1\u201d Set-up cache.</p><p>\u201cT\u201d Cycle \u201cC2\u201d Tag check (and/or other cache operations).</p><p>\u201cU\u201d Cycle \u201cC3\u201d Update registers.</p><p>Each operation is requested during cycle \u201cR\u201d based on the entry's state. Its state is modified at the end of cycle \u201cT\u201d, based on the result of the operation. Each entry is limited to a single, non-overlapped operation, so new requests are inhibited during that entry's cycles \u201cS\u201d and \u201cT\u201d. These inhibits are implemented using two pipelined busy masks: AQvBusyS[<b>15</b>:<b>0</b>] and AQvBusyT[<b>15</b>:<b>0</b>]. AQvBusyS is set for an entry either when it is issued to address calculation unit <b>418</b>, or when it is issued for a retry access. Thus, two bits in AQvBusyS may be set simultaneously. These will be distinguished by the state of the AQvCalc bit. AQvCalc is set after the address calculation finishes cycle \u201cT\u201d (regardless of whether it completed a tag check). Thus, AQvCalc is zero for the entry being calculated; it is one for any retry. AQvBusyT is simply AQvBusyS delayed one cycle.</p><p>The cache select signals are decoded during \u201cE1\u201d to determine which request generated either a tag check or data load cycle.</p><p>AccDoesTCN Instruction from address queue <b>308</b> will do tag check during next cycle.</p><p>AccDoesLdN Instruction from address queue <b>308</b> will do data load during next cycle.</p><p>ACalcDoesTCN Instruction from address calculation unit <b>418</b> will do tag check during next cycle.</p><p>ACalcDoesLdN Instruction from address calculation unit <b>418</b> will do data load during next cycle.</p><p>These signals are delayed one cycle, for use during the data cache access. (For this cycle, omit the postfix \u201cN\u201d from the signal mnemonics.)</p><p>The entry being tag checked is identified using AQvBusyT during a tag check cycle. If the tag check was issued as a retry, there is only a single operation, and the mask has only one bit set. Otherwise, the tag check used the entry with AQvCalc zero.</p><h4>III. ADDRESS STACK</h4><p>Address stack <b>420</b> (FIG. 7) is logically part of address queue <b>308</b>, but is physically separate due to layout considerations. The address stack contains the physical memory address for each instruction in address queue <b>308</b>. This address consists of two fields, the physical page number (RAdr[<b>39</b>:<b>12</b>]) and the virtual index (VAdr[<b>13</b>:<b>0</b>]). These fields overlap by two bits because data cache <b>424</b> is virtually indexed (i.e., virtual bits [<b>13</b>:<b>3</b>] select a data doubleword in the data cache) but physically tagged (i.e., cache tags store physical address bits RAdr[<b>39</b>:<b>12</b>]).</p><p>The translated real address (i.e., RAdr[<b>39</b>:<b>12</b>]) is latched from TLB <b>422</b> (FIG. <b>7</b>). The low 12 bits of the real address equal corresponding bits of the virtual address Vadr[<b>11</b>:<b>0</b>].</p><p>The low 14 bits of the virtual address (i.e., VAdr[<b>13</b>:<b>0</b>]) are latched from the calculated address. These bits select a byte within the data cache array. The low 12 bits are an offset within the smallest virtual page. and are not modified by TLB <b>422</b>.</p><p>Address stack includes additional information such as \u201caccess byte mask\u201d (indicating which of the eight bytes of the accessed doubleword are read or written), \u201caccess type\u201d (indicating which type of instruction is being executed; i.e., load, store, etc.) and \u201creference type\u201d (indicating the length of the operand.</p><p>The address stack is loaded during the address calculation sequence (FIG. <b>11</b>). Thus, it has a single write port. It has two read ports. A \u201cstack\u201d port is used when address queue <b>308</b> retries an operation. A \u201cstore\u201d port is used when a store instruction is graduated.</p><h4>IV. MEMORY DEPENDENCY</h4><p>This logic is implemented in address queue <b>308</b>; associated with segment <b>515</b> shown in FIG. <b>7</b>.</p><p>A. Memory Dependency Checks</p><p>Load and store instructions are decoded and graduated in program order. However, to improve performance, memory operations to cacheable addresses may be performed out of order, unless there is a memory dependency. There are two types of dependency. First, a true memory dependency exists between a load instruction and any previous store which altered any byte used by the load. Second, accesses to the same cache set may be delayed by previous accesses to other addresses which share that set. This is an implementation dependency which prevents unnecessary cache thrashing. It is also necessary for proper operation of the dependency check procedure.</p><p>In a cache, a \u201cset\u201d is the group of blocks selected by each index value. In a direct-mapped cache, this index selects a set consisting of a single block. In an \u201cn-way\u201d set-associative cache, this index selects a set of \u201cn\u201d blocks. Cache addressing is described above in Section II.C.</p><p>Although memory loads are performed out of order, processor <b>100</b> appears (to a programmer) to have strong memory ordering. It detects whenever strong ordering might be violated, and backs up and re-executes the affected load instruction.</p><p>Accesses to non-cacheable addresses are performed in program order, when the corresponding instruction is about to graduate. All previous instructions have been completed, so no dependency check is needed. This is discussed below.</p><p>Memory dependencies are resolved within the address queue <b>308</b>. A dependency may exist whenever two real addresses access the same cache set. Dependency checks must use real rather than virtual addresses, because two virtual addresses can be mapped to the same real address. For timing and cost reasons, however, the 40-bit real addresses are not directly compared. Instead, dependencies are detected in two steps.</p><p>Address queue <b>308</b> contains two 16-row by 16-column dependency matrixes. These matrixes are identical, except for the logic equations defining how bits are set. \u201cCache Dependency Matrix\u201d <b>2400</b>, shown in FIG. 24, keeps track of all previous entries which use the same cache set. \u201cStore Dependency Matrix\u201d <b>2450</b>, also shown in FIG. 24, keeps track of all dependencies of load instructions on previous store instructions. Because store dependencies exist only between operations on the same doubleword (i.e., all memory accesses are within doublewords), bits set in store matrix <b>2450</b> are always a subset of those set in cache matrix <b>2400</b>. The operations of store matrix <b>2450</b> and cache matrix <b>2400</b> are illustrated in FIGS. 25<i>a </i>and <b>25</b><i>b</i>, respectively.</p><p>In the first step of a dependency check, a 9-bit cache index (VAdr[<b>13</b>:<b>5</b>]) is associatively compared to each entry in address queue <b>308</b> (i.e., segment <b>516</b> of FIG. <b>7</b>). This identifies all previous entries to the same cache set. This comparison occurs while the virtual address (VAdr[<b>13</b>:<b>0</b>]) is written into stack <b>420</b> at the end of the address calculate cycle. Each matching entry is flagged by setting a corresponding dependency bit in cache matrix <b>2400</b>. (At about the same time, VAdr[<b>4</b>:<b>3</b>] and a byte mask derived from VAdr[<b>2</b>:<b>0</b>] are also associatively compared to each entry in address queue <b>308</b>. This comparison, combined with additional signals described below, enables the setting of a corresponding dependency bit in store matrix <b>2450</b>.)</p><p>Second, the translated real address (RAdr[<b>39</b>:<b>12</b>]) is associatively compared to the data cache address tags. If there is a hit on the same side (i.e., way) of the cache, the new address selects the same cache block. If there is a miss, the cache block must be refilled before all dependencies can be resolved. This tag check cycle usually occurs one cycle after the address calculation, but it may be delayed if the data cache is busy.</p><p>1. Dependency Checking if Virtual Coherency</p><p>The dependency circuit must function properly even if the program uses virtual aliases. A virtual alias occurs if two different virtual addresses are translated to the same real address. Aliases must be considered, because associative comparator uses two virtual address bits (VAdr[<b>13</b>:<b>12</b>]) as part of the translated real address. (The lower bits (<b>11</b>:<b>5</b>) are part of the page offset, which is the same in the virtual and real addresses.) If aliases differ in these bits, the dependency logic will mistake them for distinct real addresses, and will fail to detect any dependencies between them. However, Secondary Cache <b>432</b> stores the two \u201cprimary index\u201d bits (PIdx; i.e., VAdr[<b>13</b>:<b>12</b>]) and generates a \u201cVirtual Coherency\u201d exception if a different index is used. That instruction will be aborted with a soft exception, so any dependency does not matter.</p><p>2. Cache Block Dependencies</p><p>Memory dependencies are resolved by comparing cache indexes and using the cache hit signals. This method requires that each address be brought into the data cache before all dependencies can be resolved. Once an instruction has used a cache block, that block must remain in the cache until that instruction has graduated. Although the processor does not invalidate any block that is still in use, external interface <b>434</b> may. If it invalidates a block which has been used to load a register before the load instruction has graduated, that instruction is flagged with a soft exception (described above) which will prevent it from being completed.</p><p>Data cache <b>424</b> is 2-way set associative. That is, it can store two independent blocks in each cache set. One of these blocks may be used for out-of-order operations. The second block must be reserved for in-order operations within that cache set. This guarantees that the processor can complete instructions in order without having to invalidate any block while it is still in use.</p><p>If a third block is needed, there is no room to bring that block into the cache. So that instruction and all subsequent accesses to this set must be delayed until all previous accesses to this set have graduated.</p><p>3. Load Dependency on Previous Stores</p><p>Whenever data is stored and then loaded from the same location, the load must get the newly stored data. A memory dependency exists between a load and a previous store if:</p><p>a. Both reference the same cache block (i.e., the dependency bits indicate the same cache set;</p><p>the load must have a cache hit on the same way as the store);</p><p>b. Both reference the same doubleword (i.e., address bits <b>4</b>:<b>3</b> are equal) and;</p><p>c. The byte masks have at least one byte in common.</p><p>Memory dependencies at the block level are detected during tag check cycles, because the cache hit signals are required to determine the selected way (which identifies the selected block).</p><p>The remaining information necessary to determine a block-level load dependency on a previous store is represented by the dependency bits in store matrix <b>2450</b>, which are set based exclusively on virtual addresses. These dependency bits identify store-to-load dependencies based upon common cache set and doubleword addresses (i.e., VAdr[<b>13</b>:<b>3</b>]) and byte overlap (discussed below).</p><p>Referring to FIG. 25<i>a</i>, a store dependency mask <b>2501</b><i>a </i>of entry <b>2502</b><i>a </i>is used to select previous store entries #1, #2 and #3. Each selected entry (i.e., #1-#3 in this case) detects a store dependency if it has the same set, doubleword, and any of the same bytes identified in a byte mask (described below). Otherwise, the corresponding dependency bit is reset. A load instruction may be dependent on several stores; it must wait until all have graduated.</p><p>Referring again to FIG. 25<i>a</i>, a newly calculated virtual address and byte mask <b>2502</b><i>a </i>from an instruction input into address queue <b>308</b> is shown being loaded into entry #4. As represented by line <b>2503</b><i>a</i>, this value is compared with every index, doubleword and byte mask entry (i.e., entries #0-#7) in address queue <b>308</b> via comparators <b>2504</b><i>a</i>-<b>2511</b><i>a</i>. However, only those entries identified through mask <b>2501</b><i>a </i>to be associated with store instructions (i.e., store entries) that are \u201cprevious\u201d to entry #4 may be used to alter the dependency bits of entry #4. Accordingly, resulting comparison \u201cstates\u201d <b>2512</b><i>a</i>, <b>2513</b><i>a </i>and <b>2514</b><i>a </i>associated with previous store entries #1-#3 may be used to set dependency bits <b>2520</b><i>a</i>, <b>2518</b><i>a </i>and/or <b>2516</b><i>a </i>in matrix <b>2500</b><i>a </i>if entry #4 is dependent on (i.e., has the same set, doubleword and any of the same bytes as) entries #1, #2 and/or #3, respectively. Alternatively, if the address of any previous store entry (i.e., #1-#3) has not yet been calculated, the dependency bit in entry #4 is set as a default, which can be reset when this earlier address is ultimately calculated.</p><p>Entries #5 and #6 contain later instructions that may depend upon entry #4. If, for example, the address of entry #5 is calculated before entry #4, bit <b>2522</b><i>a </i>of entry #5 will be set if entry #4 is a store entry. Although the address of entry #4 is not yet calculated, store dependency matrix <b>2500</b><i>a </i>follows a procedure that presumes dependency of earlier, uncalculated store instructions until such presumption is proven false. Accordingly, bit <b>2522</b><i>a </i>may be reset when the address of entry #4 is actually calculated. Of course, if entry #5 is calculated after entry #4, the standard operation described above controls.</p><p>For clarity, matrix <b>2500</b><i>a </i>shows only eight of the sixteen rows and columns present in the store-to-load dependency matrix of the preferred embodiment.</p><p>Address queue <b>308</b> uses somewhat simplified decoding for determining byte masks. It classifies each instruction by length\u2014byte, halfword, fullword, or doubleword. The utilized bytes are identified in an 8-bit byte mask (doubleword contains 8 bytes) generated by address calculation unit <b>418</b>. For simplicity, \u201cLoad Word/Doubleword Left/Right\u201d instructions are treated as if they used the entire word or doubleword, even though some bytes may not be needed. These instructions are infrequently used in normal code, so this has negligible impact on performance. Byte masks generated from available combinations of instruction length and VAdr[<b>2</b>:<b>0</b>] values are illustrated in Table 7.</p><p><tables id=\"TABLE-US-00007\"><table colsep=\"0\" frame=\"none\" pgwide=\"1\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"280PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 7</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Byte Masks</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"126PT\"></colspec><colspec align=\"center\" colname=\"1\" colwidth=\"112PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"42PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">8-bit Byte Mask</entry><entry morerows=\"0\" valign=\"top\">Instruction</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(Little Endian)</entry><entry morerows=\"0\" valign=\"top\">Abbrevi-</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"12\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"56PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"35PT\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"35PT\"></colspec><colspec align=\"center\" colname=\"4\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"5\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"6\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"7\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"8\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"9\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"10\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"11\" colwidth=\"14PT\"></colspec><colspec align=\"left\" colname=\"12\" colwidth=\"42PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\">Instruction</entry><entry morerows=\"0\" valign=\"top\">Length</entry><entry morerows=\"0\" valign=\"top\">VAdr[2:0]</entry><entry morerows=\"0\" valign=\"top\">7</entry><entry morerows=\"0\" valign=\"top\">6</entry><entry morerows=\"0\" valign=\"top\">5</entry><entry morerows=\"0\" valign=\"top\">4</entry><entry morerows=\"0\" valign=\"top\">3</entry><entry morerows=\"0\" valign=\"top\">2</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">ations</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"12\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"13\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"1\" colwidth=\"28PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"28PT\"></colspec><colspec align=\"left\" colname=\"3\" colwidth=\"35PT\"></colspec><colspec align=\"center\" colname=\"4\" colwidth=\"35PT\"></colspec><colspec align=\"center\" colname=\"5\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"6\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"7\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"8\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"9\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"10\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"11\" colwidth=\"14PT\"></colspec><colspec align=\"center\" colname=\"12\" colwidth=\"14PT\"></colspec><colspec align=\"left\" colname=\"13\" colwidth=\"42PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\">LB</entry><entry morerows=\"0\" valign=\"top\">SB</entry><entry morerows=\"0\" valign=\"top\">Byte</entry><entry morerows=\"0\" valign=\"top\">000</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">L: Load</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">001</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">S: Store</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">010</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">B: Byte</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">011</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">100</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">101</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">110</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">111</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry></row><row><entry morerows=\"0\" valign=\"top\">LH</entry><entry morerows=\"0\" valign=\"top\">SH</entry><entry morerows=\"0\" valign=\"top\">Halfword</entry><entry morerows=\"0\" valign=\"top\">00X</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">H: Halfword</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">01X</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">10X</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">11X</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry></row><row><entry morerows=\"0\" valign=\"top\">LWR</entry><entry morerows=\"0\" valign=\"top\">SWR</entry><entry morerows=\"0\" valign=\"top\">Word</entry><entry morerows=\"0\" valign=\"top\">0XX</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">W: Word</entry></row><row><entry morerows=\"0\" valign=\"top\">LWL</entry><entry morerows=\"0\" valign=\"top\">SWL</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">1XX</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">R: Right</entry></row><row><entry morerows=\"0\" valign=\"top\">LW</entry><entry morerows=\"0\" valign=\"top\">SW</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">L: Left</entry></row><row><entry morerows=\"0\" valign=\"top\">LWC1</entry><entry morerows=\"0\" valign=\"top\">SWC1</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">C1: Copro-</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">cessor 1*</entry></row><row><entry morerows=\"0\" valign=\"top\">LD</entry><entry morerows=\"0\" valign=\"top\">SD</entry><entry morerows=\"0\" valign=\"top\">Double-</entry><entry morerows=\"0\" valign=\"top\">XXX</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">D: Double-</entry></row><row><entry morerows=\"0\" valign=\"top\">LDC1</entry><entry morerows=\"0\" valign=\"top\">SDC1</entry><entry morerows=\"0\" valign=\"top\">word</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">word</entry></row><row><entry morerows=\"0\" valign=\"top\">LDL</entry><entry morerows=\"0\" valign=\"top\">SDL</entry></row><row><entry morerows=\"0\" valign=\"top\">LDR</entry><entry morerows=\"0\" valign=\"top\">SDR</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"13\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry align=\"left\" morerows=\"0\" nameend=\"13\" namest=\"1\" valign=\"top\">*Coprocessor 1 (i.e., floating point operations) </entry></row></tbody></tgroup></table></tables></p><p>These checks cannot be completed if any previous store has not had its address calculated yet. In such case, the load will remain dependent on the previous store until the address of the store is calculated. If the new address selects the same cache set, the load will wait until the store instruction has graduated. Otherwise, the associated dependency bit is reset and the load can proceed as soon as all remaining dependency bits are reset.</p><p>4. Dependency Checks When Loading Addresses</p><p>Virtual address bits VAdr[<b>13</b>:<b>0</b>] are loaded into an entry in both address queue <b>308</b> and the address stack <b>420</b> (FIG. 7) during the second half of each address calculate cycle. (These bits are duplicated due to layout considerations for processor <b>100</b>.) At the same time, the \u201cindex\u201d address bits VAdr[<b>13</b>:<b>5</b>] are associatively compared to every other entry. These comparators define the initial value for the dependency bits in the cache-set dependency matrix <b>2400</b>. This dependency check is based solely on the cache index, because the cache hit signals are not yet known. Usually, the next cycle does a tag check which determines the cache hit signals. However, it may be delayed due to tag update cycles or external intervention.</p><p>When loading an address for entry \u201cn\u201d into address stack <b>420</b>, its fifteen dependency bits (i.e., Dep[n][k=O . . . 15, k\u2260n]) of dependency matrix <b>2400</b> are set using the following method. As noted above, the cache \u201cindex\u201d is bits VAdr[<b>13</b>:<b>5</b>]. Each dependency bit Dep[n][k] is set high (i.e., to a logic \u201c1\u201d) when:</p><p>(1) Entry \u201ck\u201d contains an instruction previous to entry \u201cn\u201d (as defined by the cache dependency mask); and</p><p>(2) The address for entry \u201ck\u201d has not been computed or index[n]=index[k].</p><p>The dependency bit of a previously calculated entry \u201ck\u201d may be reset. This dependency bit was previously set because the new entry's address had not previously been calculated. If entry \u201ck\u201d has index[k]\u2260index[n], then reset Dep[k][n]. (Note reversal of subscripts on Dep.)</p><p>FIG. 25<i>b </i>shows an example of newly calculated index <b>2502</b> (i.e., VAdr[<b>13</b>:<b>5</b>]) being loaded into entry #4. As represented by line <b>2503</b><i>b</i>, this value is compared with every index entry (i.e., entries #0-#7) in address queue <b>308</b> via comparators <b>2504</b><i>b</i>-<b>2511</b><i>b</i>. However, only those entries identified through mask <b>2501</b><i>b </i>to be \u201cprevious\u201d instructions to entry #4 may be used to alter the dependency bits of entry #4. Accordingly, resulting comparison \u201cstates\u201d <b>2512</b><i>b</i>, <b>2513</b><i>b </i>and <b>2514</b><i>b </i>associated with previous entries #1-#3 may be used to set dependency bits <b>2520</b><i>b</i>, <b>2518</b><i>b </i>and/or <b>2516</b><i>b </i>in matrix <b>2500</b><i>b </i>if entry #4 is dependent on (i.e., uses the same set as) entries #1, #2 and/or #3, respectively. Alternatively, if the address of any previous entry (i.e., #1-3) has not yet been calculated, the dependency bit in entry #4 is set as a default, which can be reset when this earlier address is ultimately calculated.</p><p>Entries #5 and #6 contain later instructions that may depend upon entry #4. If, for example, the address of entry #5 is calculated before entry #4, bit <b>2522</b><i>b </i>of entry 5 will be set. Although the address of entry #4 is not yet calculated, dependency matrix <b>2500</b><i>b </i>follows a procedure that presumes dependency of earlier, uncalculated instructions until such presumption is proven false. Accordingly, bit <b>2522</b><i>b </i>may be reset when the address of entry #4 is actually calculated. Of course, if entry #5 is calculated after entry #4, the standard operation described above controls.</p><p>For clarity, matrix <b>2500</b><i>b </i>shows only eight of the sixteen rows and columns present in the cache-set dependency matrix of the preferred embodiment.</p><p>5. Dependency Checks During Tag Check Cycles</p><p>Dependency is determined during tag check cycles using cache hit signals, and the state and dependency bits within the address stack <b>420</b>, as shown in FIG. <b>26</b>. This figure lists all legal combinations of input bits, which are not completely independent of each other. By means of explanation, abbreviations used in the figure are defined below:</p><p>\u201c-\u201d: \u201cdon't care\u201d for inputs, \u201cno action\u201d for outputs;</p><p>\u201cD\u201d: cache block dependency (i.e., a previous entry uses the same cache set);</p><p>\u201cL\u201d: indicates a store-to-load dependency, or possible dependency if previous store address not yet calculated;</p><p>\u201cS\u201d: bit is set;</p><p>\u201cC\u201d: bit is set conditionally, only if there is no store-to-load dependency (\u201cL\u201d=0);</p><p>Cache State: valid (V), refilling (R), not valid (N), valid or refilling (E); available (not refilling) (A).</p><p>Each entry includes five bits which indicate cache hit information. The two \u201cLock\u201d bits (LockA and LockB) indicate that the subject entry is using the first (random order) block within the cache set, for either side A or B (i.e., way 0 and 1, respectively). This block is locked into the cache until all entries which use it have graduated. If new entries get a hit on a locked block, they will use it and set their corresponding lock bit.</p><p>The two \u201cUse\u201d bits (UseA and UseB) indicate that this entry is using the second (sequential order) block within the cache set, for either side A or B. This block is locked into the cache until this entry has graduated. Then this block may be replaced, if necessary, by the next sequential entry to access this cache set. If new entries get a hit on a \u201cuse\u201d block, they cannot use it.</p><p>The \u201cDependency\u201d signal indicates that this entry is dependent on a previous entry.</p><p>For each cache cycle scheduled by the processor, the corresponding entry in address stack <b>420</b> is read. This provides the address and dependency information.</p><p>The procedure must identify all entries which use the same cache set so it can determine if any block has already been locked. This identification uses the dependency bits held in dependency cells <b>3004</b><i>a </i>in matrix <b>2400</b> (FIG. 24) to select all other entries with the same index, or whose addresses have not yet been calculated. Bits in both the row and column are used. For entry #n, row bits Dep[n][j] identify which other entries (#j) this entry is dependent on. Column bits Dep[j][n] identify other entries which are dependent on this entry. These bits include uncalculated entries\u2014which are reset when the address is calculated if it selects another cache set.</p><p>Before any cache block has been locked, every entry is dependent on, or depended on by, each other entry using the same cache set. When a block is locked, its dependency bits are no longer needed for sequencing. Instead, they simply identify all other entries using the same cache set. Thus, whenever any other entry selects the same set, it knows that the block is locked.</p><p>The dependency bits identify every entry which uses the same cache set, by logically ORing row and column bits. The row (bits within each entry) identify other entries on which this entry is dependent. The column identifies entries which are dependent on this entry. More specifically, the row and column dependency bits form 16-bit numbers which are ORed in a bit-wise fashion. Accordingly, the result is a single 16-bit number (i.e., mask) with a \u201c1\u201d in each position corresponding to an entry using the same cache set. This number is used to read lock/use array <b>2404</b>, illustrated in FIG. <b>24</b>.</p><p>Specifically, each bit that is set (i.e., a logic 1) in the foregoing 16-bit number (i.e., mask) enables the reading of LockA, LockB, UseA and UseB bits associated with that entry. Thereafter, all LockA bits are ORed together (i.e., column <b>2460</b>) generating a single LockA value for the associated cache set. Similarly, all LockB bits (column <b>2462</b>), UseA bits (column <b>2464</b>) and UseB bits (column <b>2466</b>) are separately ORed to generate a single value for each status bit of the associated cache set. These bits indicate current activity on this cache set.</p><p>B. Dependency Logic</p><p>FIG. 24 shows dependency matrix <b>2400</b> and <b>2450</b> disposed in address queue <b>308</b>. In the preferred embodiment, these two matrixes are configured in a single array because they share many signals. (This is discussed further in connection with FIG. 33.) Matrixes <b>2400</b> and <b>2450</b> are shown separately, however, for clarity.</p><p>Referring again to FIG. 24, a set of comparators <b>2406</b> identifies dependencies for recording in each matrix, and an array of lock and use bits <b>2404</b> describe activity for each active cache set. \u02dcDepCache[j] and DepPrevC[j] signals forwarded to matrix <b>2400</b> indicate cache set dependencies while \u02dcDepBytes[j] and DepPrevS[j] signals forwarded to matrix <b>2450</b> indicate store-to-load dependencies. (NB: a tilde (i.e.,\u02dc) placed in front of a signal name indicates a complemented value.) Each comparator in set <b>2406</b> is coupled to matrixes <b>2400</b> and <b>2450</b>, like comparator <b>2408</b> (i.e., there are sixteen \u02dcDepCache[j], DepPrevC[j], \u02dcDepBytes[j] and DepPrevS[j] lines between comparator set <b>2406</b> and matrixes <b>2400</b> and <b>2450</b>). Only a single connection is shown for purposes of discussion.</p><p>Generally, signals \u02dcDepCache[j] and \u02dcDepBytes[j] identify matching addresses. At the same time, signals DepPrevC[j] and DepPrevS[j] function as masks <b>2501</b><i>b </i>and <b>2501</b><i>a</i>, respectively (see FIGS. 25<i>b </i>and <b>25</b><i>a</i>).</p><p>\u02dcDepCache[j] signal on line <b>2410</b> in conjunction with DepPrevC[j] on line <b>2410</b>\u2032 indicate whether any dependency exists between an index address (i.e., cache set) stored in comparator <b>2408</b> and an address being newly calculated (i.e., \u201ccache-set\u201d dependency). Similarly, \u02dcDepBytes[j] signal on line <b>2412</b> in conjunction with DepPrevS[j] on line <b>2412</b>\u2032 indicate whether any dependency exists between an entry stored in comparator <b>2408</b> and an address being newly calculated based on byte overlap (i.e., store-to-load dependency). In FIG. 24, a newly calculated address is at entry 10; identified by the ACalcVec[j] signal. This signal is gated by phase 2 of the processor clock (i.e., \u03c62) through NAND gate <b>2414</b> thereby generating \u02dcACalcVecWr[j] on line <b>2420</b>\u2032. \u02dcACalcVecWr[j] passes through inverter <b>2416</b> thereby generating ACalcVecWr[j] on line <b>2420</b>. As shown in FIG. 24, these signals are applied to both matrix <b>2400</b> and <b>2450</b> over lines <b>2420</b> and <b>2420</b>\u2032.</p><p>Signal ACalcVec[j] identifies a newly calculated entry. Resulting signals ACalcVecWr[j] and \u02dcACalcVecWr[j] enable initial dependencies to be written into the corresponding row. As discussed below, signal ACalcVecWr[j] also provides a means for resetting dependency bits in other rows which erroneously indicate a dependence on this entry (i.e., through line <b>2422</b>). Signal ACalcVec[j] is generated by priority logic <b>1500</b> (FIG. <b>15</b>).</p><p>Referring to FIG. <b>15</b> and Section II.C.4 above, address queue <b>308</b> prioritizes three sets of requests that compete for queue resources: freeload, retry access and address calculate. These three sets of requests are combined into a single set at the end of cycle C0 (i.e., \u201cAccComReq\u201d), and the highest priority request in the combined set is granted by a dynamic priority encoder at the beginning of cycle C1. If an address calculate request is granted for a particular entry in address queue <b>308</b>, priority logic <b>1500</b> generates a 16-bit mask with a single set bit (logic 1) identifying the single entry whose address is being calculated (i.e., ACalcVec[j]) and fifteen reset bits (logic 0) associated with the remaining entries whose addresses are not being calculated.</p><p>Referring to matrix <b>2400</b> in FIG. 24, the \u02dcDepCache[j] signal on line <b>2410</b> and DepPrevC[j] on line <b>2410</b>\u2032 pass through every cell in row <b>7</b>. (As shown in FIG. 24, and discussed below, a row component of read bit line <b>2428</b> (i.e., \u02dcDepCRead[j]) and a combined read bit line <b>2428</b>\u2032 (produced from row and column components of line <b>2428</b>) also pass through row <b>7</b>.) \u02dcDepCache[j] and DepPrevC[j] are logically combined and the result (DepPrevC[k]) is passed through every cell in column <b>7</b>, as indicated by line <b>2418</b>. (The logical combination is described below in connection with FIG. 30<i>a</i>.) Concurrently, ACalcVecWr[j] and \u02dcACalcVecWr[j] provide a pulse (when ACalcVec[j] and \u03c62 are high) to all cells in row <b>10</b> of matrix <b>2400</b>, as shown by lines <b>2420</b> and <b>2420</b>\u2032. (Additionally, ACalcVecWr[k]\u2014which is the same signal as ACalcVecWr[j]\u2014is conveyed to all cells in column <b>10</b>, as shown by line <b>2422</b>.) If entry <b>10</b> is dependent on previous entry <b>7</b>, dependency bit at row <b>10</b>, column <b>7</b> is set through the combination of signals on lines <b>2418</b>, <b>2420</b> and <b>2420</b>\u2032. (This combination is discussed below in connection with FIG. 30<i>a</i>.)</p><p>Referring to matrix <b>2450</b> in FIG. 24, the \u02dcDepBytes[j] signal on line <b>2412</b> passes through every cell in row <b>7</b>. This same signal is logically combined with DepPrevS[j] on line <b>2412</b>\u2032, and the result (DepPrevS[k]) is passed through every cell in column <b>7</b>, as indicated by line <b>2454</b>. (The logical combination is described below in connection with FIG. 30<i>b </i>.) Concurrently, ACalcVecWr[j] and \u02dcACalcvecwr[j] provide a pulse (when ACalcVec[j] and \u03c62 are high) to all cells in row <b>10</b> of matrix <b>2450</b>, as shown by lines <b>2420</b> and <b>2420</b>\u2032. (Additionally, ACalcVecWr[k]\u2014which is the same signal as ACalcVecWr[j]\u2014is also conveyed to all cells in column <b>10</b>, as shown by line <b>2422</b>.) If entry <b>10</b> is dependent on previous entry <b>7</b>, dependency bit at row <b>10</b>, column <b>7</b> is set through the combination of signals on lines <b>2454</b>, <b>2420</b> and <b>2420</b>\u2032. (This combination is discussed below in connection with FIG. 30<i>b</i>.)</p><p>Any entries calculated prior to the newly calculated entry (i.e., entry <b>10</b> in this example) in matrixes <b>2400</b> and <b>2450</b> that could possibly depend on this entry will have previously set their dependency bit associated with this entry (i.e., bit <b>10</b>) to a logic 1. As noted above, this is a default value when the earlier address is unknown. However, once the previously uncalculated entry is calculated, defaulted bit values may be reset if no dependency in fact exists. Referring to FIG. 24, line <b>2422</b> enables bit values located in column <b>10</b> of matrixes <b>2400</b> and <b>2450</b> to be reset if the associated row had previously been calculated. In other words, the \u02dcDepCache[j] signals associated with each row j in matrix <b>2400</b> and the \u02dcDepBytes[j] signals associated with each row j in matrix <b>2450</b> indicate whether or not a dependency exists. Each such signal is combined with the signal on line <b>2422</b> (i.e., ACalcVecWr[k]) to reset the corresponding bit <b>10</b> if no dependency exists. (This combination is discussed below in connection with FIGS. 30<i>a </i>and <b>30</b><i>b</i>.) Set bits in matrix <b>2400</b> are used for, among other things, reading lock/use array <b>2404</b>.</p><p>For purposes of reading array <b>2404</b>, \u02dcDepcache[j] signal on line <b>2410</b> is forwarded to a complemented input of MUX <b>2426</b> as a bypass to the array. A second input to MUX <b>2426</b> is provided by read bit line <b>2428</b>, which contains both a row and a column. (Each read word line (e.g. line <b>2429</b>) and read bit line (e.g., line <b>2428</b>) in matrix <b>2400</b> contain both a row and a column.) More specifically, readline <b>2428</b> is constructed from corresponding \u02dcDepCRead[j] and \u02dcColumnC[k] values (signals internal to matrix cells) and enabled by a TagEntryRd[j] signal (discussed below in connection with FIGS. 30<i>a </i>and <b>30</b><i>c</i>). These values are combined into a single signal (i.e., SameCacheSet[j], see FIGS. 30<i>a </i>and <b>30</b><i>c</i>) and forwarded to MUX <b>2426</b> via latch <b>2430</b>, as represented by combined read bit line <b>2428</b>\u2032 in FIG. <b>24</b>. Latch <b>2430</b> is gated by phase 1 of the processor clock (i.e., \u03c61). The MUX outputs a signal to lock/use array <b>2404</b> (through latch <b>2432</b> gated by \u03c62). This output value enables the reading of lock or use bits <b>2440</b>-<b>2443</b>.</p><p>As noted above, dependency checking requires a two-step operation; i.e., comparing virtual address information in dependency matrixes (i.e., matrix <b>2400</b> and <b>2450</b>) and comparing an associated translated real address with data cache address tags. In the course of the latter operation, the status of an accessed cache set is determined by reading any lock or use bits (held in array <b>2404</b>) set by other entries accessing the same cache set.</p><p>Referring to FIG. 24, if the same entry is undergoing both steps of the dependency checking operation at the same time (i.e., virtual and real address comparing), signal ACalcDoesTC (generated by priority logic <b>1500</b>) selects DepCache[j] through MUX <b>2426</b>. If the entry associated with DepCache[j] matches the newly calculated entry (i.e., entry 10 in this example), DepCache[j] is high thereby reading out any associated lock or use bit (i.e., bits <b>2440</b>-<b>2443</b>) in array <b>2404</b>. An identical circuit consisting of a MUX and two latches is coupled to every row in Matrix <b>2400</b> enabling the same operation to be carried out in parallel. The net result is a 16-bit word (i.e., mask) defined by the contents of array <b>2400</b> that identifies any lock or use bits set (i.e., a logic 1) for the associated cache set.</p><p>Alternatively, if the same entry is not undergoing both steps of the dependency checking operation at the same time (i.e., there may be a pending address for tag check operations\u2014subject to a \u201cretry access\u201d request\u2014at the time matrixes <b>2400</b> and <b>2450</b> are accessed), signal ACalcDoesTC selects combined read bit line <b>2428</b>\u2032 (passing through latch <b>2430</b>) with MUX <b>2426</b>. Line <b>2428</b>\u2032 enables the reading of certain dependency bits associated with the entry undergoing tag check operations. These bits are used to access lock and use bits in array <b>2404</b>. The signal on combined read bit line <b>2428</b>\u2032 (i.e., SameCacheSet[j]; which is identified on line <b>3036</b><i>a </i>in FIG. 30<i>a</i>) is enabled by TagEntryRd[j].</p><p>More specifically, an entry separately undergoing tag check operations enables signal TagEntrySel[j], which is gated by \u03c61 through NAND <b>2434</b> and passes through inverter <b>2436</b> thereby generating TagEntryRd[j] as shown in FIG. <b>24</b>. (In the example of FIG. 24, the entry undergoing tag check operations is entry 3.) TagEntryRd[j] enables the dependency bits located on the jth row and kth column (where j=k) of matrix <b>2400</b> to be read out. The corresponding column signal (i.e., TagEntryRd[k]) is enabled through a simple electrical connection, as shown in FIG. 30<i>a</i>. (As the \u201cj\u201d designation indicates, a TagEntryRd[j] signal is available for each row (j) and corresponding column (k, where k=j) in matrix <b>2400</b>.) Signal TagEntryRd[j] is generated by priority logic <b>1500</b> (FIG. <b>15</b>).</p><p>Referring to FIG. <b>15</b> and Section II.C.4 above, address queue <b>308</b> prioritizes three sets of requests: freeload, retry access and address calculate. As discussed above, these three sets are combined and prioritized. Accordingly, if a \u201cretry access\u201d request is granted for a particular entry in address queue <b>308</b>, priority logic <b>1500</b> generates a 16-bit mask with a single set bit (logic 1) identifying the single entry whose operation (Load, for example) is being retried (i.e., TagEntrySel[j]) and fifteen reset bits (logic 0) associated with remaining entries whose operations are not being retried.</p><p>Referring to FIG. 24, read bit line <b>2428</b> combines with TagEntryRd[j] and [k]<b>0</b> to select dependency bits stored at bit locations <b>2444</b> (row <b>3</b>, col. <b>7</b>) and <b>2445</b> (row <b>7</b>, col. <b>3</b>). These values are complemented, ORed together (generating SameCacheSet[j]) and output to array <b>2404</b> (through MUX <b>2426</b>) on combined read bit line <b>2428</b>\u2032. The value on line <b>2428</b>\u2032 enables the reading of any lock or use bits that may be set if location <b>2444</b> or <b>2445</b> holds a set bit (i.e., logic 1). An identical operation is carried out for entries 0 through 2 and 4 through 15. Implementing logic and circuitry are shown in FIGS. 30<i>a </i>and <b>30</b><i>c. </i></p><p>The net effect of this operation is to produce a 16-bit word consisting of all dependency bits on row j and column k (i.e., row and column <b>3</b> in this example) ORed together in a bit-wise fashion (i.e., 16 SameCacheSet[j] values). This word is then used to read out values from array <b>2404</b> that correspond to the associated cache set.</p><p>A TagEntryWrite[j] signal (not shown) is used to set corresponding lock and use bits in array <b>2404</b> one clock cycle after the associated TagEntryRd[j] signal. Using the same logic and circuitry of the TagEntryRd[j] signal, a TagEntryWrite signal updates state bits in array <b>2404</b> based on the results of the tag checking operation.</p><p>In addition to the foregoing, FIG. 24 discloses signals DepRowC[j] and DepRowS[j] which are output from OR gates <b>2438</b> and <b>2452</b>, respectively. These signals represent the ORed value of all dependency bits in an associated row. An identical circuit is coupled to each row in matrix <b>2400</b> and <b>2450</b>. These signals are used to identify any dependencies of a particular entry and inhibit the associated cache operation. In matrix <b>2400</b>, each DepRowC[j] signal is used in combination with the lock and use bits to identify allowable operations.</p><p>The relationship between DepRowC[j] signals and corresponding lock bits of array <b>2404</b> is illustrated in Table 8. This table identifies allowable operations (i.e., refilling or hitting a way of a cache) based on associated DepRowC[j] and lock bit values.</p><p><tables id=\"TABLE-US-00008\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 8</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Allowable Operations Based on Lock Bits and DepRowC[j]</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"7\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"42PT\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"28PT\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"35PT\"></colspec><colspec align=\"center\" colname=\"4\" colwidth=\"21PT\"></colspec><colspec align=\"center\" colname=\"5\" colwidth=\"35PT\"></colspec><colspec align=\"center\" colname=\"6\" colwidth=\"28PT\"></colspec><colspec align=\"center\" colname=\"7\" colwidth=\"28PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Refill</entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">DepRowC[j]</entry><entry morerows=\"0\" valign=\"top\">Lock A</entry><entry morerows=\"0\" valign=\"top\">Lock B</entry><entry morerows=\"0\" valign=\"top\">A?</entry><entry morerows=\"0\" valign=\"top\">Refill B?</entry><entry morerows=\"0\" valign=\"top\">Hit A?</entry><entry morerows=\"0\" valign=\"top\">Hit B?</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"7\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">Yes</entry><entry morerows=\"0\" valign=\"top\">Yes</entry><entry morerows=\"0\" valign=\"top\">Yes</entry><entry morerows=\"0\" valign=\"top\">Yes</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">No</entry><entry morerows=\"0\" valign=\"top\">No</entry><entry morerows=\"0\" valign=\"top\">Yes</entry><entry morerows=\"0\" valign=\"top\">No</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">No</entry><entry morerows=\"0\" valign=\"top\">No</entry><entry morerows=\"0\" valign=\"top\">No</entry><entry morerows=\"0\" valign=\"top\">Yes</entry></row><row><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">Yes</entry><entry morerows=\"0\" valign=\"top\">Yes</entry><entry morerows=\"0\" valign=\"top\">Yes</entry><entry morerows=\"0\" valign=\"top\">Yes</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">No</entry><entry morerows=\"0\" valign=\"top\">Yes</entry><entry morerows=\"0\" valign=\"top\">Yes</entry><entry morerows=\"0\" valign=\"top\">Yes</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">0</entry><entry morerows=\"0\" valign=\"top\">1</entry><entry morerows=\"0\" valign=\"top\">Yes</entry><entry morerows=\"0\" valign=\"top\">No</entry><entry morerows=\"0\" valign=\"top\">Yes</entry><entry morerows=\"0\" valign=\"top\">Yes</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"7\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>Referring to Table 8, if a queue entry has a dependency and either way 0 or way 1 (i.e., side A or side B, respectively) of the associated cache set is locked, then refills are inhibited on both ways of the set and hits are allowed only on the locked way. Conversely, if a queue entry has no dependency (for example, the oldest instruction pending in the queue associated with the subject cache set), and either way 0 or way 1 is locked, then refills are inhibited only on the locked way and hits are allowed on both ways.</p><p>The use bit for either way is set when an instruction with no dependencies (i.e., DepRowC[j]=0) hits or refills an unlocked block. This represents \u201csequential\u201d or \u201cin-order\u201d operation, as discussed above. The use bit remains set until the associated instruction graduates, thereby preventing another instruction from refilling or hitting this block.</p><p>In matrix <b>2450</b>, each DepRowS[j] signal is used to identify load operations with dependencies. In those situations where a load instruction is dependent on another entry (i.e., DepRowS[j]=1), that load instruction is aborted since the data it seeks to load may be invalid.</p><p>During processor operation, matrix <b>2450</b> is checked only when attempting to perform a load instruction. Store instructions are not performed until they are the oldest instruction in the queue. Hence, there is no need to check the corresponding entry (i.e., row) in matrix <b>2450</b> since it will contain all zeroes.</p><p>As noted above, matrixes <b>2400</b> and <b>2450</b> are identical except for the logic equations defining how bits are set. Further, matrix <b>2450</b> is not coupled to lock/use matrix <b>2404</b> nor are TagEntryRd[j] signals applied to its entries. Aside from these differences, the foregoing discussion related to matrix <b>2400</b> applies equally to matrix <b>2450</b>. In particular, signals \u02dcDepBytes[j], DepPrevS[j], ACalcVecWr[j] and \u02dcACalcVecWr[j] interact with matrix <b>2450</b> in the same manner as signals \u02dcDepCache[j], DepPrevC[j], ACalcVecWr[j] and \u02dcACalcVecWr[j] interact with matrix <b>2400</b>.</p><p>1. Comparators</p><p>A logic block diagram of each comparator in comparator set <b>2406</b> of FIG. 24 is provided in FIG. <b>22</b>. An alternative embodiment of at least a portion of the circuit shown in FIG. 22 is provided in the comparator circuit of FIG. <b>27</b>. The basic architecture of the circuits in FIGS. 22 and 27 are analogous, including many of the same signals and equivalent portions of circuitry. (Identical signals and circuits on these drawings are identified with identical signal names and reference numbers, respectively.) A significant different between these two architectures, however, is the presence of AND gates <b>2734</b> and <b>2736</b> in FIG. <b>22</b>. FIG. 27 has no equivalent circuitry. These gates, as described below, enable the generation of signals DepPrevS[j] and DepPrevC[j], which function as masks <b>2501</b><i>a </i>and <b>2501</b><i>b</i>, respectively, when used in conjunction with matrixes <b>2450</b> and <b>2400</b> (FIG. <b>24</b>).</p><p>Referring to FIGS. 22 and 27, each comparator-in set <b>2406</b> (FIG. 24) holds set index bits <b>2702</b>, doubleword bits <b>2704</b> and byte mask value <b>2706</b> for the corresponding entry (the \u201csaved entry\u201d). These values were loaded at the time this entry was newly calculated. Each comparator also includes an AQvCalcP[j] signal <b>2726</b> whose value is determined by the presence of address information (i.e., index bits, doubleword bits and byte mask).</p><p>Should no value (i.e., address) be calculated for a particular entry (i.e., the particular comparator is inactive), the associated AQvCalcP[j] signal <b>2726</b> will be reset (logic 0) thereby setting signals Depcache[j] and DepBytes[j] high (i.e., logic 1). This facilitates a presumption of dependency for those entries whose addresses are not yet calculated, as described above in connection with FIGS. 25<i>a </i>and <b>25</b><i>b</i>. Once an entry has been calculated, the AQvCalcP[j] signal is set (i.e., logic 1), allowing comparators <b>2708</b>, <b>2710</b> and <b>2712</b> to determine the value of DepCache[j] and DepBytes[j].</p><p>The AQvCalcP[j] signal is the same as the \u201cAQvCalc\u201d signal described in Table 3, except AQvCalcP[j] is valid one cycle earlier (i.e., in cycle E1).</p><p>Returning to FIGS. 22 and 27, AND/OR gate <b>2708</b> and comparators <b>2710</b>-<b>2716</b> enable the comparison of previously loaded values <b>2702</b>, <b>2704</b> and <b>2706</b> with a newly calculated virtual address <b>2718</b>, an associated byte mask <b>2720</b> and addresses from the external interface <b>2722</b>.</p><p>The newly calculated address <b>2718</b> and external interface address <b>2722</b> each compare set address bits <b>13</b>:<b>5</b> with index address <b>2702</b> using comparators <b>2712</b> and <b>2714</b>, respectively. The calculated address <b>2718</b> also compares the doubleword within a cache block (bits <b>4</b>:<b>3</b>) using comparator <b>2710</b> and checks for byte overlap (derived from the operation type and bits <b>2</b>:<b>0</b>, as discussed above) using AND/OR gate <b>2708</b>. Bit <b>4</b> of the external address <b>2724</b> is compared using comparator <b>2716</b> to match quadwords during cache refill. The ExtCompQW signal is used to enable or disable a quadword comparison.</p><p>The comparison results are logically combined, as shown in FIG. <b>22</b>. More specifically, OR gate <b>2238</b> receives the output of comparator <b>2712</b> and a complemented AQvCalcP[j] <b>2226</b> signal, and outputs a DepCache[j] signal. AND gate <b>2240</b> receives the output of AND/OR gate <b>2708</b> and comparators <b>2710</b> and <b>2712</b>. Further, OR gate <b>2242</b> receives the output of AND gate <b>2240</b> and a complemented AQvCalcP[j] <b>2226</b> signal, and outputs a DepBytes[j] signal. Equivalent logic is illustrated in FIG. <b>27</b>.</p><p>During comparison operations, signal DepBytes[j] is high when comparators <b>2708</b>, <b>2710</b> and <b>2712</b> all identify matches\u2014thereby indicating matching VAdr[<b>13</b>:<b>3</b>] and byte overlap. This signal is inverted by inverter <b>2730</b> (i.e., producing \u02dcDepBytes[j]) before being forwarded to matrix <b>2450</b> (FIG. <b>24</b>).</p><p>As mentioned above, the comparator of FIG. 22 also generates DepPrevS[j]. Specifically, at about the same time that inverter <b>2730</b> generates \u02dcDepBytes[j], signal DepBytes[j] is combined with signals AQvstore[j] and AQvPrev[j] in AND gate <b>2734</b>, generating DepPrevS[j] (i.e., mask <b>2501</b><i>a</i>). Signals AQvStore[j] and AQvPrev[j] are generated outside the comparator of FIG. <b>22</b>. AQvStore[j] is high when the saved entry is associated with a store instruction. This signal is generated by decoding logic. Referring to FIG. 7, when an instruction is loaded into address queue <b>308</b>, a portion of the instruction (e.g., function code) is decoded in decoder <b>505</b>. As a result of this decoding operation, an \u201cAQvStore\u201d bit (held in the address queue) is set if the associated instruction is a store (see Table 1). An AQvstore bit is stored in address queue <b>308</b> for each queue entry, thereby enabling the queue to track which entries are store instructions.</p><p>AQvPrev[j] is high when the saved entry is older than the newly calculated entry. This signal is derived from the active mask shown in FIG. <b>14</b> and generated from the logic shown in FIG. <b>13</b>. Although the mask in FIG. 14 relies on the write pointer signal of the address queue (i.e., \u201cWrPtr\u201d) to identify the newest instruction, for purposes of dependency operations, ACalcVec[j] identifies the most recent instruction in queue <b>308</b>. Accordingly, any instruction falling between this entry and the read pointer signal of the address queue (i.e., \u201cRdPtr\u201d) is an active instruction and therefore is associated with a high AQvPrev[j] signal. (See the discussion of WrPtr and RdPtr in Section C.3, above.)</p><p>If \u02dcDepBytes[j] and DepPrevS[j] are low and high, respectively, then a store-to-load dependency based on common set, common doubleword and byte overlap is established, and the appropriate bit in matrix <b>2450</b> is set.</p><p>Referring again to FIGS. 22 and 27, DepCache[j] is high when comparator <b>2712</b> identifies a match\u2014thereby indicating a matching VAdr[<b>13</b>:<b>5</b>]. This signal is inverted by inverter <b>2732</b> (i.e., producing \u02dcDepCache[j]) before being forwarded to matrix <b>2400</b> (FIG. <b>24</b>).</p><p>As mentioned above, the comparator of FIG. 22 also generates DepPrevC[j]. Specifically, at about the same time that inverter <b>2732</b> generates \u02dcDepcache[j], signal DepCache[j] is combined with signal AQvPrev[j] in AND gate <b>2736</b> generating DepPrevC[j] (i.e., mask <b>2501</b><i>b</i>).</p><p>As noted above, AQvPrev[j] is high when the saved entry is older than the newly calculated entry. If \u02dcDepCache[j] and DepPrevC[j] are low and high, respectively, then a cache-set dependency is established and the appropriate bit in matrix <b>2400</b> (FIG. 24) is set.</p><p>FIG. 28 shows the circuit implementation of the dependency comparators of FIGS. 22, <b>24</b> and <b>27</b>. Each entry of comparator set <b>2406</b> in FIG. 24 (e.g. comparator <b>2408</b>) has at least two comparators and an AND/OR gate used with newly calculated addresses as shown in FIGS. 22 and 27. The index comparator <b>2712</b> compares address bits <b>13</b>:<b>5</b> to determine if the new address selects the same cache set. The doubleword comparator <b>2710</b> compares address bits <b>4</b>:<b>3</b>. The byte overlap circuit (i.e., 8-bit wide AND/OR gate <b>2708</b>) determines if the new byte mask selects any of the same bytes.</p><p>The index comparator <b>2712</b> is constructed with nine conventional comparator circuits <b>2800</b> shown in FIG. <b>28</b>. Signal ACalcVec[j] identifies the comparator circuit associated with the newly calculated entry. This entry is stored in the circuit when first calculated and functions as the index address for subsequent \u201cnew\u201d addresses. These subsequent addresses are presented on bit lines <b>2801</b> and <b>2802</b>, forcing line <b>2803</b> high if the new bit differs from the stored bit. If index address bits differ, a positive pulse coinciding with \u03c62 is presented on line <b>2804</b>.</p><p>Similarly, the doubleword comparator <b>2710</b> is constructed with two conventional comparator circuits <b>2800</b> shown in FIG. <b>28</b>.</p><p>Referring to FIG. 29, byte overlap circuit <b>2900</b> is constructed from stacks <b>2901</b> and <b>2902</b> of four transistors with complemented signals applied to their gates. Accordingly, a transistor is turned off only when an associated byte is present. If a pair of transistors are turned off (for example, transistors <b>2903</b> and <b>2904</b>), line <b>2905</b> (i.e., \u02dcDepOverlapB[j]) will remain low during \u03c62 thereby indicating an overlap (i.e., dependency). Byte overlap circuit <b>2900</b> is represented logically as AND/OR gate <b>2708</b> in FIGS. 22 and 27.</p><p>These circuits switch dynamically on the phase 2 clock edge. Their outputs are pulses which switch about 3 inverter delays later. So that these pulses can be gated together without generating glitches, each circuit generates a pulse if there is no dependency. Comparator <b>2800</b> generates a pulse if any address bit differs, using standard comparator circuits and a dynamic \u201cOR\u201d (i.e., all related comparators coupled to match line <b>2806</b> via transistor <b>2808</b>). The byte overlap circuit <b>2900</b> generates a pulse if no bit is present in both masks. This requires an \u201cAND\u201d circuit. It is impractical to wire a stack of 8 bits in series, so the outputs of two parallel 4-high stacks are ANDed. (A parallel \u201cOR\u201d gate would be faster and simpler, but it would generate a pulse if the two masks overlap.)</p><p>2. Dependency and Diagonal Cells</p><p>FIG. 30<i>a </i>shows the logic within the two types of cells included in matrix <b>2400</b>. These cell types are diagonal cell <b>3002</b><i>a </i>(one per row along the matrix diagonal) and dependency cell <b>3004</b><i>a </i>(15 per row at all locations except the matrix diagonal). Restated, diagonal cells are located at row j, column k, where j=k. In contrast, dependency cells are located at row j and column k, where j\u2260k. Each dependency cell <b>3004</b><i>a </i>stores one bit in a RAM cell using cross-coupled inverters <b>3006</b><i>a </i>and <b>3006</b><i>a</i>\u2032. This bit is written from the comparator outputs (DepPrevC[j] and \u02dcDepCache[j]) when the address is calculated. Bits can be reset when other addresses are calculated later, if it determines that there is no dependency (using signals ACalcVecWr[k] and \u02dcDepCache[j]). Diagonal cells <b>3002</b><i>a </i>do not contain RAM cells, but they do connect horizontal and vertical control lines.</p><p>Referring to FIG. 30<i>a</i>, phase 1 of the processor clock (\u03c61) on line <b>3068</b><i>a </i>periodically enables the output of dependency cells <b>3004</b><i>a </i>in an associated row (this value is used every cycle E2). Specifically, \u03c61 outputs a complemented value of the bit held at node <b>3007</b><i>a </i>to line <b>3070</b><i>a </i>every half cycle. (The method for reading this value out using \u03c61 and transistors <b>3072</b><i>a </i>and <b>3074</b><i>a </i>is the same as described below using TagEntryRd[j] and transistors <b>3064</b><i>a </i>and <b>3008</b><i>a</i>.) This complemented value is logically ORed with all other values on row j, producing a \u02dcDepRowCor[j] signal. This signal passes through an inverter and latch (not shown) to become a DepRowC[j] signal, shown in FIG. <b>24</b>.</p><p>As shown in FIG. 30<i>a</i>, signals TagEntryRd[k] on line <b>3048</b><i>a </i>and TagEntryRd[j] on line <b>3044</b><i>a </i>enable the output of dependency cells in the associated row and column, respectively. These signals output a complemented column dependency bit value (i.e., \u02dcColumnC[k]) through transistor <b>3008</b><i>a</i>, and a complemented row dependency bit value (i.e., \u02dcDepCRead[j]) through transistor <b>3010</b><i>a</i>. More specifically, a dependency bit value held at node <b>3007</b><i>a</i>, is forwarded through inverters <b>3006</b><i>a</i>\u2032 and <b>3060</b><i>a </i>and applied to the gates of transistors <b>3062</b><i>a </i>and <b>3064</b><i>a</i>. If the dependency bit value is high, transistors <b>3062</b><i>a </i>and <b>3064</b><i>a </i>conduct thereby coupling transistors <b>3010</b><i>a </i>and <b>3008</b><i>a</i>, respectively, to ground. Accordingly, when transistors <b>3010</b><i>a </i>and <b>3008</b><i>a </i>conduct, lines <b>3066</b><i>a </i>(\u02dcDepCRead[j]) and <b>3040</b><i>a </i>(\u02dcColumnC[k]) are low.</p><p>Conversely, if the dependency bit value is low, transistors <b>3062</b><i>a </i>and <b>3064</b><i>a </i>do not conduct thereby decoupling transistor <b>3010</b><i>a </i>and <b>3008</b><i>a</i>, respectively, from ground. Accordingly, when transistors <b>3010</b><i>a </i>and <b>3008</b><i>a </i>conduct, lines <b>3066</b><i>a </i>(\u02dcDepCRead[j]) and <b>3040</b><i>a </i>(\u02dcColumnC[k]) remain high. (At the beginning of a processor clock cycle, read bit lines such as <b>3066</b><i>a </i>and <b>3040</b><i>a </i>are charged high. These lines are charged periodically and therefore remain high unless pulled low (i.e., such as when transistors <b>3010</b><i>a </i>and <b>3062</b><i>a </i>conduct, and/or when transistors <b>3008</b><i>a </i>and <b>3064</b><i>a </i>conduct). This technique is called \u201cdynamic logic,\u201d and is well known in the art.)</p><p>The use of TagEntryRd[j] and [k] to read out values in associated row and column cells is more clearly illustrated in FIG. 30<i>c. </i></p><p>FIG. 30<i>c </i>shows a portion of dependency matrix <b>2400</b>, including dependency cells <b>3004</b><i>c</i>\u2032, <b>3004</b><i>c</i><b>41</b>  and diagonal cells <b>3002</b><i>c</i>\u2032 and <b>3002</b><i>c</i>\u2033. The internal circuitry of dependency cells <b>3004</b><i>c</i>\u2032 and <b>3004</b><i>c</i>\u2033 is identical to that of dependency cell <b>3004</b><i>a </i>in FIG. 30<i>a</i>. Similarly, the internal circuitry of diagonal cells <b>3002</b><i>c</i>\u2032 and <b>3002</b><i>c</i>\u2033 is identical to that of diagonal cell <b>3002</b><i>a </i>in FIG. 30<i>a</i>. For clarity, only a portion of the circuitry of dependency and diagonal cells in FIG. 30<i>c </i>is shown.</p><p>Referring to FIG. 30<i>c</i>, a TagEntryRd[j] signal on line <b>3046</b><i>c </i>and TagEntryRd[k] on line <b>3048</b><i>c </i>enables the output of bits stored in dependency cells <b>3004</b><i>c</i>\u2033 and <b>3004</b><i>c</i>\u2032. Specifically, line <b>3046</b><i>c </i>enables transistor <b>3008</b><i>c </i>while line <b>3048</b><i>c </i>enables transistor <b>3010</b><i>c</i>. The complemented values of RAM cell <b>3006</b><i>c</i>\u2033 and <b>3006</b><i>c</i>\u2032 are conveyed to the complemented inputs of OR gate <b>3038</b><i>c</i>, which outputs a value on line <b>3036</b><i>c</i>. This output (i.e., line <b>3036</b><i>c</i>) is symbolically represented by combined read bit line <b>2428</b>\u2032 in FIG. <b>24</b>.</p><p>Returning to FIG. 30<i>a</i>, assuming dependency cell <b>3004</b><i>a </i>is in a row corresponding to a newly calculated entry, a bit (held by cross-coupled inverters <b>3006</b><i>a </i>and <b>3006</b><i>a</i>\u2032; i.e., a RAM cell) may be written by enabling signals ACalcVecWr[j] (on line <b>3011</b><i>a</i>), \u02dcACalcVecWr[j] (on line <b>3012</b><i>a</i>), and data signal DepPrevC[k] (on line <b>3014</b><i>a</i>). As discussed above, ACalcVec[j] identifies a newly calculated entry. DepPrevC[k] is a product of DepPrevc[j] on line <b>3016</b><i>a </i>(which indicates whether an entry is previous to the newly-calculated entry; i.e., mask <b>2501</b><i>b </i>of FIG. 25<i>b</i>) and \u02dcDepCache[j] on line <b>3018</b><i>a </i>(which indicates whether there is a cache index match (i.e., VAdr[<b>13</b>:<b>5</b>]) with the newly-calculated entry). Signal \u02dcDepCache[j] is inverted, and then combined with DepPrevC[j] in NAND <b>3020</b><i>a</i>. The output of this NAND gate is inverted by inverter <b>3022</b><i>a</i>, generating a signal that is input to transistors <b>3024</b><i>a</i>, which feed the signal to inverters <b>3006</b><i>a </i>and <b>3006</b><i>a</i>\u2032. In short, the DepC bit (maintained by inverters <b>3006</b><i>a </i>and <b>3006</b><i>a</i>\u2032 at node <b>3007</b><i>a</i>) is set when DepPrevC[j] is high and \u02dcDepCache[j] is low.</p><p>Alternatively, assuming dependency cell <b>3004</b><i>a </i>is in a row corresponding to a previously calculated entry, and this cell was previously set based on an earlier entry whose address had not yet been calculated (as discussed above), this bit may be reset (if there is no dependency) using \u02dcDepCache[j] on line <b>3026</b><i>a </i>and ACalcVecWr[k] (on line <b>3028</b><i>a</i>) generated from the now-calculated earlier entry. If no dependency exists, lines <b>3026</b><i>a </i>and <b>3028</b><i>a </i>will be high, coupling the DepC bit to ground through transistors <b>3050</b><i>a </i>and <b>3052</b><i>a. </i></p><p>Also shown in FIG. 30<i>a </i>is signal \u02dcActive[j] on line <b>3030</b><i>a </i>which resets an entire row if entry [j] is not active by coupling each DepC bit on a particular row to ground through an associated transistor <b>3054</b><i>a</i>. This signal is derived from the active mask illustrated in FIG. 14, which distinguishes between entries in the address queue that are active and inactive. Accordingly, inactive entries may be identified and cleared. Similarly, signal \u02dcActive[k] on line <b>3032</b><i>a</i>, generated from \u02dcActive[j] on line <b>3034</b><i>a</i>, resets an entire column if an entry is not active. Specifically, \u02dcActive[k] on line <b>3032</b><i>a </i>couples each DepC bit on a particular column to ground through an associated transistor <b>3056</b><i>a</i>. Signal \u02dcActive[k] is used in such situations as initializing a matrix or clearing an entry after the associated instruction graduates or aborts.</p><p>Also shown is OR signal <b>3036</b><i>a </i>which represents row and column output values ORed together through OR gate <b>3038</b><i>a </i>(i.e., signal SameCacheSet[j]). This signal is symbolically represented as combined read bit line <b>2428</b>\u2032 in FIG. <b>24</b>.</p><p>FIG. 30<i>b </i>shows the logic within the two types of cells included in matrix <b>2450</b>. (As is apparent from FIGS. 30<i>a </i>and <b>30</b><i>b</i>, the circuitry present in the cells of matrix <b>2450</b> is identical to the corresponding circuitry in matrix <b>2400</b>.) These cell types are diagonal cell <b>3002</b><i>b </i>(one per row along the matrix diagonal) and dependency cell <b>3004</b><i>b </i>(15 per row at all locations except the matrix diagonal). Restated, diagonal cells are located at row j, column k, where j=k. In contrast, dependency cells are located at row j and column k, where j\u2260k. Each dependency cell <b>3004</b><i>b </i>stores one bit in a RAM cell using cross-coupled inverters <b>3006</b><i>b </i>and <b>3006</b><i>b</i>\u2032. This bit is written from the comparator outputs (DepPrevS[j] and \u02dcDepBytes[j]) when the address is calculated. Bits can be reset when other addresses are calculated later, if it determines that there is no dependency (using signals ACalcVecWr[k] and \u02dcDepBytes[j]). Diagonal cells <b>3002</b><i>b </i>do not contain RAM cells, but they do connect horizontal and vertical control lines.</p><p>Referring to FIG. 30<i>b</i>, phase 1 of the processor clock (\u03c61) on line <b>3068</b><i>b </i>periodically enables the output of dependency cells <b>3004</b><i>b </i>in an associated row (this value is used every cycle E2). Specifically, a dependency bit held at node <b>3007</b><i>b </i>is forwarded through inverter <b>3006</b><i>b</i>\u2032 and <b>3060</b><i>b </i>and applied to the gate of transistor <b>3072</b><i>b</i>. If the dependency bit value is high, transistor <b>3072</b><i>b </i>conducts thereby coupling transistor <b>3074</b><i>b </i>to ground. Accordingly, when transistor <b>3074</b><i>b </i>conducts (i.e., when \u03c61 is high), line <b>3070</b><i>b </i>(\u02dcDepRowCor[j]) is low.</p><p>Conversely, if the dependency bit value is low, transistor <b>3072</b><i>b </i>does not conduct and line <b>3070</b><i>b </i>remains decoupled from ground even when \u03c61 is high. Line <b>3070</b><i>b </i>(like lines <b>3070</b><i>a</i>, <b>3066</b><i>a</i>, and <b>3040</b><i>a </i>in cells <b>3004</b><i>a</i>) is periodically charged in accordance with the well-known technique of dynamic logic. Accordingly, decoupling line <b>3070</b><i>b </i>from ground thereby enables it to remain high.</p><p>In summary, \u03c61 outputs a complemented value of the bit held at node <b>3007</b><i>b </i>to line <b>3070</b><i>b </i>every half cycle. This complemented value is logically ORed with all other values on row j, producing a \u02dcDepRowSor[j] signal. This signal passes through an inverter and latch (not shown) to become a DepRowS[j] signal, shown in FIG. <b>24</b>.</p><p>Assuming dependency cell <b>3004</b><i>b </i>is in a row corresponding to a newly calculated entry, a bit (held by cross-coupled inverters <b>3006</b><i>b </i>and <b>3006</b><i>b</i>\u2032 at note <b>3007</b><i>b </i>i.e., a RAM cell) may be written by enabling signals ACalcVecWr[j] (on line <b>3011</b><i>b</i>), \u02dcACalcVecWr[j] (on line <b>3012</b><i>b</i>), and data signal DepPrevS[k] (on line <b>3014</b><i>b</i>). As discussed above, ACalcVec[j] identifies a newly calculated entry. DepPrevS[k] is a product of DepPrevS[j] on line <b>3016</b><i>b </i>(which indicates whether an entry is a store instruction and previous to the entry being calculated i.e., the mask <b>2501</b><i>a </i>of FIG. 25<i>a</i>) and \u02dcDepBytes[j] on line <b>3018</b><i>b </i>(which indicates whether there is a cache index and doubleword match and byte overlap with the newly-calculated entry). Signal \u02dcDepBytes[j] is inverted, and then combined with DepPrevS[j] in NAND <b>3020</b><i>b</i>. The output of this NAND gate is inverted by inverter <b>3022</b><i>b</i>, generating a signal that is input to transistors <b>3024</b><i>b</i>, which feed the signal to inverters <b>3006</b><i>b </i>and <b>3006</b><i>b</i>\u2032. In short, the DepS bit (maintained by inverters <b>3006</b><i>b </i>and <b>3006</b><i>b</i>\u2032 at node <b>3007</b><i>b</i>) is set when DepPrevS[j] is high and \u02dcDepBytes[j] is low.</p><p>Alternatively, assuming dependency cell <b>3004</b><i>b </i>is in a row corresponding to a previously calculated entry, and this cell was previously set based on an earlier entry that had not yet been calculated (as discussed above), this bit may be reset (if there is no dependency) using \u02dcDepBytes[j] on line <b>3026</b><i>b </i>and ACalcVecWr[k] (on line <b>3028</b><i>b</i>) generated from the now-calculated earlier entry. If no dependency exists, lines <b>3026</b><i>b </i>and <b>3028</b><i>a </i>will be high, coupling the DepS bit to ground through transistors <b>3050</b><i>b </i>and <b>3052</b><i>b. </i></p><p>ACalcVecWr[j], \u02dcACalcVecWr[j] and ACalcVecWr[k] are identified on different lines in FIG. 30<i>a </i>(i.e., lines <b>3011</b><i>a</i>, <b>3012</b><i>a </i>and <b>3028</b><i>a</i>, respectively) and FIG. 30<i>b </i>(i.e., lines <b>3011</b><i>b</i>, <b>3012</b><i>b </i>and <b>3028</b><i>b</i>, respectively) for purposes of discussion at the individual cell level. However, as shown in FIG. 24, the same signals lines convey these signals to matrix <b>2400</b> and <b>2450</b>.</p><p>Also shown in FIG. 30<i>b </i>is signal \u02dcActive[j] on line <b>3030</b><i>b </i>which resets an entire row if entry [j] is not active by coupling each DepS bit on a particular row to ground through an associated transistor <b>3054</b><i>b</i>. This signal is derived from the active mask illustrated in FIG. 14, which distinguishes between entries in the address queue that are active and inactive. Accordingly, inactive signals may be identified and cleared. Similarly, signal \u02dcActive[k] on line <b>3032</b><i>b</i>, generated from \u02dcActive[j] on line <b>3034</b><i>b</i>, resets an entire column if an entry is not active. Specifically, \u02dcActive[k] on line <b>3032</b><i>b </i>couple each DepS bit on a particular column to ground through an associated transistor <b>3056</b><i>b</i>. Signal \u02dcActive[k] is used in such situations as initializing a matrix or clearing an entry after the associated instruction graduates or aborts.</p><p>C. Dependency Logic\u2014Alternative Embodiment</p><p>FIG. 31 illustrates an alternative embodiment of the dependency matrix system shown in FIG. <b>24</b>. The systems of FIG. <b>24</b> and FIG. 31 are identical except for the logic used to generate masks <b>2501</b><i>a </i>and <b>2501</b><i>b </i>(FIGS. 25<i>a </i>and <b>25</b><i>b</i>).</p><p>As shown in FIG. 24, comparator <b>2408</b> forwards signals \u02dcDepCache[j] and DepPrevC[j] to matrix <b>2400</b>, and signals \u02dcDepBytes[j] and DepPrevS[j] to matrix <b>2450</b>. The logic used to generate these signals (in comparator <b>2408</b>) is illustrated in FIG. <b>22</b>. Referring to FIG. 22, DepPrevC[j] is constructed from signals DepCache[j] and AQvPrev[j] combined in AND gate <b>2736</b>. Similarly, DepPrevS[j] is constructed from signals DepBytes[j], AQvPrev[j] and AQvStore[j] combined in AND gate <b>2734</b>. As discussed above, signal DepPrevC[j] functions as mask <b>2501</b><i>b </i>while signal DepPrevs[j] functions as mask <b>2501</b><i>a. </i></p><p>Within cache-set matrix <b>2400</b>, DepPrevC[j] is combined with \u02dcDepCache[j] to generate DepPrevC[k], as shown in FIG. 30<i>a</i>. DepPrevC[k] is used to set a DepC bit at node <b>3007</b><i>a</i>. Within store matrix <b>2450</b>, DepPrevs[j] is combined with \u02dcDepBytes[j] to generate DepPrevS[k], as shown in FIG. 30<i>b</i>. DepPrevS[k] is used to set a Deps bit at node <b>3007</b><i>b. </i></p><p>Like the system of FIG. 24, the system of FIG. 31 also uses comparators to generate and forward signal \u02dcDepCache[j] and \u02dcDepBytes[j] to a cache-set matrix (i.e., <b>3100</b>) and a store matrix (i.e., <b>3150</b>), respectively. Comparators <b>3106</b> in FIG. 31 may use the logic disclosed in FIGS. 22 or <b>27</b> to generate these signals. However, unlike the system of FIG. 24, the system of FIG. 31 forwards signals AQvPrev[j] and AQvStore[j] directly to the dependency matrixes. (AQvStore, as discussed above, is generated from a decoder in address queue <b>308</b> at the time the associated instruction is loaded into the queue. AQvPrev, also discussed above, is generated from priority logic in queue <b>308</b> which tracks active instructions.)</p><p>Referring to FIG. 31, AQvPrev[j] is forwarded to cache-set matrix <b>3100</b> and store matrix <b>3150</b> via line <b>3110</b>, and AQvstore[j] is forwarded to matrix <b>3150</b> via line <b>3112</b>. This configuration (i.e., forwarding AQvStore[j] and AQvPrev[j] directly to dependency matrixes) represents the preferred embodiment of the invention.</p><p>FIG. 32<i>a </i>shows the logic within the two types of cells included in matrix <b>3100</b>. These cell types are diagonal cell <b>3202</b><i>a </i>(one per row) and dependency cell <b>3204</b><i>a </i>(15 per row). The architecture and operation of cell <b>3202</b><i>a </i>is the same as <b>3002</b><i>a </i>(FIG. 30<i>a</i>) except for the use of signal AQvPrev[j] on line <b>3216</b><i>a</i>. In short, cell <b>3202</b><i>a </i>receives signal AQvPrev[j] rather than DepPrevC[j] to generate mask <b>2501</b><i>b </i>(i.e., DepPrevC[k]). In contrast, cell <b>3002</b><i>a </i>receives a previously-calculated mask value (i.e., DepPrevC[j]) and simply gates this value with,a constituent element (i.e., DepCache[j]) in NAND gate <b>3020</b><i>a</i>. Referring to FIG. 32<i>a</i>, DepC#bit at node <b>3007</b><i>a </i>is set when AQvPrev[j] is high and \u02dcDepCache[j] is low.</p><p>Similarly, the architecture and operation of cell <b>3204</b><i>a </i>is the same as <b>3004</b><i>a </i>(FIG. 30<i>a</i>) except AQvPrev[j] (rather than DepPrevC[j]) passes through the cell.</p><p>FIG. 32<i>b </i>shows the logic within the two types of cells included in matrix <b>3150</b>. These cell types are diagonal cell <b>3202</b><i>b </i>(one per row) and dependency cell <b>3204</b><i>b </i>(15 per row). The architecture and operation of cell <b>3202</b><i>b </i>is the same as <b>3002</b><i>b </i>except for the use of signals AQvPrev[j] on line <b>3216</b><i>b</i>, AQvStore[j] on line <b>3217</b><i>b </i>and three-input NAND gate <b>3220</b><i>b </i>(having a complemented input for line <b>3018</b><i>b</i>). In short, cell <b>3202</b><i>b </i>receives signals AQvPrev[j] and AQvStore[j] rather than DepPrevS[j] to generate mask <b>2501</b><i>a </i>(i.e., DepPrevS[k]). In contrast, cell <b>3002</b><i>b </i>receives a previously calculated mask value (i.e., DepPrevS[j]) and simply gates this value with a constituent element (i.e., DepBytes[j]) in NAND gate <b>3020</b><i>b</i>. Referring to FIG. 32<i>b</i>, DepS bit at node <b>3007</b><i>b </i>is set when AQvPrev[j] and AQvStore[j] are high, and \u02dcDepBytes[j] is low.</p><p>Similarly, the architecture and operation of cell <b>3204</b><i>b </i>is the same as <b>3004</b><i>b </i>(FIG. 30<i>b</i>) except AQvPrev[j] and AQvStore[j] (rather than DepPrevS[j]) pass through the cell.</p><p>Aside from the differences highlighted above, the operation and architecture of cache-set matrix <b>3100</b> and store matrix <b>3150</b> (FIG. 31) is identical to cache-set matrix <b>2400</b> and store matrix <b>2450</b> (FIG. <b>24</b>), respectively. Accordingly, except for the direct use of signals AQvstore[j] and AQvPrev[j] by matrixes <b>3100</b> and <b>3150</b>, the discussion presented herein related to the architecture and operation of matrixes <b>2400</b> and <b>2450</b> applies equally to matrixes <b>3100</b> and <b>3150</b>.</p><p>As mentioned above, the preferred embodiment of the invention combines the cache-set matrix and store matrix in a single array since many signals are shared. This applies to matrixes <b>2400</b> and <b>2450</b> as well as <b>3100</b> and <b>3150</b>. The preferred embodiment also requires signals AQvStore[j] and AQvPrev[j] to be forwarded directly to the dependency matrixes. Both requirements are satisfied by combining cache-set matrix <b>3100</b> with store matrix <b>3150</b>.</p><p>FIG. 33 shows the logic within the two types of cells included in a combined matrix of matrixes <b>3100</b> and <b>3150</b>. These cell types are diagonal cell <b>3302</b> (one per row) and dependency cell <b>3304</b> (<b>15</b> per row). The architecture and operation of cell <b>3302</b> is the same as cells <b>3202</b><i>a </i>(FIG. 32<i>a</i>) and <b>3202</b><i>b </i>(FIG. 32<i>b</i>). Similarly, the architecture and operation of cell <b>3304</b> is the same as cells <b>3204</b><i>a </i>and <b>3204</b><i>b</i>. Only the layout in each cell-type is changed. Moreover, FIG. 33 expressly shows the shared use of common control signals (e.g., AQvPrev[j] on line <b>3316</b>, ACalcVecWr[j] on <b>3311</b>, \u02dcACalcVecWr[j] on line <b>3312</b>, ACalcVecWr[k] on line <b>3328</b>, \u02dcActive[j] on line <b>3334</b> and <b>3330</b>, \u02dcActive[k] on line <b>3332</b> and \u03c61 on line <b>3368</b>). These signals are also shared in matrixes <b>3100</b> and <b>3150</b> (as well as <b>2400</b> and <b>2450</b>\u2014except for signal AQvPrev[j]), although different line numbers are used in the associated figures (i.e., <b>32</b><i>a</i>, <b>32</b><i>b </i>and <b>30</b><i>a</i>, <b>30</b><i>b</i>) for purposes of discussion.</p><p>FIG. 34 shows timing for the dependency circuits.</p><p>D. Uncached Memory Dependency</p><p>Load and store instructions to uncached memory addresses are executed strictly in program order.</p><p>Processor <b>100</b> does not check for dependencies between cached and uncached accesses. This is not an issue in unmapped regions, because the cached and uncached address regions are disjoint. For mapped regions, however, TLB <b>422</b> may select different cache attributes for the same page. Processor <b>100</b> does not prevent the programmer from alternatively accessing the same memory address as \u201ccached\u201d and \u201cuncached.\u201d However, coherency is not guaranteed; the contents of the cache are not checked for any uncached address.</p><p>While the above is a complete description of the preferred embodiment of the invention, various modifications, alternatives and equivalents may be used. Therefore, the above description should not be taken as limiting the scope of the invention which is defined by the appended claims.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Kenneth", "last_name": "Yeager", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "MIPS TECHNOLOGIES, INC."}, {"first_name": "", "last_name": "MIPS TECHNOLOGIES, INC.", "name": ""}, {"first_name": "", "last_name": "JEFFERIES FINANCE LLC, AS COLLATERAL AGENT", "name": ""}, {"first_name": "", "last_name": "MIPS TECHNOLOGIES, INC.", "name": ""}, {"first_name": "", "last_name": "SILICON GRAPHICS, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F   9/20"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/38        20060101A I20051008RMEP"}, {"label": "G06F  12/08        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711100"}, {"primary": false, "label": "711003"}, {"primary": false, "label": "712E09048"}, {"primary": false, "label": "711118"}, {"primary": false, "label": "711202"}, {"primary": false, "label": "712E09049"}, {"primary": false, "label": "711104"}, {"primary": false, "label": "711E12049"}], "ecla_classes": [{"label": "G06F   9/38E1"}, {"label": "G06F   9/38D4"}, {"label": "G06F   9/38E"}, {"label": "S06F12:08B10"}, {"label": "G06F  12/08B6P"}], "cpc_classes": [{"label": "G06F   9/3857"}, {"label": "G06F   9/30043"}, {"label": "G06F   9/3838"}, {"label": "G06F  12/0855"}, {"label": "G06F   9/3836"}, {"label": "G06F   9/3824"}, {"label": "G06F   9/3834"}, {"label": "G06F  12/0864"}, {"label": "G06F   9/384"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1994-10-14", "application_date": "1995-03-14", "family_members": [{"ucid": "US-6216200-B1", "titles": [{"lang": "EN", "text": "Address queue"}]}, {"ucid": "WO-1996012227-A1", "titles": [{"lang": "EN", "text": "AN ADDRESS QUEUE CAPABLE OF TRACKING MEMORY DEPENDENCIES"}, {"lang": "FR", "text": "QUEUE D'ADRESSES CAPABLE DE FAIRE LE SUIVI DES DEPENDANCES DE MEMOIRE"}]}]}