{"patent_number": "US-6321299-B1", "publication_id": 72870568, "family_id": 9525845, "publication_date": "2001-11-20", "titles": [{"lang": "EN", "text": "Computer circuits, systems, and methods using partial cache cleaning"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72646171\"><p>A method (<b>50</b>) of operating a computing system (<b>10</b>). The computing system comprises a cache memory (<b>12</b><i>b</i>), and the cache memory has a predetermined number of cache lines. First, the method, for a plurality of write addresses, writes data (<b>64</b>) to the cache memory at a location corresponding to each of the plurality of write addresses. Second, the method cleans (<b>70</b>) a selected number (<b>68</b>) of lines in the cache memory. For each of the selected number of lines, the cleaning step evaluates a dirty indicator corresponding to data in the line and copies data from the line to another memory if the dirty indicator indicates the data in the line is dirty. Lastly, the selected number of lines which are cleaned is less than the predetermined number of cache lines.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00001\" num=\"1\"><claim-text>1. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>first, for a plurality of write addresses, writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>second, cleaning a selected number of lines in the cache memory in response to a context switch by the computing system; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines cleaned is less than the predetermined number of cache lines and further comprising the steps of: </claim-text><claim-text>storing a value in an address indicator; and </claim-text><claim-text>in response to each of the plurality of write addresses, setting the value equal to the write address if the write address is greater than what is stored in the address indicator, wherein the value in the address indicator represents a final value upon completing the plurality of write addresses; and </claim-text><claim-text>wherein the selected number of lines is responsive to the final value. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00002\" num=\"2\"><claim-text>2. The method of claim <b>1</b> wherein the step of cleaning a selected number of lines in the cache memory comprises cleaning all lines in the cache memory between a first address through and including the final value.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00003\" num=\"3\"><claim-text>3. The method of claim <b>2</b> wherein the first address is at address zero of the cache memory.</claim-text></claim>"}, {"num": 4, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00004\" num=\"4\"><claim-text>4. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>first, for a plurality of write addresses, writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>second, cleaning a selected number of lines in the cache memory in response to a context switch by the computing system; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines cleaned is less than the predetermined number of cache lines and the computing system comprises an operating system and that maintains a maximum cache line address corresponding to a context of operation for the computing system; and </claim-text><claim-text>further comprising the step of setting the selected number in response to the maximum cache line address. </claim-text></claim>"}, {"num": 5, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00005\" num=\"5\"><claim-text>5. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>first, for a plurality of write addresses, writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>second, cleaning a selected number of lines in the cache memory in response to a context switch by the computing system; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines cleaned is less than the predetermined number of cache lines; and further comprising the steps of: </claim-text><claim-text>storing a value in an address indicator; and </claim-text><claim-text>in response to each of the plurality of write addresses, setting the value equal to the write address if the write address is less than what is stored in the address indicator, wherein the value in the address indicator represents a final value upon completing the plurality of write addresses; and </claim-text><claim-text>wherein the selected number of lines is responsive to the final value. </claim-text></claim>"}, {"num": 6, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00006\" num=\"6\"><claim-text>6. The method of claim <b>5</b> wherein the step of cleaning a selected number of lines in the cache memory comprises cleaning all lines in the cache memory between the final value through and including a first address.</claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00007\" num=\"7\"><claim-text>7. The method of claim <b>6</b> wherein the first address is a highest address of the cache memory.</claim-text></claim>"}, {"num": 8, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00008\" num=\"8\"><claim-text>8. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>first, for a ilurality of write addresses, writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>second, cleaning a selected number of lines in the cache memory, wherein the selected number of lines is less than the predetermined number of cache lines; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memorv if the dirty indicator indicates the data in the line is dirty; </claim-text><claim-text>storing a value in a first address indicator; </claim-text><claim-text>storing a value in a second address indicator; in response to each of the plurality of write addresses, setting the value in the first address indicator equal to the write address if the write address is greater than what is stored in the first address indicator, wherein the value in the first address indicator represents a first final value upon completing the plurality of write addresses; </claim-text><claim-text>in response to each of the plurality of write addresses, setting the value in the second address indicator equal to the write address if the write address is less than what is stored in the second address indicator, wherein the value in the second address indicator represents a second final value upon completing the plurality of write addresses; and </claim-text><claim-text>wherein the selected number of lines is responsive to the first and second final values. </claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00009\" num=\"9\"><claim-text>9. The method of claim <b>8</b> wherein the step of cleaning a selected number of lines in the cache memory comprises cleaning all lines in the cache memory between the first final value through and including the second final value.</claim-text></claim>"}, {"num": 10, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00010\" num=\"10\"><claim-text>10. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>first, for a plurality of write addresses, writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>second, cleaning a selected number of lines in the cache memory in response to a context switch by the computing system; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines cleaned is less than the predetermined number of cache lines; and </claim-text><claim-text>wherein the computing system comprises an operating system; </claim-text><claim-text>wherein the operating system maintains a maximum cache line address corresponding to a maximum address beyond which the cache memory is not written for a context of operation for the computing system; and </claim-text><claim-text>further comprising the steps of: </claim-text><claim-text>storing a value in a first address indicator; </claim-text><claim-text>in response to each of the plurality of write addresses, setting the value in the first address indicator equal to the write address if the write address is less than what is stored in the first address indicator, wherein the value in the first address indicator represents a final value upon completing the plurality of write addresses; and </claim-text><claim-text>wherein the step of cleaning a selected number of lines in the cache memory comprises cleaning all lines in the cache memory between the final value through and including the maximum cache line address. </claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00011\" num=\"11\"><claim-text>11. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>first, for a plurality of write addresses, writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>second, cleaning a selected number of lines in the cache memory in response to a context switch by the computing system; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines cleaned is less than the predetermined number of cache lines; and </claim-text><claim-text>wherein the computing system comprises an operating system; </claim-text><claim-text>wherein the operating system maintains a minimum cache line address corresponding to a minimum address beyond which the cache memory is not written for a context of operation for the computing system; and </claim-text><claim-text>further comprising the steps of: </claim-text><claim-text>storing a value in a first address indicator; </claim-text><claim-text>in response to each of the plurality of write addresses, setting the value in the first address indicator equal to the write address if the write address is greater than what is stored in the first address indicator, wherein the value in the first address indicator represents a final value upon completing the plurality of write addresses; and </claim-text><claim-text>wherein the step of cleaning a selected number of lines in the cache memory comprises cleaning all lines in the cache memory between the minimum cache line address through and including the final value. </claim-text></claim>"}, {"num": 12, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00012\" num=\"12\"><claim-text>12. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>first, for a plurality of write addresses, writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>second, cleaning a selected number of lines in the cache memory in response to a context switch by the computing system; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines deaned is less than the predetermined number of cache lines and the computing system comprises a single processor. </claim-text></claim>"}, {"num": 13, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00013\" num=\"13\"><claim-text>13. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>first, for a plurality of write addresses, writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>second, cleaning a selected number of lines in the cache memory in response to a context switch by the computing system; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines cleaned is less than the predetermined number of cache lines and the computing system comprises a plurality of processors. </claim-text></claim>"}, {"num": 14, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00014\" num=\"14\"><claim-text>14. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>first, for a plurality of write addresses, writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>second, cleaning a selected number of lines in the cache memory in response to a context switch by the computing system; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines cleaned is less than the predetermined number of cache lines and the computing system comprises a wireless data platform system. </claim-text></claim>"}, {"num": 15, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00015\" num=\"15\"><claim-text>15. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>storing a value in an address indicator; </claim-text><claim-text>for a plurality of write addresses, the steps of: </claim-text><claim-text>writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>in response to each of the plurality of write addresses, setting the value equal to the write address if the write address is greater than what is stored in the address indicator, wherein the value in the address indicator represents a final value upon completing the plurality of write addresses; and </claim-text><claim-text>responsive to a context switch by the computing system, cleaning a selected number of lines in the cache memory; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the step of cleaning a selected number of lines in the cache memory comprises cleaning all lines in the cache memory between a address zero of the cache memory through and including the final value. </claim-text></claim>"}, {"num": 16, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00016\" num=\"16\"><claim-text>16. A computing system, comprising:</claim-text><claim-text>a cache memory having a predetermined number of cache lines; </claim-text><claim-text>circuitry, responsive to a plurality of write addresses, for writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>circuitry for cleaning a selected number of lines in the cache memory in response to a context switch by the computing system, wherein for each of the selected number of lines the circuitry for cleaning comprises: </claim-text><claim-text>circuitry for evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>circuitry for copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines is less than the predetermined number of cache lines; and further comprising: </claim-text><claim-text>an address indicator for storing a value; and </claim-text><claim-text>circuitry, responsive to each of the plurality of write addresses, for setting the value equal to the write address if the write address is greater than what is stored in the address indicator, wherein the value in the address indicator represents a final value upon completing the plurality of write addresses; and </claim-text><claim-text>wherein the selected number of lines is responsive to the final value. </claim-text></claim>"}, {"num": 17, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00017\" num=\"17\"><claim-text>17. The computing system of claim <b>16</b> wherein the circuitry for cleaning a selected number of lines in the cache memory comprises circuitry for cleaning all lines in the cache memory between a first address through and including the final value.</claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00018\" num=\"18\"><claim-text>18. The computing system of claim <b>17</b> wherein the first address is at address zero of the cache memory.</claim-text></claim>"}, {"num": 19, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00019\" num=\"19\"><claim-text>19. A computing system, comprising:</claim-text><claim-text>a cache memory having a predetermined number of cache lines; </claim-text><claim-text>circuitry, responsive to a plurality of write addresses, for writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>circuitry for cleaning a selected number of lines in the cache memory in response to a context switch by the computing system, wherein for each of the selected number of lines the circuitry for cleaning comprises: </claim-text><claim-text>circuitry for evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>circuitry for copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines is less than the predetermined number of cache lines; and further: </claim-text><claim-text>wherein the computing system comprises an operating system; </claim-text><claim-text>wherein the operating system maintains a maximum cache line address corresponding to a context of operation for the computing system; and </claim-text><claim-text>further comprising circuitry for setting the selected number in response to the maximum cache line address. </claim-text></claim>"}, {"num": 20, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00020\" num=\"20\"><claim-text>20. A computing system, comprising:</claim-text><claim-text>a cache memory having a predetermined number of cache lines; </claim-text><claim-text>circuitry, responsive to a plurality of write addresses, for writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>circuitry for cleaning a selected number of lines in the cache memory in response to a context switch by the computing system, wherein for each of the selected number of lines the circuitry for cleaning comprises: </claim-text><claim-text>circuitry for evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>circuitry for copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines is less than the predetermined number of cache lines; and further comprising: </claim-text><claim-text>an address indicator for storing a value; and </claim-text><claim-text>circuitry, responsive to each of the plurality of write addresses, for setting the value equal to the write address if the write address is less than what is stored in the address indicator, wherein the value in the address indicator represents a final value upon completing the plurality of write addresses; and </claim-text><claim-text>wherein the selected number of lines is responsive to the final value. </claim-text></claim>"}, {"num": 21, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00021\" num=\"21\"><claim-text>21. The computing system of claim <b>20</b> wherein the circuitry for cleaning a selected number of lines in the cache memory comprises circuitry for cleaning all lines in the cache memory between the final value through and including a first address.</claim-text></claim>"}, {"num": 22, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00022\" num=\"22\"><claim-text>22. The computing system of claim <b>21</b> wherein the first address is a highest address of the cache memory.</claim-text></claim>"}, {"num": 23, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00023\" num=\"23\"><claim-text>23. A computing system, comprising:</claim-text><claim-text>a cache memory having a predetermined number of cache lines; </claim-text><claim-text>circuitry, responsive to a plurality of write addresses, for writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>circuitry for cleaning a selected number of lines in the cache memory, wherein the selected number of lines is less than the predetermined number of cache lines, and wherein for each of the selected number of lines the circuitry for cleaning comprises: </claim-text><claim-text>circuitry for evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>circuitry for copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; </claim-text><claim-text>a first address indicator for storing a value; </claim-text><claim-text>a second address indicator for storing a value; </claim-text><claim-text>circuitry, responsive to each of the plurality of write addresses, for setting the value in the first address indicator equal to the write address if the write address is greater than what is stored in the first address indicator, wherein the value in the first address indicator represents a first final value upon completing the plurality of write addresses; </claim-text><claim-text>circuitry, responsive to each of the plurality of write addresses, for setting the value in the second address indicator equal to the write address if the write address is less than what is stored in the second address indicator, wherein the value in the second address indicator represents a second final value upon completing the plurality of write addresses; and </claim-text><claim-text>wherein the selected number of lines is responsive to the first and second final values. </claim-text></claim>"}, {"num": 24, "parent": 23, "type": "dependent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00024\" num=\"24\"><claim-text>24. The computing system of claim <b>23</b> wherein the circuitry for cleaning a selected number of lines in the cache memory comprises circuitry for cleaning all lines in the cache memory between the first final value through and including the second final value.</claim-text></claim>"}, {"num": 25, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00025\" num=\"25\"><claim-text>25. A computing system, comprising:</claim-text><claim-text>a cache memory having a predetermined number of cache lines; </claim-text><claim-text>circuitry, responsive to a plurality of write addresses, for writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>circuitry for cleaning a selected number of lines in the cache memory in response to a context switch by the computing system, wherein for each of the selected number of lines the circuitry for cleaning comprises: </claim-text><claim-text>circuitry for evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>circuitry for copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines is less than the predetermined number of cache lines; and further: </claim-text><claim-text>wherein the computing system comprises an operating system; </claim-text><claim-text>wherein the operating system maintains a maximum cache line address corresponding to a maximum address beyond which the cache memory is not written for a context of operation for the computing system; and </claim-text><claim-text>further comprising: </claim-text><claim-text>a first address indicator for storing a value; </claim-text><claim-text>circuitry, responsive to each of the plurality of write addresses, for setting the value in the first address indicator equal to the write address if the write address is less than what is stored in the first address indicator, wherein the value in the first address indicator represents a final value upon completing the plurality of write addresses; and </claim-text><claim-text>wherein the circuitry for cleaning a selected number of lines in the cache memory comprises circuitry for cleaning all lines in the cache memory between the final value through and including the maximum cache line address. </claim-text></claim>"}, {"num": 26, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00026\" num=\"26\"><claim-text>26. A computing system, comprising:</claim-text><claim-text>a cache memory having a predetermined number of cache lines; </claim-text><claim-text>circuitry, responsive to a plurality of write addresses, for writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>circuitry for cleaning a selected number of lines in the cache memory in response to a context switch by the computing system, wherein for each of the selected number of lines the circuitry for cleaning comprises: </claim-text><claim-text>circuitry for evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>circuitry for copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines is less than the predetermined number of cache lines; and further: </claim-text><claim-text>wherein the computing system comprises an operating system; </claim-text><claim-text>wherein the operating system maintains a minimum cache line address corresponding to a minimum address beyond which the cache memory is not written for a context of operation for the computing system; and </claim-text><claim-text>further comprising: </claim-text><claim-text>a first address indicator for storing a value; </claim-text><claim-text>circuitry, responsive to each of the plurality of write addresses, for setting the value in the first address indicator equal to the write address if the write address is greater than what is stored in the first address indicator, wherein the value in the first address indicator represents a final value upon completing the plurality of write addresses; and </claim-text><claim-text>wherein the circuitry for cleaning a selected number of lines in the cache memory comprises circuitry for cleaning all lines in the cache memory between the minimum cache line address through and including the final value. </claim-text></claim>"}, {"num": 27, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00027\" num=\"27\"><claim-text>27. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>first, for a plurality of write addresses, writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>second, cleaning a selected number of lines in the cache memory in response to a context switch by the computing system; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines cleaned is less than the predetermined number of cache lines and wherein the cache memory is accessible by a single processor core. </claim-text></claim>"}, {"num": 28, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00028\" num=\"28\"><claim-text>28. A method of operating a computing system comprising a cache memory having a predetermined number of cache lines, comprising the steps of:</claim-text><claim-text>first, for a plurality of write addresses, writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>second, cleaning a selected number of lines in the cache memory in response to a context switch by the computing system; </claim-text><claim-text>wherein for each of the selected number of lines the cleaning step comprises: </claim-text><claim-text>evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the selected number of lines cleaned is less than the predetermined number of cache lines and the cache memory is accessible by a plurality of processor cores. </claim-text></claim>"}, {"num": 29, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6321299-B1-CLM-00029\" num=\"29\"><claim-text>29. A computing system, comprising:</claim-text><claim-text>cache memory having a predetermined number of cache lines; </claim-text><claim-text>an address indicator for storing a value; </claim-text><claim-text>circuitry, responsive to a plurality of write addresses, for writing data to the cache memory at a location corresponding to each of the plurality of write addresses; and </claim-text><claim-text>circuitry, responsive to each of the plurality of write addresses, for setting the value equal to the write address if the write address is greater than what is stored in the address indicator, wherein the value in the address indicator represents a final value upon completing the plurality of write addresses; and </claim-text><claim-text>circuitry, responsive to a context switch by the computing system, for cleaning a selected nunber of lines in the cache memory; </claim-text><claim-text>wherein for each of the selected number of lines the circuitry for cleaning comprises: </claim-text><claim-text>circuitry for evaluating a dirty indicator corresponding to data in the line; and </claim-text><claim-text>circuitry for copying data from the line to another memory if the dirty indicator indicates the data in the line is dirty; and </claim-text><claim-text>wherein the circuitry for cleaning a selected number of lines in the cache memory comprises circuitry for cleaning all lines in the cache memory between a address zero of the cache memory through and including the final value. </claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54760599\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>CROSS-REFERENCES TO RELATED APPLICATIONS</h4><p>This application claims a priority right from France Patent Application 09 05420, entitled Circuits, syst\u00e9m\u00e9s et proc\u00e9d\u00e9s d'ordinateur utilisant un nettoyage partiel d'une m\u00e9moire cache, having inventors G\u00e9rard Chauvel, Serge Lasserre, Dominique Beno\u00eet Jacques d'Invemo, and filed Apr. 29, 1998.</p><h4>STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT</h4><p>Not Applicable.</p><h4>BACKGROUND OF THE INVENTION</h4><p>The present embodiments relate to computing environments implementing one or more cache memories.</p><p>Cache circuits are important components which are frequently used in contemporary computing systems (e.g., microprocessors and the like) to increase system performance by reducing the potential amount of time needed to access information. Typically, a cache circuit includes various components, such as a tag memory which is commonly a random access memory (\u201cRAM\u201d). The tag RAM stores so-called tag information which corresponds to the cached data which is commonly stored in a separate cache data RAM. The tag information may include various characteristics corresponding to the cached data, such as the actual address where the cached data may be found in some other memory device (e.g., an external memory structure). Another component of a cache circuit is the hit detection circuit associated with the tag RAM. The hit detection circuit (of which there are N such circuits in an N-way set associative cache circuit) compares an incoming address with the actual address stored as part of the tag information. If the comparison matches, there is said to be a \u201chit\u201d in the cache circuit, that is, the data sought at the incoming address may be retrieved directly from the cache data RAM; on the other hand, if the comparison does not match, there is said to be a \u201cmiss\u201d in the cache circuit, that is, the data sought at the incoming address is not located, or for some other reason is not reliable, within the cache data RAM. In the event of a cache miss, then the data must be retrieved from a memory higher in the memory hierarchy, such as the main (i.e., often external) memory or in another cache located at a higher level in the system. Thus, access to data following a cache miss requires a greater amount of time then when a cache hit occurs and, indeed, if the access is from external memory the time required may be considerable as compared to the access time in the event of a cache hit.</p><p>While the above illustrates that cache memories are generally perceived as beneficial, as computing devices and environments become more complex there is a need to further scrutinize cache operations to determine whether additional efficiency may be achieved. In this regard the present inventors have recognized that a number of clock cycles may be eliminated in the context of certain operations of cache circuits. The reduction of expended clock cycles relating to cache operations improves system speed. In addition, this reduction of clock cycles also reduces overall system power consumption, which is of critical consideration in many contemporary systems such as in portable computing devices.</p><h4>BRIEF SUMMARY OF THE INVENTION</h4><p>In one preferred embodiment, there is a method of operating a computing system. The computing system comprises a cache memory, and the cache memory has a predetermined number of cache lines. First, the method, for a plurality of write addresses, writes data to the cache memory at a location corresponding to each of the plurality of write addresses. Second, the method cleans a selected number of lines in the cache memory. For each of the selected number of lines, the cleaning step evaluates a dirty indicator corresponding to data in the line and copies data from the line to another memory if the dirty indicator indicates the data in the line is dirty. Lastly, the selected number of lines which are cleaned is less than the predetermined number of cache lines. Other circuits, systems, and methods are also disclosed and claimed.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWING</h4><p>FIG. 1 illustrates a block diagram of a wireless data platform in which the present embodiments may be implemented</p><p>FIG. 2 illustrates a block diagram of a cache architecture which may be used in the platform of FIG. <b>1</b> and in other processing devices;</p><p>FIG. 3 illustrates a block diagram of portions of the cache controller of FIG. 2 where those portions are directed to a cache cleaning methodology of the preferred embodiment;</p><p>FIG. 4 illustrates a flow chart of a first embodiment for reducing clock cycles required in connection with a cache clean occurring during a context switch of the general purpose processor of FIG. 1, where the extent of the cache clean is determined by the highest address written in the cache prior to the context switch; and</p><p>FIG. 5 illustrates a flow chart of a second embodiment for reducing clock cycles required in connection with a cache clean occurring during a context switch of the general purpose processor of FIG. 1, where the extent of the cache clean is determined by the highest address available to the operating system for writing to the cache prior to the context switch.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF THE INVENTION</h4><p>FIG. 1 illustrates a preferred embodiment of a general wireless data platform <b>10</b> into which various of the cache embodiments described in this document may be implemented, and which could be used for example in the implementation of a Smartphone or a portable computing device. Wireless data platform <b>10</b> includes a general purpose (Host) processor <b>12</b> having an instruction cache <b>12</b><i>a </i>and a data cache <b>12</b><i>b</i>, each with a corresponding instruction memory management unit (\u201cMMU\u201d) <b>12</b><i>c </i>and <b>12</b><i>d</i>, and further illustrates buffer circuitry <b>12</b><i>e </i>and lastly an operating core <b>12</b><i>f</i>, all of which communicate with a system bus SBUS. The SBUS includes data SBUS<sub>d</sub>, address SBUS<sub>a</sub>, and control SBUS<sub>c </sub>conductors. A digital signal processor (\u201cDSP\u201d) <b>14</b><i>a </i>having its own internal cache (not shown), and a peripheral interface <b>14</b><i>b</i>, are coupled to the SBUS. Although not shown, various peripheral devices may therefore be coupled to peripheral interface <b>14</b><i>b</i>, including a digital to analog converter (\u201cDAC\u201d)or a network interface. DSP <b>14</b><i>a </i>and peripheral interface <b>14</b><i>b </i>are coupled to a DMA interface <b>16</b> which is further coupled to a DMA controller <b>18</b>. DMA controller <b>18</b> is also coupled to the SBUS as well as to a video or LCD controller <b>20</b> which communicates with an LCD or video display <b>22</b>. DMA controller <b>18</b> is coupled via address <b>24</b><sub>a</sub>, data <b>24</b><sub>d</sub>, and control <b>24</b><sub>c </sub>buses to a main memory which in the preferred embodiment is a synchronous dynamic random access memory (\u201cSDRAM\u201d) <b>24</b>. Similarly, DMA controller <b>18</b> is coupled via address <b>26</b><sub>a</sub>, data <b>26</b><sub>d</sub>, and control <b>26</b><sub>c </sub>buses to a flash memory <b>26</b> (or memories).</p><p>The general operational aspects of wireless data platform <b>10</b> are appreciated in connection with the present inventive concepts by noting that it utilizes both a general purpose processor <b>12</b> and a DSP <b>14</b><i>a</i>. Thus, there are multiple cores sharing a single memory, and it will be appreciated that the inventive methodology described later provides various improvements to system performance for such a multi-core system (which may be the case also for systems other than platform <b>10</b>). In addition, note that many of the inventive aspects described below also may improve operations in a mono-processor system.</p><p>Turning the focus now to cache aspects of the preferred embodiment, FIG. 2 illustrates by way of example the architecture of data cache <b>12</b><i>b </i>of general purpose processor <b>12</b> from FIG. <b>1</b>. Before detailing this structure, it should be understood that various of the present inventive teachings may be implemented in connection with other caches such as instruction cache <b>12</b><i>a</i>, either or both of the caches of DSP <b>14</b><i>a</i>, or in yet other caches (e.g., unified cache) in platform <b>10</b>. In addition, various of the inventive teachings described below may be used in conjunction with any processing device which would benefit from a cache memory, including smartphones, PDAs, palmtop computers, notebook computers, desktop computers and so on. Lastly, although various details are presented below with respect to data cache <b>12</b><i>b</i>, it also should be noted that many of those details (e.g., set association, array sizes, address and storage lengths) are for illustrative purposes only.</p><p>Looking now to the detail of data cache <b>12</b><i>b </i>shown in FIG. 2, it includes a cache controller <b>28</b> which receives a memory address, which in this case is a portion of a 32-bit data address DA[<b>31</b>:<b>0</b>], where the portion received includes bits \u201c[<b>11</b>:<b>4</b>]\u201d indicating that cache controller <b>28</b> receives bits \u201c<b>4</b>\u201d through \u201c<b>11</b>\u201d of the 32-bit address, and further receives bits DA[<b>1</b>:<b>0</b>], where similarly the \u201c[<b>1</b>:<b>0</b>]\u201d indicates that cache controller <b>28</b> receives bits \u201c<b>0</b>\u201d through \u201c<b>1</b>\u201d of the 32-bit address. Cache controller <b>28</b> is coupled to a virtual tag array <b>30</b><sub>v</sub>, and which store tags which correspond to lines in a data array <b>32</b>. In this regard and of note for later discussion, virtual tag array <b>30</b><sub>v </sub>stores a dirty bit for each line in data array <b>32</b>, where an indication of dirty data is known in the cache art to represent that data which has been brought into data array <b>32</b> has been changed but that changed copy has not been output to a memory higher in the memory system (e.g., main memory). Also including indications corresponding to each line in data array <b>32</b> are least recently used (\u201cLRU\u201d) bits in an LRU indicator array <b>34</b> and valid bits in a valid indicator array <b>36</b>.</p><p>In the preferred embodiment, data cache <b>12</b><i>b </i>is arranged as a two-way set associative cache; therefore, tag array <b>30</b><sub>v </sub>has two planes <b>30</b><i>a</i><sub>v </sub>and <b>30</b><i>b</i><sub>v</sub>, respectively. Similarly, data array <b>32</b> has two memory planes <b>32</b><i>a </i>and <b>32</b><i>b</i>. In the illustrated embodiment each plane <b>32</b><i>a </i>and <b>32</b><i>b </i>is 1024\u00d732 bits (i.e., 4 bytes) and, therefore, requires four consecutive addresses to form a line of 16 bytes. The outputs of planes <b>30</b><i>a</i><sub>v </sub>and <b>30</b><i>b</i><sub>v </sub>are output to respective comparators <b>38</b><sub>a </sub>and <b>38</b><sub>b</sub>. DA[<b>31</b>:<b>12</b>] are also coupled to both comparators <b>38</b><sub>a </sub>and <b>38</b><sub>b</sub>. Each comparator <b>38</b><sub>a </sub>and <b>38</b><sub>b </sub>generates a one-bit output designated Hit way<b>1</b> and Hit way<b>2</b>, respectively. The Hit way<b>1</b> and Hit way<b>2</b> signals are connected to the control inputs of respective passgates <b>40</b><i>a </i>and <b>40</b><i>b</i>, each of which provides as an output the addressed 32-bit data DD[<b>31</b>:<b>0</b>].</p><p>The operation of a set associative cache is well known in the art and is summarized here for a read operation merely to present a context for details to be understood in a later discussion relating more particularly to a cache cleaning process. Looking to the read operation, when an address DA[<b>31</b>:<b>0</b>] is received for a memory access, address bits DA[<b>11</b>:<b>4</b>] are used as an address into each plane <b>30</b><i>a</i><sub>v </sub>and <b>30</b><i>b</i><sub>v </sub>of virtual tag array <b>30</b><i>a</i><sub>v</sub>. Each plane <b>30</b><i>a</i><sub>v </sub>and <b>30</b><i>b</i><sub>v </sub>outputs tag bits Tag_DA[<b>31</b>:<b>12</b>] responsive to the address, where that tag includes an indication of the address of the data stored in data array <b>32</b>. Next, bits DA[<b>31</b>:<b>12</b>] are compared via comparators <b>38</b><i>a </i>and <b>38</b><i>b </i>to the tags to determine if a match (i.e., hit) occurs, and if so the output of one of comparators <b>38</b><i>a </i>and <b>38</b><i>b </i>enables the Hit way<b>1</b> or Hit way<b>2</b> signal, respectively. During this same process, note that an index portion of the address, where the index in the current example is bits DA[<b>11</b>:<b>4</b>], is applied to data array planes <b>32</b><i>a </i>and <b>32</b><i>b</i>. Thus, both planes output information from that index and the enablement of either the Hit way<b>1</b> or Hit way<b>2</b> signal causes the output of one of those planes to be presented as the output data DD[<b>31</b>:<b>0</b>]. Of course, if a cache miss occurs (i.e., neither the Hit way<b>1</b> signal nor the Hit way<b>2</b> signal is enabled), then the addressed information is sought from a memory higher in the memory hierarchy than cache <b>12</b><i>b</i>. Lastly, recall that each memory address in the tag memory has a corresponding valid bit in array indicator <b>36</b>. These bits indicate whether the data at the corresponding location in the cache is valid. The bits in LRU array indicator <b>34</b> determine which line of planes <b>32</b><i>a </i>and <b>32</b><i>b </i>is updated after a cache miss.</p><p>Data cache <b>12</b><i>b </i>also includes a cache clean feature which can significantly improve the efficiency of the operation of the cache, as is now first detailed functionally by way of the block diagram of FIG. <b>3</b>. Specifically, FIG. 3 illustrates cache controller <b>28</b> in greater detail insofar as the cache clean feature is concerned. Cache controller <b>28</b> includes an address register <b>42</b> for storing an address value designated I_MAX and which, as appreciated later, stores certain copies of the data address index (i.e., DA[<b>11</b>:<b>4</b>]) as controlled by additional circuitry now described. The address input of address register <b>42</b> is connected to the output of a passgate <b>44</b>, which has its data input connected to receive the address index DA[<b>11</b>:<b>4</b>]. Additionally, the address index DA[<b>11</b>:<b>4</b>] is connected to the input of a comparator <b>46</b>, which further is connected to receive the value of I_MAX as stored in register <b>42</b>. For reasons detailed below, when a cache hit occurs in response to write to data cache <b>12</b><i>b</i>, comparator <b>46</b> determines whether the value of I_MAX is greater than an incoming address index DA[<b>11</b>:<b>4</b>] and, if so, enables the control input of passgate <b>44</b> so that the then incoming address index DA[<b>11</b>:<b>4</b>] is copied into register <b>42</b>, thereby updating the value of I_MAX.</p><p>Completing FIG. 3, cache controller <b>28</b> also includes a cache clean process circuit <b>48</b>, which is connected to receive as one input dirty bits as provided from virtual tag array <b>30</b><sub>v</sub>, and which is further connected to receive a CACHE_LEAN signal to enable its functionality as detailed below. Indeed, note further that the CACHE_CLEAN signal is also connected to clear the value of I_MAX in register <b>42</b>. Additionally, the value of I_MAX in register <b>42</b> is also an input to cache clean process circuit <b>48</b>. The structure of cache clean process circuit <b>48</b> may be chosen by one skilled in the art from various alternatives given its functionality as will be appreciated from the following description of FIGS. 4 and 5.</p><p>FIG. 4 illustrates a flow chart of a method designated generally at <b>50</b> and which describes the preferred operation of cache controller <b>28</b> with respect to writes of data array <b>32</b>, where such a method is accomplished in large part through the operation of the circuit blocks shown in FIG. <b>3</b>. Method <b>50</b> commences with a step <b>52</b> where the value I_MAX (in register <b>42</b>) is cleared to a value of zero. In the preferred embodiment, note that this step may be achieved by asserting the CACHE_CLEAN signal. Further in this regard, it will be appreciated by the conclusion of the discussion of method <b>50</b> that the present embodiment provides improvements in connection with operations involving context switches. Indeed, although not shown, step <b>52</b> may be part of an initialization of data cache <b>12</b><i>b</i>, as in response to a first context switch. Having noted these alternatives, it may now be helpful to examine what is meant by a context switch for the sake of some readers of this document, although such terminology is known in the art. A context switch occurs in response to various events, such as an external interrupt or the expiration of a clock timer such as is often maintained by an operating system. This switch relates to a change in process, which is appreciated in platform <b>10</b> or other processor controlled systems where operations are separated into processes. Each process is defined by various matters, and these matters often include the area of memory used by the process, the input/output mapping of the process, the memory management of the process such as address translation, and other process characterizing values typically stored in general purpose registers. A context switch occurs when the current process is changed to a new process, and thus when it is necessary to store information describing each of these aspects of the current process so that after the next process (or after several other processes) is complete, this information may be retrieved when what is now the current process is once again switched to so that it again becomes the current process.</p><p>Retuming now to method <b>50</b>, after step <b>52</b> the flow continues to step <b>54</b>. Step <b>54</b> represents that a write address is issued to the memory system which includes data cache <b>12</b><i>b</i>. Briefly looking back to FIG. 1, therefore, an example of step <b>52</b> occurs when core <b>12</b><i>f </i>issues an address to write data to SDRAM <b>24</b>, and note that SDRAM <b>24</b> is higher in a memory system which includes data cache <b>12</b><i>b </i>at a lower level. Next, method <b>50</b> continues from step <b>54</b> to step <b>56</b>.</p><p>Step <b>56</b> determines whether a hit occurs in data cache <b>12</b><i>b </i>in response to the write address issued in step <b>54</b>. If a cache hit does not occur (i.e., if a cache miss occurs), then method <b>50</b> continues from step <b>56</b> to step <b>58</b>. Conversely, if a cache hit occurs, then method <b>50</b> continues from step <b>56</b> to step <b>60</b>. Each of these alternative paths is discussed below.</p><p>Looking to the instance of step <b>58</b>, and recognizing that it occurs in response to a cache miss, it alone operates in the same manner as known in the cache art. Specifically, step <b>58</b> writes the data to the address location in a storage circuit other than data cache <b>12</b><i>b</i>. For example, in platform <b>10</b>, this write is to the appropriate address in SDRAM <b>24</b>.</p><p>Looking to the instance of step <b>60</b>, which recall is reached when a cache hit occurs, it determines whether the current address index DA[<b>11</b>:<b>4</b>] value is greater than the value of I_MAX. Returning briefly to FIG. 3, note that the operation of step <b>60</b> may be achieved by comparator <b>46</b>. If the address index DA[<b>11</b>:<b>4</b>] value is greater than the value of I_MAX, then method <b>50</b> continues from step <b>60</b> to step <b>62</b>, and if not, then method <b>50</b> skips to step <b>64</b>, described below after first considering further the operation of step <b>62</b>. Step <b>62</b>, having been reached because the address index DA[<b>11</b>:<b>4</b>] value exceeds the value of I_MAX, stores the current address index DA[<b>11</b>:<b>4</b>] as the new value of I_MAX. In this regard, note two matters. First, since the value of I_MAX was cleared in step <b>52</b>, then the first time step <b>60</b> is reached and the address indexDA[<b>11</b>:<b>4</b>] is non-zero then step <b>60</b> should pass the method flow to step <b>62</b> and the value of I_MAX is thereby increased to the current address index. Second, returning again to FIG. 3, note that step <b>62</b> is achieved by the output of comparator <b>46</b> and its control of passgate <b>44</b>. Specifically, if comparator <b>46</b>, in performing step <b>60</b>, determines that DA[<b>11</b>:<b>4</b>] exceeds the value of I_MAX, then its output enables passgate <b>44</b> so that DA[ <b>11</b>:<b>4</b>] is copied into register <b>42</b> thereby becoming the new value of I_MAX. Next, method <b>50</b> continues from step <b>62</b> to step <b>64</b>.</p><p>Step <b>64</b> writes the data at issue into data array <b>32</b> at the address specified in step <b>54</b>. Additionally, the dirty bit in virtual tag array <b>30</b><sub>v </sub>and corresponding to the written data cache line is set to a state of dirty. Next, method <b>50</b> continues from step <b>64</b> to step <b>66</b>. Step <b>66</b> represents a wait state, where method <b>50</b> awaits one of two events, those being either the issuance of another write address or a context switch. If another write address is issued, then method <b>50</b> returns from step <b>66</b> to step <b>54</b>. In that event, one skilled in the art will appreciate that the preceding steps occur again, and if the index of the newly issued write address is greater than the current value of I_MAX, then that index will become the new value of I_MAX. Indeed, this looping operation may occur for numerous successive write operations, where each time the preceding steps operate such that I_MAX may be increased. Looking then to the effect of a current context switch, method <b>50</b> continues from step <b>66</b> to step <b>68</b> and, from the preceding, it should be appreciated that the value of I_MAX at this time represents the largest value of the address index which has been written since the last context switch and prior to the current context switch.</p><p>Step <b>68</b> represents a cache cleaning process, and as appreciated later, one which may dramatically improve performance and efficiency in contrast to the prior art. Particularly, step <b>68</b> illustrates that for a loop of L, from a value of L equal to I_MAX down to a value of L equal to 0, a step <b>70</b> occurs whereby each cache line having an address equal to L is cleaned. In other words, for each instance of step <b>68</b>, L is decreased starting at I_MAX, and each time through the time that the value of L equals 0, then the flow continues to a cleaning operation in step <b>70</b> and then loops back to step <b>68</b> for the next iteration. Looking to step <b>70</b>, cleaning of a cache line is known in the art, and involves evaluating the tag (or tags) for the line to determine if any data in the line is dirty. In the present embodiment, this operation is controlled by cache clean process circuit <b>48</b> of FIG. 3 as enabled by the CACHE_CLEAN signal. The process determines if the line includes dirty data, and if so, that data (or the entire line) is written to a higher memory. To the contrary, if for a given line its dirty bit(s) indicates that the entire line is clean, then the data line corresponding to the dirty bit(s) is not written out to the higher memory.</p><p>From the above, and particularly from the effect of I_MAX and steps <b>68</b> and <b>70</b>, one skilled in the art should appreciate that after a context switch, data cache <b>12</b><i>b </i>is cleaned, but the cleaning process spans only from address 0 of the cache through the highest cache address which was written prior to the context switch (i.e., as stored in I_MAX). This methodology is perhaps better understood by way of an example. Suppose that after step <b>52</b>, there are five successive cache writes to respective index addresses 0, 2, 4, 6, and 8, and following those writes there is a context switch. At this point, therefore, the value of I_MAX equals 8, and in response to the context switch steps <b>68</b> and <b>70</b> will clean data array <b>32</b> only from address 0 through address <b>8</b>. Note now that such an operation is entirely different from the prior art. Specifically, in the prior art, in response to a context switch, the entire cache is cleaned. Thus, each cache line is evaluated to determine if its contents are dirty and, if so, those dirty contents are written out to higher memory. Given this contrast, one skilled in the art should appreciate that the present inventive embodiment is considerably more efficient. By way of illustration and returning to the example of five successive addresses, suppose that the cache includes addresses through 255. In such a case, the prior art would expend additional time, which may well be a single clock cycle per addressable line, to evaluate and clean each of addresses 9 through 255. In contrast, the preferred embodiment stops the cleaning operation at some point which is less than the entirety of the cache, where in the embodiment just described the stoppage is after completing a cleaning of the highest address written prior to the context change (where that address is 8 in the current example). Thus, the number of overall clock cycles required for the cleaning operation may be significantly reduced, and this reduction also reduces overall power consumption. Additionally, note that in environments which have frequent context switches, such as may be the case for platform <b>10</b>, the efficiencies of the preferred embodiment accumulate for each context switch. This overall efficiency is even more pronounced where there are only a few cache writes between the time of context switches.</p><p>Also from the above discussion of step <b>68</b>, note that it advances the cleaning process down to address 0. This approach is preferred because it is independent from the cache size. In any event, it is therefore assumed for such an approach that completing the cleaning operation through address 0 will cause lines having dirty data to be written out to main memory. Given this observation, however, note that two alternative approaches may be used if the data at or near address 0 is not likely to have been changed. Each of these alternatives is discussed below.</p><p>In a first alternative to the approach of I_MAX as discussed above, a second address value is established to determine the lowest address index which causes a cache hit for a given context switch, and this second value is used along with the value of I_MAX. Thus, assuming that this second address value is named I_MIN, it could initially be set to a large value (e.g., the highest address of the cache) and reduced to the lowest address index value which causes a cache hit during a given context switch. By way of example, therefore, suppose the highest index address of the cache is 255, and that there are five successive cache writes to respective index addresses 8, 16, 24, 32, and 40, and following those writes there is a context switch. In this example, initially I_MAX equals0, and I_MN equals 255. Based on the five accesses, I_MAX would be increased for each access until it equals 40. On the other hand, the first access to index address 8 would decrease the value of I_MIN to a value of 8, and the remaining accesses would not affect that value because they are higher index addresses then the updated value of I_MIN. Concluding, this alternative, step <b>60</b> would be modified so that step <b>70</b> cleans all lines between and including the address from I_MIN to the address of I_MAX, thereby cleaning the number of cache lines between the lowest changed address and the highest changed address, and thereby once again cleaning a number of lines less than the entire number of lines in the cache.</p><p>In a second alternative to the approach of I_MAX as discussed above, a different address value is established to determine the lowest address index which causes a cache hit for a given context switch, and this different value is used alone and to clean to the highest address of the cache. In simple terms, this is an operation in opposite fashion of the process of using the value of I_MAX. Assuming again that this different address value is named I_MIN, it could initially be set to a large value (e.g., the highest address of the cache) and reduced to the lowest address index value which causes a cache hit during a given context switch. However, when step <b>68</b> is implemented, it would be modified so that step <b>70</b> cleans all lines from the address value of I_MIN to the top of the cache, that is, to the highest address of the cache.</p><p>Given the limiting looping operation in view of the I_MAX value, the present inventive scope includes an alternative embodiment as shown in FIG. <b>5</b>. In this alternative approach indicated as a method <b>50</b><i>a</i>, the functionality of comparator <b>46</b> of FIG. 3 is not used but instead an address value maintained by the operating system as pertaining to a current context is used to determine the value of I_MAX at the time of the context switch. This difference is noted below with respect to step <b>72</b>, where the remaining steps shown in FIG. 5 are the same as steps shown in shown in FIG. <b>4</b>.</p><p>Turning to method <b>50</b><i>a</i>, steps <b>52</b> through <b>66</b> are not detailed here since they were discussed above. Looking therefore to step <b>72</b>, note that it sets the value of I_MAX, but here that value is set based on a value which is accessible in some implementations of operating systems. Specifically, some operating systems maintain a maximum cache line value for a given context. Thus, after step <b>66</b> is satisfied, the operating system will then have available a maximum cache lineaddress which corresponds to the context which is being completed (i.e., from which the switch is occurring). Step <b>72</b> sets the value of I_MAX equal to this maximum cache line address. Accordingly, when method <b>50</b><i>a </i>continues to step <b>68</b>, and provided that this maximum cache line address was less than the number of total lines in the cache, then the looping operation caused by step <b>68</b> and step <b>70</b> once again causes a cleaning of lines in the data cache, and once again the number of lines in the which are cleaned is less than the total number of lines in the cache.</p><p>From the above, it may be appreciated that the above embodiments reduce the number of clock cycles related to cache cleaning operations after a context switch, and provide various improvements over the prior art. In addition to the above teachings, it should also be noted that while the present embodiments have been described in detail, various substitutions, modifications or alterations could be made to the descriptions set forth above without departing from the inventive scope. For example, while in the preferred embodiment the occurrence of a context switch is what triggers the resetting of I_MAX and later limits it in response to a successive context switch, one skilled in the art may ascertain some other event or events whereby a first event occurrence resets the value of I_MAX and a second event occurrence then ends the upward adjustments of the value of I_MAX, after which again the cache is cleaned from some minimum address to the last saved value of I_MAX. As another example, while FIGS. 4 and 5 illustrate a generally sequential method via flow charts, it should be understood that various circuits may be used to implement such operation such as a state machine to perform these steps and, thus, the flow may be to alternative states from each state rather than sequential as shown in the flow diagram. As yet another example, while data cache <b>18</b><i>b </i>has been used herein to demonstrate various aspects many of the present inventive teachings apply to various other cache architectures. As yet a final example, platform <b>10</b> is only by way of illustration, and it should be understood that it may be modified further and also that numerous of the inventive aspects may be implemented in other systems having one or more cache memories. Thus, the previous description, these examples, and other matters ascertainable by one skilled in the art given the present teachings should help illustrate the inventive scope, as defined by the following claims.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "G\u00e9rard", "last_name": "Chauvel", "name": ""}, {"first_name": "Serge", "last_name": "Lasserre", "name": ""}, {"first_name": "Dominique Beno\u00eet Jacques", "last_name": "d'Inverno", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "TEXAS INSTRUMENTS INCORPORATED"}, {"first_name": "", "last_name": "TEXAS INSTRUMENTS INCORPORATED", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/08"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/0804      20160101A I20201006RMEP"}, {"label": "G06F  12/08        20060101A I20051008RMUS"}], "national_classes": [{"primary": true, "label": "711135"}, {"primary": false, "label": "711218"}, {"primary": false, "label": "711203"}, {"primary": false, "label": "711E1204"}, {"primary": false, "label": "711219"}, {"primary": false, "label": "711129"}], "ecla_classes": [{"label": "G06F  12/08B2"}], "cpc_classes": [{"label": "G06F  12/0804"}, {"label": "Y02D  10/00"}, {"label": "Y02D  10/00"}, {"label": "G06F  12/0804"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1998-04-29", "application_date": "1998-11-05", "family_members": [{"ucid": "FR-2778254-A1", "titles": [{"lang": "FR", "text": "CIRCUITS,SYSTEMES ET PROCEDES D'ORDINATEUR UTILISANT UN NETTOYAGE PARTIEL D'UNE MEMOIRE CACHE"}, {"lang": "EN", "text": "Memory cache cleaning method to reduce clock cycles in computer system with cache memories"}]}, {"ucid": "US-6321299-B1", "titles": [{"lang": "EN", "text": "Computer circuits, systems, and methods using partial cache cleaning"}]}, {"ucid": "FR-2778254-B1", "titles": [{"lang": "EN", "text": "COMPUTER CIRCUITS, SYSTEMS, AND METHODS USING PARTIAL CLEANING OF A HIDDEN MEMORY"}, {"lang": "FR", "text": "CIRCUITS,SYSTEMES ET PROCEDES D'ORDINATEUR UTILISANT UN NETTOYAGE PARTIEL D'UNE MEMOIRE CACHE"}]}]}