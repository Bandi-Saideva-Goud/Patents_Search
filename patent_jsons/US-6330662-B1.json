{"patent_number": "US-6330662-B1", "publication_id": 72861329, "family_id": 22972936, "publication_date": "2001-12-11", "titles": [{"lang": "EN", "text": "Apparatus including a fetch unit to include branch history information to increase performance of multi-cylce pipelined branch prediction structures"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72653267\"><p>An instruction fetch unit for fetching instructions from an instruction cache of a processor. The fetch unit includes a next fetch address mechanism generating predicted next fetch addresses, the next fetch address mechanism generating a next fetch address for a fetch bundle over at least two cycles of the processor. The next fetch address mechanism determines the next fetch address based on whether a control transfer instruction from an intermediate set of fetched instructions is taken.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6330662-B1-CLM-00001\" num=\"1\"><claim-text>1. An instruction fetch unit for fetching instructions from an instruction cache of a processor, comprising:</claim-text><claim-text>a next fetch address mechanism generating predicted next fetch addresses, the next fetch address mechanism generating a next fetch address for a fetch bundle over at least two cycles of the processor; or </claim-text><claim-text>the next fetch address mechanism generating a next fetch address for a fetch bundle over at least three cycles of the processor when two intervening addresses are between a present address and the next fetch address; </claim-text><claim-text>wherein said next fetch address mechanism determines said next fetch address based on whether a control transfer instruction from an intermediate set of fetched instructions is taken. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6330662-B1-CLM-00002\" num=\"2\"><claim-text>2. The instruction fetch unit of claim <b>1</b>, wherein:</claim-text><claim-text>said fetch bundle comprises at least eight instructions. </claim-text></claim>"}, {"num": 3, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6330662-B1-CLM-00003\" num=\"3\"><claim-text>3. A processor that executes coded instructions, comprising:</claim-text><claim-text>an instruction fetch unit for fetching instructions from an instruction cache of a processor, the fetch unit including a next fetch address mechanism generating predicted next fetch addresses, the next fetch address mechanism generating a next fetch address for a fetch bundle over at least two cycles of the processor, or </claim-text><claim-text>the next fetch address mechanism generating a next fetch address for a fetch bundle over at least three cycles of the processor when two intervening addresses are between a present address and the next fetch address; </claim-text><claim-text>wherein said next fetch address mechanism determines said next fetch address based on whether a control transfer instruction from an intermediate set of fetched instructions is taken; </claim-text><claim-text>an instruction scheduling unit receiving the coded instructions and issuing received instructions for execution; and </claim-text><claim-text>an instruction execution unit generating data access requests in response to the issued instructions. </claim-text></claim>"}, {"num": 4, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6330662-B1-CLM-00004\" num=\"4\"><claim-text>4. A computer system comprising:</claim-text><claim-text>a processor formed on an integrated circuit chip, said processor including an instruction fetch unit for fetching instructions from an instruction cache of a processor, the fetch unit including a next fetch address mechanism generating predicted next fetch addresses, the next fetch address mechanism generating a next fetch address for a fetch bundle over at least two cycles of the processor, or </claim-text><claim-text>the next fetch address mechanism generating a next fetch address for a fetch bundle over at least three cycles of the processor when two intervening addresses are between a present address and the next fetch address; </claim-text><claim-text>wherein said next fetch address mechanism determines said next fetch address based on whether a control transfer instruction from an intermediate set of fetched instructions is taken; </claim-text><claim-text>an instruction scheduling unit receiving the coded instructions and issuing received instructions for execution; and </claim-text><claim-text>an instruction execution unit generating data access requests in response to the issued instructions. </claim-text></claim>"}, {"num": 5, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6330662-B1-CLM-00005\" num=\"5\"><claim-text>5. An instruction fetch means for fetching instructions from an instruction cache of a processor, comprising:</claim-text><claim-text>a next fetch address means for generating predicted next fetch addresses, the next fetch address means generating a next fetch address for a fetch bundle over at least two cycles of the processor; or </claim-text><claim-text>the next fetch address means generating a next fetch address for a fetch bundle over at least three cycles of the processor when two intervening addresses are between a present address and the next fetch address; </claim-text><claim-text>wherein said next fetch address means determines said next fetch address based on whether a control transfer instruction from an intermediate set of fetched instructions is taken. </claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6330662-B1-CLM-00006\" num=\"6\"><claim-text>6. The instruction fetch means of claim <b>1</b>, wherein:</claim-text><claim-text>said fetch bundle comprises at least eight instructions. </claim-text></claim>"}, {"num": 7, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6330662-B1-CLM-00007\" num=\"7\"><claim-text>7. A processor that executes coded instructions, comprising:</claim-text><claim-text>an instruction fetch means for fetching instructions from an instruction cache of a processor, the fetch means including a next fetch address means for generating predicted next fetch addresses, the next fetch address means generating a next fetch address for a fetch bundle over at least two cycles of the processor, or </claim-text><claim-text>the next fetch address means generating a next fetch address for a fetch bundle over at least three cycles of the processor when two intervening addresses are between a present address and the next fetch address; </claim-text><claim-text>wherein said next fetch address means determines said next fetch address based on whether a control transfer instruction from an intermediate set of fetched instructions is taken; </claim-text><claim-text>an instruction scheduling means for receiving the coded instructions and issuing received instructions for execution; and </claim-text><claim-text>an instruction execution means for generating data access requests in response to the issued instructions. </claim-text></claim>"}, {"num": 8, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6330662-B1-CLM-00008\" num=\"8\"><claim-text>8. A computer system comprising:</claim-text><claim-text>a processor formed on an integrated circuit chip, said processor including instruction fetch means for fetching instructions from an instruction cache of a processor, the fetch means including a next fetch address means for generating predicted next fetch addresses, the next fetch address means generating a next fetch address for a fetch bundle over at least two cycles of the processor, or </claim-text><claim-text>the next fetch address means generating a next fetch address for the fetch bundle over at least three cycles of the processor when two intervening addresses are between a present address and the next fetch address; </claim-text><claim-text>wherein said next fetch address means determines said next fetch address based on whether a control transfer instruction from an intermediate set of fetched instructions is taken; </claim-text><claim-text>an instruction scheduling means for receiving the coded instructions and issuing received instructions for execution; and </claim-text><claim-text>an instruction execution means for generating data access requests in response to the issued instructions.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54768843\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>This invention relates in general to microprocessors, and more particularly, to branch prediction structures in a microprocessor which use branch history information to increase performance of multi-cycle branch prediction structures.</p><p>2. Relevant Background</p><p>To improve overall performance of modern processors, also called microprocessors, to execute instructions, processors use techniques including pipe lining, super scalar execution, speculative instruction execution, and out-of-order instruction issue to enable multiple instructions to be issued and executed each clock cycle. As used herein the term processor includes complex instruction set computers (CISC), reduced instruction set computers (RISC), and hybrids thereof.</p><p>Super scalar processors achieve higher performance by executing many instructions simultaneously at high frequencies. As cycle times for high performance processors decreases, the functions performed by the structures in the microprocessor need to be distributed over multiple cycles. However, information relating to the function must be obtained from the structures every cycle based on updated input per cycle. Accordingly, the coordination of data available at each cycle is critical to the proper operation of the processor. In particular, a processor which utilizes speculative instruction execution must be capable of accurately predicting the instructions to be speculatively executed so that the number of branch misdirections is reduced. In this way, the processor's performance is not adversely affected by excessive branch misdirections.</p><p>FIG. 1 illustrates, as an example, five cycles of a microprocessor (shown as cycles 1 to 5), wherein three cycles are required to obtain or \u201cfetch\u201d a set of instructions known as a \u201cfetch bundle\u201d. For each fetch bundle of instructions obtained shown in FIG. 1, as an example, one cycle is required to generate the address, and two cycles are required to access the instruction cache (I$) to obtain the actual instructions of the bundle. As shown in FIG. 1, when the address generation step A<sub>1</sub>, is complete during cycle 1, the next cycle (cycle 2) can begin generating the address for obtaining fetch bundle \u201ca\u201d. This address generation step is shown as A<sub>a</sub>. When the address generation step A<sub>a </sub>for fetch bundle \u201ca\u201d is complete at the end of cycle 2, on the next cycle (cycle 3) the address generation step for fetch bundle \u201cb\u201d can be commenced. As used herein, \u201cz\u201d, \u201ca\u201d, and \u201cb\u201d are fetch addresses corresponding to their respective fetch bundle.</p><p>The process shown in FIG. 1 assumes that the step of address generation only requires a single cycle to determine the next fetch address. In this sense, FIG. 1 is an ideal situation at reduced cycle times where at the end of cycle 1, the predictive decision as to the next fetch address is complete, and that information can be used at the beginning of cycle 2 for the address generation for fetch bundle \u201ca\u201d. At the end of cycle 2, the predictive decision as to the next fetch address for fetch bundle \u201cb\u201d is completed and that information can be used to generate the address for fetch bundle \u201cb\u201d during cycle 3.</p><p>However, because of increased frequency of operation and greater complexity of pipelines within a processor, the decision as to the next fetch address can be a complex decision, and may be split over more than one cycle. Accordingly, what is needed is a method and apparatus for increasing the accuracy of multi-cycled, pipeline branch prediction structures.</p><h4>SUMMARY OF THE INVENTION</h4><p>In light of the above, therefore, according to one broad aspect of the invention, a method and apparatus of including branch history to increase accuracy of multi-cycled, pipeline branch prediction structures is disclosed. In particular, an instruction fetch unit for fetching instructions from an instruction cache of a processor is disclosed. The fetch unit includes a next fetch address mechanism generating predicted next fetch addresses, the next fetch address mechanism generating a next fetch address for a fetch bundle over at least two cycles of the processor. The next fetch address mechanism determines the next fetch address based on whether a control transfer instruction from an intermediate set of fetched instructions is taken.</p><p>According to another broad aspect of the present invention, a processor is disclosed that executes coded instructions including an instruction scheduling unit receiving the coded instructions and issuing the received instruction for execution. An instruction execution unit generates data accesses in response to the issued instructions. The processor includes an instruction fetch unit for fetching instructions from an instruction cache of a processor. The fetch unit includes a next fetch address mechanism generating predicted next fetch addresses, the next fetch address mechanism generating a next fetch address for a fetch bundle over at least two cycles of the processor. The next fetch address mechanism determines the next fetch address based on whether a control transfer instruction from an intermediate set of fetched instructions is taken.</p><p>A computer system incorporating the features of the present invention is also disclosed.</p><p>The foregoing and other features, utilities and advantages of the invention will be apparent from the following more particular description of a preferred embodiment of the invention as illustrated in the accompanying drawings.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>FIG. 1 illustrates the address generation steps and instruction cache access steps to obtain a plurality of instruction fetch bundles over five cycles of a microprocessor.</p><p>FIG. 2 shows in block diagram form a computer system in accordance with one embodiment of the present invention.</p><p>FIG. 3 shows a processor in block diagram form in accordance with one embodiment of the present invention.</p><p>FIG. 4 illustrates in block diagram form an instruction fetch unit with branch prediction in accordance with one embodiment of the present invention.</p><p>FIG. 5 illustrates the address generation steps (2 cycles/bundle) and instruction cache access steps (2 cycles/bundle) to obtain a plurality of instruction fetch bundles over eight cycles of a microprocessor, in accordance with one embodiment of the present invention.</p><p>FIG. 6 illustrates in block diagram form an array structure for the next fetch address table (NFT) in accordance with one embodiment of the present invention.</p><p>FIG. 7A illustrates a tree structure showing the nine possible addresses which can be predicted after a two cycle address generation step.</p><p>FIG. 7B shows a tree structure of a single cycle implementation if no branch history is used.</p><p>FIG. 8 illustrates a decision tree utilized by one embodiment of the present invention in order to reduce the possibilities of branch misdirections.</p><p>FIG. 9 illustrates in block diagram form a hardware structure for implementing the decision tree of FIG. 8 in accordance with one embodiment of the present invention.</p><p>FIG. 10 illustrates an alternative decision tree utilized by one embodiment of the present invention in order to reduce the possibilities of branch misdirections.</p><p>FIG. 11 illustrates in block diagram form a hardware structure for implementing the decision tree of FIG. 10 in accordance with one embodiment of the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>As will be explained below, the present invention provides a next fetch address mechanism which determines the next fetch address based on whether a control transfer instruction from an intermediate set of fetched instructions is taken. As used herein, the term \u201ccontrol transfer instruction\u201d includes instructions which alter the sequential program order, such as branch instructions or the like, including conditional branches which are taken or not taken, unconditional branches which are always taken, and a return which is always taken. Using the structure of the present invention, the instruction cache I$ of a processor can be accessed on every cycle, and a multi-cycle next fetch address table is incorporated into a branch prediction scheme using per cycle branch prediction information.</p><p>Processor architectures can be represented as a collection of interacting functional units as shown in FIG. <b>2</b>. These functional units, discussed in greater detail below, perform the functions of fetching instructions and data from memory, decoding fetched instructions, scheduling instructions to be executed, executing the instructions, managing memory transactions, and interfacing with external circuitry and devices.</p><p>The present invention is described in terms of apparatus and methods particularly useful in a highly pipeline and super scalar processor <b>102</b> shown in block diagram form in FIG. <b>2</b> and FIG. <b>3</b>. The particular examples represent implementations that can be used to issue and execute multiple instructions per cycle and are amenable to high clock frequency operations. However, it is expressly understood that the inventive features of the present invention may be usefully embodied in a number of alternative processor architectures that will benefit from the performance features of the present invention. Accordingly, these alternative embodiments are equivalent to the particular embodiments shown and described herein.</p><p>FIG. 2 shows a typical general purpose computer system <b>100</b> incorporating a processor <b>102</b> in accordance with the present invention. Computer system <b>100</b> in accordance with the present invention comprises an address/data bus <b>101</b> for communicating information, processor <b>102</b> coupled with bus <b>101</b> through input/output (I/O) interface <b>103</b> for processing data and executing instructions, and memory system <b>104</b> coupled with bus <b>101</b> for storing information and instructions for processor <b>102</b>. Memory system <b>104</b> comprises, for example, cache memory <b>105</b> and main memory <b>107</b>. As will be described below, cache memory <b>105</b> includes one or more levels of cache memory. In a typical embodiment, processor <b>102</b>, I/O interface <b>103</b>, and some or all of cache memory <b>105</b> may be integrated in a single integrated circuit, although the specific components and integration density are a matter of design choice selected to meet the needs of a particular application.</p><p>User I/O devices <b>106</b> are coupled to bus <b>101</b> and are operative to communicate information in appropriately structured form to and from the other parts of computer <b>100</b>. User I/O devices may include a keyboard, mouse, card reader, magnetic or paper tape, magnetic disk, optical disk, or other available devices, including another computer. Mass storage device <b>117</b> is coupled to bus <b>101</b>, and may be implemented using one or more magnetic hard disks, magnetic tapes, CDROMs, large banks of random access memory, or the like. A wide variety of random access and read only memory technologies are available and are equivalent for purposes of the present invention. Mass storage <b>117</b> may include computer programs and data stored therein. Some or all of mass storage <b>117</b> may be configured to be incorporated as a part of memory system <b>104</b>.</p><p>In a typical computer system <b>100</b>, processor <b>102</b>, I/O interface <b>103</b>, memory system <b>104</b>, and mass storage device <b>117</b>, are coupled to bus <b>101</b> formed on a printed circuit board and integrated into a single housing as suggested by the dashed-line box <b>108</b>. However, the particular components chosen to be integrated into a single housing is based upon market and design choices. Accordingly, it is expressly understood that fewer or more devices may be incorporated within the housing suggested by dashed line <b>108</b>.</p><p>Display device <b>109</b> is used to display messages, data, a graphical or command line user interface, or other communications with the user. Display device <b>109</b> may be implemented, for example, by a cathode ray tube (CRT) monitor, liquid crystal display (LCD) or any available equivalent.</p><p>FIG. 3 illustrates the principle components of processor <b>102</b> in greater detail in block diagram form. It is contemplated that processor <b>102</b> may be implemented with more or fewer functional components and still benefit from the apparatus and methods of the present invention unless expressly specified herein. Also, functional units are identified using a precise nomenclature for ease of description and understanding, but other nomenclature often is used to identify equivalent functional units.</p><p>Instruction fetch unit (IFU) <b>202</b> comprises instruction fetch mechanisms and includes, among other things, an instruction cache for storing instructions, branch prediction logic, and address logic for addressing selected instructions in the instruction cache. The instruction cache (I$) is commonly referred to as a portion of the level one cache (L<b>1</b>$), with another portion of the Lc cache dedicated to data storage (D$). IFU <b>202</b> fetches one or more instructions at a time by appropriately addressing the instruction cache. The instruction cache feeds addressed instructions to instruction rename unit (IRU) <b>204</b>. Preferably, IFU <b>202</b> fetches multiple instructions each cycle, and in a specific example fetches eight instructions each cycle.</p><p>In the absence of a branch instruction, IFU <b>202</b> addresses the instruction cache sequentially. The branch prediction logic in IFU <b>202</b> handles branch instructions, including unconditional branches. An outcome tree of each branch instruction is formed using any of a variety of available branch prediction algorithms and mechanisms. More than one branch can be predicted simultaneously by supplying sufficient branch prediction resources. After the branches are predicted, the address of the predicted branch is applied to the instruction cache rather than the next sequential address.</p><p>IRU <b>204</b> comprises one or more pipeline stages that include instruction renaming and dependency checking mechanisms. The instruction renaming mechanism is operative to map register specifiers in the instructions to physical register locations and to perform register renaming to minimize dependencies. IRU <b>204</b> further comprises dependency checking mechanisms that analyze the instructions fetched by IFU <b>202</b> amongst themselves, and against those instructions installed in ISU <b>206</b>, to establish true dependencies. IRU <b>204</b> outputs renamed instructions to instruction scheduling unit (ISU) <b>206</b>.</p><p>Program code may contain complex instructions, also called \u201cmacroinstructions\u201d, from the running object code. It is desirable in many applications to break these complex instructions into a plurality of simple instructions or \u201cmacroinstructions\u201d to simplify and expedite execution. In a specific implementation, the execution units are optimized to precisely handle instructions with a limited number of dependencies using a limited number of resources (e.g., registers). Complex instructions include any instructions that require more than the limited number of resources or involve more than the limited number of dependencies. IRU <b>204</b> includes mechanisms to translate or expand complex instructions into a plurality of macroinstructions. These macroinstructions are executed more efficiently in the execution units (e.g., floating point and graphics execution unit (FGU) <b>210</b> and integer execution unit (IEU) <b>208</b>), than could the macroinstructions.</p><p>ISU <b>206</b> receives renamed instructions from IRU <b>204</b> and registers them for execution. Upon registration, instructions are deemed \u201clive instructions\u201d in a specific example. ISU <b>206</b> is operative to schedule and dispatch instructions as soon as their dependencies have been satisfied into an appropriate execution unit (e.g., integer execution unit (IEU) <b>208</b>, or floating point and graphics unit (FGU) <b>210</b>). ISU <b>206</b> also maintains trap status of live instructions. ISU <b>206</b> may perform other functions such as maintaining the correct architectural state of processor <b>102</b>, including state maintenance when out-of-order instruction issue logic is used. ISU <b>206</b> may include mechanisms to redirect execution appropriately when traps or interrupts occur and to ensure efficient execution of multiple threads where multiple threaded operation is used. Multiple thread operation means that processor <b>102</b> is running multiple substantially independent processes simultaneously. Multiple thread operation is consistent with but not required by the present invention.</p><p>ISU <b>206</b> also operates to retire executed instructions when completed by IEU <b>208</b> and FGU <b>210</b>. ISU <b>206</b> performs the appropriate updates to architectural register files and condition code registers upon complete execution of an instruction. ISU <b>206</b> is responsive to exception conditions and discards or flushes operations being performed on instructions subsequent to an instruction generating an exception in the program order. ISU <b>206</b> quickly removes instructions from a misprinted branch path and initiates IFU <b>202</b> to fetch from the correct branch address. An instruction is retired when it has finished execution and all older instructions have retired. Upon retirement the instruction's result is written into the appropriate register file and it is no longer deemed a \u201clive instruction\u201d.</p><p>IEU <b>208</b> includes one or more pipelines, each pipeline comprising one or more stages that implement integer instructions. IEU <b>208</b> also includes mechanisms for holding the results and state of speculatively executed integer instructions. IEU <b>208</b> functions to perform final decoding of integer instructions before they are executed on the execution units and to determine operand bypassing amongst instructions concurrently in execution on the processor pipelines. IEU <b>208</b> executes all integer instructions including determining correct virtual addresses for load/store instructions. IEU <b>208</b> also maintains correct architectural register state for a plurality of integer registers in processor <b>102</b>. IEU <b>208</b> preferably includes mechanisms to access single and/or double precision architectural registers as well as single and/or double precision rename registers.</p><p>FGU <b>210</b> includes one or more pipelines, each comprising one or more stages that implement floating point instructions. FGU <b>210</b> also includes mechanisms for holding the results and state of speculatively executed floating point and graphics instructions. FGU <b>210</b> functions to perform final decoding of floating point instructions before they are executed on the execution units and to determine operand bypassing amongst instructions concurrently in execution on the processor pipelines. In the specific example, FGU <b>210</b> includes one or more pipelines dedicated to implementing special purpose multimedia and graphics instructions that are extensions to standard architectural instructions for a processor. FGU <b>210</b> may be equivalently substituted with a floating point unit (FPU) in designs in which special purpose graphics and multimedia instructions are not used. FGU <b>210</b> preferably includes mechanisms to access single and/or double precision architectural registers as well as single and/or double precision rename registers.</p><p>A data cache memory unit (DCU) <b>212</b> shown in FIG. 3, including cache memory <b>105</b> shown in FIG. 2, functions to buffer memory reads from off-chip memory through external interface unit (EIU) <b>214</b>. Optionally, DCU <b>212</b> also buffers memory write transactions. DCU <b>212</b> can comprise, in one example, two hierarchical levels of cache memory on-chip (L<b>1</b>$ and L<b>2</b>$) and a third cache level (L<b>3</b>$) accessible through EIU <b>214</b>.</p><p>FIG. 4 illustrates the instruction fetch unit (IFU) <b>202</b> in greater detail. IFU <b>202</b> comprises instruction fetch mechanisms and includes, among other things, an instruction cache I$ for storing instructions, branch prediction logic <b>230</b>, a branch prediction table <b>232</b>, a next fetch address table (NFAT or NFT) <b>234</b>, and logic for addressing selected instructions in the instruction cache I$.</p><p>IFU <b>202</b> fetches one or more instructions each clock cycle by appropriately addressing the instruction cache I$ via MUX <b>240</b> and MUX <b>242</b> under the control of branch logic <b>230</b> shown in FIG. <b>4</b>. In the absence of a conditional branch instruction, IFU <b>202</b> addresses the instruction cache I$ sequentially. Fetched instructions are passed to IRU <b>204</b> shown in FIG. 3. A fetch bundle may include multiple control-flow (i.e., conditional or unconditional branch) instructions. Hence, IFU <b>202</b> desirably bases the next fetch address decision upon the simultaneously predicted outcomes of multiple branch instructions, as will be described below.</p><p>The branch prediction logic <b>230</b> and table <b>232</b> handle branch instructions, including unconditional branch instructions. In one example, an outcome for each branch instruction is predicted using a variety of available branch prediction algorithms and mechanisms, along with the inventive techniques disclosed herein. In the example shown in FIG. 4, an exclusive-OR operation is performed on the current fetch address (cfa) And a value from a selected branch history register (BHR) to generate an index to the branch prediction table <b>232</b>. The BHR comprises information about the outcomes of a pre-selected number of most-recently executed conditional and unconditional branch instructions, and an outcome can be represented as taken or not taken.</p><p>In accordance with the present invention and as will be described below with reference to FIG. 5, the next fetch address table (NFT or NFAT) <b>234</b> uses two-cycles to generate the next fetch address. The branch prediction table <b>232</b> provides updated information every cycle that will be used to select information from the NFT <b>234</b> in every cycle of the two cycle access. Consequently, an instruction cache access can occur every cycle based on an address obtained from the NFT <b>234</b>.</p><p>When a conditional branch instruction is predicted, the predicted outcome is used to speculatively update the appropriate BHR so that the outcome will be part of the information used by the next access to BPT <b>232</b>. When a branch is misprinted, however, the appropriate BHR must be repaired by transferring the BHR value from a branch repair table BRT <b>244</b>, along with the actual outcome of the misprinted branch loaded into the BHR.</p><p>The NFT <b>234</b> determines the next fetch address based upon the current fetch address (cfa) Received from the output of MUX <b>246</b>. For example, the NFT may comprise 2048 entries, each of which comprises two multi-bit values corresponding to a predicted address for instructions in two-halves of the current fetch bundle, shown as nfa0 and nfa1 in FIG. <b>4</b> and in FIG. <b>6</b>. It is understood that, depending upon the particular implementation, more next fetch addresses beyond nfa1, nfa0 could be used if desired.</p><p>As shown in FIG. 6, each row of the NFT corresponds to a single fetch bundle, and each column corresponds to four instructions (shown as nfa1 and nfa0). The nfa1, nfa0in FIG. 6 are each mapped to any branch instruction within a set of four sequentially located instructions. It is understood that this mapping is a matter of choice depending upon cost and performance and can be altered with different levels of granularity as desired. FIG. 6 also shows that the NFT <b>234</b> can have decoding logic for decoding the current fetch address to point to the proper nfa1/nfa0 pair in the NFT. In one example, the multi-bit values nfa1/nfa0 can include set prediction for the next fetch, along with an index to the instruction cache I$.</p><p>In accordance with the present invention, novel structures for the NFT <b>234</b> are shown in FIGS. 9 and 11 and are explained below in greater detail.</p><p>The branch repair table (BRT) <b>244</b> comprises entries or slots for a number of unresolved branch instructions, in one example. The BRT <b>244</b> determines when a branch instruction is misprinted based upon information from IEU <b>208</b>, for example. BRT <b>244</b>, operating through the branch logic, redirects IFU <b>202</b> down the correct branch path.</p><p>Once a branch is resolved, the address of the path this branch actually follows is communicated from IEU <b>208</b> and compared against the predicted path address. If these two addresses differ, those instructions down the misprinted path are flushed from the processor and the IFU <b>202</b> redirects the instruction fetch from the instruction cache I$ down the correct path using the BRT input into the MUX <b>246</b>.</p><p>In one example, a return address stack (RAS) <b>248</b> can be used to contain the return addresses of the eight most recently executed branch and control transfer instructions, and whenever a \u201ccall\u201d, \u201clink\u201d, \u201cjump\u201d or similar instruction is executed, the program counter can be pushed onto the RAS. When a subsequent \u201creturn\u201d instruction is executed, the program counter value on the top of the RAS is popped, and the IFU <b>202</b> begins to fetch the instructions at this address.</p><p>In accordance with the present invention, the novel structures and methods for the NFT <b>234</b> will now be described. As discussed above, the NFT stores next fetch addresses which could be branch targets or sequential addresses, and supplies the appropriate predicted branch addresses for use by the address generation portion of the processor. In one example and in accordance with the present invention, the NFT will have a latency of two cycles, such that the decision as to the next fetch address is split over two cycles. FIG. 5 illustrates the coordination of addresses and instruction cache access steps where the address generation step occurs over two cycles, and the instruction cache access step also occurs over two cycles. In accordance with short cycle time and high frequency pipeline operation of a processor of the present invention, a step of generating an instruction address is commenced every cycle.</p><p>Referring to cycle 2, the address generation step for fetch bundle \u201cz\u201d is spread over cycles 2 and 3 and shown as steps A<sub>0z</sub>, and A<sub>1z</sub>. At the beginning of cycle 3, the address generation step for fetch bundle \u201ca\u201d commences through cycles 3 and 4 as shown by steps A<sub>oaf </sub>and A<sub>1a</sub>. Cycles 4 and 5 show the address generation steps for fetch bundle \u201cb\u201d, shown as A<sub>0b </sub>and A<sub>1b</sub>.</p><p>When cycle 3 is complete, the fetch address for fetch bundle \u201cz\u201d has been calculated and fetch bundle \u201cz\u201d is obtained from the instruction cache over cycles 4 and 5. However, when cycle 3 is complete, the address generation steps for fetch bundle \u201ca\u201d have already commenced, since the processor utilizes pipe lining to achieve a throughput of one fetch bundle per cycle. Similarly, when cycle 4 is complete and the fetch address for fetch bundle \u201ca\u201d has been calculated, the address generation steps for fetch bundle \u201cb\u201d have already commenced.</p><p>Accordingly, if fetch bundle \u201ca\u201d introduced an instruction resulting in a taken branch, then the predicted and generated addresses for the instructions in fetch bundle \u201cb\u201d beginning at cycle 4 may be inaccurate. This is because under ideal circumstances \u201cz\u201d, not \u201cx\u201d, should be used to predict \u201ca\u201d. Likewise, if fetch bundle \u201cb\u201d introduced an instruction resulting in a taken branch, then the predicted and generated addresses for the instructions in fetch bundle \u201cc\u201d beginning at cycle 5 may be inaccurate.</p><p>The NFT stores next fetch addresses (nfa) associated with a cache line. In one example, two branch addresses are stored, and referred to herein as nfa1 and nfa0. Over a single cycle, three addresses are required: a sequential address (representing the address if no branch is taken), nfa1 and nfa0. It follows that over two cycles, nine addresses would be required. Over three cycles, 27 addresses would be required to be stored for a specific index into the NFT.</p><p>Stated differently, because an address generation/prediction completed every two cycles as shown in FIG. 5, there are a greater number of possible next fetch addresses which correspond to a particular current fetch address. This effect is shown in FIG. 7A, where a tree structure shows the nine possible addresses which can be predicted after a two cycle address generation step. The current fetch address is shown as \u201cz\u201d. Although the NFT completes the address generation process every 2 cycles, the NFT does generate addresses every cycle in a pipeline manner. For each address presented to the NFT, the NFT generated nfa1 (next fetch address 1), nfa0 (next fetch address 0), and sequential address (branch not taken). As can be seen in FIG. 7A, after two cycles, there are nine possible predicted addresses which correspond to the current fetch address \u201cz\u201d. Accordingly, after a two cycle address generation step, because of aliasing there are a greater number of possible predicted addresses to choose from, which may increase the likelihood that a branch misdirection may occur.</p><p>FIG. 7B shows a tree structure if the NFT is implemented without branch history. Fetch address \u201cb\u201d would have to be stored in nfa1, nfa0. So from FIG. 7A, there are three nfa1's from FIG. 7A which can alias to nfa1 in FIG. <b>7</b>B. There are also three nfa0's from FIG. 7A which can alias to nfa0 in FIG. <b>7</b>B. Depending on which is chosen, there are three sequential addresses from FIG. 7A which alias to either of nfa1, nfa0 of FIG. <b>7</b>B. Accordingly, this aliasing reduces the accuracy of prediction.</p><p>FIGS. 8-11 illustrate embodiments of the present invention which can be implemented in NFT to address this problem.</p><p>In a preferred embodiment, shown in FIGS. 8-11, the present invention utilizes branch history information (i.e. Whether a branch was taken or not taken) in order to help determine and predict the next fetch addresses. In particular, the branch history information is derived from intermediate fetch bundles to help predict the next fetch addresses.</p><p>FIG. 8 illustrates a preferred embodiment of the present invention, wherein a decision tree is utilized to reduce the degree of aliasing in a processor with a multi-cycle NFT. As shown, from a current fetch address for an instruction \u201cz\u201d, information as to whether a branch/control transfer instruction from an intermediate fetch bundle \u201ca\u201d was, or was not, taken is used to determine the next fetch addresses for fetch bundle \u201cb\u201d. These fetch addresses for fetch bundle \u201cb\u201d will be selected depending upon whether the CTI (control transfer instruction) was present in NFA1 or NFA0 of the fetch bundle \u201ca\u201d.</p><p>As shown in FIG. 8, starting with an instruction \u201cz\u201d, if a branch/CTI from bundle \u201cz\u201d was taken, then the predicted next fetch addresses are nfa1 and nfa0, shown as <b>300</b> and <b>302</b>. However, if a branch/control transfer instruction in fetch bundle \u201ca\u201d was not taken, then the predicted next fetch addresses are nfa1 and nfa0, shown as <b>304</b> and <b>306</b>. Therefore, instead of nine possible next fetch addresses (as previously shown in FIG. <b>7</b>A), four next fetch addresses <b>300</b>, <b>302</b>, <b>304</b>, and <b>306</b> are provided in accordance with the present invention. Accordingly, the amount of aliasing is reduced.</p><p>In particular, the selection process is as follows. The nfa1 <b>300</b> is selected if \u201ca\u201d was taken and \u201cb\u201d is in the upper half of fetch bundle \u201ca\u201d. The nfa0 <b>302</b> is selected if \u201ca\u201d was taken and \u201cb\u201d is in the lower half of fetch bundle \u201ca\u201d. The nfa1 <b>304</b> is selected if \u201ca\u201d was not taken and \u201cb\u201d is in the upper half of fetch bundle \u201ca\u201d. The nfa0 <b>306</b> is selected if \u201ca\u201d was not taken and \u201cb\u201d is in the lower half of fetch bundle \u201ca\u201d.</p><p>Aliasing is reduced as shown by comparing FIG. 8 to FIG. <b>7</b>A. When \u201ca\u201d was taken, two nfa1's of FIG. 7A map to one nfa1 of FIG. 8; two nfa0's of FIG. 7A map to one nfa0 of FIG. 8; and two sequential of FIG. 7A map to either of nfa1 or nfa0 of FIG. <b>8</b>. When \u201ca\u201d was not taken, one nfa1 of FIG. 7A maps to one nfa1 of FIG. 8; one nfa0 maps to one nfa0, and one sequential of FIG. 7A maps to either nfa1 or nfa0 of FIG. <b>8</b>. This particular mapping is logically dictated by the fact that there is always a (not taken)/(taken) path for the intermediate fetch address.</p><p>Therefore, an improvement in performance of the processor can be achieved by reducing aliasing from all possible next fetch addresses after two cycles of address generation, thereby reducing the probability of taking a misprinted branch. In this way, by using branch history information (i.e., whether or not a branch from an intermediate fetch bundle was taken or not taken) the decision-making for prediction is improved.</p><p>FIG. 9 illustrates one embodiment of an architecture for an NFT in accordance with the present invention to implement the decision tree shown in FIG. <b>8</b>. In FIG. 9, two tables or data arrays <b>400</b>, <b>402</b> containing taken and not taken information are provided. Each table <b>400</b>,<b>402</b> stores nfa1 and nfa0 corresponding to the output of address generation for the current fetch address of instruction \u201cz\u201d. Multiplexers or selectors <b>404</b>, <b>406</b> are used with a select line coupled to the branch history information as to whether a branch instruction in the intermediate fetch bundle \u201cz\u201d (which supplies \u201ca\u201d) was taken or not. In this manner, nfa1 and nfa0 can be selected using the branch history information as described above.</p><p>While the invention has been shown with respect to an NFT, the invention could be applied to a branch prediction table BPT (FIG. 4) or to any multi-cycle structure which stores or has access to information regarding the branches taken or not taken. Further, the invention could be also utilized with a system where three cycle address generation is used, where there are two intervening addresses between the present address and the predicted address. In other words, the invention could be stretched over multiple cycles and incorporated in a processor where address generation occurs over more than two cycles.</p><p>FIG. 10 illustrates an alternative decision tree utilized by one embodiment of the present invention in order to reduce the possibilities of branch misdirections. Branch history information (i.e., whether or not a branch was taken or not taken) is used in order to help determine and predict the next fetch addresses. In particular, the branch history information is derived from whether a branch/control transfer instruction from an intermediate fetch bundle was taken, either from the upper half or lower half of the bundle, or no branch/control transfer instructions from an intermediate fetch bundle were taken (i.e., sequential operation).</p><p>As shown in FIG. 10, from a current fetch address for an instruction \u201cz\u201d, information as to whether a branch/control transfer instruction from the upper half or lower half of an intermediate fetch bundle \u201ca\u201d was, or was not, taken is used to determine the next fetch addresses for fetch bundle \u201cb\u201d. Starting with an instruction \u201cz\u201d, if a branch/control transfer instruction from the lower half of bundle \u201ca\u201d was taken, then the predicted next fetch addresses are nfa1 and nfa0, shown as <b>500</b> and <b>502</b>. If a branch/control transfer instruction from the upper half of bundle \u201ca\u201d was taken, then the predicted next fetch addresses are nfa1 and nfa0, shown as <b>504</b> and <b>506</b>.</p><p>However, if no branch/control transfer instructions in fetch bundle \u201ca\u201d were taken, then the predicted next fetch addresses are nfa1 and nfa0, shown as <b>508</b> and <b>510</b>. Therefore, instead of nine possible next fetch addresses (as previously shown in FIG. <b>7</b>A), six next fetch addresses <b>500</b>, <b>502</b>, <b>504</b>, <b>506</b>, <b>508</b>, and <b>510</b> are provided in accordance with the present invention, which reduces aliasing.</p><p>In particular, the process to select is as follows. nfa0 <b>500</b> or nfa1 <b>502</b> is selected if \u201ca\u201d is taken and \u201ca\u201d is in the upper half of fetch bundle \u201cz\u201d. nfa0 <b>500</b> is chosen if \u201cb\u201d is in the lower half of fetch bundle \u201ca\u201d. nfa1 <b>502</b> is chosen if \u201cb\u201d is in the upper half of fetch bundle \u201ca\u201d. nfa0 <b>500</b> is written with an updated target address if \u201ca\u201d was in the lower half of fetch bundle \u201cz\u201d, and if \u201cb\u201d was in the lower half of fetch bundle \u201ca\u201d. The process can be similarly extended to the other portions of the decision tree of FIG. <b>10</b>.</p><p>Therefore, an improvement in performance of the processor can be achieved by reducing aliasing from all possible next fetch addresses after two cycles of address generation, thereby reducing the probability of taking a misprinted branch. In this way, by using branch history information (i.e. Whether or not a branch from an intermediate fetch bundle was taken or not taken) the decision-making for prediction is improved.</p><p>FIG. 11 illustrates an embodiment of an architecture for an NFT in accordance with the present invention to implement the decision tree shown in FIG. <b>10</b>. In FIG. 11, three tables or data arrays <b>600</b>, <b>602</b>, and <b>604</b> are provided, each table <b>600</b>, <b>602</b>, and <b>604</b> storing nfa1 and nfa0 addresses corresponding to the output of address generation for the current fetch address of instruction \u201cz\u201d. The table <b>600</b> stores nfa1/nfa0 addresses where a branch/control transfer instruction from the lower half of bundle \u201ca\u201d was taken; table <b>602</b> stores nfa1/nfa0 addresses where a branch/control transfer instruction from the upper half of bundle \u201ca\u201d was taken; and table <b>604</b> stores nfa1 /nfa0 addresses where no branch/control transfer instructions from bundle \u201ca\u201d were taken.</p><p>A multiplexer or selector <b>606</b> is used with select lines coupled to the branch history information as to whether a branch instruction from the upper or lower half in the intermediate fetch bundle \u201ca\u201d was taken or not. In this manner, nfa1 and nfa0 can be selected using the branch history information.</p><p>While the method disclosed herein has been described and shown with reference to particular steps performed in a particular order, it will be understood that these steps may be combined, sub-divided, or re-ordered to form an equivalent method without departing from the teachings of the present invention. Accordingly, unless specifically indicated herein, the order and grouping of the steps is not a limitation of the present invention.</p><p>While the invention has been particularly shown and described with reference to a preferred embodiment thereof, it will be understood by those skilled in the art that various other changes in the form and details may be made without departing from the spirit and scope of the invention. For instance, while the present invention has been described with reference to a processor architecture shown in FIG. 2, it will be understood that the present invention could be used in other equivalent processor designs.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Sanjay", "last_name": "Patel", "name": ""}, {"first_name": "Adam", "last_name": "Talcott", "name": ""}, {"first_name": "Rajasekhar", "last_name": "Cherabuddi", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "SUN MICROSYSTEMS, INC."}, {"first_name": "", "last_name": "Oracle America, Inc.", "name": ""}, {"first_name": "", "last_name": "SUN MICROSYSTEMS, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F   9/30"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/38        20060101A I20051008RMEP"}, {"label": "G06F   9/32        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "712236"}, {"primary": false, "label": "712234"}, {"primary": false, "label": "712E09077"}, {"primary": false, "label": "712E09051"}, {"primary": false, "label": "712E09056"}], "ecla_classes": [{"label": "G06F   9/38E2T"}, {"label": "G06F   9/38B2"}, {"label": "G06F   9/30A3C"}], "cpc_classes": [{"label": "G06F   9/30058"}, {"label": "G06F   9/3804"}, {"label": "G06F   9/3848"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1999-02-23", "application_date": "1999-02-23", "family_members": [{"ucid": "US-6330662-B1", "titles": [{"lang": "EN", "text": "Apparatus including a fetch unit to include branch history information to increase performance of multi-cylce pipelined branch prediction structures"}]}]}