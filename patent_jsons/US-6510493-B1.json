{"patent_number": "US-6510493-B1", "publication_id": 73278388, "family_id": 23391982, "publication_date": "2003-01-21", "titles": [{"lang": "EN", "text": "Method and apparatus for managing cache line replacement within a computer system"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50437945\"><p>A cache memory having a mechanism for managing cache lines replacement is disclosed. The cache memory comprises multiple cache lines partitioned into a first group and a second group. The number of cache lines in the second group is preferably larger than the number of cache lines in the first group. A replacement logic block selectively chooses a cache line from one of the two groups of cache lines for replacement during an allocation cycle.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6510493-B1-CLM-00001\" num=\"1\"><claim-text>1. A cache memory, comprising:</claim-text><claim-text>a plurality of cache lines partitioned into a first group and a second group, wherein said second group contains more cache lines than said first group; and </claim-text><claim-text>a replacement logic block for selectively choosing a cache line from one of said two groups of cache lines for replacement during an allocation cycle, wherein said replacement logic block includes a first replacement logic means for replacing cache lines from said first group and a second replacement logic means for replacing cache lines from said second group. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6510493-B1-CLM-00002\" num=\"2\"><claim-text>2. The cache memory according to <claim-ref idref=\"US-6510493-B1-CLM-00001\">claim 1</claim-ref>, wherein said replacement logic block includes an allocation input and a selection input.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6510493-B1-CLM-00003\" num=\"3\"><claim-text>3. The cache memory according to <claim-ref idref=\"US-6510493-B1-CLM-00002\">claim 2</claim-ref>, wherein an assertion of said selection input is controlled by a lifetime requirement of an instruction/data within a software application.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6510493-B1-CLM-00004\" num=\"4\"><claim-text>4. The cache memory according to <claim-ref idref=\"US-6510493-B1-CLM-00001\">claim 1</claim-ref>, wherein said first replacement logic means and said second replacement logic means utilize an identical cache line replacement algorithm.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6510493-B1-CLM-00005\" num=\"5\"><claim-text>5. The cache memory according to <claim-ref idref=\"US-6510493-B1-CLM-00001\">claim 1</claim-ref>, wherein said first replacement logic means and said second replacement logic means utilize different cache line replacement algorithms.</claim-text></claim>"}, {"num": 6, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6510493-B1-CLM-00006\" num=\"6\"><claim-text>6. A method for managing cache line replacement within a cache memory, said method comprising the steps of:</claim-text><claim-text>partitioning a plurality of cache lines into a first group and a second group, wherein said second group contains more cache lines than said first group; </claim-text><claim-text>providing a first cache line replacement algorithm for replacing cache lines from said first group and a second cache line replacement algorithm for replacing cache lines from said second group; and </claim-text><claim-text>selectively choosing a cache line from one of said two groups of cache lines for replacement during an allocation cycle. </claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6510493-B1-CLM-00007\" num=\"7\"><claim-text>7. The method according to <claim-ref idref=\"US-6510493-B1-CLM-00006\">claim 6</claim-ref>, wherein said choosing step further includes a step of choosing a cache line from one of said two groups of cache lines for replacement in accordance with a lifetime requirement of an instruction/data within a software application.</claim-text></claim>"}, {"num": 8, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6510493-B1-CLM-00008\" num=\"8\"><claim-text>8. The method for according to <claim-ref idref=\"US-6510493-B1-CLM-00006\">claim 6</claim-ref>, wherein said first cache line replacement algorithm is identical to said second cache line replacement algorithm.</claim-text></claim>"}, {"num": 9, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6510493-B1-CLM-00009\" num=\"9\"><claim-text>9. The cache memory according to <claim-ref idref=\"US-6510493-B1-CLM-00006\">claim 6</claim-ref>, wherein said first cache line</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53863715\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Technical Field</p><p>The present invention relates to data storage in general and, in particular, to a cache memory for storing data within a computer system. Still more particularly, the present invention relates to a mechanism for managing cache line replacement within a computer system.</p><p>2. Description of the Prior Art</p><p>Many high-performance data processing systems include both a system memory and a cache memory. A cache memory is a relatively high-speed memory that stores a copy of information that is also stored in one or more portions of the system memory. A cache line typically includes a directory for storing address tags, and a cache entry array for storing instructions or data. A compare match of an incoming address with one of the tags within the directory indicates a cache \u201chit;\u201d otherwise, it is considered as a cache \u201cmiss.\u201d</p><p>Typically, if there is a cache miss when all the cache lines within the cache memory are filled, then one of the cache lines within the cache memory must be selected for replacement. There are different cache replacement algorithms that are well-known to those skilled in the relevant art, none of which can provide optimal results on all software applications. For example, a least-recently used (LRU) replacement algorithm removes data from a cache memory if the re-use latency of the data is relatively long. However, if the data is required between groups of data stream that completely fill the cache memory, the data will be flushed from the cache memory, forcing the data to be reloaded back to the cache memory again after each large volume of data stream. Consequently, it is desirable to provide an improved mechanism for managing cache line replacement within a computer system such that optimal results can be achieved for a larger group of software applications.</p><h4>SUMMARY OF THE INVENTION</h4><p>In accordance with a preferred embodiment of the present invention, a cache memory comprises multiple cache lines that are partitioned into a first group and a second group. The number of cache lines in the second group is preferably larger than the number of cache lines in the first group. A replacement logic block selectively chooses a cache line from one of the two groups of cache lines for replacement during an allocation cycle.</p><p>All objects, features, and advantages of the present invention will become apparent in the following detailed written description.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The invention itself, as well as a preferred mode of use, further objects, and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein:</p><p>FIG. 1 is a block diagram of a data processing system having a multi-level cache memory hierarchy in which the present invention may be incorporated;</p><p>FIG. 2 is a block diagram of a cache memory in accordance with the present invention;</p><p>FIG. 3 is a detailed logical block diagram of cache memory <b>30</b> from FIG. 2, in accordance with a preferred embodiment of the present invention; and</p><p>FIG. 4 is a high-level flow diagram of an exemplary application of a preferred embodiment of the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF A PREFERRED EMBODIMENT</h4><p>The present invention may be implemented in any data processing system having a cache memory. Also, it should be understood that the features of the present invention may be applicable in various data processing systems having a multi-level cache memory hierarchy.</p><p>Referring now to the drawings and in particular to FIG. 1, there is illustrated a block diagram of a data processing system <b>10</b> having a multi-level cache memory hierarchy in which the present invention may be incorporated. Processor <b>23</b> is constructed as a monolithic integrated circuit device comprising a processing unit <b>11</b>, an on-chip instruction cache <b>12</b>, and an on-chip data cache <b>13</b>. Both caches <b>12</b>, <b>13</b> are primary caches and are connected to processing unit <b>11</b> by paths within a local bus structure. As shown, instruction cache <b>12</b> is coupled to processing unit <b>11</b> via a local bus <b>15</b> and a cache controller <b>17</b>, while data cache <b>13</b> is coupled to processing unit <b>11</b> via local bus <b>15</b> and a cache controller <b>18</b>.</p><p>A secondary cache <b>14</b> is coupled to processing unit <b>11</b> via local bus <b>15</b> and a cache controller <b>19</b>. Secondary cache <b>14</b> is typically much larger than either instruction cache <b>12</b> or data cache <b>13</b>, and access to secondary cache <b>14</b> is somewhat slower than to either instruction cache <b>12</b> or data cache <b>13</b>. Processing unit <b>11</b> is connected to a system bus <b>20</b> via a bus interface <b>16</b> in which timing and control translations between local bus <b>15</b> and system bus <b>20</b> take place. A main memory <b>21</b> and a disk storage device <b>22</b> are also coupled to system bus <b>20</b>.</p><p>As depicted in FIG. 1, the memory hierarchy is organized from the fastest to the slowest, from the smallest to the largest, and from the most expensive per bit to the least expensive per bit, in progression from primary caches <b>12</b>, <b>13</b> to secondary cache <b>14</b>, to main memory <b>21</b>, and to disk storage device <b>22</b>. Typically, main memory <b>21</b> contains a subset of what is in disk storage device <b>22</b>, and secondary cache <b>14</b> contains a subset of what is in main memory <b>21</b>.</p><p>Depending on the relationship between primary caches <b>13</b>, <b>14</b> and secondary cache <b>16</b>, each of primary caches <b>13</b>, <b>14</b> may contain information independent of what is in secondary cache <b>14</b> (i.e., primary caches may not enforce strong inclusivity).</p><p>Processing unit <b>11</b> can access primary caches <b>12</b>, <b>13</b> within a single processor cycle, while it may take several processor cycles to access secondary cache <b>14</b>. If a cache \u201cmiss\u201d occurs in primary caches <b>12</b>, <b>13</b> and secondary cache <b>14</b>, then main memory <b>21</b> is accessed to perform a cache allocation (or linefill operation), which entails replacing a cache line with a group of data from main memory <b>21</b> that contains the information requested by processing unit <b>11</b>. This cache allocation must be performed in order to satisfy the attempted cache access for which a cache \u201cmiss\u201d occurred. If main memory <b>21</b> does not contain the information for which the cache allocation is attempted, then a page containing this data is obtained from disk storage device <b>22</b> such that the cache allocation can be completed.</p><p>As mentioned previously, there is not a single cache replacement algorithm that can provide optimal results for all application software. This is because each application software has a different data demand requirement from the others. Thus, optimal results can only be achieved if application software can inform the cache memory as to what information should be replaced quickly, and what information should be replaced slowly. This lifetime requirement of an instruction within a software application can be included with page information in a translation lookaside buffer (TLB), or with compiler hints within the instruction.</p><p>In light of the above, the present invention provides an improved method of managing the replacement of cache entries, which allows certain instruction or data to have a longer temporal locality than others at any time, while still providing a gradual fair replacement of the cache lines within the entire cache memory.</p><p>With reference now to FIG. 2, there is depicted a block diagram of a cache memory in accordance with the present invention. This cache memory is preferably a primary cache memory such as instruction cache <b>12</b> and data cache <b>13</b> from FIG. <b>1</b>. As shown, a cache memory <b>30</b> includes a directory <b>31</b> for storing address tags and a cache entry array <b>32</b> for storing instructions (or data). Directory <b>31</b> also contains state bits and inclusivity bits (not shown) as they are known to those skilled in the art. Thus, each cache line within cache memory <b>30</b> includes an address tag and its associated data. Furthermore, cache memory <b>30</b> is divided into two partitions, namely, a first partition <b>33</b> and a second partition <b>34</b>. It is preferred to have an unequal number of cache lines between first partition <b>33</b> and second partition <b>34</b>. In the present embodiment, first partition <b>33</b> contains eight cache lines and second partition <b>34</b> contains <b>128</b> cache lines, though any number of cache lines is acceptable in each partition.</p><p>Cache memory <b>30</b> is preferably a content-addressable memory (CAM). As shown, a CAM control block <b>38</b> is associated with first partition <b>33</b>, and a CAM control block <b>39</b> is associated with second partition <b>34</b>.</p><p>In addition to directory <b>31</b> and cache entry array <b>32</b>, a replacement logic block <b>35</b> provides the necessary logic to allocate a cache line within cache memory <b>30</b>. Replacement logic block <b>35</b> is controlled by an allocation input <b>36</b> and a selection input <b>37</b>. Preferably, allocation input <b>36</b> is inactive during a normal cache access cycle (such as a read or write cycle), and is active when a cache line needs to be replaced. Selection input <b>37</b> is a software controllable signal input through which a cache line from one of the two partitions can be selected for allocation. For example, during an allocation cycle (i.e., when allocation input <b>36</b> is active), a cache line from first partition <b>33</b> is selected for allocation when selection input <b>37</b> is asserted, and a cache line from second partition <b>34</b> is selected for allocation when selection input <b>37</b> is not asserted.</p><p>In the present embodiment, any instruction (or data) that require a longer temporal locality will preferably be stored in first partition <b>33</b> while other data will be stored in second partition <b>34</b>. The cache replacement algorithm utilized in first partition <b>33</b> can be identical or different from the cache replacement algorithm utilized in second partition <b>34</b>. For example, first partition <b>33</b> and second partition <b>34</b> can both utilize a least-recently used (LRU) replacement algorithm; or as another example, first partition <b>33</b> utilizes the LRU replacement while second partition <b>34</b> utilizes a first-in-first-out (FIFO) algorithm.</p><p>Referring now to FIG. 3, there is illustrated a detailed logical block diagram of cache memory <b>30</b> from FIG. 2, in accordance with a preferred embodiment of the present invention. As shown, a replacement logic <b>47</b> for first partition <b>33</b> (from FIG. 2) is coupled to eight three-input AND gates <b>41</b>, a CAM control block <b>38</b> for first partition <b>33</b> is coupled to eight two-input AND gates <b>42</b>, a replacement logic <b>48</b> for second partition <b>34</b> (from FIG. 2) is coupled to 128 three-input AND gates <b>43</b>, and a CAM control block <b>39</b> for second partition <b>34</b> is coupled to 128 two-input AND gates <b>44</b>. Each of AND gates <b>41</b>, <b>42</b> is respectively coupled to directory <b>31</b> and cache entry array <b>32</b> via eight two-input OR gates <b>45</b>. Similarly, each of AND gates <b>43</b>, <b>44</b> is respectively coupled to directory <b>31</b> and cache entry array <b>32</b> via 128 two-input OR gate <b>46</b>. Replacement logic <b>47</b>-<b>48</b> and gates <b>41</b>-<b>46</b> are preferably part of replacement logic block <b>35</b> from FIG. <b>2</b>.</p><p>During a normal cache access cycle, a cache hit or cache miss is determined by comparing an address from memory management unit (MMU) <b>51</b> with the tag from directory <b>31</b> via a comparator <b>52</b>. Allocation input <b>36</b> is not asserted during the normal cache access cycle. During an allocation cycle after a cache miss, allocation input <b>36</b> is first asserted. Then, a cache line from first partition <b>33</b> will be selected for allocation if selection input <b>37</b> is asserted; otherwise, a cache line from second partition <b>34</b> is selected for allocation when selection input <b>37</b> is not asserted. Whether or not selection input <b>37</b> should be asserted during the allocation cycle is determined by a lifetime requirement of an instruction or data within a software application. As such, the assertion election of selection input <b>37</b> may be controlled by a software application. Once the election is made, replacement logic <b>47</b> is responsible for selecting a cache line from first partition <b>33</b> for replacement, and replacement logic <b>48</b> is responsible for selecting a cache line from second partition <b>34</b> for replacement. As mentioned previously, the replacement algorithm used within replacement logic for first partition <b>33</b> can be identical or different from the replacement algorithm used within replacement logic for second partition <b>34</b>.</p><p>The application of the present invention can be illustrated by way of a matrix multiplication example of multiplying matrix A=[a<sub>ij</sub>](0\u2267i\u2267n, 0\u2267j\u2267m) and matrix B=[b<sub>jk</sub>](0\u2267j\u2267m, <b>0\u2267k\u2267l)) to generate matrix C=[c</b><sub>ik</sub>](0\u2267i\u2267n, 0\u2267k\u2267l). With reference now to FIG. 4, there is illustrated a high-level flow diagram of an exemplary application of a preferred embodiment of the present invention. Starting at block <b>40</b>, a variable i is initialized to zero, as shown in block <b>41</b>. At this point, selection input <b>37</b> (from FIG. 3) is not asserted, and elements of the i<sup>th </sup>row of matrix A are loaded into second partition <b>34</b> (from FIG. 2) of the cache memory, as depicted in block <b>42</b>. Then, selection input <b>37</b> is asserted, and all elements of matrix B are loaded into first partition <b>33</b> (from FIG. 2) of the cache memory, as depicted in block <b>43</b>. Subsequently, the matrix multiplication is performed until i=n. In this example, the more \u201cpermanent\u201d data are loaded into the smaller section of the cache, and the other data are loaded into the larger section of the cache.</p><p>As has been described, the present invention provides a method and apparatus for managing cache line replacement within a cache memory. The present invention overcomes a prior art cache design problem of inadvertent premature removal of instruction (or data) from a cache memory during a cache line allocation. The present invention provides an enhanced cache line replacement process by allowing some critical by not frequently used data to be remain in the cache memory. While the additional control logic may complicate the cache memory construction (and requires extra silicon area), the benefits derived from the present invention generally outweigh such concerns, especially when the cost of additional circuitry diminishes with respect to the demand for higher processor performance.</p><p>While the invention has been particularly shown and described with reference to a preferred embodiment, it will be understood by those skilled in the art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Peichun Peter", "last_name": "Liu", "name": ""}, {"first_name": "Francis Patrick", "last_name": "O'Connell", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTERNATIONAL BUSINESS MACHINES CORPORATION"}, {"first_name": "", "last_name": "INTERNATIONAL BUSINESS MACHINES CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/12        20060101A I20051008RMJP"}, {"label": "G06F  12/08        20060101ALI20051220RMJP"}], "national_classes": [{"primary": true, "label": "711133"}, {"primary": false, "label": "711136"}, {"primary": false, "label": "711135"}, {"primary": false, "label": "711159"}, {"primary": false, "label": "711160"}, {"primary": false, "label": "711E12077"}], "ecla_classes": [{"label": "G06F  12/12B8"}, {"label": "S06F12:08B22L"}], "cpc_classes": [{"label": "G06F  12/0897"}, {"label": "G06F  12/128"}, {"label": "G06F  12/0897"}, {"label": "G06F  12/08"}, {"label": "G06F  12/128"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1999-07-15", "application_date": "1999-07-15", "family_members": [{"ucid": "JP-2001043134-A", "titles": [{"lang": "JA", "text": "\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fb\u30b7\u30b9\u30c6\u30e0\u5185\u3067\u30ad\u30e3\u30c3\u30b7\u30e5\u30fb\u30e9\u30a4\u30f3\u7f6e\u63db\u3092\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u65b9\u6cd5\u304a\u3088\u3073\u88c5\u7f6e"}, {"lang": "EN", "text": "METHOD AND DEVICE FOR MANAGEMENT OF CACHE LINE SUBSTITUTION IN COMPUTER SYSTEM"}]}, {"ucid": "KR-100379993-B1", "titles": [{"lang": "EN", "text": "METHOD AND APPARATUS FOR MANAGING CACHE LINE REPLACEMENT WITHIN A COMPUTER SYSTEM"}, {"lang": "KO", "text": "\ucef4\ud4e8\ud130 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uce90\uc2dc \ub77c\uc778 \uad50\uccb4\ub97c \uad00\ub9ac\ud558\uae30 \uc704\ud55c \ubc29\ubc95\ubc0f \uc7a5\uce58"}]}, {"ucid": "KR-20010021053-A", "titles": [{"lang": "KO", "text": "\ucef4\ud4e8\ud130 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uce90\uc2dc \ub77c\uc778 \uad50\uccb4\ub97c \uad00\ub9ac\ud558\uae30 \uc704\ud55c \ubc29\ubc95\ubc0f \uc7a5\uce58"}, {"lang": "EN", "text": "METHOD AND APPARATUS FOR MANAGING CACHE LINE REPLACEMENT WITHIN A COMPUTER SYSTEM"}]}, {"ucid": "US-6510493-B1", "titles": [{"lang": "EN", "text": "Method and apparatus for managing cache line replacement within a computer system"}]}]}