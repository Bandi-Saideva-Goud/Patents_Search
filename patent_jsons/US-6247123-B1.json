{"patent_number": "US-6247123-B1", "publication_id": 72701803, "family_id": 25027374, "publication_date": "2001-06-12", "titles": [{"lang": "EN", "text": "Branch prediction mechanism employing branch selectors to select a branch prediction"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11200528\" source=\"national office\"><p>A branch prediction apparatus is provided which stores multiple branch selectors corresponding to instruction bytes within a cache line of instructions or portion thereof. The branch selectors identify a branch prediction to be selected if the corresponding instruction byte is the byte indicated by the offset of the fetch address used to fetch the cache line. Instead of comparing pointers to the branch instructions with the offset of the fetch address, the branch prediction is selected simply by decoding the offset of the fetch address and choosing the corresponding branch selector. The branch prediction apparatus may operate at a higher frequencies (i.e. lower clock cycles) than if the pointers to the branch instruction and the fetch address were compared (a greater than or less than comparison). The branch selectors directly determine which branch prediction is appropriate according to the instructions being fetched, thereby decreasing the amount of logic employed to select the branch prediction.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72569651\"><p>A branch prediction apparatus is provided which stores multiple branch selectors corresponding to instruction bytes within a cache line of instructions or portion thereof. The branch selectors identify a branch prediction to be selected if the corresponding instruction byte is the byte indicated by the offset of the fetch address used to fetch the cache line. Instead of comparing pointers to the branch instructions with the offset of the fetch address, the branch prediction is selected simply by decoding the offset of the fetch address and choosing the corresponding branch selector. The branch prediction apparatus may operate at a higher frequencies (i.e. lower clock cycles) than if the pointers to the branch instruction and the fetch address were compared (a greater than or less than comparison). The branch selectors directly determine which branch prediction is appropriate according to the instructions being fetched, thereby decreasing the amount of logic employed to select the branch prediction.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00001\" num=\"1\"><claim-text>1. A microprocessor comprising:</claim-text><claim-text>an instruction cache configured to store a group of contiguous instruction bytes, said instruction cache coupled to receive a fetch address identifying a first instruction within said group of contiguous instruction bytes, wherein said instruction cache is configured to output said group of contiguous instruction bytes responsive to said fetch address; and </claim-text><claim-text>a branch prediction unit including a branch prediction storage configured to store a plurality of branch selectors corresponding to said group of contiguous instruction bytes, wherein said branch prediction unit is configured to select a first branch selector of said plurality of branch selectors responsive to said fetch address, and wherein said first branch selector identifies a first branch prediction for a first-encountered predicted-taken branch instruction within said group of contiguous instruction bytes and subsequent to said first instruction. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00002\" num=\"2\"><claim-text>2. The microprocessor as recited in claim <b>1</b> wherein said first branch selector corresponds to said first instruction.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00003\" num=\"3\"><claim-text>3. The microprocessor as recited in claim <b>1</b> wherein said plurality of branch selectors comprises at least one branch selector per instruction within said group of contiguous instruction bytes.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00004\" num=\"4\"><claim-text>4. The microprocessor as recited in claim <b>1</b> wherein said plurality of branch selectors comprises one branch selector per byte within said group of contiguous instruction bytes.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00005\" num=\"5\"><claim-text>5. The microprocessor as recited in claim <b>1</b> wherein, if no predicted-taken branch instruction is within said group of contiguous instruction bytes and subsequent to said first instruction, said first branch predictor identifies a sequential address as said first branch prediction.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00006\" num=\"6\"><claim-text>6. The microprocessor as recited in claim <b>1</b> wherein, if said first-encountered predicted-taken branch instruction is a return instruction, said first branch selector indicates a return stack as a source of said first branch prediction.</claim-text></claim>"}, {"num": 7, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00007\" num=\"7\"><claim-text>7. The microprocessor as recited in claim <b>1</b> further comprising one or more functional units configured to execute instructions, wherein said branch prediction unit is configured to update said first branch selector responsive to execution, by one of said one or more functional units, of a second branch instruction if said second branch instruction is taken and said second branch instruction is: (i) within said group of contiguous instruction bytes; (ii) prior to said first branch instruction; and (iii) subsequent to said first instruction.</claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00008\" num=\"8\"><claim-text>8. The microprocessor as recited in claim <b>7</b> wherein said first branch selector is updated to identify a second branch prediction corresponding to said second branch instruction.</claim-text></claim>"}, {"num": 9, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00009\" num=\"9\"><claim-text>9. The microprocessor as recited in claim <b>1</b> wherein said group of contiguous instruction bytes is a portion of a cache line in said instruction cache.</claim-text></claim>"}, {"num": 10, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00010\" num=\"10\"><claim-text>10. The microprocessor as recited in claim <b>1</b> wherein said branch prediction unit is coupled to receive said fetch address concurrent with said instruction cache receiving said fetch address.</claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00011\" num=\"11\"><claim-text>11. A computer system comprising:</claim-text><claim-text>a microprocessor including: </claim-text><claim-text>an instruction cache configured to store a group of contiguous instruction bytes, said instruction cache coupled to receive a fetch address identifying a first instruction within said group of contiguous instruction bytes, wherein said instruction cache is configured to output said group of contiguous instruction bytes responsive to said fetch address; and </claim-text><claim-text>a branch prediction unit including a branch prediction storage configured to store a plurality of branch selectors corresponding to said group of contiguous instruction bytes, wherein said branch prediction unit is configured to select a first branch selector of said plurality of branch selectors responsive to said fetch address, and wherein said first branch selector identifies a first branch prediction for a first-encountered predicted-taken branch instruction within said group of contiguous instruction bytes and subsequent to said first instruction; and </claim-text><claim-text>an input/output (I/O) device configured to communicate between said computer system and another computer system to which said I/O device is couplable. </claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00012\" num=\"12\"><claim-text>12. The computer system as recited in claim <b>11</b> wherein said I/O device is a modem.</claim-text></claim>"}, {"num": 13, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00013\" num=\"13\"><claim-text>13. The computer system as recited in claim <b>11</b> further comprising an audio I/O device.</claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00014\" num=\"14\"><claim-text>14. The computer system as recited in claim <b>13</b> wherein said audio I/O device comprises a sound card.</claim-text></claim>"}, {"num": 15, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00015\" num=\"15\"><claim-text>15. A method comprising:</claim-text><claim-text>receiving a fetch address identifying a first instruction within a group of contiguous instruction bytes; and </claim-text><claim-text>selecting a first branch selector of a plurality of branch selectors corresponding to said group of contiguous instruction bytes responsive to said fetch address, said first branch selector identifying a first branch prediction corresponding to a first-encountered predicted-taken branch instruction within said group of contiguous instruction bytes and subsequent to said first instruction. </claim-text></claim>"}, {"num": 16, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00016\" num=\"16\"><claim-text>16. The method as recited in claim <b>15</b> wherein said first branch selector corresponds to said first instruction.</claim-text></claim>"}, {"num": 17, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00017\" num=\"17\"><claim-text>17. The method as recited in claim <b>15</b> wherein said plurality of branch selectors comprises at least one branch selector per instruction within said group of contiguous instruction bytes.</claim-text></claim>"}, {"num": 18, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00018\" num=\"18\"><claim-text>18. The method as recited in claim <b>15</b> wherein said plurality of branch selectors comprises one branch selector per byte within said group of contiguous instruction bytes.</claim-text></claim>"}, {"num": 19, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00019\" num=\"19\"><claim-text>19. The method as recited in claim <b>15</b> wherein, if no predicted-taken branch instruction is within said group of contiguous instruction bytes and subsequent to said first instruction, said first branch predictor identifies a sequential address as said first branch prediction.</claim-text></claim>"}, {"num": 20, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00020\" num=\"20\"><claim-text>20. The method as recited in claim <b>15</b> wherein, if said first-encountered predicted-taken branch instruction is a return instruction, said first branch selector indicates a return stack as a source of said first branch prediction.</claim-text></claim>"}, {"num": 21, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00021\" num=\"21\"><claim-text>21. The method as recited in claim <b>15</b> further comprising:</claim-text><claim-text>executing a second branch instruction within said group of contiguous instruction bytes, subsequent to said first instruction, and prior to said second branch instruction; </claim-text><claim-text>determining that said second branch instruction is taken during said executing; and </claim-text><claim-text>updating said first branch selector to identify a second branch prediction corresponding to said second branch instruction responsive to said determining. </claim-text></claim>"}, {"num": 22, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6247123-B1-CLM-00022\" num=\"22\"><claim-text>22. The method as recited in claim <b>15</b> further comprising fetching said group of contiguous instruction bytes concurrent with said receiving and said selecting.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54559994\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><p>this application is a continuation of U.S. patent application Ser. No. 08/752,691, filed Nov. 19, 1996(which includes a continued prosecution application filed Jul. 14, 1999), now U.S. Pat. No. 5,995,749, issued Nov. 30, 1999.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>This invention relates to the field of microprocessors and, more particularly, to branch prediction mechanisms within microprocessors.</p><p>2. Description of the Related Art</p><p>Superscalar microprocessors achieve high performance by executing multiple instructions per clock cycle and by choosing the shortest possible clock cycle consistent with the design. As used herein, the term \u201cclock cycle\u201d refers to an interval of time accorded to various stages of an instruction processing pipeline within the microprocessor. Storage devices (e.g. registers and arrays) capture their values according to the clock cycle. For example, a storage device may capture a value according to a rising or falling edge of a clock signal defining the clock cycle. The storage device then stores the value until the subsequent rising or falling edge of the clock signal, respectively. The term \u201cinstruction processing pipeline\u201d is used herein to refer to the logic circuits employed to process instructions in a pipelined fashion. Although the pipeline may be divided into any number of stages at which portions of instruction processing are performed, instruction processing generally comprises fetching the instruction, decoding the instruction, executing the instruction, and storing the execution results in the destination identified by the instruction.</p><p>An important feature of a superscalar microprocessor (and a superpipelined microprocessor as well) is its branch prediction mechanism. The branch prediction mechanism indicates a predicted direction (taken or not-taken) for a branch instruction, allowing subsequent instruction fetching to continue within the predicted instruction stream indicated by the branch prediction. A branch instruction is an instruction which causes subsequent instructions to be fetched from one of at least two addresses: a sequential address identifying an instruction stream beginning with instructions which directly follow the branch instruction; and a target address identifying an instruction stream beginning at an arbitrary location in memory. Unconditional branch instructions always branch to the target address, while conditional branch instructions may select either the sequential or the target address based on the outcome of a prior instruction. Instructions from the predicted instruction stream may be speculatively executed prior to execution of the branch instruction, and in any case are placed into the instruction processing pipeline prior to execution of the branch instruction. If the predicted instruction stream is correct, then the number of instructions executed per clock cycle is advantageously increased. However, if the predicted instruction stream is incorrect (i.e. one or more branch instructions are predicted incorrectly), then the instructions from the incorrectly predicted instruction stream are discarded from the instruction processing pipeline and the number of instructions executed per clock cycle is decreased.</p><p>In order to be effective, the branch prediction mechanism must be highly accurate such that the predicted instruction stream is correct as often as possible. Typically, increasing the accuracy of the branch prediction mechanism is achieved by increasing the complexity of the branch prediction mechanism. For example, a cache-line based branch prediction scheme may be employed in which branch predictions are stored with a particular cache line of instruction bytes in an instruction cache. A cache line is a number of contiguous bytes which are treated as a unit for allocation and deallocation of storage space within the instruction cache. When the cache line is fetched, the corresponding branch predictions are also fetched. Furthermore, when the particular cache line is discarded, the corresponding branch predictions are discarded as well. The cache line is aligned in memory. A cache-line based branch prediction scheme may be made more accurate by storing a larger number of branch predictions for each cache line. A given cache line may include multiple branch instructions, each of which is represented by a different branch prediction. Therefore, more branch predictions allocated to a cache line allows for more branch instructions to be represented and predicted by the branch prediction mechanism. A branch instruction which cannot be represented within the branch prediction mechanism is not predicted, and subsequently a \u201cmisprediction\u201d may be detected if the branch is found to be taken. However, complexity of the branch prediction mechanism is increased by the need to select between additional branch predictions. As used herein, a \u201cbranch prediction\u201d is a value which may be interpreted by the branch prediction mechanism as a prediction of whether or not a branch instruction is taken or not taken. Furthermore, a branch prediction may include the target address. For cache-line based branch prediction mechanisms, a prediction of a sequential line to the cache line being fetched is a branch prediction when no branch instructions are within the instructions being fetched from the cache line.</p><p>A problem related to increasing the complexity of the branch prediction mechanism is that the increased complexity generally requires an increased amount of time to form the branch prediction. For example, selecting among multiple branch predictions may require a substantial amount of time. The offset of the fetch address identifies the first byte being fetched within the cache line: a branch prediction for a branch instruction prior to the offset should not be selected. The offset of the fetch address within the cache line may need to be compared to the offset of the branch instructions represented by the branch predictions stored for the cache line in order to determine which branch prediction to use. The branch prediction corresponding to a branch instruction subsequent to the fetch address offset and nearer to the fetch address offset than other branch instructions which are subsequent to the fetch address offset should be selected. As the number of branch predictions is increased, the complexity (and time required) for the selection logic increases. When the amount of time needed to form a branch prediction for a fetch address exceeds the clock cycle time of the microprocessor, performance of the microprocessor may be decreased. Because the branch prediction cannot be formed in a single clock cycle, \u201cbubbles\u201d are introduced into the instruction processing pipeline during clock cycles that instructions cannot be fetched due to a lack of a branch prediction corresponding to a previous fetch address. The bubble occupies various stages in the instruction processing pipeline during subsequent clock cycles, and no work occurs at the stage including the bubble because no instructions are included in the bubble. Performance of the microprocessor may thereby be decreased.</p><h4>SUMMARY OF THE INVENTION</h4><p>The problems outlined above are in large part solved by a branch prediction apparatus in accordance with the present invention. The branch prediction apparatus stores multiple branch selectors corresponding to instruction bytes within a cache line of instructions or portion thereof. The branch selectors identify a branch prediction to be selected if the corresponding instruction byte is the byte indicated by the offset of the fetch address used to fetch the cache line. Instead of comparing pointers to the branch instructions with the offset of the fetch address, the branch prediction is selected simply by decoding the offset of the fetch address and choosing the corresponding branch selector. Advantageously, the branch prediction apparatus may operate at a higher frequencies (i.e. lower clock cycles) than if the pointers to the branch instruction and the fetch address were compared (a greater than or less than comparison). The branch selectors directly determine which branch prediction is appropriate according to the instructions being fetched, thereby decreasing the amount of logic employed to select the branch prediction.</p><p>Broadly speaking, the present invention contemplates a method for selecting a branch prediction corresponding to a group of contiguous instruction bytes including a plurality of instructions. A plurality of branch selectors are stored in a branch prediction storage, wherein at least one of the plurality of branch selectors corresponds to a first one of the plurality of instructions. The branch selector identifies a particular branch prediction to be selected if the first one of the plurality of instructions is fetched. The group of contiguous instruction bytes is fetched concurrent with fetching the plurality of branch selectors. The fetch address identifies the group of contiguous instruction bytes. One of the plurality of branch selectors is selected in response to the fetch address. The branch prediction is selected in response to the one of the plurality of the branch selectors.</p><p>The present invention further contemplates a branch prediction apparatus, comprising a branch prediction storage and a selection mechanism. The branch prediction storage is coupled to receive a fetch address corresponding to a group of contiguous instruction bytes being fetched from an instruction cache. The branch prediction storage is configured to store a plurality of branch selectors wherein at least one of the plurality of branch selectors corresponds to a first instruction within the group of contiguous instruction bytes. The at least one of the plurality of branch selectors identifies a particular branch prediction to be selected if the first instruction is fetched. Coupled to the branch prediction storage to receives the plurality of branch selectors, the selection mechanism is configured to select a particular one of the plurality of branch selectors in response to a plurality of least significant bits of a fetch address used to fetch the group of contiguous instruction bytes.</p><p>The present invention still further contemplates a microprocessor comprising an instruction cache and a branch prediction unit. The instruction cache is configured to store a plurality of cache lines of instruction bytes and to provide a group of instruction bytes upon receipt of a fetch address to an instruction processing pipeline of the microprocessor. Coupled to the instruction cache and coupled to receive the fetch address concurrent with the instruction cache, the branch prediction unit is configured to store a plurality of branch selectors with respect to the group of instruction bytes and is configured to select one of the plurality of branch selectors in response to the fetch address. The one of the plurality of branch selectors identifies a branch prediction which is used as a subsequent fetch address by the instruction cache.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Other objects and advantages of the invention will become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p><p>FIG. 1 is a block diagram of one embodiment of a superscalar microprocessor.</p><p>FIG. 2 is a block diagram of one embodiment of a pair of decode units shown in FIG. <b>1</b>.</p><p>FIG. 3 is a diagram of a group of contiguous instruction bytes and a corresponding set of branch selectors.</p><p>FIG. 4 is a block diagram of a portion of one embodiment of a branch prediction unit shown in FIG. <b>1</b>.</p><p>FIG. 5 is a diagram of a prediction block for a group of contiguous instruction bytes as stored in the branch prediction unit shown in FIG. <b>4</b>.</p><p>FIG. 6 is a table showing an exemplary encoding of a branch selector.</p><p>FIG. 7 is a flowchart depicting steps performed in order to update a set of branch selectors corresponding to a group of contiguous instruction bytes.</p><p>FIG. 8 is a first example of updating the set of branch selectors.</p><p>FIG. 9 is a second example of updating the set of branch selectors.</p><p>FIG. 10 is a third example of updating the set of branch selectors.</p><p>FIG. 11 is a block diagram of a computer system including the microprocessor shown in FIG. <b>1</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><p>While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><h4>DETAILED DESCRIPTION OF THE INVENTION</h4><p>Turning now to FIG. 1, a block diagram of one embodiment of a microprocessor <b>10</b> is shown. Microprocessor <b>10</b> includes a prefetch/predecode unit <b>12</b>, a branch prediction unit <b>14</b>, an instruction cache <b>16</b>, an instruction alignment unit <b>18</b>, a plurality of decode units <b>20</b>A-<b>20</b>C, a plurality of reservation stations <b>22</b>A-<b>22</b>C, a plurality of functional units <b>24</b>A-<b>24</b>C, a load/store unit <b>26</b>, a data cache <b>28</b>, a register file <b>30</b>, a reorder buffer <b>32</b>, and an MROM unit <b>34</b>. Elements referred to herein with a particular reference number followed by a letter will be collectively referred to by the reference number alone. For example, decode units <b>20</b>A-<b>20</b>C will be collectively referred to as decode units <b>20</b>.</p><p>Prefetch/predecode unit <b>12</b> is coupled to receive instructions from a main memory subsystem (not shown), and is further coupled to instruction cache <b>16</b> and branch prediction unit <b>14</b>. Similarly, branch prediction unit <b>14</b> is coupled to instruction cache <b>16</b>. Still further, branch prediction unit <b>14</b> is coupled to decode units <b>20</b> and functional units <b>24</b>. Instruction cache <b>16</b> is further coupled to MROM unit <b>34</b> and instruction alignment unit <b>18</b>. Instruction alignment unit <b>18</b> is in turn coupled to decode units <b>20</b>. Each decode unit <b>20</b>A-<b>20</b>C is coupled to load/store unit <b>26</b> and to respective reservation stations <b>22</b>A-<b>22</b>C. Reservation stations <b>22</b>A-<b>22</b>C are further coupled to respective functional units <b>24</b>A-<b>24</b>C. Additionally, decode units <b>20</b> and reservation stations <b>22</b> are coupled to register file <b>30</b> and reorder buffer <b>32</b>. Functional units <b>24</b> are coupled to load/store unit <b>26</b>, register file <b>30</b>, and reorder buffer <b>32</b> as well. Data cache <b>28</b> is coupled to load/store unit <b>26</b> and to the main memory subsystem. Finally, MROM unit <b>34</b> is coupled to decode units <b>20</b>.</p><p>Generally speaking, branch prediction unit <b>14</b> employs a cache-line based branch prediction mechanism for predicting branch instructions. Multiple branch predictions may be stored for each cache line. Additionally, a branch selector is stored for each byte within the cache line. The branch selector for a particular byte indicates which of the branch predictions which may be stored with respect to the cache line is the branch prediction appropriate for an instruction fetch address which fetches the particular byte. The appropriate branch prediction is the branch prediction for the first predicted-taken branch instruction encountered within the cache line subsequent to the particular byte. As used herein, the terms \u201csubsequent\u201d and \u201cprior to\u201d refer to an ordering of bytes within the cache line. A byte stored at a memory address which is numerically smaller than the memory address at which a second byte is stored is prior to the second byte. Conversely, a byte stored at a memory address which is numerically larger than the memory address of a second byte is subsequent to the second byte. Similarly, a first instruction is prior to a second instruction in program order if the first instruction is encountered before the second instruction when stepping one at a time through the sequence of instructions forming the program.</p><p>In one embodiment, microprocessor <b>10</b> employs a microprocessor architecture in which the instruction set is a variable byte length instruction set (e.g. the x86 microprocessor architecture). When a variable byte length instruction set is employed, any byte within the cache line may be identified as the first byte to be fetched by a given fetch address. For example, a branch instruction may have a target address at byte position two within a cache line. In such a case, the bytes at byte positions zero and one are not being fetched during the current cache access. Additionally, bytes subsequent to a predicted-taken branch which is subsequent to the first byte are not fetched during the current cache access. Since branch selectors are stored for each byte, the branch prediction for the predicted taken branch can be located by selecting the branch selector of the first byte to be fetched from the cache line. The branch selector is used to select the appropriate branch prediction, which is then provided to the instruction fetch logic in instruction cache <b>16</b>. During the succeeding clock cycle, the branch prediction is used as the fetch address. Advantageously, the process of comparing the byte position of the first byte being fetched to the byte positions of the predicted-taken branch instructions is eliminated from the generation of a branch prediction in response to a fetch address. The amount of time required to form a branch prediction may be reduced accordingly, allowing the branch prediction mechanism to operate at higher clock frequencies (i.e. shorter clock cycles) while still providing a single cycle branch prediction.</p><p>It is noted that, although the term \u201ccache line\u201d has been used in the preceding discussion, some embodiments of instruction cache <b>16</b> may not provide an entire cache line at its output during a given clock cycle. For example, in one embodiment instruction cache <b>16</b> is configured with 32 byte cache lines. However, only 16 bytes are fetched in a given clock cycle (either the upper half or the lower half of the cache line). The branch prediction storage locations and branch selectors are allocated to the portion of the cache line being fetched. As used herein, the term \u201cgroup of contiguous instruction bytes\u201d is used to refer to the instruction bytes which are provided by the instruction cache in a particular clock cycle in response to a fetch address. A group of contiguous instruction bytes may be a portion of a cache line or an entire cache line, according to various embodiments. When a group of contiguous instruction bytes is a portion of a cache line, it is still an aligned portion of a cache line. For example, if a group of contiguous instruction bytes is half a cache line, it is either the upper half of the cache line or the lower half of the cache line. A number of branch prediction storage locations are allocated to each group of contiguous instruction bytes, and branch selectors indicate one of the branch prediction storage locations associated with that group. Furthermore, branch selectors may indicate a return stack address from a return stack structure or a sequential address if no branch instructions are encountered between the corresponding byte and the last byte in the group of contiguous instruction bytes.</p><p>Instruction cache <b>16</b> is a high speed cache memory provided to store instructions. Instructions are fetched from instruction cache <b>16</b> and dispatched to decode units <b>20</b>. In one embodiment, instruction cache <b>16</b> is configured to store up to 32 kilobytes of instructions in a 4 way set associative structure having 32 byte lines (a byte comprises 8 binary bits). Instruction cache <b>16</b> may additionally employ a way prediction scheme in order to speed access times to the instruction cache. Instead of accessing tags identifying each line of instructions and comparing the tags to the fetch address to select a way, instruction cache <b>16</b> predicts the way that is accessed. In this manner, the way is selected prior to accessing the instruction storage. The access time of instruction cache <b>16</b> may be similar to a direct-mapped cache. A tag comparison is performed and, if the way prediction is incorrect, the correct instructions are fetched and the incorrect instructions are discarded. It is noted that instruction cache <b>16</b> may be implemented as a fully associative, set associative, or direct mapped configuration.</p><p>Instructions are fetched from main memory and stored into instruction cache <b>16</b> by prefetch/predecode unit <b>12</b>. Instructions may be prefetched prior to the request thereof from instruction cache <b>16</b> in accordance with a prefetch scheme. A variety of prefetch schemes may be employed by prefetch/predecode unit <b>12</b>. As prefetch/predecode unit <b>12</b> transfers instructions from main memory to instruction cache <b>16</b>, prefetch/predecode unit <b>12</b> generates three predecode bits for each byte of the instructions: a start bit, an end bit, and a functional bit. The predecode bits form tags indicative of the boundaries of each instruction. The predecode tags may also convey additional information such as whether a given instruction can be decoded directly by decode units <b>20</b> or whether the instruction is executed by invoking a microcode procedure controlled by MROM unit <b>34</b>, as will be described in greater detail below. Still further, prefetch/predecode unit <b>12</b> may be configured to detect branch instructions and to store branch prediction information corresponding to the branch instructions into branch prediction unit <b>14</b>.</p><p>One encoding of the predecode tags for an embodiment of microprocessor <b>10</b> employing the x86 instruction set will next be described. If a given byte is the first byte of an instruction, the start bit for that byte is set. If the byte is the last byte of an instruction, the end bit for that byte is set. Instructions which may be directly decoded by decode units <b>20</b> are referred to as \u201cfast path\u201d instructions. The remaining x86 instructions are referred to as MROM instructions, according to one embodiment. For fast path instructions, the functional bit is set for each prefix byte included in the instruction, and cleared for other bytes. Alternatively, for MROM instructions, the functional bit is cleared for each prefix byte and set for other bytes. The type of instruction may be determined by examining the functional bit corresponding to the end byte. If that functional bit is clear, the instruction is a fast path instruction. Conversely, if that functional bit is set, the instruction is an MROM instruction. The opcode of an instruction may thereby be located within an instruction which may be directly decoded by decode units <b>20</b> as the byte associated with the first clear functional bit in the instruction. For example, a fast path instruction including two prefix bytes, a Mod R/M byte, and an SIB byte would have start, end, and functional bits as follows:</p><p><tables id=\"TABLE-US-00001\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"49PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"49PT\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"119PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></thead><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Start bits</entry><entry morerows=\"0\" valign=\"top\">10000</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">End bits</entry><entry morerows=\"0\" valign=\"top\">00001</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Functional bits</entry><entry morerows=\"0\" valign=\"top\">11000</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>MROM instructions are instructions which are determined to be too complex for decode by decode units <b>20</b>. MROM instructions are executed by invoking MROM unit <b>34</b>. More specifically, when an MROM instruction is encountered, MROM unit <b>34</b> parses and issues the instruction into a subset of defined fast path instructions to effectuate the desired operation. MROM unit <b>34</b> dispatches the subset of fast path instructions to decode units <b>20</b>. A listing of exemplary x86 instructions categorized as fast path instructions will be provided further below.</p><p>Microprocessor <b>10</b> employs branch prediction in order to speculatively fetch instructions subsequent to conditional branch instructions. Branch prediction unit <b>14</b> is included to perform branch prediction operations. In one embodiment, up to two branch target addresses are stored with respect to each 16 byte portion of each cache line in instruction cache <b>16</b>. Prefetch/predecode unit <b>12</b> determines initial branch targets when a particular line is predecoded. Subsequent updates to the branch targets corresponding to a cache line may occur due to the execution of instructions within the cache line. Instruction cache <b>16</b> provides an indication of the instruction address being fetched, so that branch prediction unit <b>14</b> may determine which branch target addresses to select for forming a branch prediction. Decode units <b>20</b> and functional units <b>24</b> provide update information to branch prediction unit <b>14</b>. Because branch prediction unit <b>14</b> stores two targets per 16 byte portion of the cache line, some branch instructions within the line may not be stored in branch prediction unit <b>14</b>. Decode units <b>20</b> detect branch instructions which were not predicted by branch prediction unit <b>14</b>. Functional units <b>24</b> execute the branch instructions and determine if the predicted branch direction is incorrect. The branch direction may be \u201ctaken\u201d, in which subsequent instructions are fetched from the target address of the branch instruction. Conversely, the branch direction may be \u201cnot taken\u201d, in which subsequent instructions are fetched from memory locations consecutive to the branch instruction. When a mispredicted branch instruction is detected, instructions subsequent to the mispredicted branch are discarded from the various units of microprocessor <b>10</b>. A variety of suitable branch prediction algorithms may be employed by branch prediction unit <b>14</b>.</p><p>Instructions fetched from instruction cache <b>16</b> are conveyed to instruction alignment unit <b>18</b>. As instructions are fetched from instruction cache <b>16</b>, the corresponding predecode data is scanned to provide information to instruction alignment unit <b>18</b> (and to MROM unit <b>34</b>) regarding the instructions being fetched. Instruction alignment unit <b>18</b> utilizes the scanning data to align an instruction to each of decode units <b>20</b>. In one embodiment, instruction alignment unit <b>18</b> aligns instructions from three sets of eight instruction bytes to decode units <b>20</b>. Instructions are selected independently from each set of eight instruction bytes into preliminary issue positions. The preliminary issue positions are then merged to a set of aligned issue positions corresponding to decode units <b>20</b>, such that the aligned issue positions contain the three instructions which are prior to other instructions within the preliminary issue positions in program order. Decode unit <b>20</b>A receives an instruction which is prior to instructions concurrently received by decode units <b>20</b>B and <b>20</b>C (in program order). Similarly, decode unit <b>20</b>B receives an instruction which is prior to the instruction concurrently received by decode unit <b>20</b>C in program order.</p><p>Decode units <b>20</b> are configured to decode instructions received from instruction alignment unit <b>18</b>. Register operand information is detected and routed to register file <b>30</b> and reorder buffer <b>32</b>. Additionally, if the instructions require one or more memory operations to be performed, decode units <b>20</b> dispatch the memory operations to load/store unit <b>26</b>. Each instruction is decoded into a set of control values for functional units <b>24</b>, and these control values are dispatched to reservation stations <b>22</b> along with operand address information and displacement or immediate data which may be included with the instruction.</p><p>Microprocessor <b>10</b> supports out of order execution, and thus employs reorder buffer <b>32</b> to keep track of the original program sequence for register read and write operations, to implement register renaming, to allow for speculative instruction execution and branch misprediction recovery, and to facilitate precise exceptions. A temporary storage location within reorder buffer <b>32</b> is reserved upon decode of an instruction that involves the update of a register to thereby store speculative register states. If a branch prediction is incorrect, the results of speculatively-executed instructions along the mispredicted path can be invalidated in the buffer before they are written to register file <b>30</b>. Similarly, if a particular instruction causes an exception, instructions subsequent to the particular instruction may be discarded. In this manner, exceptions are \u201cprecise\u201d (i.e. instructions subsequent to the particular instruction causing the exception are not completed prior to the exception). It is noted that a particular instruction is speculatively executed if it is executed prior to instructions which precede the particular instruction in program order. Preceding instructions may be a branch instruction or an exception-causing instruction, in which case the speculative results may be discarded by reorder buffer <b>32</b>.</p><p>The instruction control values and immediate or displacement data provided at the outputs of decode units <b>20</b> are routed directly to respective reservation stations <b>22</b>. In one embodiment, each reservation station <b>22</b> is capable of holding instruction information (i.e., instruction control values as well as operand values, operand tags and/or immediate data) for up to three pending instructions awaiting issue to the corresponding functional unit. It is noted that for the embodiment of FIG. 1, each reservation station <b>22</b> is associated with a dedicated functional unit <b>24</b>. Accordingly, three dedicated \u201cissue positions\u201d are formed by reservation stations <b>22</b> and functional units <b>24</b>. In other words, issue position <b>0</b> is formed by reservation station <b>22</b>A and functional unit <b>24</b>A. Instructions aligned and dispatched to reservation station <b>22</b>A are executed by functional unit <b>24</b>A. Similarly, issue position <b>1</b> is formed by reservation station <b>22</b>B and functional unit <b>24</b>B; and issue position <b>2</b> is formed by reservation station <b>22</b>C and functional unit <b>24</b>C.</p><p>Upon decode of a particular instruction, if a required operand is a register location, register address information is routed to reorder buffer <b>32</b> and register file <b>30</b> simultaneously. Those of skill in the art will appreciate that the x86 register file includes eight 32 bit real registers (i.e., typically referred to as EAX, EBX, ECX, EDX, EBP, ESI, EDI and ESP). In embodiments of microprocessor <b>10</b> which employ the x86 microprocessor architecture, register file <b>30</b> comprises storage locations for each of the 32 bit real registers. Additional storage locations may be included within register file <b>30</b> for use by MROM unit <b>34</b>. Reorder buffer <b>32</b> contains temporary storage locations for results which change the contents of these registers to thereby allow out of order execution. A temporary storage location of reorder buffer <b>32</b> is reserved for each instruction which, upon decode, is determined to modify the contents of one of the real registers. Therefore, at various points during execution of a particular program, reorder buffer <b>32</b> may have one or more locations which contain the speculatively executed contents of a given register. If following decode of a given instruction it is determined that reorder buffer <b>32</b> has a previous location or locations assigned to a register used as an operand in the given instruction, the reorder buffer <b>32</b> forwards to the corresponding reservation station either: 1) the value in the most recently assigned location, or 2) a tag for the most recently assigned location if the value has not yet been produced by the functional unit that will eventually execute the previous instruction. If reorder buffer <b>32</b> has a location reserved for a given register, the operand value (or reorder buffer tag) is provided from reorder buffer <b>32</b> rather than from register file <b>30</b>. If there is no location reserved for a required register in reorder buffer <b>32</b>, the value is taken directly from register file <b>30</b>. If the operand corresponds to a memory location, the operand value is provided to the reservation station through load/store unit <b>26</b>.</p><p>In one particular embodiment, reorder buffer <b>32</b> is configured to store and manipulate concurrently decoded instructions as a unit. This configuration will be referred to herein as \u201cline-oriented\u201d. By manipulating several instructions together, the hardware employed within reorder buffer <b>32</b> may be simplified. For example, a line-oriented reorder buffer included in the present embodiment allocates storage sufficient for instruction information pertaining to three instructions (one from each decode unit <b>20</b>) whenever one or more instructions are dispatched by decode units <b>20</b>. By contrast, a variable amount of storage is allocated in conventional reorder buffers, dependent upon the number of instructions actually dispatched. A comparatively larger number of logic gates may be required to allocate the variable amount of storage. When each of the concurrently decoded instructions has executed, the instruction results are stored into register file <b>30</b> simultaneously. The storage is then free for allocation to another set of concurrently decoded instructions. Additionally, the amount of control logic circuitry employed per instruction is reduced because the control logic is amortized over several concurrently decoded instructions. A reorder buffer tag identifying a particular instruction may be divided into two fields: a line tag and an offset tag. The line tag identifies the set of concurrently decoded instructions including the particular instruction, and the offset tag identifies which instruction within the set corresponds to the particular instruction. It is noted that storing instruction results into register file <b>30</b> and freeing the corresponding storage is referred to as \u201cretiring\u201d the instructions. It is further noted that any reorder buffer configuration may be employed in various embodiments of microprocessor <b>10</b>.</p><p>As noted earlier, reservation stations <b>22</b> store instructions until the instructions are executed by the corresponding functional unit <b>24</b>. An instruction is selected for execution if: (i) the operands of the instruction have been provided; and (ii) the operands have not yet been provided for instructions which are within the same reservation station <b>22</b>A-<b>22</b>C and which are prior to the instruction in program order. It is noted that when an instruction is executed by one of the functional units <b>24</b>, the result of that instruction is passed directly to any reservation stations <b>22</b> that are waiting for that result at the same time the result is passed to update reorder buffer <b>32</b> (this technique is commonly referred to as \u201cresult forwarding\u201d). An instruction may be selected for execution and passed to a functional unit <b>24</b>A-<b>24</b>C during the clock cycle that the associated result is forwarded. Reservation stations <b>22</b> route the forwarded result to the functional unit <b>24</b> in this case.</p><p>In one embodiment, each of the functional units <b>24</b> is configured to perform integer arithmetic operations of addition and subtraction, as well as shifts, rotates, logical operations, and branch operations. The operations are performed in response to the control values decoded for a particular instruction by decode units <b>20</b>. It is noted that a floating point unit (not shown) may also be employed to accommodate floating point operations. The floating point unit may be operated as a coprocessor, receiving instructions from MROM unit <b>34</b> and subsequently communicating with reorder buffer <b>32</b> to complete the instructions. Additionally, functional units <b>24</b> may be configured to perform address generation for load and store memory operations performed by load/store unit <b>26</b>.</p><p>Each of the functional units <b>24</b> also provides information regarding the execution of conditional branch instructions to the branch prediction unit <b>14</b>. If a branch prediction was incorrect, branch prediction unit <b>14</b> flushes instructions subsequent to the mispredicted branch that have entered the instruction processing pipeline, and causes fetch of the required instructions from instruction cache <b>16</b> or main memory. It is noted that in such situations, results of instructions in the original program sequence which occur after the mispredicted branch instruction are discarded, including those which were speculatively executed and temporarily stored in load/store unit <b>26</b> and reorder buffer <b>32</b>.</p><p>Results produced by functional units <b>24</b> are sent to reorder buffer <b>32</b> if a register value is being updated, and to load/store unit <b>26</b> if the contents of a memory location are changed. If the result is to be stored in a register, reorder buffer <b>32</b> stores the result in the location reserved for the value of the register when the instruction was decoded. A plurality of result buses <b>38</b> are included for forwarding of results from functional units <b>24</b> and load/store unit <b>26</b>. Result buses <b>38</b> convey the result generated, as well as the reorder buffer tag identifying the instruction being executed.</p><p>Load/store unit <b>26</b> provides an interface between functional units <b>24</b> and data cache <b>28</b>. In one embodiment, load/store unit <b>26</b> is configured with a load/store buffer having eight storage locations for data and address information for pending loads or stores. Decode units <b>20</b> arbitrate for access to the load/store unit <b>26</b>. When the buffer is full, a decode unit must wait until load/store unit <b>26</b> has room for the pending load or store request information. Load/store unit <b>26</b> also performs dependency checking for load memory operations against pending store memory operations to ensure that data coherency is maintained. A memory operation is a transfer of data between microprocessor <b>10</b> and the main memory subsystem. Memory operations may be the result of an instruction which utilizes an operand stored in memory, or may be the result of a load/store instruction which causes the data transfer but no other operation. Additionally, load/store unit <b>26</b> may include a special register storage for special registers such as the segment registers and other registers related to the address translation mechanism defined by the x86 microprocessor architecture.</p><p>In one embodiment, load/store unit <b>26</b> is configured to perform load memory operations speculatively. Store memory operations are performed in program order, but may be speculatively stored into the predicted way. If the predicted way is incorrect, the data prior to the store memory operation is subsequently restored to the predicted way and the store memory operation is performed to the correct way. In another embodiment, stores may be executed speculatively as well. Speculatively executed stores are placed into a store buffer, along with a copy of the cache line prior to the update. If the speculatively executed store is later discarded due to branch misprediction or exception, the cache line may be restored to the value stored in the buffer. It is noted that load/store unit <b>26</b> may be configured to perform any amount of speculative execution, including no speculative execution.</p><p>Data cache <b>28</b> is a high speed cache memory provided to temporarily store data being transferred between load/store unit <b>26</b> and the main memory subsystem. In one embodiment, data cache <b>28</b> has a capacity of storing up to sixteen kilobytes of data in an eight way set associative structure. Similar to instruction cache <b>16</b>, data cache <b>28</b> may employ a way prediction mechanism. It is understood that data cache <b>28</b> may be implemented in a variety of specific memory configurations, including a set associative configuration.</p><p>In one particular embodiment of microprocessor <b>10</b> employing the x86 microprocessor architecture, instruction cache <b>16</b> and data cache <b>28</b> are linearly addressed. The linear address is formed from the offset specified by the instruction and the base address specified by the segment portion of the x86 address translation mechanism. Linear addresses may optionally be translated to physical addresses for accessing a main memory. The linear to physical translation is specified by the paging portion of the x86 address translation mechanism. It is noted that a linear addressed cache stores linear address tags. A set of physical tags (not shown) may be employed for mapping the linear addresses to physical addresses and for detecting translation aliases. Additionally, the physical tag block may perform linear to physical address translation.</p><p>Turning now to FIG. 2, a block diagram of one embodiment of decode units <b>20</b>B and <b>20</b>C is shown. Each decode unit <b>20</b> receives an instruction from instruction alignment unit <b>18</b>. Additionally, MROM unit <b>34</b> is coupled to each decode unit <b>20</b> for dispatching fast path instructions corresponding to a particular MROM instruction. Decode unit <b>20</b>B comprises early decode unit <b>40</b>B, multiplexer <b>42</b>B, and opcode decode unit <b>44</b>B. Similarly, decode unit <b>20</b>C includes early decode unit <b>40</b>C, multiplexer <b>42</b>C, and opcode decode unit <b>44</b>C.</p><p>Certain instructions in the x86 instruction set are both fairly complicated and frequently used. In one embodiment of microprocessor <b>10</b>, such instructions include more complex operations than the hardware included within a particular functional unit <b>24</b>A-<b>24</b>C is configured to perform. Such instructions are classified as a special type of MROM instruction referred to as a \u201cdouble dispatch\u201d instruction. These instructions are dispatched to a pair of opcode decode units <b>44</b>. It is noted that opcode decode units <b>44</b> are coupled to respective reservation stations <b>22</b>. Each of opcode decode units <b>44</b>A-<b>44</b>C forms an issue position with the corresponding reservation station <b>22</b>A-<b>22</b>C and functional unit <b>24</b>A-<b>24</b>C. Instructions are passed from an opcode decode unit <b>44</b> to the corresponding reservation station <b>22</b> and further to the corresponding functional unit <b>24</b>.</p><p>Multiplexor <b>42</b>B is included for selecting between the instructions provided by MROM unit <b>34</b> and by early decode unit <b>40</b>B. During times in which MROM unit <b>34</b> is dispatching instructions, multiplexor <b>42</b>B selects instructions provided by MROM unit <b>34</b>. At other times, multiplexor <b>42</b>B selects instructions provided by early decode unit <b>40</b>B. Similarly, multiplexor <b>42</b>C selects between instructions provided by MROM unit <b>34</b>, early decode unit <b>40</b>B, and early decode unit <b>40</b>C. The instruction from MROM unit <b>34</b> is selected during times in which MROM unit <b>34</b> is dispatching instructions. During times in which the early decode unit within decode unit <b>20</b>A (not shown) detects a double dispatch instruction, the instruction from early decode unit <b>40</b>B is selected by multiplexor <b>42</b>C. Otherwise, the instruction from early decode unit <b>40</b>C is selected. Selecting the instruction from early decode unit <b>40</b>B into opcode decode unit <b>44</b>C allows a fast path instruction decoded by decode unit <b>20</b>B to be dispatched concurrently with a double dispatch instruction decoded by decode unit <b>20</b>A.</p><p>According to one embodiment employing the x86 instruction set, early decode units <b>40</b> perform the following operations:</p><p>(i) merge the prefix bytes of the instruction into an encoded prefix byte;</p><p>(ii) decode unconditional branch instructions (which may include the unconditional jump, the CALL, and the RETURN) which were not detected during branch prediction;</p><p>(iii) decode source and destination flags;</p><p>(iv) decode the source and destination operands which are register operands and generate operand size information; and</p><p>(v) determine the displacement and/or immediate size so that displacement and immediate data may be routed to the opcode decode unit. Opcode decode units <b>44</b> are configured to decode the opcode of the instruction, producing control values for functional unit <b>24</b>. Displacement and immediate data are routed with the control values to reservation stations <b>22</b>.</p><p>Since early decode units <b>40</b> detect operands, the outputs of multiplexors <b>42</b> are routed to register file <b>30</b> and reorder buffer <b>32</b>. Operand values or tags may thereby be routed to reservation stations <b>22</b>. Additionally, memory operands are detected by early decode units <b>40</b>. Therefore, the outputs of multiplexors <b>42</b> are routed to load/store unit <b>26</b>. Memory operations corresponding to instructions having memory operands are stored by load/store unit <b>26</b>.</p><p>Turning now to FIG. 3, a diagram of an exemplary group of contiguous instruction bytes <b>50</b> and a corresponding set of branch selectors <b>52</b> are shown. In FIG. 3, each byte within an instruction is illustrated by a short vertical line (e.g. reference number <b>54</b>). Additionally, the vertical lines separating instructions in group <b>50</b> delimit bytes (e.g. reference number <b>56</b>). The instructions shown in FIG. 3 are variable in length, and therefore the instruction set including the instructions shown in FIG. 3 is a variable byte length instruction set. In other words, a first instruction within the variable byte length instruction set may occupy a first number of bytes which is different than a second number of bytes occupied by a second instruction within the instruction set. Other instruction sets may be fixed-length, such that each instruction within the instruction set occupies the same number of bytes as each other instruction.</p><p>As illustrated in FIG. 3, group <b>50</b> includes non-branch instructions IN0-IN5. Instructions IN0, IN3, IN4, and IN5 are two byte instructions. Instruction IN1 is a one byte instruction and instruction IN2 is a three byte instruction. Two predicted-taken branch instructions PB0 and PB1 are illustrated as well, each shown as occupying two bytes. It is noted that both non-branch and branch instructions may occupy various numbers of bytes.</p><p>The end byte of each predicted-taken branch PB0 and PB1 provides a division of group <b>50</b> into three regions: a first region <b>58</b>, a second region <b>60</b>, and a third region <b>62</b>. If a fetch address identifying group <b>50</b> is presented, and the offset of the fetch address within the group identifies a byte position within first region <b>58</b>, then the first predicted-taken branch instruction to be encountered is PB0 and therefore the branch prediction for PB0 is selected by the branch prediction mechanism. Similarly, if the offset of the fetch address identifies a byte within second region <b>60</b>, the appropriate branch prediction is the branch prediction for PB1. Finally, if the offset of the fetch address identifies a byte within third region <b>62</b>, then there is no predicted-taken branch instruction within the group of instruction bytes and subsequent to the identified byte. Therefore, the branch prediction for third region <b>62</b> is sequential. The sequential address identifies the group of instruction bytes which immediately follows group <b>50</b> within main memory.</p><p>As used herein, the offset of an address comprises a number of least significant bits of the address. The number is sufficient to provide different encodings of the bits for each byte within the group of bytes to which the offset relates. For example, group <b>50</b> is 16 bytes. Therefore, four least significant bits of an address within the group form the offset of the address. The remaining bits of the address identify group <b>50</b> from other groups of instruction bytes within the main memory. Additionally, a number of least significant bits of the remaining bits form an index used by instruction cache <b>16</b> to select a row of storage locations which are eligible for storing group <b>50</b>.</p><p>Set <b>52</b> is an exemplary set of branch selectors for group <b>50</b>. One branch selector is included for each byte within group <b>50</b>. The branch selectors within set <b>52</b> use the encoding shown in FIG. 6 below. In the example, the branch prediction for PB0 is stored as the second of two branch predictions associated with group <b>50</b> (as indicated by a branch selector value of \u201c3\u201d). Therefore, the branch selector for each byte within first region <b>58</b> is set to \u201c3\u201d. Similarly, the branch prediction for PB1 is stored as the first of the branch predictions (as indicated by a branch selector value of \u201c2\u201d). Therefore, the branch selector for each byte within second region <b>60</b> is set to \u201c2\u201d. Finally, the sequential branch prediction is indicated by the branch selectors for bytes within third region <b>62</b> by a branch selector encoding of \u201c0\u201d.</p><p>It is noted that, due to the variable byte length nature of the x86 instruction set, a branch instruction may begin within one group of contiguous instruction bytes and end within a second group of contiguous instruction bytes. In such a case, the branch prediction for the branch instruction is stored with the second group of contiguous instruction bytes. Among other things, the bytes of the branch instruction which are stored within the second group of contiguous instruction bytes need to be fetched and dispatched. Forming the branch prediction in the first group of contiguous instruction bytes would cause the bytes of the branch instruction which lie within the second group of instruction bytes not to be fetched.</p><p>Turning now to FIG. 4, a portion of one embodiment of branch prediction unit <b>14</b> is shown. Other embodiments of branch prediction unit <b>14</b> and the portion shown in FIG. 4 are contemplated. As shown in FIG. 4, branch prediction unit <b>14</b> includes a branch prediction storage <b>70</b>, a way multiplexor <b>72</b>, a branch selector multiplexor <b>74</b>, a branch prediction multiplexor <b>76</b>, a sequential/return multiplexor <b>78</b>, a final prediction multiplexor <b>80</b>, an update logic block <b>82</b>, and a decoder <b>84</b>. Branch prediction storage <b>70</b> and decoder <b>84</b> are coupled to a fetch address bus <b>86</b> from instruction cache <b>16</b>. A fetch address concurrently provided to the instruction bytes storage within instruction cache <b>16</b> is conveyed upon fetch address bus <b>86</b>. Decoder block <b>84</b> provides selection controls to prediction selector multiplexer <b>74</b>. Prediction controls for way multiplexor <b>72</b> are provided via a way selection bus <b>88</b> from instruction cache <b>16</b>. Way selection bus <b>88</b> provides the way of instruction cache <b>16</b> which is storing the cache line corresponding to the fetch address provided on fetch address bus <b>86</b>. Additionally, a selection control is provided by decoder <b>84</b> based upon which portion of the cache line is being fetched. Way multiplexor <b>72</b> is coupled to receive the contents of each storage location within the row of branch prediction storage <b>70</b> which is indexed by the fetch address upon fetch address bus <b>86</b>. Branch selector multiplexor <b>74</b> and branch prediction multiplexor <b>76</b> are coupled to receive portions of the output of way multiplexor <b>72</b> as inputs. Additionally, the output of branch selector multiplexor <b>74</b> provides selection controls for multiplexors <b>76</b>, <b>78</b>, and <b>80</b>. Sequential/return multiplexor <b>78</b> selects between a sequential address provided upon a sequential address bus <b>90</b> from instruction cache <b>16</b> and a return address provided upon a return address bus <b>92</b> from a return stack. The output of multiplexors <b>76</b> and <b>78</b> is provided to final prediction multiplexor <b>80</b>, which provides a branch prediction bus <b>94</b> to instruction cache <b>16</b>. Instruction cache <b>16</b> uses the branch prediction provided upon branch prediction bus <b>94</b> as the fetch address for the subsequent clock cycle. Update logic block <b>82</b> is coupled to branch prediction storage <b>70</b> via an update bus <b>96</b> used to update branch prediction information stored therein. Update logic block <b>82</b> provides updates in response to a misprediction signalled via a mispredict bus <b>98</b> from functional units <b>24</b> and decode units <b>20</b>. Additionally, update logic block <b>82</b> provides updates in response to newly predecoded instruction indicated by prefetch/predecode unit <b>12</b> upon a predecode bus <b>100</b>.</p><p>Branch prediction storage <b>70</b> is arranged with a number of ways equal to the number of ways in instruction cache <b>16</b>. For each way, a prediction block is stored for each group of contiguous instruction bytes existing within a cache line. In the embodiment of FIG. 4, two groups of instruction bytes are included in each cache line. Therefore, prediction block P<sub>00 </sub>is the prediction block corresponding to the first group of contiguous instruction bytes in the first way and prediction block P<sub>01</sub>, is the prediction block corresponding to the second group of contiguous instruction bytes in the first way. Similarly, prediction block P<sub>10 </sub>is the prediction block corresponding to the first group of contiguous instruction bytes in the second way and prediction block P<sub>11 </sub>is the prediction block corresponding to the second group of contiguous instruction bytes in the second way, etc. Each prediction block P<sub>00 </sub>to P<sub>31 </sub>in the indexed row is provided as an output of branch prediction storage <b>70</b>, and hence as an input to way multiplexor <b>72</b>. The indexed row is similar to indexing into a cache: a number of bits which are not part of the offset portion of the fetch address are used to select one of the rows of branch prediction storage <b>70</b>. It is noted that branch prediction storage <b>70</b> may be configured with fewer rows than instruction cache <b>16</b>. For example, branch prediction storage <b>70</b> may include \u00bc the number of rows of instruction cache <b>16</b>. In such a case, the address bits which are index bits of instruction cache <b>16</b> but which are not index bits of branch prediction storage <b>70</b> may be stored with the branch prediction information and checked against the corresponding bits of the fetch address to confirm that the branch prediction information is associated with the row of instruction cache <b>16</b> which is being accessed.</p><p>Way multiplexor <b>72</b> selects one of the sets of branch prediction information P<sub>00</sub>-P<sub>31 </sub>based upon the way selection provided from instruction cache <b>16</b> and the group of instruction bytes referenced by the fetch address. In the embodiment shown, for example, a 32 byte cache line is divided into two 16 byte groups. Therefore, the fifth least significant bit of the address is used to select which of the two groups contains the fetch address. If the fifth least significant bit is zero, then the first group of contiguous instruction bytes is selected. If the fifth least significant bit is one, then the second group of contiguous instruction bytes is selected. It is noted that the way selection provided upon way select bus <b>88</b> may be a way prediction produced by a branch prediction from the previous clock cycle, according to one embodiment. Alternatively, the way selection may be generated via tag comparisons between the fetch address and the address tags identifying the cache lines stored in each way of the instruction cache. It is noted that an address tag is the portion of the address which is not an offset within the cache line nor an index into the instruction cache.</p><p>The selected prediction block provided by way multiplexor <b>72</b> includes branch selectors for each byte in the group of contiguous instruction bytes, as well as branch predictions BP1 and BP2. The branch selectors are provided to branch selector multiplexor <b>74</b>, which selects one of the branch selectors based upon selection controls provided by decoder <b>84</b>. Decoder <b>84</b> decodes the offset of the fetch address into the group of contiguous instruction bytes to select the corresponding branch selector. For example, if a group of contiguous instruction bytes is 16 bytes, then decoder <b>84</b> decodes the four least significant bits of the fetch address. In this manner, a branch selector is chosen.</p><p>The selected branch selector is used to provide selection controls to branch prediction multiplexor <b>76</b>, sequential/return multiplexor <b>78</b>, and final prediction multiplexor <b>80</b>. In one embodiment, the encoding of the branch selector can be used directly as the multiplexor select controls. In other embodiments, a logic block may be inserted between branch selector multiplexor <b>74</b> and multiplexors <b>76</b>, <b>78</b>, and <b>80</b>. For the embodiment shown, branch selectors comprise two bits. One bit of the selected branch selector provides the selection control for prediction multiplexor <b>76</b> and sequential/return selector <b>78</b>. The other bit provides a selection control for final prediction multiplexor <b>80</b>. A branch prediction is thereby selected from the multiple branch predictions stored in branch prediction storage <b>70</b> corresponding to the group of contiguous instruction bytes being fetched, the sequential address of the group of contiguous instruction bytes sequential to the group of contiguous instruction bytes being fetched, and a return stack address from a return stack structure. It is noted that multiplexors <b>76</b>, <b>78</b>, and <b>80</b> may be combined into a single 4 to 1 multiplexor for which the selected branch selector provides selection controls to select between the two branch predictions from branch prediction storage <b>70</b>, the sequential address, and the return address.</p><p>The return stack structure (not shown) is used to store return addresses corresponding to subroutine call instructions previously fetched by microprocessor <b>10</b>. In one embodiment, the branch predictions stored by branch prediction storage <b>70</b> include an indication that the branch prediction corresponds to a subroutine call instruction. Subroutine call instructions are a subset of branch instructions which save the address of the sequential instruction (the return address) in addition to redirecting the instruction stream to the target address of the subroutine call instruction. For example, the in the x86 microprocessor architecture, the subroutine call instruction (CALL) pushes the return address onto the stack indicated by the ESP register.</p><p>A subroutine return instruction is another subset of the branch instructions. The subroutine return instruction uses the return address saved by the most recently executed subroutine call instruction as a target address. Therefore, when a branch prediction includes an indication that the branch prediction corresponds to a subroutine call instruction, the sequential address to the subroutine call instruction is placed at the top of the return stack. When a subroutine return instruction is encountered (as indicted by a particular branch selector encoding), the address nearest the top of the return stack which has not previously been used as a prediction is used as the prediction of the address. The address nearest the top of the return stack which has not previously been used as a prediction is conveyed by the return stack upon return address bus <b>92</b> (along with the predicted way of the return address, provided to the return stack similar to its provision upon way select bus <b>88</b>). Branch prediction unit <b>14</b> informs the return stack when the return address is selected as the prediction. Additional details regarding an exemplary return stack structure may be found in the commonly assigned, co-pending patent application entitled: \u201cSpeculative Return Address Prediction Unit for a Superscalar Microprocessor\u201d, Ser. No. 08/550,296, filed Oct. 30, 1995 by Mahalingaiah, et al. The disclosure of the referenced patent application is incorporated herein by reference in its entirety.</p><p>The sequential address is provided by instruction cache <b>16</b>. The sequential address identifies the next group of contiguous instruction bytes within main memory to the group of instruction bytes indicated by the fetch address upon fetch address bus <b>86</b>. It is noted that, according to one embodiment, a way prediction is supplied for the sequential address when the sequential address is selected. The way prediction may be selected to be the same as the way selected for the fetch address. Alternatively, a way prediction for the sequential address may be stored within branch prediction storage <b>70</b>.</p><p>As mentioned above, update logic block <b>82</b> is configured to update a prediction block upon detection of a branch misprediction or upon detection of a branch instruction while predecoding the corresponding group of contiguous instruction bytes in prefetch/predecode unit <b>12</b>. The prediction block corresponding to each branch prediction is stored in update logic block <b>82</b> as the prediction is performed. A branch tag is conveyed along with the instructions being fetched (via a branch tag bus <b>102</b>), such that if a misprediction is detected or a branch instruction is detected during predecoding, the corresponding prediction block can be identified via the branch tag. In one embodiment, the prediction block as shown in FIG. 5 is stored, as well as the index of the fetch address which cause the prediction block to be fetched and the way in which the prediction block is stored.</p><p>When a branch misprediction is detected, the corresponding branch tag is provided upon mispredict bus <b>98</b> from either the functional unit <b>24</b> which executes the branch instruction or from decode units <b>20</b>. If decode units <b>20</b> provide the branch tag, then the misprediction is of the previously undetected type (e.g. there are more branch instructions in the group than can be predicted using the corresponding branch predictions). Decode units <b>20</b> detect mispredictions of unconditional branch instructions (i.e. branch instructions which always select the target address) Functional units <b>24</b> may detect a misprediction due to a previously undetected conditional branch instruction or due to an incorrect taken/not-taken prediction. Update logic <b>82</b> selects the corresponding prediction block out of the aforementioned storage. In the case of a previously undetected branch instruction, one of the branch predictions within the prediction block is assigned to the previously undetected branch instruction. According to one embodiment, the algorithm for selecting one of the branch predictions to store the branch prediction for the previously undetected branch instruction is as follows: If the branch instruction is a subroutine return instruction, the branch selector for the instruction is selected to be the value indicating the return stack. Otherwise, a branch prediction which is currently predicted not-taken is selected. If each branch prediction is currently predicted-taken, then a branch prediction is randomly selected. The branch selector for the new prediction is set to indicate the selected branch prediction. Additionally, the branch selectors corresponding to bytes between the first branch instruction prior to the newly detected branch instruction and the newly detected branch instruction are set to the branch selector corresponding to the new prediction. FIG. 7 below describes one method for updating the branch selectors. For a mispredicted taken prediction which causes the prediction to become predicted not-taken, the branch selectors corresponding to the mispredicted prediction are set to the branch selector corresponding to the byte subsequent to the mispredicted branch instruction. In this manner, a prediction for a subsequent branch instruction will be used if the instructions are fetched again at a later clock cycle.</p><p>When prefetch/predecode unit <b>12</b> detects a branch instruction while predecoding a group of contiguous instruction bytes, prefetch/predecode unit <b>12</b> provides the branch tag for the group of contiguous instruction bytes if the predecoding is performed because invalid predecode information is stored in the instruction cache for the cache line (case (i)). Alternatively, if the predecoding is being performed upon a cache line being fetched from the main memory subsystem, prefetch/predecode unit <b>12</b> provides the address of the group of contiguous instruction bytes being predecoded, the offset of the end byte of the branch instruction within the group, and the way of the instruction cache selected to store the group (case (ii)). In case (i), the update is performed similar to the branch misprediction case above. In case (ii), there is not yet a valid prediction block stored in branch prediction storage <b>70</b> for the group of instructions. For this case, update logic block <b>82</b> initializes the branch selectors prior to the detected branch to the branch selector selected for the detected branch. Furthermore, the branch selectors subsequent to the detected branch are initialized to the sequential value. Alternatively, each of the branch selectors may be initialized to sequential when the corresponding cache line in instruction cache <b>16</b> is allocated, and subsequently updated via detection of a branch instructions during predecode in a manner similar to case (i).</p><p>Upon generation of an update, update logic block <b>82</b> conveys the updated prediction block, along with the fetch address index and corresponding way, upon update bus <b>96</b> for storage in branch prediction storage <b>70</b>. It is noted that, in order to maintain branch prediction storage <b>70</b> as a single ported storage, branch prediction storage <b>70</b> may employ a branch holding register. The updated prediction information is stored into the branch holding register and updated into the branch prediction storage upon an idle cycle on fetch address bus <b>86</b>. An exemplary cache holding register structure is described in the commonly assigned, co-pending patent application entitled: \u201cDelayed Update Register for an Array\u201d, U.S. Ser. No. 08/481,914, filed Jun. 7, 1995, by Tran, et al., incorporated herein by reference in its entirety.</p><p>It is noted that a correctly predicted branch instruction may result in an update to the corresponding branch prediction as well. A counter indicative of previous executions of the branch instruction (used to form the taken/not-taken prediction of the branch instruction) may need to be incremented or decremented, for example. Such updates are performed upon retirement of the corresponding branch prediction. Retirement is indicated via a branch tag upon retire tag bus <b>104</b> from reorder buffer <b>32</b>.</p><p>It is noted that the structure of FIG. 4 may be further accelerated through the use of a predicted branch selector. The predicted branch selector is stored with each prediction block and is set to the branch selector selected in a previous fetch of the corresponding group of contiguous instruction bytes. The predicted branch selector is used to select the branch prediction, removing branch selector multiplexor <b>74</b> from the path of branch prediction generation. Branch selector multiplexor <b>74</b> is still employed, however, to verify the selected branch selector is equal to the predicted branch selector. If the selected branch selector and the predicted branch selector are not equal, then the selected branch selector is used to provide the correct branch prediction during the succeeding clock cycle and the fetch of the incorrect branch prediction is cancelled.</p><p>Turning now to FIG. 5, an exemplary prediction block <b>110</b> employed by one embodiment of the branch prediction unit <b>14</b> as shown in FIG. 4 is shown. Prediction block <b>110</b> includes a set of branch selectors <b>112</b>, a first branch prediction (BP<b>1</b>) <b>114</b>, and a second branch prediction (BP<b>2</b>) <b>116</b>. Set of branch selectors <b>112</b> includes a branch selector for each byte of the group of contiguous instruction bytes corresponding to prediction block <b>110</b>.</p><p>First branch prediction <b>114</b> is shown in an exploded view in FIG. <b>5</b>. Second branch prediction <b>116</b> is configured similarly. First branch prediction <b>114</b> includes an index <b>118</b> for the cache line containing the target address, and a way selection <b>120</b> for the cache line as well. According to one embodiment, index <b>118</b> includes the offset portion of the target address, as well as the index. Index <b>118</b> is concatenated with the tag of the way indicated by way selection <b>120</b> to form the branch prediction address. Additionally, a prediction counter <b>122</b> is stored for each branch prediction. The prediction counter is incremented each time the corresponding branch instruction is executed and is taken, and is decremented each time the corresponding branch instruction is executed and is not-taken. The most significant bit of the prediction counter is used as the taken/not-taken prediction. If the most significant bit is set, the branch instruction is predicted taken. Conversely, the branch instruction is predicted not-taken if the most significant bit is clear. In one embodiment, the prediction counter is a two bit saturating counter. The counter saturates when incremented at binary \u201811\u2019 and saturates when decremented at a binary \u201801\u2019. In another embodiment, the prediction counter is a single bit which indicates a strong (a binary one) or a weak (a binary zero) taken prediction. If a strong taken prediction is mispredicted, it becomes a weak taken prediction. If a weak taken prediction is mispredicted, the branch becomes predicted not taken and the branch selector is updated (i.e. the case of a mispredicted branch that becomes not-taken). Finally, a call bit <b>124</b> is included in first branch prediction <b>114</b>. Call bit <b>124</b> is indicative, when set, that the corresponding branch instruction is a subroutine call instruction. If call bit <b>124</b> is set, the current fetch address and way are stored into the return stack structure mentioned above.</p><p>Turning next to FIG. 6, a table <b>130</b> illustrating an exemplary branch selector encoding is shown. A binary encoding is listed (most significant bit first), followed by the branch prediction which is selected when the branch selector is encoded with the corresponding value. As table <b>130</b> illustrates, the least significant bit of the branch selector can be used as a selection control for branch prediction multiplexor <b>76</b> and sequential/return multiplexor <b>78</b>. If the least significant bit is clear, then the first branch prediction is selected by branch prediction multiplexor <b>76</b> and the sequential address is selected by sequential/return multiplexor <b>78</b>. On the other hand, the second branch prediction is selected by branch prediction multiplexor <b>76</b> and the return address is selected by sequential/return multiplexor if the least significant bit is set. Furthermore, the most significant bit of the branch selector can be used as a selection control for final prediction multiplexor <b>80</b>. If the most significant bit is set, the output of branch prediction multiplexor <b>76</b> is selected. If the most significant bit is clear, the output of sequential/return multiplexor <b>78</b> is selected.</p><p>Turning now to FIG. 7, a flow chart depicting the steps employed to update the branch selectors of a group of contiguous instruction bytes in response to a mispredicted branch instruction is shown. Updating due to a branch instruction discovered during predecoding may be performed similarly. The misprediction may be the result of detecting a branch instruction for which prediction information is not stored in branch prediction storage <b>70</b>, or may be the result of an incorrect taken/not-taken prediction which causes the corresponding prediction counter to indicate not-taken.</p><p>Upon detection of the misprediction, branch prediction unit <b>14</b> uses an \u201cend pointer\u201d: the offset of the end byte of the mispredicted branch instruction within the corresponding group of contiguous instruction bytes. Additionally, the prediction block is selected for update using the branch tag received in response to the misprediction. Branch prediction unit <b>14</b> decodes the end pointer into an update mask (step <b>140</b>). The update mask comprises a binary digit for each byte within the group of continuous instruction bytes. Digits corresponding to bytes prior to and including the end byte of the branch instruction within the cache line are set, and the remaining digits are clear.</p><p>Branch prediction unit <b>14</b> identifies the current branch selector. For mispredicted taken/not-taken predictions, the current branch selector is the branch selector corresponding to the mispredicted branch instruction. For misprediction due to an undetected branch, the current branch selector is the branch selector corresponding to the end byte of the undetected branch instruction. The current branch selector is XNOR'd with each of the branch selectors to create a branch mask (step <b>142</b>). The branch mask includes binary digits which are set for each byte having a branch selector which matches the current branch selector and binary digits which are clear for each byte having a branch selector which does not match the current branch selector.</p><p>The update mask created in step <b>140</b> and the branch mask created in step <b>142</b> are subsequently ANDed, producing a final update mask (step <b>144</b>). The final update mask includes binary digits which are set for each byte of the group of contiguous instruction bytes which is to be updated to the new branch selector. For a mispredicted taken branch, the new branch selector is the branch selector of the byte subsequent to the end byte of the mispredicted taken branch instruction. For an undetected branch, the new branch selector is the branch selector indicating the branch prediction storage assigned to the previously undetected branch by update logic block <b>82</b>.</p><p>An extended mask is also generated (steps <b>146</b> and <b>148</b>). The extended mask indicates which branch selectors are to be erased because the branch prediction corresponding to the branch selector has been reallocated to the newly discovered branch instruction or because the branch prediction now indicates not taken. The extended mask is generated by first creating a second branch mask similar to the branch mask, except using the new branch selector instead of the current branch selector (i.e. the mask is created by XNORing the branch selectors corresponding to the cache line with the new branch selector (step <b>146</b>)). The resulting mask is then ANDed with the inversion of the final update mask to create the extended mask (step <b>148</b>). Branch selectors corresponding to bits in the extended mask which are set are updated to indicate the branch selector of the byte immediately subsequent to the last byte for which a bit in the extended mask is set. In this manner, the branch prediction formerly indicated by the branch selector is erased and replaced with the following branch selector within the cache line. During a step <b>150</b>, the branch selectors are updated in response to the final update mask and the extended mask.</p><p>Turning now to FIG. 8, an example of the update of the branch selectors using the steps shown in the flowchart of FIG. 7 is shown. Each byte position is listed (reference number <b>160</b>), followed by a set of branch selectors prior to update (reference number <b>162</b>). In the initial set of branch selectors <b>162</b>, a subroutine return instruction ends at byte position <b>1</b> as well as a first branch instruction ending at byte position <b>8</b> (indicated by branch selector number <b>3</b>) and a second branch instruction ending at byte position <b>11</b> (indicated by branch selector number <b>2</b>).</p><p>For the example of FIG. 8, a previously undetected branch instruction is detected ending at byte position <b>6</b>. The second branch prediction is selected to represent the branch prediction for the previously undetected branch instruction. The update mask is generated as shown at reference number <b>164</b>, given the end pointer of the previously undetected branch instruction is byte position <b>6</b>. Since the example is a case of misprediction due to a previously undetected branch instruction and the branch selector at byte position <b>6</b> is \u201c3\u201d, the current branch selector is \u201c3\u201d. The XNORing of the current branch selector with the initial branch selectors <b>162</b> yields the branch mask depicted at reference number <b>166</b>. The subsequent ANDing of the update mask and the branch mask yields the final update mask shown at reference number <b>168</b>. As indicated by final update mask <b>168</b>, byte positions <b>2</b> through <b>6</b> are updated to the new branch selector.</p><p>The second branch mask is generated by XNORing the new branch selector with initial branch selectors <b>162</b> (reference number <b>170</b>). The new branch selector is \u201c3\u201d, so second branch mask <b>170</b> is equal to branch mask <b>166</b> in this example. ANDing branch mask <b>170</b> with the logical inversion of the final update mask <b>168</b> produces the extended mask shown at reference number <b>172</b>. As extended mask <b>172</b> indicates, byte positions <b>7</b> and <b>8</b> are to be updated to indicate the first branch prediction, since the second branch prediction has been assigned to the branch instruction ending at byte position <b>6</b> and the branch instruction represented by the first branch prediction ends at byte <b>11</b>. An updated set of branch selectors is shown at reference number <b>174</b>. The updated set of branch selectors at reference number <b>174</b> reflects choosing the branch prediction corresponding to branch selector \u201c3\u201d for storing branch prediction information corresponding to the previously undetected branch instruction.</p><p>Turning next to FIG. 9, a second example of the update of the branch selectors using the steps shown in the flowchart of FIG. 7 is shown. Similar to the example of FIG. 8, each byte position is listed (reference number <b>160</b>), followed by a set of branch selectors prior to update (reference number <b>162</b>). In the initial set of branch selectors <b>162</b>, a subroutine return instruction ends at byte position <b>1</b> as well as a first branch instruction ending at byte position <b>8</b> (indicated by branch selector number <b>3</b>) and a second branch instruction ending at byte position <b>11</b> (indicated by branch selector number <b>2</b>).</p><p>For the example of FIG. 9, a previously undetected branch instruction is again detected ending at byte position <b>6</b>. However, the first branch prediction is selected to represent the branch prediction for the previously undetected branch instruction (as opposed to the second branch prediction as shown in FIG. <b>8</b>). Since the misprediction is at the same byte position as FIG. 8, the same update mask, branch mask, and final update mask are generated as in FIG. 8 (reference numbers <b>164</b>, <b>166</b>, and <b>168</b>).</p><p>The second branch mask is generated by XNORing the new branch selector with initial branch selectors <b>162</b> (reference number <b>180</b>). The new branch selector is \u201c2\u201d in this example, so second branch mask <b>180</b> indicates byte positions <b>9</b> through <b>11</b>. ANDing branch mask <b>180</b> with the logical inversion of the final update mask <b>168</b> produces the extended mask shown at reference number <b>182</b>. As extended mask <b>182</b> indicates, byte positions <b>9</b> through <b>11</b> are to be updated to indicate the branch prediction following byte position <b>11</b> (i.e. the sequential branch prediction), since the first branch prediction has been assigned to the branch instruction ending at byte position <b>6</b> and the branch instruction represented by the second branch prediction ends at byte <b>8</b>. An updated set of branch selectors is shown at reference number <b>184</b>. The updated set of branch selectors at reference number <b>184</b> reflects choosing the branch prediction corresponding to branch selector \u201c2\u201d for storing branch prediction information corresponding to the previously undetected branch instruction.</p><p>Turning now to FIG. 10, a third example of the update of the branch selectors using the steps shown in the flowchart of FIG. 7 is shown. Similar to the example of FIG. 8, each byte position is listed (reference number <b>160</b>), followed by a set of branch selectors prior to update (reference number <b>162</b>). In the initial set of branch selectors <b>162</b>, a subroutine return instruction ends at byte position <b>1</b> as well as a first branch instruction ending at byte position <b>8</b> (indicated by branch selector number <b>3</b>) and a second branch instruction ending at byte position <b>11</b> (indicated by branch selector number <b>2</b>).</p><p>For the example of FIG. 10, a the branch instruction ending at byte position <b>8</b> is mispredicted as taken, and the ensuing update of the second branch prediction causes the prediction counter to indicate not-taken. Since the branch prediction is not taken, the branch selectors indicating the branch prediction should be updated to indicate the subsequent branch instruction (or update to indicate sequential, if there is no subsequent branch instruction within the group of contiguous instruction bytes). In cases in which a branch prediction becomes not-taken, the end pointer of the \u201cnew\u201d branch instruction is invalid, since there is no newly detected branch instruction. Therefore, the update mask is generated as all zero (reference number <b>190</b>). Since the current branch selector is \u201c3\u201d, the branch mask is generated as shown at reference number <b>191</b>. Therefore, the final update mask (reference number <b>192</b>) is all zeros.</p><p>The second branch mask is generated by XNORing the new branch selector with initial branch selectors <b>162</b> (reference number <b>194</b>). The new branch selector is set to \u201c3\u201d in this example, such that each of the branch selectors coded to \u201c3\u201d are indicated by second branch mask <b>194</b>. ANDing branch mask <b>180</b> with the logical inversion of the final update mask <b>192</b> produces the extended mask shown at reference number <b>196</b>. As extended mask <b>196</b> indicates, byte positions <b>2</b> through <b>8</b> are to be updated to indicate the branch prediction following byte position <b>8</b> (i.e. the first branch prediction), since the first branch prediction is assigned to the branch instruction ending at byte position <b>11</b>. An updated set of branch selectors is shown at reference number <b>198</b>. The updated set of branch selectors at reference number <b>198</b> reflects deleting branch selector \u201c3\u201d from the set of branch selectors corresponding to the group of contiguous instruction bytes, since the first branch prediction is not storing a predicted-taken branch prediction.</p><p>As FIG. 10 illustrates, the procedure for removing a branch selector when a prediction indicates not-taken is similar to the procedure for reassigning a branch prediction. The differences in the two procedures are that the update mask for removing a branch selector is always generated as zeros, and the current branch selector is provided as the \u201cnew\u201d branch selector in order to generate the extended mask.</p><p>It is noted that, although the preceding discussion has focused on an embodiment in which a variable byte-length instruction set is employed (e.g. the x86 instruction set), branch selectors may be employed in branch prediction mechanisms for fixed byte length instruction sets as well. An embodiment for fixed byte length instruction sets may store a branch selector for each instruction, since the instructions are stored at constant offsets within cache lines or groups of contiguous instruction bytes.</p><p>It is further noted that, although the embodiment above shows multiple branch predictions per group of contiguous instruction bytes, branch selectors may be employed even when only one branch prediction is stored for each group. The branch selectors in this case may be a single bit. If the bit is set, then the branch prediction is selected. If the bit is clear, then the sequential prediction is selected.</p><p>It is noted that, as referred to above, a previously undetected branch instruction is a branch instruction represented by none of the branch predictions within the corresponding prediction block. The previously undetected branch instruction may be previously undetected (i.e. not executed since the corresponding cache line was stored into instruction cache <b>16</b>). Alternatively, the branch prediction corresponding to the previously undetected branch instruction may have been reassigned to a different branch instruction within the corresponding group of contiguous instruction bytes.</p><p>Turning now to FIG. 11, a computer system <b>200</b> including microprocessor <b>10</b> is shown. Computer system <b>200</b> further includes a bus bridge <b>202</b>, a main memory <b>204</b>, and a plurality of input/output (I/O) devices <b>206</b>A-<b>206</b>N. Plurality of I/O devices <b>206</b>A-<b>206</b>N will be collectively referred to as I/O devices <b>206</b>. Microprocessor <b>10</b>, bus bridge <b>202</b>, and main memory <b>204</b> are coupled to a system bus <b>208</b>. I/O devices <b>206</b> are coupled to an I/O bus <b>210</b> for communication with bus bridge <b>202</b>.</p><p>Bus bridge <b>202</b> is provided to assist in communications between I/O devices <b>206</b> and devices coupled to system bus <b>208</b>. I/O devices <b>206</b> typically require longer bus clock cycles than microprocessor <b>10</b> and other devices coupled to system bus <b>208</b>. Therefore, bus bridge <b>202</b> provides a buffer between system bus <b>208</b> and input/output bus <b>210</b>. Additionally, bus bridge <b>202</b> translates transactions from one bus protocol to another. In one embodiment, input/output bus <b>210</b> is an Enhanced Industry Standard Architecture (EISA) bus and bus bridge <b>202</b> translates from the system bus protocol to the EISA bus protocol. In another embodiment, input/output bus <b>210</b> is a Peripheral Component Interconnect (PCI) bus and bus bridge <b>202</b> translates from the system bus protocol to the PCI bus protocol. It is noted that many variations of system bus protocols exist. Microprocessor <b>10</b> may employ any suitable system bus protocol.</p><p>I/O devices <b>206</b> provide an interface between computer system <b>200</b> and other devices external to the computer system. Exemplary I/O devices include a modem, a serial or parallel port, a sound card, etc. I/O devices <b>206</b> may also be referred to as peripheral devices. Main memory <b>204</b> stores data and instructions for use by microprocessor <b>10</b>. In one embodiment, main memory <b>204</b> includes at least one Dynamic Random Access Memory (DRAM) and a DRAM memory controller.</p><p>It is noted that although computer system <b>200</b> as shown in FIG. 11 includes one bus bridge <b>202</b>, other embodiments of computer system <b>200</b> may include multiple bus bridges <b>202</b> for translating to multiple dissimilar or similar I/O bus protocols. Still further, a cache memory for enhancing the performance of computer system <b>200</b> by storing instructions and data referenced by microprocessor <b>10</b> in a faster memory storage may be included. The cache memory may be inserted between microprocessor <b>10</b> and system bus <b>208</b>, or may reside on system bus <b>208</b> in a \u201clookaside\u201d configuration.</p><p>Although various components above have been described as multiplexors, it is noted that multiple multiplexors, in series or in parallel, may be employed to perform the selection represented by the multiplexors shown.</p><p>It is still further noted that the present discussion may refer to the assertion of various signals. As used herein, a signal is \u201casserted\u201d if it conveys a value indicative of a particular condition. Conversely, a signal is \u201cdeasserted\u201d if it conveys a value indicative of a lack of a particular condition. A signal may be defined to be asserted when it conveys a logical zero value or, conversely, when it conveys a logical one value. Additionally, various values have been described as being discarded in the above discussion. A value may be discarded in a number of manners, but generally involves modifying the value such that it is ignored by logic circuitry which receives the value. For example, if the value comprises a bit, the logic state of the value may be inverted to discard the value. If the value is an n-bit value, one of the n-bit encodings may indicate that the value is invalid. Setting the value to the invalid encoding causes the value to be discarded. Additionally, an n-bit value may include a valid bit indicative, when set, that the n-bit value is valid. Resetting the valid bit may comprise discarding the value. Other methods of discarding a value may be used as well.</p><p>Table 1 below indicates fast path, double dispatch, and MROM instructions for one embodiment of microprocessor <b>10</b> employing the x86 instruction set:</p><p><tables id=\"TABLE-US-00002\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 1</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">x86 Fast Path, Double Dispatch, and MROM</entry></row><row><entry morerows=\"0\" valign=\"top\">Instructions</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"28PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"84PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"105PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">X86 Instruction</entry><entry morerows=\"0\" valign=\"top\">Instruction Category</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AAA</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AAD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AAM</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AAS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ADC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ADD</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AND</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ARPL</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BOUND</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BSF</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BSR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BSWAP</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BT</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BTC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BTR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BTS</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CALL</entry><entry morerows=\"0\" valign=\"top\">fast path/double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CBW</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CWDE</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CLC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CLD</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CLI</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CLTS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMP</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPXCHG</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPXCHG8B</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CPUID</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CWD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CWQ</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">DDA</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">DAS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">DEC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">DIV</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ENTER</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">HLT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">IDIV</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">IMUL</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">IN</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INTO</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INVD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INVLPG</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">IRET</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">IRETD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Jcc</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">JCXZ</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">JECXZ</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">JMP</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LAHF</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LAR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LDS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LES</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LFS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LGS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LSS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LEA</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LEAVE</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LGDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LIDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LLDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LMSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LODS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LODSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LODSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LODSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LOOP</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LOOPcond</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LSL</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LTR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOV</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVCC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOV.CR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOV.DR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVSX</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVZX</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MUL</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">NEG</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">NOP</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">NOT</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OUT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OUTS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OUTSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OUTSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OUTSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">POP</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">POPA</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">POPAD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">POPF</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">POPFD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">PUSH</entry><entry morerows=\"0\" valign=\"top\">fast path/double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">PUSHA</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">PUSHAD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">PUSHF</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">PUSHFD</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">RCL</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">RCR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ROL</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ROR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">RDMSR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">REP</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">REPE</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">REPZ</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">REPNE</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">REPNZ</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">RET</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">RSM</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SAHF</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SAL</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SAR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SHL</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SHR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SBB</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SCAS</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SCASB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SCASW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SCASD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SETcc</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SGDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SIDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SHLD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SHRD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SLDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SMSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STD</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STI</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STOS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STOSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STOSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STOSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SUB</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">TEST</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">VERR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">VERW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">WBINVD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">WRMSR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">XADD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">XCHG</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">XLAT</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">XLATB</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">XOR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"left\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" valign=\"top\">Note: Instructions including an SIB byte are also considered double dispatch instructions. </entry></row></tbody></tgroup></table></tables></p><p>It is noted that a superscalar microprocessor in accordance with the foregoing may further employ the latching structures as disclosed within the commonly assigned patent application entitled \u201cConditional Latching Mechanism and Pipelined Microprocessor Employing the Same\u201d, U.S. Ser. No. 08/400,608 field Mar. 8, 1995, by Pflum et al., which was abandoned for U.S. patent application Ser. No. 08/744,707, filed Oct. 31, 1996, now U.S. Pat. No. 5,831,462, issued Nov. 3, 1998. The disclosure of this patent application is incorporated herein by reference in its entirety.</p><p>It is further noted that aspects regarding array circuitry may be found in the commonly assigned patent application entitled \u201cHigh Performance Ram Array Circuit Employing Self-Time Clock Generator for Enabling Array Access\u201d, U.S. Ser. No. 08/473,103 filed Jun. 7, 1995 by Tran., now U.S. Pat. No. 5,619,464, issued Apr. 9, 1997. The disclosure of this patent application is incorporated herein by reference in its entirety.</p><p>It is additionally noted that other aspects regarding superscalar microprocessors may be found in the following co-pending, commonly assigned patent applications: \u201cLinearly Addressable Microprocessor Cache\u201d, U.S. Ser. No. 08/146,381, filed Oct. 29, 1993 by Witt, which was abandoned for U.S. application Ser. No. 08/506,509, filed Jul. 24, 1995, now U.S. Pat. No. 5,623,619, issued Apr. 22, 1997; \u201cSuperscalar Microprocessor Including a High Performance Instruction Alignment Unit\u201d, U.S. Ser. No. 08/377,843, filed Jan. 25, 1995 by Witt, et al, which was abandoned for U.S. application Ser. No. 08/887,818, filed Jun. 30, 1997, now U.S. Pat. No. 5,819,057, issued Oct. 6, 1998; \u201cA Way Prediction Structure\u201d, U.S. Ser. No. 08/522,181, filed Aug. 31, 1995 by Roberts, et al, which was abandoned for U.S. application Ser. No. 08/884,819, filed Jun. 30, 1997, now U.S. Pat. No. 5,845,323, issued Dec. 1, 1998; \u201cA Data Cache Capable of Performing Store Accesses in a Single Clock Cycle\u201d, U.S. Ser. No. 08/521,627, filed Aug. 31, 1995 by Witt, et al, now U.S. Pat. No. 5,860,104, issued Jan. 12, 1999; \u201cA Parallel and Scalable Instruction Scanning Unit\u201d, U.S. Ser. No. 08/475,400, filed Jun. 7, 1995 by Narayan, which was abandoned for U.S. Pat. application Ser. No. 08/915,092, filed Aug. 20, 1997, now U.S. Pat. No. 5,875,315, issued Feb. 23, 1999; and \u201cAn Apparatus and Method for Aligning Variable-Byte Length Instructions to a Plurality of Issue Positions\u201d, U.S. Ser. No. 08/582,473, filed Jan. 2, 1996 by Tran, et al., now U.S. Pat. No. 5,822,559, issued Oct. 13, 1998. The disclosure of these patent applications are incorporated herein by reference in their entirety.</p><p>In accordance with the above disclosure, a branch prediction mechanism using branch selectors is described. The branch prediction mechanism quickly locates the branch prediction corresponding to a given fetch address by selecting the branch selector corresponding to the byte indicated by the given fetch address and selecting the branch prediction indicated by that branch selector. The branch prediction mechanism may be capable of operating at a higher frequency than previous branch prediction mechanisms.</p><p>Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Thang M.", "last_name": "Tran", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "ADVANCED MICRO DEVICES, INC."}, {"first_name": "", "last_name": "GLOBALFOUNDRIES U.S. INC.", "name": ""}, {"first_name": "", "last_name": "GLOBALFOUNDRIES INC.", "name": ""}, {"first_name": "", "last_name": "AMD TECHNOLOGIES HOLDINGS, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F   9/00"}, {"primary": false, "label": "G06F   9/44"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/38        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "712239"}, {"primary": false, "label": "712E09057"}, {"primary": false, "label": "712E09051"}], "ecla_classes": [{"label": "G06F   9/38B2B"}, {"label": "G06F   9/38E2D"}], "cpc_classes": [{"label": "G06F   9/3844"}, {"label": "G06F   9/3844"}, {"label": "G06F   9/3806"}, {"label": "G06F   9/3806"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1996-11-19", "application_date": "1999-09-22", "family_members": [{"ucid": "WO-1998022873-A1", "titles": [{"lang": "FR", "text": "DISPOSITIF D'ANTICIPATION DE SAUT UTILISANT DES SELECTEURS DE SAUTS POUR SELECTIONNER UNE ANTICIPATION DE SAUT"}, {"lang": "EN", "text": "BRANCH PREDICTION MECHANISM EMPLOYING BRANCH SELECTORS TO SELECT A BRANCH PREDICTION"}]}, {"ucid": "DE-69710503-D1", "titles": [{"lang": "EN", "text": "BRANCH FORECASTING MECHANISM WITH SELECTOR SWITCHES FOR SELECTING A BRANCHING FORECAST"}, {"lang": "DE", "text": "VERZWEIGUNGSVORHERSAGEMECHANISMUS MIT AUSWAHLSCHALTERN ZUM AUSW\u00c4HLEN EINER VERZWEIGUNGSVORHERSAGE"}]}, {"ucid": "DE-69710503-T2", "titles": [{"lang": "EN", "text": "BRANCH FORECASTING MECHANISM WITH SELECTOR SWITCHES FOR SELECTING A BRANCHING FORECAST"}, {"lang": "DE", "text": "VERZWEIGUNGSVORHERSAGEMECHANISMUS MIT AUSWAHLSCHALTERN ZUM AUSW\u00c4HLEN EINER VERZWEIGUNGSVORHERSAGE"}]}, {"ucid": "JP-3803723-B2", "titles": [{"lang": "JA", "text": "\u5206\u5c90\u4e88\u6e2c\u3092\u9078\u629e\u3059\u308b\u5206\u5c90\u30bb\u30ec\u30af\u30bf\u3092\u63a1\u7528\u3059\u308b\u5206\u5c90\u4e88\u6e2c\u6a5f\u69cb"}, {"lang": "EN", "text": "Branch prediction mechanism that employs a branch selector that selects branch prediction"}]}, {"ucid": "EP-1008036-A1", "titles": [{"lang": "FR", "text": "DISPOSITIF D'ANTICIPATION DE SAUT UTILISANT DES SELECTEURS DE SAUTS POUR SELECTIONNER UNE ANTICIPATION DE SAUT"}, {"lang": "EN", "text": "BRANCH PREDICTION MECHANISM EMPLOYING BRANCH SELECTORS TO SELECT A BRANCH PREDICTION"}, {"lang": "DE", "text": "VERZWEIGUNGSVORHERSAGEMECHANISMUS MIT AUSWAHLSCHALTERN ZUM AUSW\u00c4HLEN EINER VERZWEIGUNGSVORHERSAGE"}]}, {"ucid": "US-6247123-B1", "titles": [{"lang": "EN", "text": "Branch prediction mechanism employing branch selectors to select a branch prediction"}]}, {"ucid": "JP-2001503899-A", "titles": [{"lang": "JA", "text": "\u5206\u5c90\u4e88\u6e2c\u3092\u9078\u629e\u3059\u308b\u5206\u5c90\u30bb\u30ec\u30af\u30bf\u3092\u63a1\u7528\u3059\u308b\u5206\u5c90\u4e88\u6e2c\u6a5f\u69cb"}, {"lang": "EN", "text": "A branch prediction mechanism employing a branch selector to select a branch prediction"}]}, {"ucid": "EP-1008036-B1", "titles": [{"lang": "FR", "text": "DISPOSITIF D'ANTICIPATION DE SAUT UTILISANT DES SELECTEURS DE SAUTS POUR SELECTIONNER UNE ANTICIPATION DE SAUT"}, {"lang": "EN", "text": "BRANCH PREDICTION MECHANISM EMPLOYING BRANCH SELECTORS TO SELECT A BRANCH PREDICTION"}, {"lang": "DE", "text": "VERZWEIGUNGSVORHERSAGEMECHANISMUS MIT AUSWAHLSCHALTERN ZUM AUSW\u00c4HLEN EINER VERZWEIGUNGSVORHERSAGE"}]}, {"ucid": "US-5995749-A", "titles": [{"lang": "EN", "text": "Branch prediction mechanism employing branch selectors to select a branch prediction"}]}, {"ucid": "US-5961638-A", "titles": [{"lang": "EN", "text": "Branch prediction mechanism employing branch selectors to select a branch prediction"}, {"lang": "EN", "text": "MICROPROCESSOR"}]}]}