{"patent_number": "US-6212544-B1", "publication_id": 72631911, "family_id": 25501232, "publication_date": "2001-04-03", "titles": [{"lang": "EN", "text": "Altering thread priorities in a multithreaded processor"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11168613\" source=\"national office\"><p>A system and method for performing computer processing operations in a data processing system includes a multithreaded processor and thread switch logic. The multithreaded processor is capable of switching between two or more threads of instructions which can be independently executed. Each thread has a corresponding state in a thread state register depending on its execution status. The thread switch logic contains a thread switch control register to store the conditions upon which a thread switch will occur. The thread switch logic has a time-out register which forces a thread switch when execution of the active thread in the multithreaded processor exceeds a programmable period of time. Thread switch logic also has a forward progress count register to prevent repetitive unproductive thread switching between threads in the multithreaded processor. Thread switch logic also is responsive to a software manager capable of changing the priority of the different threads and thus superseding thread switch events.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72536220\"><p>A system and method for performing computer processing operations in a data processing system includes a multithreaded processor and thread switch logic. The multithreaded processor is capable of switching between two or more threads of instructions which can be independently executed. Each thread has a corresponding state in a thread state register depending on its execution status. The thread switch logic contains a thread switch control register to store the conditions upon which a thread switch will occur. The thread switch logic has a time-out register which forces a thread switch when execution of the active thread in the multithreaded processor exceeds a programmable period of time. Thread switch logic also has a forward progress count register to prevent repetitive unproductive thread switching between threads in the multithreaded processor. Thread switch logic also is responsive to a software manager capable of changing the priority of the different threads and thus superseding thread switch events.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00001\" num=\"1\"><claim-text>1. A method of computer processing, comprising:</claim-text><claim-text>(a) executing at least one of a plurality of threads of instructions in a multithreaded processor, each of said plurality of threads having a priority bit; and </claim-text><claim-text>(b) altering the relative priority bit of one of said plurality of threads using either or both a thread switch event experienced by said one of said plurality of threads and an instruction executed by said processor; </claim-text><claim-text>(c) determining the applicability of a plurality of hardware thread switch conditions that affect when said multithreaded processor switches threads. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00002\" num=\"2\"><claim-text>2. The method of claim <b>1</b>, further comprising:</claim-text><claim-text>(d) switching from one of the plurality of threads to another of the plurality based on the priority bit of each thread. </claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00003\" num=\"3\"><claim-text>3. The method of claim <b>1</b>, wherein a signal generated by hardware alters the priority bit of one or more threads.</claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00004\" num=\"4\"><claim-text>4. The method of claim <b>3</b>, wherein the signal results from an event external to the multithreaded processor.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00005\" num=\"5\"><claim-text>5. The method of claim <b>1</b>, wherein a thread switch manager alters the priority bit of one or more threads.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00006\" num=\"6\"><claim-text>6. The method of claim <b>1</b>, wherein the priority of one of the plurality of threads is higher than another of the plurality of threads.</claim-text></claim>"}, {"num": 7, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00007\" num=\"7\"><claim-text>7. The method of claim <b>1</b>, wherein at least two of the plurality of threads can have equal priority.</claim-text></claim>"}, {"num": 8, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00008\" num=\"8\"><claim-text>8. The method of claim <b>1</b>, wherein the priority of any of the plurality of threads can be high, medium, or low.</claim-text></claim>"}, {"num": 9, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00009\" num=\"9\"><claim-text>9. The method of claim <b>2</b>, wherein switching from a first of the plurality of threads to a second of the plurality occurs when the first thread has a lower priority than the second thread and the second thread is in a ready state.</claim-text></claim>"}, {"num": 10, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00010\" num=\"10\"><claim-text>10. The method of claim <b>1</b>, wherein switching from the one of the plurality of threads having a higher priority than another of the plurality of threads is disabled for one of said plurality of hardware thread switch conditions.</claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00011\" num=\"11\"><claim-text>11. A method of computer processing, comprising:</claim-text><claim-text>(a) executing at least one of a plurality of threads of instructions in a multithreaded processor, each of the threads having a priority bit in a thread state register in said multithreaded processor; </claim-text><claim-text>(b) altering the priority bit of one or more of the plurality of threads using a thread switch manager; </claim-text><claim-text>(c) switching execution from a first of the plurality of threads to a second of the plurality of threads if the first thread has a lower priority than the second thread and the second thread is in a ready state; </claim-text><claim-text>(d) not switching execution from the one of the plurality of threads based on the relative priority of said plurality of threads despite the occurrence of thread switch events which would otherwise cause a thread switch as determined by hardware thread switch conditions. </claim-text></claim>"}, {"num": 12, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00012\" num=\"12\"><claim-text>12. A thread switch manager for a computer processing system, comprising:</claim-text><claim-text>a multithreaded processor; </claim-text><claim-text>a plurality of threads of instructions comprising at least one active thread in the multithreaded processor; </claim-text><claim-text>at least one priority switch instruction; </claim-text><claim-text>at least one thread switch priority control bit in a hardware thread switch control register storing thread switch conditions in the multithreaded processor; </claim-text><claim-text>a plurality of priority state bits in a thread state register in the multithreaded processor, each of the plurality of priority state bits corresponding to each of the plurality of threads in the multithreaded processor wherein the priority switch instruction can change any of the plurality of priority state bits depending upon the at least one thread switch priority control bit in the thread switch control register. </claim-text></claim>"}, {"num": 13, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00013\" num=\"13\"><claim-text>13. The thread switch manager of claim <b>12</b>,</claim-text><claim-text>wherein the priority switch instruction changes one of the plurality of priority state bits and in response thereto at least one of the plurality of threads of instructions in the multithreaded processor changes priority. </claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00014\" num=\"14\"><claim-text>14. The thread switch manager of claim <b>13</b>, wherein the multithreaded processor switches processing from the active thread in response to the change of priority of the at least one thread of instructions if the at least one thread switch priority control bit is enabled.</claim-text></claim>"}, {"num": 15, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00015\" num=\"15\"><claim-text>15. The thread switch manager of claim <b>13</b>, wherein the plurality of priority state bits further comprises a plurality of sets, each one of the sets corresponding to a respective one of the plurality of threads of instructions.</claim-text></claim>"}, {"num": 16, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00016\" num=\"16\"><claim-text>16. The thread switch manager of claim <b>12</b>, wherein said first register and said second register are unitary.</claim-text></claim>"}, {"num": 17, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00017\" num=\"17\"><claim-text>17. The thread switch manager of claim <b>12</b>, wherein the priority switch instruction does not change any of a plurality of architected registers of the multithreaded processor.</claim-text></claim>"}, {"num": 18, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00018\" num=\"18\"><claim-text>18. The thread switch manager of claim <b>12</b>, wherein the priority switch instruction can execute on the multithreaded processor without illegal instruction interrupts.</claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00019\" num=\"19\"><claim-text>19. The thread switch manager of claim <b>18</b>, wherein the switch instruction is a no-op instruction.</claim-text></claim>"}, {"num": 20, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00020\" num=\"20\"><claim-text>20. A computer processor, comprising:</claim-text><claim-text>(a) means for processing a plurality of threads of instructions in a hardware multithreaded processor; </claim-text><claim-text>(b) means for storing a priority state of each of the plurality of threads in a first register; </claim-text><claim-text>(c) means for storing a plurality of thread switch conditions in a hardware thread switch control register in the multithreaded processor to cause the multithreaded processor to switch processing from one to another of the plurality of threads; </claim-text><claim-text>(d) means for changing the priority state of one of the plurality of threads based on a thread switch event experienced by said one of the plurality of threads or by a thread switch instruction; </claim-text><claim-text>(e) means for altering the applicability of at least one thread switch condition to said plurality of threads; </claim-text><claim-text>(f) means, responsive to the changing means and the altering means, for determining whether the processing means will switch processing from a first thread to a second thread of the plurality of threads. </claim-text></claim>"}, {"num": 21, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00021\" num=\"21\"><claim-text>21. The computer processor of claim <b>20</b> wherein said means for changing the priority state further comprises a signal from an interrupt request.</claim-text></claim>"}, {"num": 22, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00022\" num=\"22\"><claim-text>22. The computer processor of claim <b>20</b> wherein said means for changing the priority state comprises at least one software instruction to manipulate priority bits in at least one hardware register.</claim-text></claim>"}, {"num": 23, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6212544-B1-CLM-00023\" num=\"23\"><claim-text>23. A computer data processing system, comprising:</claim-text><claim-text>a hardware multithreaded processor capable of processing at least one thread of instructions and of switching between at least two threads of instructions; </claim-text><claim-text>the hardware multithreaded processor comprising a hardware thread switch control register to store thread switch conditions whereupon a thread switch event corresponding to a thread switch condition will result in the multithreaded processor switching between the at least two threads; </claim-text><claim-text>a plurality of internal memory units; </claim-text><claim-text>a system bus interconnecting the internal memory units to each other and to the multithreaded processor; </claim-text><claim-text>a plurality of external connections connecting the multithreaded processor to one or more of the following external devices: a memory device, a communication device, a computer network, and an input/output device; </claim-text><claim-text>a bus interface connecting the external connections to the multithreaded processor; and </claim-text><claim-text>a thread switch manager operably connected to the multithreaded processor to change priority of at least one thread of instruction based on a thread switch event experienced by said at least one thread of instruction and/or by a thread switch instruction interacting with a thread switch priority bit in a register thereby changing the applicability of the thread switch conditions to the at least two threads of instructions.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54522842\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>RELATED APPLICATION DATA</h4><p>The present invention relates to the following U.S. applications, the subject matter of which is hereby incorporated by reference: (1) U.S. Ser. No. 958,716 filed Oct. 23, 1997 entitled <i>Method and Apparatus for Selecting Thread Switch Events in a Multithreaded Processor, </i>Ser. No. 08/958,716, filed concurrently herewith; (2) U.S. Ser. No. 956,875 filed Oct. 23, 1997 entitled <i>An Apparatus and Method to Guarantee Forward Progress in a Multithreaded Processor, </i>Ser. No. 08/956,875, filed concurrently herewith; (3) U.S. Ser. No. 957,002 filed Oct. 23, 1997 entitled <i>Thread Switch Control in a Multithreaded Processor System, </i>Ser. No. 08/957,002, filed concurrently herewith; (4) U.S. Ser. No. 956,577 filed Oct. 23, 1997 entitled <i>Method and Apparatus to Force a Thread Switch in a Multithreaded Processor, </i>Ser. No. 08/956,577, filed concurrently herewith; (5) U.S. Ser. No. 773,572 filed Dec. 27, 1996 entitled <i>Background Completion of Instruction and Associated Fetch Request in a Multithread Processor, </i>(6) U.S. Ser. No. 761,378 filed Dec. 9, 1996 entitled <i>Multi</i>-<i>Entry Fully Associative Transition Cache; </i>(7) U.S. Ser. No. 761,380 filed Dec. 9, 1996 entitled <i>Method and Apparatus for Prioritizing and Routing Commands from a Command Source to a Command Sink, </i>(8) U.S. Ser. No. 761,379 filed Dec. 9, 1996 entitled <i>Method and Apparatus for Tracking Processing of a Command; </i>(9) U.S. Ser. No. 473,692 filed Jun. 7, 1995 entitled <i>Method and System for Enhanced Multithread Operation in a Data Processing System by Reducing Memory Access Latency Delays; </i>and (10) U.S. Ser. No. 675,315 filed Jul. 3, 1996 entitled <i>Multithreaded Storage Cell. </i></p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>The present invention relates in general to an improved method for and apparatus of a computer data processing system; and in particular, to an improved high performance multithreaded computer data processing system and method embodied in the hardware of the processor.</p><p>The fundamental structure of a modern computer includes peripheral devices to communicate information to and from the outside world; such peripheral devices may be keyboards, monitors, tape drives, communication lines coupled to a network, etc. Also included in the basic structure of the computer is the hardware necessary to receive, process, and deliver this information from and to the outside world, including busses, memory units, input/output (I/O) controllers, storage devices, and at least one central processing unit (CPU), etc. The CPU is the brain of the system. It executes the instructions which comprise a computer program and directs the operation of the other system components.</p><p>From the standpoint of the computer's hardware, most systems operate in fundamentally the same manner. Processors actually perform very simple operations quickly, such as arithmetic, logical comparisons, and movement of data from one location to another. Programs which direct a computer to perform massive numbers of these simple operations give the illusion that the computer is doing something sophisticated. What is perceived by the user as a new or improved capability of a computer system, however, may actually be the machine performing the same simple operations, but much faster. Therefore continuing improvements to computer systems require that these systems be made ever faster.</p><p>One measurement of the overall speed of a computer system, also called the throughput, is measured as the number of operations performed per unit of time. Conceptually, the simplest of all possible improvements to system speed is to increase the clock speeds of the various components, particularly the clock speed of the processor. So that if everything runs twice as fast but otherwise works in exactly the same manner, the system will perform a given task in half the time. Computer processors which were constructed from discrete components years ago performed significantly faster by shrinking the size and reducing the number of components; eventually the entire processor was packaged as an integrated circuit on a single chip. The reduced size made it possible to increase the clock speed of the processor, and accordingly increase system speed.</p><p>Despite the enormous improvement in speed obtained from integrated circuitry, the demand for ever faster computer systems still exists. Hardware designers have been able to obtain still further improvements in speed by greater integration, by further reducing the size of the circuits, and by other techniques. Designers, however, think that physical size reductions cannot continue indefinitely and there are limits to continually increasing processor clock speeds. Attention has therefore been directed to other approaches for further improvements in overall speed of the computer system.</p><p>Without changing the clock speed, it is still possible to improve system speed by using multiple processors. The modest cost of individual processors packaged on integrated circuit chips has made this practical. The use of slave processors considerably improves system speed by off-loading work from the CPU to the slave processor. For instance, slave processors routinely execute repetitive and single special purpose programs, such as input/output device communications and control. It is also possible for multiple CPUs to be placed in a single computer system, typically a host-based system which services multiple users simultaneously. Each of the different CPUs can separately execute a different task on behalf of a different user, thus increasing the overall speed of the system to execute multiple tasks simultaneously. It is much more difficult, however, to improve the speed at which a single task, such as an application program, executes. Coordinating the execution and delivery of results of various functions among multiple CPUs is a tricky business. For slave I/O processors this is not so difficult because the functions are pre-defined and limited but for multiple CPUs executing general purpose application programs it is much more difficult to coordinate functions because, in part, system designers do not know the details of the programs in advance. Most application programs follow a single path or flow of steps performed by the processor. While it is sometimes possible to break up this single path into multiple parallel paths, a universal application for doing so is still being researched. Generally, breaking a lengthy task into smaller tasks for parallel processing by multiple processors is done by a software engineer writing code on a case-by-case basis. This ad hoc approach is especially problematic for executing commercial transactions which are not necessarily repetitive or predictable.</p><p>Thus, while multiple processors improve overall system performance, there are still many reasons to improve the speed of the individual CPU. If the CPU clock speed is given, it is possible to further increase the speed of the CPU, i.e., the number of operations executed per second, by increasing the average number of operations executed per clock cycle. A common architecture for high performance, single-chip microprocessors is the reduced instruction set computer (RISC) architecture characterized by a small simplified set of frequently used instructions for rapid execution, those simple operations performed quickly as mentioned earlier. As semiconductor technology has advanced, the goal of RISC architecture has been to develop processors capable of executing one or more instructions on each clock cycle of the machine. Another approach to increase the average number of operations executed per clock cycle is to modify the hardware within the CPU. This throughput measure, clock cycles per instruction, is commonly used to characterize architectures for high performance processors. Instruction pipelining and cache memories are computer architectural features that have made this achievement possible. Pipeline instruction execution allows subsequent instructions to begin execution before previously issued instructions have finished. Cache memories store frequently used and other data nearer the processor and allow instruction execution to continue, in most cases, without waiting the full access time of a main memory. Some improvement has also been demonstrated with multiple execution units with look ahead hardware for finding instructions to execute in parallel.</p><p>The performance of a conventional RISC processor can be further increased in the superscalar computer and the Very Long Instruction Word (VLIW) computer, both of which execute more than one instruction in parallel per processor cycle. In these architectures, multiple functional or execution units are provided to run multiple pipelines in parallel. In a superscalar architecture, instructions may be completed in-order and out-of-order. In-order completion means no instruction can complete before all instructions dispatched ahead of it have been completed. Out-of-order completion means that an instruction is allowed to complete before all instructions ahead of it have been completed, as long as a predefined rules are satisfied.</p><p>For both in-order and out-of-order completion of instructions in superscalar systems, pipelines will stall under certain circumstances. An instruction that is dependent upon the results of a previously dispatched instruction that has not yet completed may cause the pipeline to stall. For instance, instructions dependent on a load/store instruction in which the necessary data is not in the cache, i.e., a cache miss, cannot be completed until the data becomes available in the cache. Maintaining the requisite data in the cache necessary for continued execution and to sustain a high hit ratio, i.e., the number of requests for data compared to the number of times the data was readily available in the cache, is not trivial especially for computations involving large data structures. A cache miss can cause the pipelines to stall for several cycles, and the total amount of memory latency will be severe if the data is not available most of the time. Although memory devices used for main memory are becoming faster, the speed gap between such memory chips and high-end processors is becoming increasingly larger. Accordingly, a significant amount of execution time in current high-end processor designs is spent waiting for resolution of cache misses and these memory access delays use an increasing proportion of processor execution time.</p><p>And yet another technique to improve the efficiency of hardware within the CPU is to divide a processing task into independently executable sequences of instructions called threads. This technique is related to breaking a larger task into smaller tasks for independent execution by different processors except here the threads are to be executed by the same processor. When a CPU then, for any of a number of reasons, cannot continue the processing or execution of one of these threads, the CPU switches to and executes another thread. The term \u201cmultithreading\u201d as defined in the computer architecture community is not the same as the software use of the term which means one task subdivided into multiple related threads. In the architecture definition, the threads may be independent. Therefore \u201chardware multithreading\u201d is often used to distinguish the two uses of the term. Within the context of the present invention, the term multithreading connotes hardware multithreading to tolerate memory latency.</p><p>Multithreading permits the processors' pipeline(s) to do useful work on different threads when a pipeline stall condition is detected for the current thread. Multithreading also permits processors implementing non-pipeline architectures to do useful work for a separate thread when a stall condition is detected for a current thread. There are two basic forms of multithreading. A traditional form is to keep N threads, or states, in the processor and interleave the threads on a cycle-by-cycle basis. This eliminates all pipeline dependencies because instructions in a single thread are separated. The other form of multithreading, and the one considered by the present invention, is to interleave the threads on some long-latency event.</p><p>Traditional forms of multithreading involves replicating the processor registers for each thread. For instance, for a processor implementing the architecture sold under the trade name PowerPC\u2122 to perform multithreading, the processor must maintain N states to run N threads. Accordingly, the following are replicated N times: general purpose registers, floating point registers, condition registers, floating point status and control register, count register, link register, exception register, save/restore registers, and special purpose registers. Additionally, the special buffers, such as a segment lookaside buffer, can be replicated or each entry can be tagged with the thread number and, if not, must be flushed on every thread switch. Also, some branch prediction mechanisms, e.g., the correlation register and the return stack, should also be replicated. Fortunately, there is no need to replicate some of the larger functions of the processor such as: level one instruction cache (L<b>1</b> I-cache), level one data cache (L<b>1</b> D-cache), instruction buffer, store queue, instruction dispatcher, functional or execution units, pipelines, translation lookaside buffer (TLB), and branch history table. When one thread encounters a delay, the processor rapidly switches to another thread. The execution of this thread overlaps with the memory delay on the first thread.</p><p>Existing multithreading techniques describe switching threads on a cache miss or a memory reference. A primary example of this technique may be reviewed in \u201cSparcle: An Evolutionary Design for Large-Scale Multiprocessors,\u201d by Agarwal et al., IEEE Micro Volume 13, No. 3, pp. 48-60, June 1993. As applied in a RISC architecture, multiple register sets normally utilized to support function calls are modified to maintain multiple threads. Eight overlapping register windows are modified to become four non-overlapping register sets, wherein each register set is a reserve for trap and message handling. This system discloses a thread switch which occurs on each first level cache miss that results in a remote memory request. While this system represents an advance in the art, modern processor designs often utilize a multiple level cache or high speed memory which is attached to the processor. The processor system utilizes some well-known algorithm to decide what portion of its main memory store will be loaded within each level of cache and thus, each time a memory reference occurs which is not present within the first level of cache the processor must attempt to obtain that memory reference from a second or higher level of cache.</p><p>It should thus be apparent that a need exists for an improved data processing system which can reduce delays due to memory latency in a multilevel cache system utilized in conjunction with a multithread data processing system.</p><h4>SUMMARY OF THE INVENTION</h4><p>An object of the present invention is to provide an improved data processing system and method for multithreaded processing embodied in the hardware of the processor. This object is achieved by a multithreaded processor capable of switching execution between two threads of instructions, and thread switch logic embodied in hardware registers with optional software override of thread switch conditions. An added advantage of the thread switch logic is that processing of various threads of instructions allows optimization of the use of the processor among the threads.</p><p>Another object of the present invention is to improve multithreaded computer processing by allowing the processor to execute a second thread of instructions thereby increasing processor utilization which is otherwise idle because it is retrieving necessary data and/or instructions from various memory elements, such as caches, memories, external I/O, direct access storage devices for a first thread.</p><p>An additional object of the present invention is to provide a multithread data processing system and method which performs conditional thread switching wherein the conditions of thread switching can be different per thread or can be changed during processing by the use of a software thread control manager.</p><p>It is yet another object of the invention to allow processing of a second thread when a first thread has a latency event, such as a cache miss, which requires a large number of cycles to complete, during which time the second thread may experience a cache miss at the same cache level which can be completed in much less time.</p><p>It is yet another object of the invention to prevent thrashing wherein each thread is locked in a repetitive cycle of switching threads without any instructions executing. The invention provides a forward progress count register and method which allows up to a programmable maximum number of thread switches called the forward progress threshold after which the processor stops switching threads until one thread is able to execute. The forward progress register and its threshold monitors the number of thread switches that have occurred without an instruction having been executed and when that number is equal to a threshold no further thread switching occurs until an instruction is executed. An added advantage of the forward progress count register is that the register and threshold can be customized for certain latency events, such as one threshold value for a very long latency event such as access to external computer networks; and another forward progress threshold for shorter latency events such as cache misses.</p><p>It is yet another object to prevent computer processing on a thread from being inactive for an excessive period of time. The feature of the invention to achieve this object is to force a thread switch after waiting the number of cycles specified in a thread switch time-out register. An added advantage of this feature is that the computer processing system does not experience hangs resulting from shared resource contention. Fairness of allocating processor cycles between threads is accomplished and the maximum response latency to external interrupts and other events external to the processor is limited.</p><p>It is a further object of the invention to provide for rapid thread switching conditions. This object is achieved by hardware registers which stores the state of threads, the priority of threads, and thread switch conditions.</p><p>It is yet another object of the invention to provide flexibility to modify the results of the thread switch hardware registers. This object is achieved by altering the priority of one or more of the threads in the processor. Either a signal from an interrupt request or a software instruction can be used to modify bits in a state register indicating the priority of each thread. Then depending upon the priority of each thread, a thread switch may occur to allow a higher priority thread to have more processing cycles. The advantage to altering the priority allows changing the frequency of thread switching, increasing execution cycles for a critical task, and decreasing the number of processing cycles lost by the high priority thread because of thread switch latency.</p><p>These and other related objects are achieved by providing a method for performing multithread operations in a data processing system by executing at least one of a plurality of threads of instructions, each having a priority, in a multithreaded processor connected to at least one memory unit and altering the priority of one or more of the plurality of threads. Based on the priority of all the threads, the multiprocessor can switch from one thread to another. Priority can be changed by either a signal from an interrupt or by a hardware and software arrangement called a thread switch manager.</p><p>Priority of the threads implies that one thread can have a higher priority than another, but the threads can also have equal priority. A priority scheme is presented wherein each thread can have one of three priorities. Switching from a first thread of lower priority occurs when a second thread is in a ready state. Switching from a first thread of higher priority can be disabled when the high priority thread has a miss from a level one cache or other memory unit which has a quick response time.</p><p>Simply, the invention is a method of computer processing by allocating processor cycles to each of a plurality of threads of instructions in a multithreaded processor based on the relative priority of all the threads in the processor.</p><p>The thread switch manager is an arrangement of hardware and software having at least one priority switch instruction, at least one thread switch priority bit in a first register such as a machine state register and/or a thread switch control register, and a number of priority state bits in a second register such as a thread state register, wherein the priority switch instruction can change any of the plurality of priority state bits. In a multithreaded processing system having more than one thread, the thread executing is referred to as the active thread. The priority switch instruction can change the priority state bits corresponding to the active thread or any other thread in the multithreading processing system. Then if a thread switch priority bit is enabled, the multithreaded processor switches processing from the active thread in response to the change of priority of either the active thread or another thread.</p><p>To retrofit an existing processor to accommodate the thread switch manager, the priority switch instruction should not change any of a plurality of architected registers of a multithreaded processor and should execute on a multithreaded processor without illegal instruction interrupts, e.g., the priority switch instruction can be a preferred no op instruction.</p><p>The invention is also a computer processing system comprising means for processing a plurality of threads of instructions; means for storing a priority state of each of the plurality of threads; means for changing the priority state of each of the plurality of threads and in response thereto, a means for enabling the processing means to switch processing from a first thread to a second thread of the plurality of threads.</p><p>Other objects, features and characteristics of the present invention; methods, operation, and functions of the related elements of the structure; combination of parts; and economies of manufacture will become apparent from the following detailed description of the preferred embodiments and accompanying drawings, all of which form a part of this specification, wherein like reference numerals designate corresponding parts in the various figures.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The invention itself, however, as well as a preferred mode of use, further objectives and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein:</p><p>FIG. 1 is a block diagram of a computer system capable of implementing the invention described herein.</p><p>FIG. 2 illustrates a high level block diagram of a multithreaded data processing system according to the present invention.</p><p>FIG. 3 illustrates a block diagram of the storage control unit of FIG. <b>2</b>.</p><p>FIG. 4 illustrates a block diagram of the thread switch logic, the storage control unit and the instruction unit of FIG. <b>2</b>.</p><p>FIG. 5 illustrate the changes of state of a thread as the thread experiences different thread switch events shown in FIG. <b>4</b>.</p><p>FIG. 6 is a flow chart of the forward progress count of the invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>With reference now to the figures and in particular with reference to FIG. 1, there is depicted a high level block diagram of a computer data processing system <b>10</b> which may be utilized to implement the method and system of the present invention. The primary hardware components and interconnections of a computer data processing system <b>10</b> capable of utilizing the present invention are shown in FIG. <b>1</b>. Central processing unit (CPU) <b>100</b> for processing instructions is coupled to caches <b>120</b>, <b>130</b>, and <b>150</b>. Instruction cache <b>150</b> stores instructions for execution by CPU <b>100</b>. Data caches <b>120</b>, <b>130</b> store data to be used by CPU <b>100</b>. The caches communicate with random access memory in main memory <b>140</b>. CPU <b>100</b> and main memory <b>140</b> also communicate via bus interface <b>152</b> with system bus <b>155</b>. Various input/output processors (IOPs) <b>160</b>-<b>168</b> attach to system bus <b>155</b> and support communication with a variety of storage and input/output (I/O) devices, such as direct access storage devices (DASD) <b>170</b>, tape drives <b>172</b>, remote communication lines <b>174</b>, workstations <b>176</b>, and printers <b>178</b>. It should be understood that FIG. 1 is intended to depict representative components of a computer data processing system <b>10</b> at a high level, and that the number and types of such components may vary.</p><p>Within the CPU <b>100</b>, a processor core <b>110</b> contains specialized functional units, each of which perform primitive operations, such as sequencing instructions, executing operations involving integers, executing operations involving real numbers, transferring values between addressable storage and logical register arrays. FIG. 2 illustrates a processor core <b>100</b>. In a preferred embodiment, the processor core <b>100</b> of the data processing system <b>10</b> is a single integrated circuit, pipelined, superscalar microprocessor, which may be implemented utilizing any computer architecture such as the family of RISC processors sold under the trade name PowerPC\u2122; for example, the PowerPC\u2122 <b>604</b> microprocessor chip sold by IBM.</p><p>As will be discussed below, the data processing system <b>10</b> preferably includes various units, registers, buffers, memories, and other sections which are all preferably formed by integrated circuitry. It should be understood that in the figures, the various data paths have been simplified; in reality, there are many separate and parallel data paths into and out of the various components. In addition, various components not germane to the invention described herein have been omitted, but it is to be understood that processors contain additional units for additional functions. The data processing system <b>10</b> can operate according to reduced instruction set computing, RISC, techniques or other computing techniques.</p><p>As represented in FIG. 2, the processor core <b>100</b> of the data processing system <b>10</b> preferably includes a level one data cache, L<b>1</b> D-cache <b>120</b>, a level two L<b>2</b> cache <b>130</b>, a main memory <b>140</b>, and a level one instruction cache, L<b>1</b> I-cache <b>150</b>, all of which are operationally interconnected utilizing various bus connections to a storage control unit <b>200</b>. As shown in FIG. 1, the storage control unit <b>200</b> includes a transition cache <b>210</b> for interconnecting the L<b>1</b> D-cache <b>120</b> and the L<b>2</b> cache <b>130</b>, the main memory <b>140</b>, and a plurality of execution units. The L<b>1</b> D-cache <b>120</b> and L<b>1</b> I-cache <b>150</b> preferably are provided on chip as part of the processor <b>100</b> while the main memory <b>140</b> and the L<b>2</b> cache <b>130</b> are provided off chip. Memory system <b>140</b> is intended to represent random access main memory which may or may not be within the processor core <b>100</b> and, and other data buffers and caches, if any, external to the processor core <b>100</b>, and other external memory, for example, DASD <b>170</b>, tape drives <b>172</b>, and workstations <b>176</b>, shown in FIG. <b>1</b>. The L<b>2</b> cache <b>130</b> is preferably a higher speed memory system than the main memory <b>140</b>, and by storing selected data within the L<b>2</b> cache <b>130</b>, the memory latency which occurs as a result of a reference to the main memory <b>140</b> can be minimized. As shown in FIG. 1, the L<b>2</b> cache <b>130</b> and the main memory <b>140</b> are directly connected to both the L<b>1</b> I-cache <b>150</b> and an instruction unit <b>220</b> via the storage control unit <b>200</b>.</p><p>Instructions from the L<b>1</b> I-cache <b>150</b> are preferably output to an instruction unit <b>220</b> which, in accordance with the method and system of the present invention, controls the execution of multiple threads by the various subprocessor units, e.g., branch unit <b>260</b>, fixed point unit <b>270</b>, storage control unit <b>200</b>, and floating point unit <b>280</b> and others as specified by the architecture of the data processing system <b>10</b>. In addition to the various execution units depicted within FIG. 1, those skilled in the art will appreciate that modem superscalar microprocessor systems often include multiple versions of each such execution unit which may be added without departing from the spirit and scope of the present invention. Most of these units will have as an input source operand information from various registers such as general purpose registers GPRs <b>272</b>, and floating point registers FPRs <b>282</b>. Additionally, multiple special purpose register SPRs <b>274</b> may be utilized. As shown in FIG. 1, the storage control unit <b>200</b> and the transition cache <b>210</b> are directly connected to general purpose registers <b>272</b> and the floating point registers <b>282</b>. The general purpose registers <b>272</b> are connected to the special purpose registers <b>274</b>.</p><p>Among the functional hardware units unique to this multithreaded processor <b>100</b> is the thread switch logic <b>400</b> and the transition cache <b>210</b>. The thread switch logic <b>400</b> contains various registers that determine which thread will be the active or the executing thread. Thread switch logic <b>400</b> is operationally connected to the storage control unit <b>200</b>, the execution units <b>260</b>, <b>270</b>, and <b>280</b>, and the instruction unit <b>220</b>. The transition cache <b>210</b> within the storage control unit <b>200</b> must be capable of implementing multithreading. Preferably, the storage control unit <b>200</b> and the transition cache <b>210</b> permit at least one outstanding data request per thread. Thus, when a first thread is suspended in response to, for example, the occurrence of L<b>1</b> D-cache miss, a second thread would be able to access the L<b>1</b> D-cache <b>120</b> for data present therein. If the second thread also results in L<b>1</b> D-cache miss, another data request will be issued and thus multiple data requests must be maintained within the storage control unit <b>200</b> and the transition cache <b>210</b>. Preferably, transition cache <b>210</b> is the transition cache of U.S. application Ser. No. 08/761,378 filed Dec. 9, 1996 entitled <i>Multi</i>-<i>Entry Fully Associative Transition Cache, </i>hereby incorporated by reference. The storage control unit <b>200</b>, the execution units <b>260</b>, <b>270</b>, and <b>280</b> and the instruction unit <b>220</b> are all operationally connected to the thread switch logic <b>400</b> which determines which thread to execute.</p><p>As illustrated in FIG. 2, a bus <b>205</b> is provided between the storage control unit <b>200</b> and the instruction unit <b>220</b> for communication of, e.g., data requests to the storage control unit <b>200</b>, and a L<b>2</b> cache <b>130</b> miss to the instruction unit <b>220</b>. Further, a translation lookaside buffer TLB <b>250</b> is provided which contains virtual-to-real address mapping. Although not illustrated within the present invention various additional high level memory mapping buffers may be provided such as a segment lookaside buffer which will operate in a manner similar to the translation lookaside buffer <b>250</b>.</p><p>FIG. 3 illustrates the storage control unit <b>200</b> in greater detail, and, as the name implies, this unit controls the input and output of data and instructions from the various storage units, which include the various caches, buffers and main memory. As shown in FIG. 3, the storage control unit <b>200</b> includes the transition cache <b>210</b> functionally connected to the L<b>1</b> D-cache <b>120</b>, multiplexer <b>360</b>, the L<b>2</b> cache <b>130</b>, and main memory <b>140</b>. Furthermore, the transition cache <b>210</b> receives control signals from sequencers <b>350</b>. The sequencers <b>350</b> include a plurality of sequencers, preferably three, for handling instruction and/or data fetch requests. Sequencers <b>350</b> also output control signals to the transition cache <b>210</b>, the L<b>2</b> cache <b>130</b>, as well as receiving and transmitting control signals to and from the main memory <b>140</b>.</p><p>Multiplexer <b>360</b> in the storage control unit <b>200</b> shown in FIG. 3 receives data from the L<b>1</b> D-cache <b>120</b>, the transition cache <b>210</b>, the L<b>2</b> cache <b>130</b>, main memory <b>140</b>, and, if data is to be stored to memory, the execution units <b>270</b> and <b>280</b>. Data from one of these sources is selected by the multiplexer <b>360</b> and is output to the L<b>1</b> D-cache <b>120</b> or the execution units in response to a selection control signal received from the sequencers <b>350</b>. Furthermore, as shown in FIG. 3, the sequencers <b>350</b> output a selection signal to control a second multiplexer <b>370</b>. Based on this selection signal from the sequencers <b>350</b>, the multiplexer <b>370</b> outputs the data from the L<b>2</b> cache <b>130</b> or the main memory <b>140</b> to the L<b>1</b> I-cache <b>150</b> or the instruction unit <b>220</b>. In producing the above-discussed control and selection signals, the sequencers <b>350</b> access and update the L<b>1</b> directory <b>320</b> for the L<b>1</b> D-cache <b>120</b> and the L<b>2</b> directory <b>330</b> for the L<b>2</b> cache <b>130</b>.</p><p>With respect to the multithreading capability of the processor described herein, sequencers <b>350</b> of the storage control unit <b>200</b> also output signals to thread switch logic <b>400</b> which indicate the state of data and instruction requests. So, feedback from the caches <b>120</b>, <b>130</b> and <b>150</b>, main memory <b>140</b>, and the translation lookaside buffer <b>250</b> is routed to the sequencers <b>350</b> and is then communicated to thread switch logic <b>400</b> which may result in a thread switch, as discussed below. Note that any device wherein an event designed to cause a thread switch in a multithreaded processor occurs will be operationally connected to sequencers <b>350</b>.</p><p>FIG. 4 is a logical representation and block diagram of the thread switch logic hardware <b>400</b> that determines whether a thread will be switched and, if so, what thread. Storage control unit <b>200</b> and instruction unit <b>220</b> are interconnected with thread switch logic <b>400</b>. Thread switch logic <b>400</b> preferably is incorporated into the instruction unit <b>220</b> but if there are many threads the complexity of the thread switch logic <b>400</b> may increase so that the logic is external to the instruction unit <b>220</b>. For ease of explanation, thread switch logic <b>400</b> is illustrated external to the instruction unit <b>220</b>.</p><p>Some events which result in a thread to be switched in this embodiment are communicated on lines <b>470</b>, <b>472</b>, <b>474</b>, <b>476</b>, <b>478</b>, <b>480</b>, <b>482</b>, <b>484</b>, and <b>486</b> from the sequencers <b>350</b> of the storage control unit <b>200</b> to the thread switch logic <b>400</b>. Other latency events can cause thread switching; this list is not intended to be inclusive; rather it is only representative of how the thread switching can be implemented. A request for an instruction by either the first thread T<b>0</b> or the second thread T<b>1</b> which is not in the instruction unit <b>220</b> is an event which can result in a thread switch, noted by <b>470</b> and <b>472</b> in FIG. 4, respectively. Line <b>474</b> indicates when the active thread, whether T<b>0</b> or T<b>1</b>, experiences a L<b>1</b> D-cache <b>120</b> miss. Cache misses of the L<b>2</b> cache <b>130</b> for either thread T<b>0</b> or T<b>1</b> is noted at lines <b>476</b> and <b>478</b>, respectively. Lines <b>480</b> and <b>482</b> are activated when data is returned for continued execution of the T<b>0</b> thread or for the T<b>1</b> thread, respectively. Translation lookaside buffer misses and completion of a table walk are indicated by lines <b>484</b> and <b>486</b>, respectively.</p><p>These events are all fed into the thread switch logic <b>400</b> and more particularly to the thread state registers <b>440</b> and the thread switch controller <b>450</b>. Thread switch logic <b>400</b> has one thread state register for each thread. In the embodiment described herein, two threads are represented so there is a T<b>0</b> state register <b>442</b> for a first thread T<b>0</b> and a T<b>1</b> state register <b>444</b> for a second thread T<b>1</b>, to be described herein. Thread switch logic <b>400</b> comprises a thread switch control register <b>410</b> which controls what events will result in a thread switch. For instance, the thread switch control register <b>410</b> can block events that cause state changes from being seen by the thread switch controller <b>450</b> so that a thread may not be switched as a result of a blocked event. The thread state registers and the logic and operation of changing threads are the subject of a U.S. patent application Ser. No. 957,002 filed concurrently and herein incorporated by reference. The thread switch control register <b>410</b> is the subject of a U.S. patent application Ser. No. 958,716 filed concurrently and <b>40</b> herein incorporated by reference. The forward progress count register <b>420</b> is used to prevent thrashing and may be included in the thread switch control register <b>410</b>. The forward progress count register <b>420</b> is the subject of a U.S. patent application Ser. No. 956,875 filed concurrently and herein incorporated by reference. Thread switch time-out register <b>430</b>, the subject of a U.S. patent application Ser. No. 956,577 filed concurrently and herein incorporated by reference, allocates fairness and livelock issues. Finally, but not to be limitative, the thread switch controller <b>450</b> comprises a myriad of logic gates which represents the culmination of all logic which actually determines whether a thread is switched, what thread, and under what circumstances. Each of these logic components and their functions are set forth in further detail.</p><h4>Thread State Registers</h4><p>Thread state registers <b>440</b> comprise a state register for each thread and, as the name suggests, store the state of the corresponding thread; in this case, a T<b>0</b> thread state register <b>442</b> and a T<b>1</b> thread state register <b>444</b>. The number of bits and the allocation of particular bits to describe the state of each thread can be customized for a particular architecture and thread switch priority scheme. An example of the allocation of bits in the thread state registers <b>442</b>, <b>444</b> for a multithreaded processor having two threads is set forth in the table below.</p><p><tables id=\"TABLE-US-00001\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Thread State Register Bit Allocation</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></thead><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"2\" colsep=\"0\" rowsep=\"0\"><colspec align=\"right\" colname=\"1\" colwidth=\"28PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"189PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\">(0)</entry><entry morerows=\"0\" valign=\"top\">Instruction/Data</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">0 = Instruction</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">1 = Data</entry></row><row><entry morerows=\"0\" valign=\"top\">(1:2)</entry><entry morerows=\"0\" valign=\"top\">Miss type sequencer</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">00 = None</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">01 = Translation lookaside buffer miss (check bit 0 for I/D)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">10 = L1 cache miss</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">11 = L2 cache miss</entry></row><row><entry morerows=\"0\" valign=\"top\">(3)</entry><entry morerows=\"0\" valign=\"top\">Transition</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">0 = Transition to current state does not result in thread switch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">1 = Transition to current state results in thread switch</entry></row><row><entry morerows=\"0\" valign=\"top\">(4:7)</entry><entry morerows=\"0\" valign=\"top\">Reserved</entry></row><row><entry morerows=\"0\" valign=\"top\">(8)</entry><entry morerows=\"0\" valign=\"top\">0 = Load</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">1 = Store</entry></row><row><entry morerows=\"0\" valign=\"top\">(9:14)</entry><entry morerows=\"0\" valign=\"top\">Reserved</entry></row><row><entry morerows=\"0\" valign=\"top\">(15:17)</entry><entry morerows=\"0\" valign=\"top\">Forward progress counter</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">111 =Reset (instruction has completed during this thread)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">000 = 1st execution of this thread w/o instruction complete</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">001 = 2nd execution of this thread w/o instruction complete</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">010 = 3rd execution of this thread w/o instruction complete</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">011 = 4th execution of this thread w/o instruction complete</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">100 = 5th execution of this thread w/o instruction complete</entry></row><row><entry morerows=\"0\" valign=\"top\">(18:19)</entry><entry morerows=\"0\" valign=\"top\">Priority (could be set by software)</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">00 = Medium</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">01 = Low</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">10 = High</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">11 = &lt;Illegal&gt;</entry></row><row><entry morerows=\"0\" valign=\"top\">(20:31)</entry><entry morerows=\"0\" valign=\"top\">Reserved</entry></row><row><entry morerows=\"0\" valign=\"top\">(32:63)</entry><entry morerows=\"0\" valign=\"top\">Reserved if 64 bit implementation</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>In the embodiment described herein, bit <b>0</b> identifies whether the miss or the reason the processor stalled execution is a result of a request for an instruction or for data. Bits <b>1</b> and <b>2</b> indicate if the requested information was not available and if so, from what hardware, i.e., whether the translated address of the data or instruction was not in the translation lookaside buffer <b>250</b>, or the data or instruction itself was not in the L<b>1</b> D-cache <b>120</b> or the L<b>2</b> cache <b>130</b>, as further explained in the description of FIG. <b>5</b>. Bit <b>3</b> indicates whether the change of state of a thread results in a thread switch. A thread may change state without resulting in a thread switch. For instance, if a thread switch occurs when thread T<b>1</b> experiences an L<b>1</b> cache miss, then if thread T<b>1</b> experiences a L<b>2</b> cache miss, there will be no thread switch because the thread already switched on a L<b>1</b> cache miss. The state of T<b>1</b>, however, still changes. Alternatively, if by choice, the thread switch logic <b>400</b> is configured or programmed not to switch on a L<b>1</b> cache miss, then when a thread does experience an L<b>1</b> cache miss, there will be no thread switch even though the thread changes state. Bit <b>8</b> of the thread state registers <b>442</b> and <b>444</b> is assigned to whether the information requested by a particular thread is to be loaded into the processor core or stored from the processor core into cache or main memory. Bits <b>15</b> through <b>17</b> are allocated to prevent thrashing, as discussed later with reference to the forward progress count register <b>420</b>. Bits <b>18</b> and <b>19</b> can be set in the hardware or could be set by software to indicate the priority of the thread.</p><p>FIG. 5 represents four states in the present embodiment of a thread processed by the data processing system <b>10</b> and these states are stored in the thread state registers <b>440</b>, bit positions <b>1</b>:<b>2</b>. State 00 represents the \u201cready\u201d state, i.e., the thread is ready for processing because all data and instructions required are available; state 10 represents the thread state wherein the execution of the thread within the processor is stalled because the thread is waiting for return of data into either the L<b>1</b> D-cache <b>120</b> or the return of an instruction into the L<b>1</b> I-cache <b>150</b>; state 11 represents that the thread is waiting for return of data into the L<b>2</b> cache <b>130</b>; and the state 01 indicates that there is a miss on the translation lookaside buffer <b>250</b>, i.e., the virtual address was in error or wasn't available, called a table walk. Also shown in FIG. 5 is the hierarchy of thread states wherein state 00, which indicates the thread is ready for execution, has the highest priority. Short latency events are preferably assigned a higher priority.</p><p>FIG. 5 also illustrates the change of states when data is retrieved from various sources. The normal uninterrupted execution of a thread T<b>0</b> is represented in block <b>510</b> as state 00. If a L<b>1</b> D-cache or I-cache miss occurs, the thread state changes to state 10, as represented in block <b>512</b>, pursuant to a signal sent on line <b>474</b> (FIG. 4) from the storage control unit <b>200</b> or line <b>470</b> (FIG. 4) from the instruction unit <b>220</b>, respectively. If the required data or instruction is in the L<b>2</b> cache <b>130</b> and is retrieved, then normal execution of T<b>0</b> resumes at block <b>510</b>. Similarly block <b>514</b> of FIG. 5 represents a L<b>2</b> cache miss which changes the state of thread of either T<b>0</b> or T<b>1</b> to state 11 when storage control unit <b>200</b> signals the miss on lines <b>476</b> or <b>478</b> (FIG. <b>4</b>). When the instructions or data in the L<b>2</b> cache are retrieved from main memory <b>140</b> and loaded into the processor core <b>100</b> as indicated on lines <b>480</b> and <b>482</b> (FIG. <b>4</b>), the state again changes back to state 00 at block <b>510</b>. The storage control unit <b>200</b> communicates to the thread registers <b>440</b> on line <b>484</b> (FIG. 4) when the virtual address for requested information is not available in the translation lookaside buffer <b>250</b>, indicated as block <b>516</b>, as a TLB miss or state 01. When the address does become available or if there is a data storage interrupt instruction as signaled by the storage control unit <b>200</b> on line <b>486</b> (FIG. <b>4</b>), the state of the thread then returns to state 00, meaning ready for execution.</p><p>The number of states, and what each state represents is freely selectable by the computer architect. For instance, if a thread has multiple L<b>1</b> cache misses, such as both a L<b>1</b> I-cache miss and L<b>1</b> D-cache miss, a separate state can be assigned to each type of cache miss. Alternatively, a single thread state could be assigned to represent more than one event or occurrence.</p><p>An example of a thread switch algorithm for two threads of equal priority which determines whether to switch threads is given. The algorithm can be expanded and modified accordingly for more threads and thread switch conditions according to the teachings of the invention. The interactions between the state of each thread stored in the thread state registers <b>440</b> (FIG. 4) and the priority of each thread by the thread switching algorithm are dynamically interrogated each cycle. If the active thread T<b>0</b> has a L<b>1</b> miss, the algorithm will cause a thread switch to the dormant thread T<b>1</b> unless the dormant thread T<b>1</b> is waiting for resolution of a L<b>2</b> miss. If a switch did not occur and the L<b>1</b> cache miss of active thread T<b>0</b> turns into a L<b>2</b> cache miss, the algorithm then directs the processor to switch to the dormant thread T<b>1</b> regardless of the T<b>1</b>'s state. If both threads are waiting for resolution of a L<b>2</b> cache miss, the thread first having the L<b>2</b> miss being resolved becomes the active thread. At every switch decision time, the action taken is optimized for the most likely case, resulting in the best performance. Note that thread switches resulting from a L<b>2</b> cache miss are conditional on the state of the other thread, if not extra thread switches would occur resulting in loss of performance.</p><h4>Thread Switch Control Register</h4><p>In any multithreaded processor, there are latency and performance penalties associated with switching threads. In the multithreaded processor in the preferred embodiment described herein, this latency includes the time required to complete execution of the current thread to a point where it can be interrupted and correctly restarted when it is next invoked, the time required to switch the thread-specific hardware facilities from the current thread's state to the new thread's state, and the time required to restart the new thread and begin its execution. Preferably the thread-specific hardware facilities operable with the invention include the thread state registers described above and the memory cells described in U.S. patent application Ser. No. 675,315 filed Jul. 3, 1996 entitled <i>Multithreaded Storage Cell, </i>herein incorporated by reference. In order to achieve optimal performance in a coarse grained multithreaded data processing system, the latency of an event which generates a thread switch must be greater than the performance cost associated with switching threads in a multithreaded mode, as opposed to the normal single-threaded mode.</p><p>The latency of an event used to generate a thread switch is dependent upon both hardware and software. For example, specific hardware considerations in a multithreaded processor include the speed of external SRAMs used to implement an L<b>2</b> cache external to the processor chip. Fast SRAMs in the L<b>2</b> cache reduce the average latency of an L<b>1</b> miss while slower SRAMS increase the average latency of an L<b>1</b> miss. Thus, performance is gained if one thread switch event is defined as a L<b>1</b> cache miss in hardware having an external L<b>2</b> cache data access latency greater than the thread switch penalty. As an example of how specific software code characteristics affect the latency of thread switch events, consider the L<b>2</b> cache hit-to-miss ratio of the code, i.e., the number of times data is actually available in the L<b>2</b> cache compared to the number of times data must be retrieved from main memory because data is not in the L<b>2</b> cache. A high L<b>2</b> hit-to-miss ratio reduces the average latency of an L<b>1</b> cache miss because the L<b>1</b> cache miss seldom results in a longer latency L<b>2</b> miss. A low L<b>2</b> hit-to-miss ratio increases the average latency of an L<b>1</b> miss because more L<b>1</b> misses result in longer latency L<b>2</b> misses. Thus, a L<b>1</b> cache miss could be disabled as a thread switch event if the executing code has a high L<b>2</b> hit-to-miss ratio because the L<b>2</b> cache data access latency is less than the thread switch penalty. A L<b>1</b> cache miss would be enabled as a thread switch event when executing software code with a low L<b>2</b> hit-to-miss ratio because the L<b>1</b> cache miss is likely to turn into a longer latency L<b>2</b> cache miss.</p><p>Some types of latency events are not readily detectable. For instance, in some systems the L<b>2</b> cache outputs a signal to the instruction unit when a cache miss occurs. Other L<b>2</b> caches, however, do not output such a signal, as in for example, if the L<b>2</b> cache controller were on a separate chip from the processor and accordingly, the processor cannot readily determine a state change. In these architectures, the processor can include a cycle counter for each outstanding L<b>1</b> cache miss. If the miss data has not been returned from the L<b>2</b> cache after a predetermined number of cycles, the processor acts as if there had been a L<b>2</b> cache miss and changes the thread's state accordingly. This algorithm is also applicable to other cases where there are more than one distinct type of latency. As an example only, for a L<b>2</b> cache miss in a multiprocessor, the latency of data from main memory may be significantly different than the latency of data from another processor. These two events may be assigned different states in the thread state register. If no signal exists to distinguish the states, a counter may be used to estimate which state the thread should be in after it encounters a L<b>2</b> cache miss.</p><p>The thread switch control register <b>410</b> is a software programmable register which selects the events to generate thread switching and has a separate enable bit for each defined thread switch control event. Although the embodiment described herein does not implement a separate thread switch control register <b>410</b> for each thread, separate thread switch control registers <b>410</b> for each thread could be implemented to provide more flexibility and performance at the cost of more hardware and complexity. Moreover, the thread switch control events in one thread switch control register need not be identical to the thread switch control events in any other thread switch control register.</p><p>The thread switch control register <b>410</b> can be written by a service processor with software such as a dynamic scan communications interface disclosed in U.S. Pat. No. 5,079,725 entitled <i>Chip Identification Method for Use with Scan Design Systems and Scan Testing Techniques </i>or by the processor itself with software system code. The contents of the thread switch control register <b>410</b> is used by the thread switch controller <b>450</b> to enable or disable the generation of a thread switch. A value of one in the register <b>410</b> enables the thread switch control event associated with that bit to generate a thread switch. A value of zero in the thread switch control register <b>410</b> disables the thread switch control event associated with that bit from generating a thread switch. Of course, an instruction in the executing thread could disable any or all of the thread switch conditions for that particular or for other threads. The following table shows the association between thread switch events and their enable bits in the register <b>410</b>.</p><p><tables id=\"TABLE-US-00002\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Thread Switch Control Register Bit Assignment</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></thead><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"2\" colsep=\"0\" rowsep=\"0\"><colspec align=\"right\" colname=\"1\" colwidth=\"28PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"189PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\">(0)</entry><entry morerows=\"0\" valign=\"top\">Switch on L1 data cache fetch miss</entry></row><row><entry morerows=\"0\" valign=\"top\">(1)</entry><entry morerows=\"0\" valign=\"top\">Switch on L1 data cache store miss</entry></row><row><entry morerows=\"0\" valign=\"top\">(2)</entry><entry morerows=\"0\" valign=\"top\">Switch on L1 instruction cache miss</entry></row><row><entry morerows=\"0\" valign=\"top\">(3)</entry><entry morerows=\"0\" valign=\"top\">Switch on instruction TLB miss</entry></row><row><entry morerows=\"0\" valign=\"top\">(4)</entry><entry morerows=\"0\" valign=\"top\">Switch on L2 cache fetch miss</entry></row><row><entry morerows=\"0\" valign=\"top\">(5)</entry><entry morerows=\"0\" valign=\"top\">Switch on L2 cache store miss</entry></row><row><entry morerows=\"0\" valign=\"top\">(6)</entry><entry morerows=\"0\" valign=\"top\">Switch on L2 instruction cache miss</entry></row><row><entry morerows=\"0\" valign=\"top\">(7)</entry><entry morerows=\"0\" valign=\"top\">Switch on data TLB/segment lookaside buffer miss</entry></row><row><entry morerows=\"0\" valign=\"top\">(8)</entry><entry morerows=\"0\" valign=\"top\">Switch on L2 cache miss and dormant thread not L2 cache miss</entry></row><row><entry morerows=\"0\" valign=\"top\">(9)</entry><entry morerows=\"0\" valign=\"top\">Switch when thread switch time-out value reached</entry></row><row><entry morerows=\"0\" valign=\"top\">(10)</entry><entry morerows=\"0\" valign=\"top\">Switch when L2 cache data returned</entry></row><row><entry morerows=\"0\" valign=\"top\">(11)</entry><entry morerows=\"0\" valign=\"top\">Switch on IO external accesses</entry></row><row><entry morerows=\"0\" valign=\"top\">(12)</entry><entry morerows=\"0\" valign=\"top\">Switch on double-X store: miss on first of two*</entry></row><row><entry morerows=\"0\" valign=\"top\">(13)</entry><entry morerows=\"0\" valign=\"top\">Switch on double-X store: miss on second of two*</entry></row><row><entry morerows=\"0\" valign=\"top\">(14)</entry><entry morerows=\"0\" valign=\"top\">Switch on store multiple/string: miss on any access</entry></row><row><entry morerows=\"0\" valign=\"top\">(15)</entry><entry morerows=\"0\" valign=\"top\">Switch on load multiple/string: miss on any access</entry></row><row><entry morerows=\"0\" valign=\"top\">(16)</entry><entry morerows=\"0\" valign=\"top\">Reserved</entry></row><row><entry morerows=\"0\" valign=\"top\">(17)</entry><entry morerows=\"0\" valign=\"top\">Switch on double-X load: miss on first of two*</entry></row><row><entry morerows=\"0\" valign=\"top\">(18)</entry><entry morerows=\"0\" valign=\"top\">Switch on double-X load: miss on second of two*</entry></row><row><entry morerows=\"0\" valign=\"top\">(19)</entry><entry morerows=\"0\" valign=\"top\">Switch on or 1,1,1 instruction if machine state register (problem</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">state) bit, msr(pr)=1. Allows software priority change indepen-</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">dent of msr(pr). If bit 19 is one, or 1,1,1 instruction sets low</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">priority. If bit 19 is zero, priority is set to low oniy if msr(pr)=0</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">when the or 1,1,1 instruction is executed. See changing priority</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">with software, to be discussed later.</entry></row><row><entry morerows=\"0\" valign=\"top\">(20)</entry><entry morerows=\"0\" valign=\"top\">Reserved</entry></row><row><entry morerows=\"0\" valign=\"top\">(21)</entry><entry morerows=\"0\" valign=\"top\">Thread switch priority enable</entry></row><row><entry morerows=\"0\" valign=\"top\">(22:29)</entry><entry morerows=\"0\" valign=\"top\">Reserved</entry></row><row><entry morerows=\"0\" valign=\"top\">(30:31)</entry><entry morerows=\"0\" valign=\"top\">Forward progress count</entry></row><row><entry morerows=\"0\" valign=\"top\">(32:63)</entry><entry morerows=\"0\" valign=\"top\">Reserved in 64 bit register implementation</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry align=\"left\" morerows=\"0\" nameend=\"2\" namest=\"1\" valign=\"top\">*A double-X load/store refers to loading or storing an elementary halfword, a word, or a double word, that crosses a doubleword boundary. A double-X load/store in this context is not a load or store of multiple words or a string of words. </entry></row></tbody></tgroup></table></tables></p><h4>Thread Switch Time-Out Register</h4><p>As discussed above, coarse grained multithreaded processors rely on long latency events to trigger thread switching. Sometimes during execution, a processor in a multiprocessor environment or a background thread in a multithreaded architecture, has ownership of a resource that can have only a single owner and another processor or active thread requires access to the resource before it can make forward progress. Examples include updating a memory page table or obtaining a task from a task dispatcher. The inability of the active thread to obtain ownership of the resource does not result in a thread switch event, nonetheless, the thread is spinning in a loop unable to do useful work. In this case, the background thread that holds the resource does not obtain access to the processor so that it can free up the resource because it never encountered a thread switch event and does not become the active thread.</p><p>Allocating processing cycles among the threads is another concern; if software code running on a thread seldom encounters long latency switch events compared to software code running on the other threads in the same processor, that thread will get more than it's fair share of processing cycles. Yet another excessive delay that may exceed the maximum acceptable time is the latency of an inactive thread waiting to service an external interrupt within a limited period of time or some other event external to the processor. Thus, it becomes preferable to force a thread switch to the dormant thread after some time if no useful processing is being accomplished to prevent the system from hanging.</p><p>The logic to force a thread switch after a period of time is a thread switch time-out register <b>430</b> (FIG. <b>4</b>), a decrementer, and a decrementer register to hold the decremented value. The thread switch time-out register <b>430</b> holds a thread switch time-out value. The thread switch time-out register <b>430</b> implementation used in this embodiment is shown in the following table:</p><p><tables id=\"TABLE-US-00003\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">Thread Switch Time-out Register Bits</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></thead><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"35PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"56PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"126PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">\u2002(0:21)</entry><entry morerows=\"0\" valign=\"top\">Reserved</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">(22:31)</entry><entry morerows=\"0\" valign=\"top\">Thread switch time-out value</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>The embodiment of the invention described herein does not implement a separate thread switch time-out register <b>430</b> for each thread, although that could be done to provide more flexibility. Similarly, if there are multiple threads, each thread need not have the same thread switch time-out value. Each time a thread switch occurs, the thread switch time-out value from the thread switch time-out register <b>430</b> is loaded by hardware into the decrement register. The decrement register is decremented once each cycle until the decrement register value equals zero, then a signal is sent to the thread switch controller <b>450</b> which forces a thread switch unless no other thread is ready to process instructions. For example, if all other threads in the system are waiting on a cache miss and are not ready to execute instructions, the thread switch controller <b>450</b> does not force a thread switch. If no other thread is ready to process instructions when the value in the decrement register reaches zero, the decremented value is frozen at zero until another thread is ready to process instructions, at which point a thread switch occurs and the decrement register is reloaded with a thread switch time-out value for that thread. Similarly, the decrement register could just as easily be named an increment register and when a thread is executing the register could increment up to some predetermined value when a thread switch would be forced.</p><p>The thread switch time-out register <b>430</b> can be written by a service processor as described above or by the processor itself with software code. The thread switch time-out value loaded into the thread switch time-out register <b>430</b> can be customized according to specific hardware configuration and/or specific software code to minimize wasted cycles resulting from unnecessary thread switching. Too high of a value in the thread switch time-out register <b>430</b> can result in reduced performance when the active thread is waiting for a resource held by another thread or if response latency for an external interrupt or some other event external to the processor is too long. Too high of a value can also prevent fairness if one thread experiences a high number of thread switch events and the other does not A thread switch time-out value twice to several times longer than the most frequent longest latency event that causes a thread switch is recommended, e.g., access to main memory. Forcing a thread switch after waiting the number of cycles specified in the thread switch time-out register <b>430</b> prevents system hangs due to shared resource contention, enforces fairness of processor cycle allocation between threads, and limits the maximum response latency to external interrupts and other events external to the processor.</p><h4>Forward Progress Guarantee</h4><p>That at least one instruction must be executed each time a thread switch occurs and a new thread becomes active is too restrictive in certain circumstances, such as when a single instruction generates multiple cache accesses and/or multiple cache misses. For example, a fetch instruction may cause an L<b>1</b> I-cache <b>150</b> miss if the instruction requested is not in the cache; but when the instruction returns, required data may not be available in the L<b>1</b> D-cache <b>120</b>. Likewise, a miss in translation lookaside buffer <b>250</b> can also result in a data cache miss. So, if forward progress is strictly enforced, misses on subsequent accesses do not result in thread switches. A second problem is that some cache misses may require a large number of cycles to complete, during which time another thread may experience a cache miss at the same cache level which can be completed in much less time. If, when returning to the first thread, the strict forward progress is enforced, the processor is unable to switch to the thread with the shorter cache miss.</p><p>To remedy the problem of thrashing wherein each thread is locked in a repetitive cycle of switching threads without any instructions executing, there exists a forward progress count register <b>420</b> (FIG. 4) which allows up to a programmable maximum number of thread switches called the forward progress threshold value. After that maximum number of thread switches, an instruction must be completed before switching can occur again. In this way, thrashing is prevented. Forward progress count register <b>420</b> may actually be bits <b>30</b>:<b>31</b> in the thread switch control register <b>410</b> or a software programmable forward progress threshold register for the processor. The forward progress count logic uses bits <b>15</b>:<b>17</b> of the thread state registers <b>442</b>, <b>444</b> that indicate the state of the threads and are allocated for the number of thread switches a thread has experienced without an instruction executing. Preferably, then these bits comprise the forward progress counter.</p><p>When a thread changes state invoking the thread switch algorithm, if at least one instruction has completed in the active thread, the forward-progress counter for the active thread is reset and the thread switch algorithm continues to compare thread states between the threads in the processor. If no instruction has completed, the forward-progress counter value in the thread state register of the active thread is compared to the forward progress threshold value. If the counter value is not equal to the threshold value, the thread switch algorithm continues to evaluate the thread states of the threads in the processor. Then if a thread switch occurs, the forward-progress counter is incremented. If, however, the counter value or state is equal to the threshold value, no thread switch will occur until an instruction can execute, i.e., until forward progress occurs. Note that if the threshold register has value zero, at least one instruction must complete within the active thread before switching to another thread. If each thread switch requires three processor cycles and if there are two threads and if the thread switch logic is programmed to stop trying to switch threads after five tries; then the maximum number of cycles that the processor will thrash is thirty cycles. One of skill in the art can appreciate that there a potential conflict exists between prohibiting a thread switch because no forward progress will be made on one hand and, on the other hand, forcing a thread switch because the time-out count has been exceeded. Such a conflict can easily be resolved according to architecture and software.</p><p>FIG. 6 is a flowchart of the forward progress count feature of thread switch logic <b>400</b> which prevents thrashing. At block <b>610</b>, bits <b>15</b>:<b>17</b> in thread state register <b>442</b> pertaining to thread T<b>0</b> are reset to state 111. Execution of this thread is attempted in block <b>620</b> and the state changes to 000. If an instruction successfully executes on thread T<b>0</b>, the state of thread T<b>0</b> returns to 111 and remains so. If, however, thread T<b>0</b> cannot execute an instruction, a thread switch occurs to thread T<b>1</b>, or another background thread if more than two threads are permitted in the processor architecture. When a thread switch occurs away from T<b>1</b> or the other background thread and execution returns to thread T<b>0</b>, a second attempt to execute thread T<b>0</b> occurs and the state of thread T<b>0</b> becomes 001 as in block <b>630</b>. Again, if thread T<b>0</b> encounters a thread switch event, control of the processor is switched away from thread T<b>0</b> to another thread. Similarly, whenever a thread switch occurs from the other thread, e.g., T<b>1</b>, back to thread T<b>0</b>, the state of T<b>0</b> changes to 010 on this third attempt to execute T<b>0</b> (block <b>640</b>); to 011 on the fourth attempt to execute T<b>0</b> (block <b>650</b>), and to state 100 on the fifth attempt to execute T<b>0</b> (block <b>660</b>).</p><p>In this implementation, there are five attempts to switch to thread T<b>0</b>. After the fifth attempt or whenever the value of bits <b>15</b>:<b>17</b> in the thread state register (TSR) <b>442</b> is equal to the value of bits <b>30</b>:<b>31</b> plus one in the thread switch control register (TSC) <b>410</b>, i.e., whenever TSC(<b>30</b>:<b>31</b>)+1=TSR (<b>15</b>:<b>17</b>), no thread switch away from thread T<b>0</b> occurs. It will be appreciated that five attempts is an arbitrary number, the maximum number of allowable switches with unsuccessful execution, i.e., the forward progress threshold value, is programmable and it may be realized in certain architectures that five is too many switches, and in other architectures, five is too few. In any event, the relationship between the number of times that an attempt to switch to a thread with no instructions executing must be compared with a threshold value and once that threshold value has been reached, no thread switch occurs away from that thread and the processor waits until the latency associated with that thread is resolved. In the embodiment described herein, the state of the thread represented by bits <b>15</b>:<b>17</b> of the thread state register <b>442</b> is compared with bits <b>30</b>:<b>31</b> in the thread switch control register <b>410</b>. Special handling for particular events that have extremely long latency, such as interaction with input/output devices, to prevent prematurely blocking thread switching with forward progress logic improves processor performance. One way to handle these extremely long latency events is to block the incrementing of the forward progress counter or ignore the output signal of the comparison between the forward progress counter and the threshold value if data has not returned. Another way to handle extremely long latency events is to use a separate larger forward progress count for these particular events.</p><h4>Thread Switch Manager</h4><p>The thread state for all software threads dispatched to the processor is preferably maintained in the thread state registers <b>442</b> and <b>444</b> of FIG. 4 as described. In a single processor one thread executes its instructions at a time and all other threads are dormant. Execution is switched from the active thread to a dormant thread when the active thread encounters a long-latency event as discussed above with respect to the forward progress register <b>420</b>, the thread switch control register <b>410</b>, or the thread switch time-out register <b>430</b>. Independent of which thread is active, these hardware registers use conditions that do not dynamically change during the course of execution.</p><p>Flexibility to change thread switch conditions by a thread switch manager improves overall system performance. A software thread switch manager can alter the frequency of thread switching, increase execution cycles available for a critical task, and decrease the overall cycles lost because of thread switch latency. The thread switch manager can be programmed either at compile time or during execution by the operating system, e.g., a locking loop can change the frequency of thread switches; or an operating system task can be dispatched because a dormant thread in a lower priority state is waiting for an external interrupt or is otherwise ready. It may be advantageous to disallow or decrease the frequency of thread switches away from an active thread so that performance of the current instruction stream does not suffer the latencies resulting from switching into and out of it. Alternatively, a thread can forgo some or all of its execution cycles by essentially lowering its priority, and as a result, decrease the frequency of switches into it or increase the frequency of switches out of the thread to enhance overall system performance. The thread switch manager may also unconditionally force or inhibit a thread switch, or influence which thread is next selected for execution.</p><p>A multiple-priority thread switching scheme assigns a priority value to each thread to qualify the conditions that cause a switch. It may also be desirable in some cases to have the hardware alter thread priority. For instance, a low-priority thread may be waiting on some event, which when it occurs, the hardware can raise the priority of the thread to influence the response time of the thread to the event, such as an external interrupt. Relative priorities between threads or the priority of a certain thread will influence the handling of such an event. The priorities of the threads can be adjusted by hardware in response to an event or by the thread switch manager software through the use of one or more instructions. The thread switch manager alters the actions performed by the hardware thread switch logic to effectively change the relative priority of the threads.</p><p>Three priorities are used with the embodiment described herein of two threads and provide sufficient distinction between threads to allow tuning of performance without adversely affecting system performance. With three priorities, two threads can have an equal status of medium priority. The choice of three priorities for two threads is not intended to be limiting. In some architectures a \u201cnormal\u201d state may be that one thread always has a higher priority than the other threads. It is intended to be within the scope of the invention to cover more than two threads of execution having one or multiple priorities that can be set in hardware or programmed by software.</p><p>The three priorities of each thread are high, medium, and low. When the priority of thread T<b>0</b> is the same as thread T<b>1</b>, there is no effect on the thread switching logic. Both threads have equal priority so neither is given an execution time advantage. When the priority of thread T<b>0</b> is greater than the priority of thread T<b>1</b>, thread switching from T<b>0</b> to T<b>1</b> is disabled for certain thread switch events, for example all L<b>1</b> cache misses, i.e., data load, data store, and instruction fetch, because L<b>1</b> cache misses are resolved much faster than other conditions such as L<b>2</b> misses and translates. Any thread switch event may be disabled so that thread T<b>0</b> is given a better chance of receiving more execution cycles than thread T<b>1</b> which allows thread T<b>0</b> to continue execution so long as it does not waste an excessive number of execution cycles. The processor, however, will still relinquish control to thread T<b>1</b> if thread T<b>0</b> experiences a relatively long execution latency, e.g., a L<b>2</b> cache miss or retrieving data from a source external to the computer system. Thread switching from T<b>1</b> to T<b>0</b> is unaffected, except that a switch occurs when dormant thread T<b>0</b> is ready in which case thread T<b>0</b> preempts thread T<b>1</b>. This case would be expected to occur when thread T<b>0</b> switches away because of an L<b>2</b> cache miss or translation request, and the condition is resolved in the background while thread T<b>1</b> is executing. The case of thread T<b>0</b> having a priority less than thread T<b>1</b> is analogous to the case above, with the thread designation reversed.</p><p>There are different possible approaches to implementing management of thread switching by changing thread priority. New instructions can be added to the processor architecture. Existing processor instructions having side effects that have the desired actions can also be used. Among the factors that influence the choice among the methods of allowing software control are: (a) the ease of redefining architecture to include new instructions and the effect of architecture changes on existing processors; (b) the desirability of running identical software on different versions of processors; (c) the performance tradeoffs between using new, special purpose instructions versus reusing existing instructions and defining resultant side effects; (d) the desired level of control by the software, e.g., whether the effect can be caused by every execution of some existing instruction, such as a specific load or store, or whether more control is needed, by adding an instruction to the stream to specifically cause the effect.</p><p>The architecture described herein preferably takes advantage of an unused instruction whose values do not change the architected general purpose registers of the processor; this feature is critical for retrofitting multithreading capabilities into a processor architecture. Otherwise special instructions can be coded. The instruction is a \u201cpreferred no-op\u201d or 0,0,0; other instructions, however, can effectively act as a no-op. A no-op or nop is an instruction whose execution cause the computer to proceed to a next instruction to be executed, without performing an operation. In an embodiment of the preferred architecture, by using different versions of the or instruction, or 0,0,0 or 1,1,1 or any existing instruction that can have the additional priority switch meaning attached to it to alter thread priority, the same instruction stream may execute on a processor without adverse effects such as illegal instruction interrupts. An illegal instruction interrupt is generated when execution is attempted of an illegal instruction, or of a reserved or optional instruction that is not provided by the implementation. An extension uses the state of the machine state register to alter the meaning of these instructions. For example, it may be undesirable to allow a user to code some or all of these thread priority instructions and access the functions they provide. The special functions they provide may be defined to occur only in certain modes of execution, they will have no effect in other modes and will be executed normally, as a no-op.</p><p>One possible implementation, using a dual-thread multithreaded processor, uses three priority switch instructions which become part of the executing software itself to change the priority of itself:</p><p><tables id=\"TABLE-US-00004\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"2\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"1\" colwidth=\"49PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"168PT\"></colspec><thead valign=\"bottom\"><row><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></thead><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\">tsop 1 or 1,1,1</entry><entry morerows=\"0\" valign=\"top\">Switch to dormant thread</entry></row><row><entry morerows=\"0\" valign=\"top\">tsop 2 or 1,1,1</entry><entry morerows=\"0\" valign=\"top\">Set active thread to LOW priority</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Switch to dormant thread</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">NOTE: Only valid in privileged mode unless TSC[19]=1</entry></row><row><entry morerows=\"0\" valign=\"top\">tsop 3 or 2,2,2</entry><entry morerows=\"0\" valign=\"top\">Set active thread to MEDIUM priority</entry></row><row><entry morerows=\"0\" valign=\"top\">tsop 4 or 3,3,3</entry><entry morerows=\"0\" valign=\"top\">Set active thread to HIGH priority</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">NOTE: Only valid in privileged mode</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>Priority switch instructions tsop <b>1</b> and tsop <b>2</b> can be the same instruction as embodied herein as or 1,1,1 but they can also be separate instructions. These instructions interact with bits <b>19</b> and <b>21</b> of the thread switch control register <b>410</b> and the problem/privilege bit of the machine state register as described herein. If bit <b>21</b> of the thread switch control register <b>410</b> has a value of one, the thread switch manager can set the priority of its thread to one of three priorities represented in the thread state register at bits <b>18</b>:<b>19</b>. If bit <b>19</b> of the thread switch control register <b>410</b> has a value zero, then the instruction tsop <b>2</b> thread switch and thread priority setting is controlled by the problem/privilege bit of the machine state register. On the other hand, if bit <b>19</b> of the thread switch control register <b>410</b> has a value one, or if the problem/privilege bit of the machine state register has a value zero and the instruction or 1,1,1 is present in the code, the priority for the active thread is set to low and execution is immediately switched to the dormant or background thread if the dormant thread is enabled. The instruction or 2,2,2 sets the priority of the active thread to medium regardless of the value of the problem/privilege bit of the machine state register. And the instruction or 3,3,3, when the problem/privilege bit of the machine state register bit has a value of zero, sets the priority of the active thread to high. If bit <b>21</b> of the thread switch control register <b>320</b> is zero, the priority for both threads is set to medium and the effect of the or x,x,x instructions on the priority is blocked. If an external interrupt request is active, and if the corresponding thread's priority is low, that thread's priority is set to medium.</p><p>The events altered by the thread priorities are: (1) switch on L<b>1</b> D-cache miss to load data; (2) switch on L<b>1</b> D-cache miss for storing data; (3) switch on L<b>1</b> I-cache miss on an instruction fetch; and (4) switch if the dormant thread in ready state. In addition, external interrupt activation may alter the corresponding thread's priority. The following table shows the effect of priority on conditions that cause a thread switch. A simple TSC entry in columns three and four means to use the conditions set forth in the thread switch control (TSC) register <b>410</b> to initiate a thread switch. An entry of TSC[0:2] treated as 0 means that bits <b>0</b>:<b>2</b> of the thread switch control register <b>410</b> are treated as if the value of those bits are zero for that thread and the other bits in the thread switch control register <b>410</b> are used as is for defining the conditions that cause thread switches. The phrase when thread T<b>0</b> ready in column four means that a switch to thread T<b>0</b> occurs as soon as thread T<b>0</b> is no longer waiting on the miss event that caused it to be switched out. The phrase when thread T<b>1</b> ready in column 3 means that a switch to thread T<b>1</b> occurs as soon as thread T<b>1</b> is no longer waiting on the miss event that caused it to be switched out. If the miss event is a thread switch time-out, there is no guarantee that the lower priority thread completes an instruction before the higher priority thread switches back in.</p><p><tables id=\"TABLE-US-00005\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"4\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"42PT\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"42PT\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"63PT\"></colspec><colspec align=\"center\" colname=\"4\" colwidth=\"70PT\"></colspec><thead valign=\"bottom\"><row><entry align=\"center\" morerows=\"0\" nameend=\"4\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">T0 Thread Switch</entry><entry morerows=\"0\" valign=\"top\">T1 Thread Switch</entry></row><row><entry morerows=\"0\" valign=\"top\">T0 Priority</entry><entry morerows=\"0\" valign=\"top\">T1 Priority</entry><entry morerows=\"0\" valign=\"top\">Conditions</entry><entry morerows=\"0\" valign=\"top\">Conditions</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"4\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></thead><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\">High</entry><entry morerows=\"0\" valign=\"top\">High</entry><entry morerows=\"0\" valign=\"top\">TSC</entry><entry morerows=\"0\" valign=\"top\">TSC</entry></row><row><entry morerows=\"0\" valign=\"top\">High</entry><entry morerows=\"0\" valign=\"top\">Medium</entry><entry morerows=\"0\" valign=\"top\">TSC[0:2] treated</entry><entry morerows=\"0\" valign=\"top\">TSC or if T0 ready</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">as 0</entry></row><row><entry morerows=\"0\" valign=\"top\">High</entry><entry morerows=\"0\" valign=\"top\">Low</entry><entry morerows=\"0\" valign=\"top\">TSC[0:2] treated</entry><entry morerows=\"0\" valign=\"top\">TSC or if T0 ready</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">as 0</entry></row><row><entry morerows=\"0\" valign=\"top\">Medium</entry><entry morerows=\"0\" valign=\"top\">High</entry><entry morerows=\"0\" valign=\"top\">TSC or if T1 ready</entry><entry morerows=\"0\" valign=\"top\">TSC[0:2] treated</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">as 0</entry></row><row><entry morerows=\"0\" valign=\"top\">Medium</entry><entry morerows=\"0\" valign=\"top\">Medium</entry><entry morerows=\"0\" valign=\"top\">TSC</entry><entry morerows=\"0\" valign=\"top\">TSC</entry></row><row><entry morerows=\"0\" valign=\"top\">Medium</entry><entry morerows=\"0\" valign=\"top\">Low</entry><entry morerows=\"0\" valign=\"top\">TSC[0:2] treated</entry><entry morerows=\"0\" valign=\"top\">TSC or if T0 ready</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">as 0</entry></row><row><entry morerows=\"0\" valign=\"top\">Low</entry><entry morerows=\"0\" valign=\"top\">High</entry><entry morerows=\"0\" valign=\"top\">TSC or if T1 ready</entry><entry morerows=\"0\" valign=\"top\">TSC[0:2] treated</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">as 0</entry></row><row><entry morerows=\"0\" valign=\"top\">Low</entry><entry morerows=\"0\" valign=\"top\">Medium</entry><entry morerows=\"0\" valign=\"top\">TSC or if T1 ready</entry><entry morerows=\"0\" valign=\"top\">TSC[0:2] treated</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">as 0</entry></row><row><entry morerows=\"0\" valign=\"top\">Low</entry><entry morerows=\"0\" valign=\"top\">Low</entry><entry morerows=\"0\" valign=\"top\">TSC</entry><entry morerows=\"0\" valign=\"top\">TSC</entry></row><row><entry align=\"center\" morerows=\"0\" nameend=\"4\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>It is recommended that a thread doing no productive work be given low priority to avoid a loss in performance even if every instruction in the idle loop causes a thread switch. Yet, it is still important to allow hardware to alter thread priority if an external interrupt is requested to a thread set at low priority. In this case the thread is raised to medium priority, to allow a quicker response to the interrupt. This allows a thread waiting on an external event to set itself at low priority, where it will stay until the event is signalled.</p><p>While the invention has been described in connection with what is presently considered the most practical and preferred embodiments, it is to be understood that the invention is not limited to the disclosed embodiments, but on the contrary, is intended to cover various modifications and equivalent arrangements included within the spirit and scope of the appended claims.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "John Michael", "last_name": "Borkenhagen", "name": ""}, {"first_name": "William Thomas", "last_name": "Flynn", "name": ""}, {"first_name": "Andrew Henry", "last_name": "Wottreng", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTERNATIONAL BUSINESS MACHINES CORPORATION"}, {"first_name": "", "last_name": "GOOGLE LLC", "name": ""}, {"first_name": "", "last_name": "GOOGLE INC.", "name": ""}, {"first_name": "", "last_name": "INTERNATIONAL BUSINESS MACHINES CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F   9/46"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/30        20060101A I20070721RMEP"}, {"label": "G06F   9/46        20060101ALI20060310RMJP"}, {"label": "G06F  12/08        20060101AFI20060310RMJP"}, {"label": "G06F   9/38        20060101A I20070721RMEP"}, {"label": "G06F   9/48        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "718103"}, {"primary": false, "label": "712E09032"}, {"primary": false, "label": "718102"}, {"primary": false, "label": "712E09053"}], "ecla_classes": [{"label": "G06F   9/30A8T"}, {"label": "G06F   9/38E4"}, {"label": "G06F   9/48C4"}], "cpc_classes": [{"label": "G06F   9/3009"}, {"label": "G06F   9/4843"}, {"label": "G06F   9/3851"}, {"label": "G06F   9/3009"}, {"label": "G06F   9/3851"}, {"label": "G06F   9/4843"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1997-10-23", "application_date": "1997-10-23", "family_members": [{"ucid": "JP-2001521219-A", "titles": [{"lang": "JA", "text": "\u30de\u30eb\u30c1\u30b9\u30ec\u30c3\u30c9\u5f0f\u30d7\u30ed\u30bb\u30c3\u30b5\u3067\u306e\u30b9\u30ec\u30c3\u30c9\u512a\u5148\u9806\u4f4d\u306e\u5909\u66f4"}, {"lang": "EN", "text": "Changing thread priority on multithreaded processors"}]}, {"ucid": "CN-1276890-A", "titles": [{"lang": "EN", "text": "Method and apparatus for altering thread priorities in multithreaded processor"}, {"lang": "ZH", "text": "\u5728\u591a\u7ebf\u7a0b\u5904\u7406\u5668\u4e2d\u6539\u53d8\u7ebf\u7a0b\u4f18\u5148\u7ea7\u7684\u65b9\u6cd5\u548c\u88c5\u7f6e"}]}, {"ucid": "IL-135460-D0", "titles": []}, {"ucid": "TW-432327-B", "titles": [{"lang": "EN", "text": "Altering thread priorities in a multithreaded processor"}]}, {"ucid": "US-6212544-B1", "titles": [{"lang": "EN", "text": "Altering thread priorities in a multithreaded processor"}]}, {"ucid": "CN-1117319-C", "titles": [{"lang": "EN", "text": "Method and apparatus for altering thread priorities in multithreaded processor"}, {"lang": "ZH", "text": "\u5728\u591a\u7ebf\u7a0b\u5904\u7406\u5668\u4e2d\u6539\u53d8\u7ebf\u7a0b\u4f18\u5148\u7ea7\u7684\u65b9\u6cd5\u548c\u88c5\u7f6e"}]}, {"ucid": "EP-1027650-A1", "titles": [{"lang": "FR", "text": "PROCEDE ET APPAREIL POUR MODIFIER L'ORDRE DE PRIORITE DES UNITES D'EXECUTION UN PROCESSEUR A UNITES D'EXECUTION MULTIPLES"}, {"lang": "EN", "text": "METHOD AND APPARATUS FOR ALTERING THREAD PRIORITIES IN A MULTITHREADED PROCESSOR"}, {"lang": "DE", "text": "VERFAHREN UND VORRICHTUNG ZUR VER\u00c4NDERUNG VON THREAD-PRIORIT\u00c4TEN IN EINEM MULTI-THREAD PROZESSOR"}]}, {"ucid": "WO-1999021089-A1", "titles": [{"lang": "EN", "text": "METHOD AND APPARATUS FOR ALTERING THREAD PRIORITIES IN A MULTITHREADED PROCESSOR"}, {"lang": "FR", "text": "PROCEDE ET APPAREIL POUR MODIFIER L'ORDRE DE PRIORITE DES UNITES D'EXECUTION UN PROCESSEUR A UNITES D'EXECUTION MULTIPLES"}]}, {"ucid": "IL-135460-A0", "titles": [{"lang": "EN", "text": "A SYSTEM AND METHOD FOR PERFORMING COMPUTER PROCESSING OPERATIONS"}]}, {"ucid": "JP-3714598-B2", "titles": [{"lang": "JA", "text": "\u30de\u30eb\u30c1\u30b9\u30ec\u30c3\u30c9\u5f0f\u30d7\u30ed\u30bb\u30c3\u30b5\u3067\u306e\u30b9\u30ec\u30c3\u30c9\u512a\u5148\u9806\u4f4d\u306e\u5909\u66f4"}, {"lang": "EN", "text": "Changing thread priorities on multithreaded processors"}]}, {"ucid": "EP-1027650-B1", "titles": [{"lang": "FR", "text": "PROCEDE ET APPAREIL POUR MODIFIER L'ORDRE DE PRIORITE DES UNITES D'EXECUTION UN PROCESSEUR A UNITES D'EXECUTION MULTIPLES"}, {"lang": "EN", "text": "METHOD AND APPARATUS FOR ALTERING THREAD PRIORITIES IN A MULTITHREADED PROCESSOR"}, {"lang": "DE", "text": "VERFAHREN UND VORRICHTUNG ZUR VER\u00c4NDERUNG VON THREAD-PRIORIT\u00c4TEN IN EINEM MULTI-THREAD PROZESSOR"}]}]}