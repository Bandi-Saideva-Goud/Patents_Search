{"patent_number": "US-6715057-B1", "publication_id": 73754269, "family_id": 31994557, "publication_date": "2004-03-30", "titles": [{"lang": "EN", "text": "Efficient translation lookaside buffer miss processing in computer systems with a large range of page sizes"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50643126\"><p>A system and method is disclosed to efficiently translate virtual-to-physical addresses of large size pages of data by eliminating one level of a multilevel page table. A computer system containing a processor includes a translation lookaside buffer (\u201cTLB\u201d) in the processor. The processor is connected to a system memory that contains a page table with multiple levels. The page table translates the virtual address of a page of data stored in system memory into the corresponding physical address of the page of data. If the size of the page is above a certain threshold value, then translation of the page using the multilevel page table occurs by eliminating one or more levels of the page table. The threshold value preferably is 512 Megabytes. The multilevel page table is only used for translation of the virtual address of the page of data stored in system memory into the corresponding physical address of the page of data if a lookup of the TLB for the virtual address of the page of data results in a miss. The TLB also contains entries from the final level of the page table (i.e., physical addresses of pages of data) corresponding to a subfield of bits from corresponding virtual addresses of the page of data. Virtual-to-physical address translation using the multilevel page table is not required if the TLB contains the needed physical address of the page of data corresponding to the subfield of bits from the virtual address of the page of data.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6715057-B1-CLM-00001\" num=\"1\"><claim-text>1. A computer system, comprising:</claim-text><claim-text>a processor including a translation lookaside buffer; </claim-text><claim-text>a system memory coupled to said processor and containing pages in a pre-designated space that are larger than or equal to a threshold size, said system memory containing a page table with multiple levels, said page table able to translate an original virtual address of a page of data stored in system memory into a corresponding physical address of the page of data, wherein translation of a page of data above said threshold size occurs by determining from the original virtual address whether the virtual address corresponds to a page above the threshold size and, if so, by eliminating one or more levels of said page table; </claim-text><claim-text>wherein said processor is capable of selectively generating a first virtual address or a second virtual address depending on whether the original virtual address corresponds to a page above the threshold size, said first and second virtual addresses corresponding to entries in said page table, and said first or second virtual addresses being used to translate said original virtual address to the physical address. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6715057-B1-CLM-00002\" num=\"2\"><claim-text>2. The computer system of <claim-ref idref=\"US-6715057-B1-CLM-00001\">claim 1</claim-ref> wherein eliminating one or more levels of said page table with multiple levels occurs for translation of a page of data 512 Megabytes in size or above.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6715057-B1-CLM-00003\" num=\"3\"><claim-text>3. The computer system of <claim-ref idref=\"US-6715057-B1-CLM-00001\">claim 1</claim-ref> wherein translation of the original virtual address of the page of data stored in system memory into the corresponding physical address of the page of data occurs only if a lookup of the translation lookaside buffer for the original virtual address of the page of data results in a miss.</claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6715057-B1-CLM-00004\" num=\"4\"><claim-text>4. The computer system of <claim-ref idref=\"US-6715057-B1-CLM-00003\">claim 3</claim-ref> wherein the page table with multiple levels does not translate the original virtual address of the page of data if the lookup of the translation lookaside buffer for the original virtual address of the final level of the page table results in a hit.</claim-text></claim>"}, {"num": 5, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6715057-B1-CLM-00005\" num=\"5\"><claim-text>5. A computer system, comprising:</claim-text><claim-text>a processor including a translation lookaside buffer; </claim-text><claim-text>a system memory coupled to said processor and containing pages in a pre-designated space that are larger than or equal to a threshold size, said system memory containing a page table with multiple levels, said page table able to convert an original virtual address of a page of data stored in system memory into a corresponding physical address of the page of data, wherein conversion of a page of data above the threshold size occurs by determining from the original virtual address whether the virtual address corresponds to a page above the threshold size and, if so, by eliminating one or more levels of said page table; and </claim-text><claim-text>a disk drive coupled to said processor; </claim-text><claim-text>wherein said processor is capable of generating a first virtal address or a second virtual address, depending on whether the original virtual address corresponds to a page above the threshold size, said first and second virtual addresses corresponding to entries in said page table, and said first or second virtual addresses used to convert said original virtual address to the physical address. </claim-text></claim>"}, {"num": 6, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6715057-B1-CLM-00006\" num=\"6\"><claim-text>6. A method, comprising:</claim-text><claim-text>by examination of first virtual address, determining whether the first virtual address corresponds to a page in memory that is of a size greater than or equal to a threshold; </claim-text><claim-text>if the virtual address corresponds to a page that is of a size greater than or equal to the threshold, generating a second virtual address or a third virtual address of a page table entry based on whether the size of the page exceeds the threshold as encoded in one or more bits in said first virtual address; and </claim-text><claim-text>translating the first virtual address to a physical address using, depending on the page size, said second or third virtual address and using a multi-level page table in which at least one level of the table is not used in the translation. </claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6715057-B1-CLM-00007\" num=\"7\"><claim-text>7. The method of <claim-ref idref=\"US-6715057-B1-CLM-00006\">claim 6</claim-ref> wherein, if the first virtual address corresponds to a page that is of a size that is less than the threshold, translating the virtual address to a physical address using all levels of the multilevel page table.</claim-text></claim>"}, {"num": 8, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6715057-B1-CLM-00008\" num=\"8\"><claim-text>8. The computer system of <claim-ref idref=\"US-6715057-B1-CLM-00001\">claim 1</claim-ref> wherein said processor generates said first or second virtual address based on one or more bits in said original virtual address.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54154517\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>CROSS-REFERENCE TO RELATED APPLICATIONS</h4><p>This application relates to the following commonly assigned co-pending applications entitled:</p><p>\u201cApparatus And Method For Interfacing A High Speed Scan-Path With Slow-Speed Test Equipment,\u201d Ser. No. 09/653,642, filed Aug. 31, 2000, \u201cPriority Rules For Reducing Network Message Routing Latency,\u201d Ser. No. 09/652,322, filed Aug. 31, 2000, \u201cScalable Directory Based Cache Coherence Protocol,\u201d Ser. No. 09/652,703, filed Aug. 31, 2000, \u201cScalable Efficient I/O Port Protocol,\u201d Ser. No. 09/652,391, filed Aug. 31, 2000, \u201cFault Containment And Error Recovery Techniques In A Scalable Multiprocessor,\u201d Ser. No. 09/651,949, filed Aug. 31, 2000, \u201cSpeculative Directory Writes In A Directory Based Cache Coherent Nonuniform Memory Access Protocol,\u201d Ser. No. 09/652,834, filed Aug. 31, 2000, \u201cSpecial Encoding Of Known Bad Data,\u201d Ser. No. 09/652,314, filed Aug. 31, 2000, \u201cBroadcast Invalidate Scheme,\u201d Ser. No. 09/652,165, filed Aug. 31, 2000, \u201cMechanism To Track All Open Pages In A DRAM Memory System,\u201d Ser. No. 09/652,704, filed Aug. 31, 2000, \u201cProgrammable DRAM Address Mapping Mechanism,\u201d Ser. No. 09/653,093, filed Aug. 31, 2000, \u201cComputer Architecture And System For Efficient Management Of Bi-Directional Bus,\u201d Ser. No. 09/652,323, filed Aug. 31, 2000, \u201cAn Efficient Address Interleaving With Simultaneous Multiple Locality Options,\u201d Ser. No. 09/652,452, filed Aug. 31, 2000, \u201cA High Performance Way Allocation Strategy For A Multi-Way Associative Cache System,\u201d Ser. No. 09/653,092, filed Aug. 31, 2000, \u201cMethod And System For Absorbing Defects In High Performance Microprocessor With A Large N-Way Set Associative Cache,\u201d Ser. No. 09/651,948, filed Aug. 31, 2000, \u201cA Method For Reducing Directory Writes And Latency In A High Performance, Directory-Based, Coherency Protocol,\u201d Ser. No. 09/652,324, filed Aug. 31, 2000, \u201cMechanism To Reorder Memory Read and Write Transactions For Reduced Latency And Increased Bandwidth,\u201d Ser. No. 09/653,094, filed Aug. 31, 2000, \u201cSystem For Minimizing Memory Bank Conflicts In A Computer System,\u201d Ser. No. 09/652,325, filed Aug. 31, 2000, \u201cComputer Resource Management And Allocation System,\u201d Ser. No. 09/651,945, filed Aug. 31, 2000, \u201cInput Data Recovery Scheme,\u201d Ser. No. 09/653,643, filed Aug. 31, 2000, \u201cFast Lane Prefetching,\u201d Ser. No. 09/652,451, filed Aug. 31, 2000, \u201cMechanism For Synchronizing Multiple Skewed Source-Synchronous Data Channels With Automatic Initialization Feature,\u201d Ser. No. 09/652,480, filed Aug. 31, 2000, \u201cMechanism To Control The Allocation Of An N-Source Shared Buffer,\u201d Ser. No. 09/651,924, filed Aug. 31, 2000, and \u201cChaining Directory Reads And Writes To Reduce DRAM Bandwidth In A Directory Based CC-NUMA Protocol,\u201d Ser. No. 09/652,315, filed Aug. 31, 2000, all of which are incorporated by reference herein.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT</h4><p>Not applicable.</p><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>The present invention generally relates to a processor that includes a large range of page sizes stored in main memory. More particularly, the invention relates to a computer system with a multi-level page table and translation lookaside buffer (\u201cTLB\u201d) that efficiently maps virtual page addresses to physical page addresses for a memory system containing variable sized pages. Still more particularly, the present invention relates to a system that eliminates one level of the page table to efficiently map addresses of large pages in the memory system.</p><p>2. Background of the Invention</p><p>Almost all computer systems include a processor and a main memory. The main memory functions as the physical working memory of the computer system, where data is stored that has been or will be used by the processor and other system components. In computer systems that implement \u201cvirtual memory,\u201d software programs executing on the computer system reference main memory through the use of virtual addresses. A memory management unit (\u201cMMU\u201d) translates each virtual address specified by a software program instruction to a physical address that is passed to the main memory in order to retrieve the requested data. The use of virtual memory permits the size of programs to greatly exceed the size of the physical main memory and provides flexibility in the placement of programs in the main memory.</p><p>Implementing a virtual memory system requires establishing a correspondence between virtual address space and physical address space in the main memory. The most common technique by which to have virtual address space correspond with physical address space is by using a paging system. A paging system involves separately dividing virtual address space and its corresponding physical address space into contiguous blocks called pages. Each page has a virtual page number (\u201cVPN\u201d) address in virtual address space that corresponds to the physical page number (\u201cPPN\u201d) address of the page in physical address space.</p><p>For each access to main memory, a virtual page number address in virtual address space is translated into the corresponding physical page number address in physical address space and a page offset within the physical page is appended to the physical page number address. Thus, the virtual address subdivided into a Virtual Page Number Address:Page Offset is translated into a physical address consisting of Physical Page Number Address:Page Offset. The physical address is then used to access main memory. Translation of the virtual page number address into its corresponding physical page number address occurs through the use of page tables stored in physical main memory.</p><p>In order to reduce the total number of page table main memory accesses required per virtual-to-physical address translation, one or more translation-lookaside buffers are often provided in the MMU. TLB accesses reduce the overall average time required to perform the steps of a virtual-to-physical address translation. A TLB is a cache-like memory, typically implemented in Static Random Access Memory (\u201cSRAM\u201d) and/or Content Addressable Memory (\u201cCAM\u201d), that holds virtual page number address to physical page number address translations that have recently been fetched from the page table in physical main memory.</p><p>Access to a TLB entry holding an output physical page number address corresponding to an input virtual page number address obviates the need for and is typically many orders of magnitude faster than access to the page table in main memory.</p><p>If the TLB does not contain the requested translation (i.e., a TLB \u201cmiss\u201d occurs) then the MMU initiates a search of page tables stored in main memory for the requested virtual page number address. TLB miss handler software executing on the MMU then loads the physical page number address referenced by the virtual page number address into the TLB, where it may be available for subsequent fast access should translation for the same input virtual page number address be required at some future point.</p><p>Modem day computer systems implement large virtual address spaces requiring many virtual address bits. A simple page table array with one entry for each possible input virtual page number address, as commonly used in the prior art, is not a feasible solution for implementing the page table because of the slow translation times for such large input addresses and the enormous size of the page table array. To keep page tables required for address translation to a reasonable size and reduce translation times, some virtual to physical address translation schemes implement address translation in multiple stages. In a typical implementation, each stage of the virtual-to-physical address translation requires one or more accesses to the page table that is held in physical main memory. Each stage of the translation requires accessing a different level of the page table using a subfield of bits from the virtual address. Thus, for a virtual memory system that incorporates three stage address translation, the page table may be broken up into three levels with the virtual page number address field from the virtual address being divided into three subfields of bits. One advantage of multistage address translation is the reduction of the amount of main memory needed to store the page tables. The reduction of main memory needed to store the page tables comes from the ability to sparsely populate the page tables and the ability to page out parts of the page table.</p><p>The final stage of address translation implemented by the bottom level of the page table (e.g., three level system this would be the third level) prior to generating the physical page number address may be virtually mapped to provide quick access to the page table entries on a TLB miss. Prior to walking each level of the page table to generate the physical page number address, a page table lookup of the virtually mapped bottom level page table entry would occur. The virtually mapped page table lookup to the TLB may also result in a miss, thus resulting in a double translation lookaside buffer miss (virtual page number address TLB miss and virtually mapped final level of the page table TLB miss). Such double TLB misses are slow since a complete walk of the page table structure is then required. Thus, for the three level page table example, a double translation lookaside buffer miss would result in the physical page number address being generated by sequential multiple accesses to each of the three levels of the page table.</p><p>One solution to reduce translation lookaside buffer misses is to use larger page sizes so that the same physical main memory can be described by many fewer virtual page number addresses. TLB misses for a system with large page sizes are much less likely. For example, if the small page sizes are such that physical main memory can be mapped into a total of 16 pages while the TLB can only hold eight virtual-to-physical page translations, on the average a random TLB access will miss 50% of the time. Alternatively, if the virtual memory system is implemented with large page sizes such that physical main memory can be mapped into a total of eight pages while the TLB can hold eight virtual-to-physical page translations, an access to the TLB will never miss. However, large page sizes also result in more expensive and complex hardware to access the page offset within the physical page and increase unused fields within the pages (due to internal fragmentation). For this reason, high-performance processors generally allow any of a plurality of page sizes to be selected for different purposes.</p><p>High performance processors implementing a virtual memory system that allow multiple page sizes regardless of the page size use the same strategy for all page sizes to translate the virtual page number address into the physical page number address. In such systems, accesses to large size pages using the same translation mapping as small size pages may result in a TLB miss for the virtually mapped final level page table and for every virtual page number address TLB miss (a double TLB miss). This is because the page table is structured for small pages and the page table entries for large page sizes may be duplicated many times. Thus, using the same virtual-to-physical translation scheme for different size pages in a multiple page size virtual memory system may effectively waste half the entries in the TLB (one physical page number address entry corresponds to a virtual page number address and the same physical page number address entry corresponds to a subfield of bits in the virtual page number address) because with large page sizes a double TLB miss is more likely. The second unneeded access to the TLB would further reduce memory system performance and increase average memory access time for data. Finally, modern day virtual memory systems typically include a data cache that contains the data for the most recently translated virtual-to-physical page number addresses. A virtual memory system that supports multiple page sizes but structures the page table for small pages, thus containing duplicate entries for large page sizes, would include in the data cache duplicate copies of the data for each of the large size page entries. A virtual address translation resulting in a double miss to the TLB would also likely result in a miss to the data cache because of the unnecessary duplication of pages.</p><p>It would be advantageous if a virtual memory system could perform virtual-to-physical address translation using a multilevel page table that effectively eliminates the problems and disadvantages described above. The address translation scheme must be able to differentiate large page sizes from small page sizes and treat the virtual-to-physical translation of each type of page separately. Separate translation would avoid the duplication of large pages and allow the TLB to map much larger amounts of physical main memory. Despite the apparent performance advantages of such a virtual memory system, to date no such system has been implemented.</p><h4>BRIEF SUMMARY OF THE INVENTION</h4><p>The problems noted above are solved in large part by a computer system that includes a processor containing a translation lookaside buffer. The processor is connected to a system memory that contains a page table with multiple levels. The page table translates the virtual address of a page of data stored in system memory into the corresponding physical address of the page of data. If the size of the page is above a certain threshold value, then translation of the page using the multilevel page table occurs by eliminating one or more levels of the page table. In the preferred embodiment, the threshold value is 512 Megabytes. The multilevel page table is only used for translation of the virtual address of the page of data stored in system memory into the corresponding physical address of the page of data if a lookup of the translation lookaside buffer for the virtual address of the page of data results in a miss. The translation lookaside buffer also contains entries from the final level of the page table (i.e., physical addresses of pages of data) that correspond to a subfield of bits from the corresponding virtual addresses of the page of data. Virtual-to-physical address translation using the multilevel page table is not required if the translation lookaside buffer contains the needed physical address of the page of data corresponding to the subfield of bits from the virtual address of the page of data.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>For a detailed description of the preferred embodiments of the invention, reference will now be made to the accompanying drawings in which:</p><p>FIG. 1 shows a system diagram of a plurality of processors coupled together;</p><p>FIGS. 2<i>a </i>and <b>2</b><i>b </i>show a block diagram of the processors of FIG. 1;</p><p>FIG. 3 shows the translation of a virtual address to a physical address using a translation lookaside buffer and page table;</p><p>FIG. 4 shows the translation of a virtual address to a physical address using a translation lookaside buffer and multilevel page table; and</p><p>FIG. 5 shows the translation of a virtual address to a physical address using a variable level page table in which one level is eliminated for large page sizes.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>NOTATION AND NOMENCLATURE</h4><p>Certain terms are used throughout the following description and claims to refer to particular system components. As one skilled in the art will appreciate, computer companies may refer to a component by different names. This document does not intend to distinguish between components that differ in name but not function. In the following discussion and in the claims, the terms \u201cincluding\u201d and \u201ccomprising\u201d are used in an open-ended fashion, and thus should be interpreted to mean \u201cincluding, but not limited to . . . \u201d. Also, the term \u201ccouple\u201d or \u201ccouples\u201d is intended to mean either an indirect or direct electrical connection. Thus, if a first device couples to a second device, that connection may be through a direct electrical connection, or through an indirect electrical connection via other devices and connections.</p><h4>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>Referring now to FIG. 1, in accordance with the preferred embodiment of the invention, computer system <b>90</b> constructed in accordance with the preferred embodiment comprises one or more processors <b>100</b> coupled to a memory <b>102</b> and an input/output (\u201cI/O\u201d) controller <b>104</b>. As shown in FIG. 1, computer system <b>90</b> includes 12 processors <b>100</b>, each processor coupled to a memory and an I/O controller. Although the computer system <b>90</b> is shown as a multiple processor system in FIG. 1, it should be understood that the present invention also may be implemented on a single processor system, and thus the following disclosure is intended to be illustrative of the preferred embodiment of practicing the invention, and is not intended to imply that the invention is limited to use in a multi-processor system.</p><p>According to the preferred embodiment, each processor preferably includes four ports for connection to adjacent processors. The inter-processor ports are designated \u201cnorth,\u201d \u201csouth,\u201d \u201ceast,\u201d and \u201cwest\u201d in accordance with the well-known Manhattan grid architecture. As such, each processor <b>100</b> can be connected to four other processors. The processors on both end of the system layout wrap around and connect to processors on the opposite side to implement a 2D torus-type connection. Although twelve processors <b>100</b> are shown in the exemplary embodiment of FIG. 1, any desired number of processors (e.g., 256) can be included. In the preferred embodiment, computer system <b>90</b> is designed to accommodate either 256 processors or 128 processors, depending on the size of the memory associated with the processors.</p><p>The I/O controller <b>104</b> provides an interface to various input/output devices such as disk drives <b>105</b> and <b>106</b> as shown. Data from the I/O devices thus enters the 2D torus via the I/O controllers.</p><p>In accordance with the preferred embodiment, the memory <b>102</b> preferably comprises RAMbus\u2122 memory devices, but other types of memory devices can be used if desired. The capacity of the memory devices <b>102</b> can be any suitable size. Further, memory devices <b>102</b> preferably are implemented as Rambus Interface Memory Modules (\u201cRIMMs\u201d).</p><p>In general, computer system <b>90</b> can be configured so that any processor <b>100</b> can access its own memory <b>102</b> and I/O devices as well as the memory and I/O devices of all other processors in the network. Preferably, the computer system may have physical connections between each processor resulting in low interprocessor communication times and improved memory and I/O device access reliability. If physical connections are not present between each pair of processors, a pass-through or bypass path is preferably implemented in each processor that permits accesses to a processor's memory and I/O devices by another processor through one or more pass-through processors.</p><p>Referring now to FIGS. 2<i>a </i>and <b>2</b><i>b</i>, each processor <b>100</b> preferably includes an instruction cache <b>110</b>, an instruction fetch, issue and retire unit (\u201cIbox\u201d) <b>120</b>, an integer execution unit (\u201cEbox\u201d) <b>130</b>, a floating-point execution unit (\u201cFbox\u201d) <b>140</b>, a memory reference unit (\u201cMbox\u201d) <b>150</b>, a data cache <b>160</b>, an L<b>2</b> instruction and data cache control unit (\u201cCbox\u201d) <b>170</b>, a level L<b>2</b> cache <b>180</b>, two memory controllers (\u201cZbox<b>0</b>\u201d and \u201cZbox<b>1</b>\u201d) <b>190</b>, and an interprocessor and I/O router unit (\u201cRbox\u201d) <b>200</b>. The following discussion describes each of these units.</p><p>Each of the various functional units <b>110</b>-<b>200</b> contains control logic that communicate with various other functional units control logic as shown. The instruction cache control logic <b>110</b> communicates with the Ibox <b>120</b>, Cbox <b>170</b>, and L<b>2</b> Cache <b>180</b>. In addition to the control logic communicating with the instruction cache <b>110</b>, the Ibox control logic <b>120</b> communicates with Ebox <b>130</b>, Fbox <b>140</b> and Cbox <b>170</b>. The Ebox <b>130</b> and Fbox <b>140</b> control logic both communicate with the Mbox <b>150</b>, which in turn communicates with the data cache <b>160</b> and Cbox <b>170</b>. The Cbox control logic also communicates with the L<b>2</b> cache <b>180</b>, Zboxes <b>190</b>, and Rbox <b>200</b>.</p><p>Referring still to FIGS. 2<i>a </i>and <b>2</b><i>b</i>, the Ibox <b>120</b> preferably includes a fetch unit <b>121</b> which contains a virtual program counter (\u201cVPC\u201d) <b>122</b>, a branch predictor <b>123</b>, an instruction-stream translation buffer <b>124</b>, an instruction predecoder <b>125</b>, a retire unit <b>126</b>, decode and rename registers <b>127</b>, an integer instruction queue <b>128</b>, and a floating point instruction queue <b>129</b>. Generally, the VPC <b>122</b> maintains virtual addresses for instructions that are in flight. An instruction is said to be \u201cin-flight\u201d from the time it is fetched until it retires or aborts. The Ibox <b>120</b> can accommodate as many as 80 instructions, in 20 successive fetch slots, in flight between the decode and rename registers <b>127</b> and the end of the pipeline. The VPC preferably includes a 20-entry table to store these fetched VPC addresses.</p><p>The Ibox <b>120</b> with regard to branch instructions uses the branch predictor <b>123</b>. A branch instruction requires program execution either to continue with the instruction immediately following the branch instruction if a certain condition is met, or branch to a different instruction if the particular condition is not met. Accordingly, the outcome of a branch instruction is not known until the instruction is executed. In a pipelined architecture, a branch instruction (or any instruction for that matter) may not be executed for at least several, and perhaps many, clock cycles after the fetch unit in the processor fetches the branch instruction. In order to keep the pipeline full, which is desirable for efficient operation, the processor includes branch prediction logic that predicts the outcome of a branch instruction before it is actually executed (also referred to as \u201cspeculating\u201d). The branch predictor <b>123</b>, which receives addresses from the VPC queue <b>122</b>, preferably bases its speculation on short and long-term history of prior instruction branches. As such, using branch prediction logic, a processor's fetch unit can speculate the outcome of a branch instruction before it is actually executed. The speculation, however, may or may not turn out to be accurate. That is, the branch predictor logic may guess wrong regarding the direction of program execution following a branch instruction. If the speculation proves to have been accurate, which is determined when the processor executes the branch instruction, then the next instructions to be executed have already been fetched and are working their way through the pipeline.</p><p>If, however, the branch speculation performed by the branch predictor <b>123</b> turns out to have been the wrong prediction (referred to as \u201cmisprediction\u201d or \u201cmisspeculation\u201d), many or all of the instructions behind the branch instruction may have to be flushed from the pipeline (i.e., not executed) because of the incorrect fork taken after the branch instruction. Branch predictor <b>123</b> uses any suitable branch prediction algorithm, however, that results in correct speculations more often than misspeculations, and the overall performance of the processor is better (even in the face of some misspeculations) than if speculation was turned off.</p><p>The instruction translation buffer (\u201cITB\u201d) <b>124</b> couples to the instruction cache <b>110</b> and the fetch unit <b>121</b>. The ITB <b>124</b> comprises a 128-entry, fully associative instruction-stream translation buffer that is used to store recently used instruction-stream address translations and page protection information. Preferably, each of the entries in the ITB <b>124</b> may be 1, 8, 64 or 512 contiguous 8-kilobyte (\u201cKB\u201d) pages or 1, 32, 512, 8192 contiguous 64-kilobyte pages. The allocation scheme used for the ITB <b>124</b> is a round-robin scheme, although other schemes can be used as desired.</p><p>The predecoder <b>125</b> reads an octaword (16 contiguous bytes) from the instruction cache <b>110</b>. Each octaword read from instruction cache may contain up to four naturally aligned instructions per cycle. Branch prediction and line prediction bits accompany the four instructions fetched by the predecoder <b>125</b>. The branch prediction scheme implemented in branch predictor <b>123</b> generally works most efficiently when only one branch instruction is contained among the four fetched instructions. The predecoder <b>125</b> predicts the instruction cache line that the branch predictor <b>123</b> will generate. The predecoder <b>125</b> generates fetch requests for additional instruction cache lines and stores the instruction stream data in the instruction cache.</p><p>Referring still to FIGS. 2<i>a </i>and <b>2</b><i>b</i>, the retire unit <b>126</b> fetches instructions in program order, executes them out of order, and then retires (also called \u201ccommitting\u201d an instruction) them in order. The Ibox <b>120</b> logic maintains the architectural state of the processor by retiring an instruction only if all previous instructions have executed without generating exceptions or branch mispredictions. An exception is any event that causes suspension of normal instruction execution. Retiring an instruction commits the processor to any changes that the instruction may have made to the software accessible registers and memory. The processor <b>100</b> preferably includes the following three machine code accessible hardware: integer and floating-point registers, memory, internal processor registers. The retire unit <b>126</b> of the preferred embodiment can retire instructions at a sustained rate of eight instructions per cycle, and can retire as many as 11 instructions in a single cycle.</p><p>The decode and rename registers <b>127</b> contain logic that forwards instructions to the integer and floating-point instruction queues <b>128</b>, <b>129</b>. The decode and rename registers <b>127</b> perform preferably the following two functions. First, the decode and rename registers <b>127</b> eliminate register write-after-read (\u201cWAR\u201d) and write-after-write (\u201cWAW\u201d) data dependency while preserving true read-after-write (\u201cRAW\u201d) data dependencies. This permits instructions to be dynamically rescheduled. Second, the decode and rename registers <b>127</b> permit the processor to speculatively execute instructions before the control flow previous to those instructions is resolved.</p><p>The logic in the decode and rename registers <b>127</b> preferably translates each instruction's operand register specifiers from the virtual register numbers in the instruction to the physical register numbers that hold the corresponding architecturally-correct values. The logic also renames each instruction destination register specifier from the virtual number in the instruction to a physical register number chosen from a list of free physical registers, and updates the register maps. The decode and rename register logic can process four instructions per cycle. Preferably, the logic in the decode and rename registers <b>127</b> does not return the physical register, which holds the old value of an instruction's virtual destination register, to the free list until the instruction has been retired, indicating that the control flow up to that instruction has been resolved.</p><p>If a branch misprediction or exception occurs, the register logic backs up the contents of the integer and floating-point rename registers to the state associated with the instruction that triggered the condition, and the fetch unit <b>121</b> restarts at the appropriate Virtual Program Counter (\u201cVPC\u201d). Preferably, as noted above, 20 valid fetch slots containing up to 80 instructions can be in flight between the registers <b>127</b> and the end of the processor's pipeline, where control flow is finally resolved. The register <b>127</b> logic is capable of backing up the contents of the registers to the state associated with any of these 80 instructions in a single cycle. The register logic <b>127</b> preferably places instructions into the integer or floating-point issue queues <b>128</b>, <b>129</b>, from which they are later issued to functional units <b>130</b> or <b>136</b> for execution.</p><p>The integer instruction queue <b>128</b> preferably includes capacity for 20 integer instructions. The integer instruction queue <b>128</b> issues instructions at a maximum rate of four instructions per cycle. The specific types of instructions processed through queue <b>128</b> include: integer operate commands, integer conditional branches, unconditional branches (both displacement and memory formats), integer and floating-point load and store commands, Privileged Architecture Library (\u201cPAL\u201d) reserved instructions, integer-to-floating-point and floating-point-integer conversion commands.</p><p>Referring still to FIGS. 2<i>a </i>and <b>2</b><i>b</i>, the integer execution unit (\u201cEbox\u201d) <b>130</b> includes arithmetic logic units (\u201cALUs\u201d) <b>131</b>, <b>132</b>, <b>133</b>, and <b>134</b> and two integer register files <b>135</b>. Ebox <b>130</b> preferably comprises a 4-path integer execution unit that is implemented as two functional-unit \u201cclusters\u201d labeled 0 and 1. Each cluster contains a copy of an 80-entry, physical-register file and two subclusters, named upper (\u201cU\u201d) and lower (\u201cL\u201d). As such, the subclusters <b>131</b>-<b>134</b> are labeled U<b>0</b>, L<b>0</b>, U<b>1</b>, and L<b>1</b>. Bus <b>137</b> provides cross-cluster communication for moving integer result values between the clusters.</p><p>The subclusters <b>131</b>-<b>134</b> include various components that are not specifically shown in FIG. 2<i>a</i>. For example, the subclusters preferably include four 64-bit adders that are used to calculate results for integer add instructions, logic units, barrel shifters and associated byte logic, conditional branch logic, a pipelined multiplier for integer multiply operations, and other components known to those of ordinary skill in the art.</p><p>Each entry in the integer instruction queue <b>128</b> preferably asserts four request signals\u2014one for each of the Ebox <b>130</b> subclusters <b>131</b>, <b>132</b>, <b>133</b>, and <b>134</b>. A queue entry asserts a request when it contains an instruction that can be executed by the subcluster, if the instruction's operand register values are available within the subcluster. The integer instruction queue <b>128</b> includes two arbiters\u2014-one for the upper subclusters <b>132</b> and <b>133</b> and another arbiter for the lower subclusters <b>131</b> and <b>134</b>. Each arbiter selects two of the possible 20 requesters for service each cycle. Preferably, the integer instruction queue <b>128</b> arbiters choose between simultaneous requesters of a subcluster based on the age of the request\u2014older requests are given priority over newer requests. If a given instruction requests both lower subclusters, and no older instruction requests a lower subcluster, then the arbiter preferably assigns subcluster <b>131</b> to the instruction. If a given instruction requests both upper subclusters, and no older instruction requests an upper subcluster, then the arbiter preferably assigns subcluster <b>133</b> to the instruction.</p><p>The floating-point instruction queue <b>129</b> preferably comprises a 15-entry queue and issues the following types of instructions: floating-point operates, floating-point conditional branches, floating-point stores, and floating-point register to integer register transfers. Each queue entry preferably includes three request lines\u2014one for the add pipeline, one for the multiply pipeline, and one for the two store pipelines. The floating-point instruction queue <b>129</b> includes three arbiters\u2014one for each of the add, multiply, and store pipelines. The add and multiply arbiters select one requester per cycle, while the store pipeline arbiter selects two requesters per cycle, one for each store pipeline. As with the integer instruction queue <b>128</b> arbiters, the floating-point instruction queue arbiters select between simultaneous requesters of a pipeline based on the age of the request\u2014older request are given priority. Preferably, floating-point store instructions and floating-point register to integer register transfer instructions in even numbered queue entries arbitrate for one store port. Floating-point store instructions and floating-point register to integer register transfer instructions in odd numbered queue entries arbitrate for the second store port.</p><p>Floating-point store instructions and floating-point register to integer register transfer instructions are queued in both the integer and floating-point queues. These instructions wait in the floating-point queue until their operand register values are available from the floating-point execution unit (\u201cFbox\u201d) registers. The instructions subsequently request service from the store arbiter. Upon being issued from the floating-point queue <b>129</b>, the instructions signal the corresponding entry in the integer queue <b>128</b> to request service. Finally, upon being issued from the integer queue <b>128</b>, the operation is completed.</p><p>The integer registers <b>135</b>, <b>136</b> preferably contain storage for the processor's integer registers, results written by instructions that have not yet been retired, and other information as desired. The two register files <b>135</b>, <b>136</b> preferably contain identical values. Each register file preferably includes four read ports and six write ports. The four read ports are used to source operands to each of the two subclusters within a cluster. The six write ports are used to write results generated within the cluster or another cluster and to write results from load instructions.</p><p>The floating-point execution queue (\u201cFbox\u201d) <b>129</b> contains a floating-point add, divide and square-root calculation unit <b>142</b>, a floating-point multiply unit <b>144</b> and a register file <b>146</b>. Floating-point add, divide and square root operations are handled by the floating-point add, divide and square root calculation unit <b>142</b> while floating-point operations are handled by the multiply unit <b>144</b>.</p><p>The register file <b>146</b> preferably provides storage for 72 entries including 31 floating-point registers and 41 values written by instructions that have not yet been retired. The Fbox register file <b>146</b> contains six read ports and four write ports (not specifically shown). Four read ports are used to source operands to the add and multiply pipelines, and two read ports are used to source data for store instructions. Two write ports are used to write results generated by the add and multiply pipelines, and two write ports are used to write results from floating-point load instructions.</p><p>Referring still to FIG. 2<i>a</i>, the Mbox <b>150</b> controls the L<b>1</b> data cache <b>160</b> and ensures architecturally correct behavior for load and store instructions. The Mbox <b>150</b> preferably contains a datastream translation buffer (\u201cDTB\u201d) <b>151</b>, a load queue (\u201cLQ\u201d) <b>152</b>, a store queue (\u201cSQ\u201d) <b>153</b>, and a miss address file (\u201cMAF\u201d) <b>154</b>. The DTB <b>151</b> preferably comprises a fully associative translation buffer that is used to store data stream address translations and page protection information. Each of the entries in the DTB <b>151</b> can map 1, 8, 64, or 512 contiguous 8-KB pages. The allocation scheme preferably is round robin, although other suitable schemes could also be used. The DTB <b>151</b> also supports an 8-bit Address Space Number (\u201cASN\u201d) and contains an Address Space Match (\u201cASM\u201d) bit. The ASN is an optionally implemented register used to reduce the need for invalidation of cached address translations for process-specific addresses when a context switch occurs.</p><p>The LQ <b>152</b> preferably is a reorder buffer used for load instructions. It contains 32 entries and maintains the state associated with load instructions that have been issued to the Mbox <b>150</b>, but for which results have not been delivered to the processor and the instructions retired. The Mbox <b>150</b> assigns load instructions to LQ slots based on the order in which they were fetched from the instruction cache <b>110</b>, and then places them into the LQ <b>152</b> after they are issued by the integer instruction queue <b>128</b>. The LQ <b>152</b> also helps to ensure correct memory reference behavior for the processor.</p><p>The SQ <b>153</b> preferably is a reorder buffer and graduation unit for store instructions. It contains 32 entries and maintains the state associated with store instructions that have been issued to the Mbox <b>150</b>, but for which data has not been written to the data cache <b>160</b> and the instruction retired. The Mbox <b>150</b> assigns store instructions to SQ slots based on the order in which they were fetched from the instruction cache <b>110</b> and places them into the SQ <b>153</b> after they are issued by the instruction cache <b>110</b>. The SQ <b>153</b> holds data associated with the store instructions issued from the integer instruction unit <b>128</b> until they are retired, at which point the store can be allowed to update the data cache <b>160</b>. The LQ <b>152</b> also helps to ensure correct memory reference behavior for the processor.</p><p>The MAF <b>154</b> preferably comprises a 16-entry file that holds physical addresses associated with pending instruction cache <b>110</b> and data cache <b>160</b> fill requests and pending input/output (\u201cI/O\u201d) space read transactions.</p><p>Processor <b>100</b> preferably includes two on-chip primary-level (\u201cL<b>1</b>\u201d) instruction and data caches <b>110</b> and <b>160</b>, and single secondary-level, unified instruction/data (\u201cL<b>2</b>\u201d) cache <b>180</b> (FIG. 2<i>b</i>). The L<b>1</b> instruction cache <b>110</b> preferably is a 64-KB virtual-addressed, two-way set-associative cache. Prediction is used to improve the performance of the two-way set-associative cache without slowing the cache access time. Each instruction cache block preferably contains a plurality (preferably 16) instructions, virtual tag bits, an address space number, an address space match bit, a one-bit PALcode bit to indicate physical addressing, a valid bit, data and tag parity bits, four access-check bits, and predecoded information to assist with instruction processing and fetch control.</p><p>The L<b>1</b> data cache <b>160</b> preferably is a 64-KB, two-way set associative, virtually indexed, physically tagged, write-back, read/write allocate cache with 64-byte cache blocks. During each cycle the data cache <b>160</b> preferably performs one of the following transactions: two quadword (or shorter) read transactions to arbitrary addresses, two quadword write transactions to the same aligned octaword, two non-overlapping less-than quadword writes to the same aligned quadword, one sequential read and write transaction from and to the same aligned octaword. Preferably, each data cache block contains 64 data bytes and associated quadword ECC bits, physical tag bits, valid, dirty, shared, and modified bits, tag parity bit calculated across the tag, dirty, shared, and modified bits, and one bit to control round-robin set allocation. The data cache <b>160</b> is organized to contain two sets, each with 512 rows containing 64-byte blocks per row (i.e., 32 KB of data per set). The processor <b>100</b> uses two additional bits of virtual address beyond the bits that specify an 8-KB page in order to specify the data cache row index. A given virtual address might be found in four unique locations in the data cache <b>160</b>, depending on the virtual-to-physical translation for those two bits. The processor <b>100</b> prevents this aliasing by keeping only one of the four possible translated addresses in the cache at any time.</p><p>The L<b>2</b> cache <b>180</b> preferably is a 1.75-MB, seven-way set associative write-back mixed instruction and data cache. Preferably, the L<b>2</b> cache holds physical address data and coherence state bits for each block.</p><p>Referring now to FIG. 2<i>b</i>, the L<b>2</b> instruction and data cache control unit (\u201cCbox\u201d) <b>170</b> controls the L<b>2</b> instruction and data cache <b>190</b> and system ports. As shown, the Cbox <b>170</b> contains a fill buffer <b>171</b>, a data cache victim buffer <b>172</b>, a system victim buffer <b>173</b>, a cache miss address file (\u201cCMAF\u201d) <b>174</b>, a system victim address file (\u201cSVAF\u201d) <b>175</b>, a data victim address file (\u201cDVAF\u201d) <b>176</b>, a probe queue (\u201cPRBQ\u201d) <b>177</b>, a requester miss-address file (\u201cRMAF\u201d) <b>178</b>, a store to I/O space (\u201cSTIO\u201d) <b>179</b>, and an arbitration unit <b>181</b>.</p><p>The fill buffer <b>171</b> preferably in the Cbox is used to buffer data that comes from other functional units outside the Cbox. The data and instructions get written into the fill buffer and other logic units in the Cbox process the data and instructions before sending to another functional unit or the L<b>1</b> cache. The data cache victim buffer (\u201cVDF\u201d) <b>172</b> preferably stores data flushed from the L<b>1</b> cache or sent to the System Victim Data Buffer <b>173</b>. The System Victim Data Buffer (\u201cSVDB\u201d) <b>173</b> is used to send data flushed from the L<b>2</b> cache to other processors in the system and to memory. Cbox Miss-Address File (\u201cCMAF\u201d) <b>174</b> preferably holds addresses of L<b>1</b> cache misses. CMAF updates and maintains the status of these addresses. The System Victim-Address File (\u201cSVAF\u201d) <b>175</b> in the Cbox preferably contains the addresses of all SVDB data entries. Data Victim-Address File (\u201cDVAF\u201d) <b>176</b> preferably contains the addresses of all data cache victim buffer (\u201cVDF\u201d) data entries.</p><p>The Probe Queue (\u201cPRBQ\u201d) <b>177</b> preferably comprises a 18-entry queue that holds pending system port cache probe commands and addresses. This queue includes 10 remote request entries, 8 forward entries, and lookup L<b>2</b> tags and requests from the PRBQ content addressable memory (\u201cCAM\u201d) against the RMAF, CMAF and SVAF. Requestor Miss-Address Files (\u201cRMAF\u201d) <b>178</b> in the Cbox preferably accepts requests and responds with data or instructions from the L<b>2</b> cache. Data accesses from other functional units in the processor, other processors in the computer system or any other devices that might need data out of the L<b>2</b> cache are sent to the RMAF for service. The Store Input/Output (\u201cSTIO\u201d) <b>179</b> preferably transfer data from the local processor to I/O cards in the computer system. Finally, arbitration unit <b>181</b> in the Cbox preferably arbitrates between load and store accesses to the same memory location of the L<b>2</b> cache and informs other logic blocks in the Cbox and computer system functional units of the conflict.</p><p>Referring still to FIG. 2<i>b</i>, processor <b>100</b> preferably includes dual, integrated RAMbus memory controllers <b>190</b> (Zbox<b>0</b> and Zbox<b>1</b>). Each Zbox <b>190</b> controls 4 or 5 channels of information flow with the main memory <b>102</b> (FIG. <b>1</b>). Each Zbox preferably includes a front-end directory in-flight table (\u201cDIFT\u201d) <b>191</b>, a middle mapper <b>192</b>, and a back end <b>193</b>. The front-end DIFT <b>191</b> performs a number of functions such as managing the processor's directory-based memory coherency protocol, processing request commands from the Cbox <b>170</b> and Rbox <b>200</b>, sending forward commands to the Rbox, sending response commands to and receiving packets from the Cbox and Rbox, and tracking up to 32 in-flight transactions. The front-end DIFT <b>191</b> also sends directory read and write requests to the Zbox and conditionally updates directory information based on request type, Local Probe Response (\u201cLPR\u201d) status and directory state.</p><p>The middle mapper <b>192</b> maps the physical address into RAMbus device format by device, bank, row, and column. The middle mapper <b>192</b> also maintains an open-page table to track all open pages and to close pages on demand if bank conflicts arise. The mapper <b>192</b> also schedules RAMbus transactions such as timer-base request queues. The Zbox back end <b>193</b> preferably packetizes the address, control, and data into RAMbus format and provides the electrical interface to the RAMbus devices themselves.</p><p>The Rbox <b>200</b> provides the interfaces to as many as four other processors and one I/O controller <b>104</b> (FIG. <b>1</b>). The inter-processor interfaces are designated as North (\u201cN\u201d), South (\u201cS\u201d), East (\u201cE\u201d), and West (\u201cW\u201d) and provide two-way communication between adjacent processors.</p><p>Turning now to FIG. 3, translation of a virtual address to a physical address using a single level page table and translation lookaside buffer is shown for a virtual memory system supporting a single page size. A virtual address <b>310</b> can be subdivided into two subfields of bits, virtual page number address field <b>320</b> and page-offset field <b>325</b>. The virtual page number (\u201cVPN\u201d) address <b>320</b> is used in the translation lookaside buffer <b>340</b> to lookup the physical page number (\u201cPPN\u201d) address <b>345</b>. If the TLB <b>340</b> contains the particular PPN address <b>345</b> corresponding to the VPN address <b>320</b> (a TLB \u201chit\u201d), then the PPN address <b>345</b> is retrieved from the TLB <b>340</b> and appended with the page offset field <b>325</b> in appending circuit <b>350</b>. If the TLB <b>340</b> does not contain the particular PPN address <b>345</b> corresponding to the VPN address <b>320</b> (a TLB \u201cmiss\u201d) then a lookup of the page table <b>330</b> to determine the PPN address <b>345</b> occurs. The page table <b>330</b> contains all possible VPN addresses <b>320</b> of the virtual memory system. Once the PPN address <b>345</b> is determined from the page table <b>330</b>, the PPN address <b>345</b> corresponding to the VPN address <b>320</b> is loaded into the TLB <b>340</b>. TLB <b>340</b> is again accessed with the VPN address <b>320</b> to generate the recently loaded PPN address <b>345</b> at the TLB <b>340</b> output. The physical address <b>360</b> consisting of the physical page number address:page offset (PPN address:page offset) is then used to access physical main memory <b>370</b> of the computer system. The PPN address <b>345</b> determines the particular page <b>380</b> and the page offset determines the offset within the page <b>380</b> that the memory access is to.</p><p>Turning now to FIG. 4, in the preferred embodiment, translation of a virtual address to a physical address using a multilevel page table and translation lookaside buffer is shown for a virtual memory system supporting multiple page sizes. TLB <b>470</b> is organized such that each TLB entry contains a physical page number (\u201cPPN\u201d) address <b>480</b> for a 64-Kilobyte physical memory page <b>492</b>. Thus, larger size pages (e.g., 128-Kilobyte page <b>494</b>, 256-Kilobyte page, 256-Megabyte page <b>496</b>, etc.) may also be supported but with duplicate entries in the page table and other disadvantages as discussed above.</p><p>As shown in FIG. 4, a virtual address for pages of size less than 512 Megabyte can be subdivided into two subfields of bits, a VPN address field <b>410</b> and page-offset field <b>430</b>. The VPN address field <b>410</b> can be further subdivided into L<b>1</b> subfield <b>415</b>, L<b>2</b> subfield <b>420</b>, and L<b>3</b> subfield <b>425</b> of bits. The VPN address <b>410</b> consisting of subfields L<b>1</b>:L<b>2</b>:L<b>3</b> is provided to the multilevel page table that preferably is a three level page table <b>434</b>. The VPN address <b>410</b> with subfields L<b>1</b>:L<b>2</b>:L<b>3</b> is also provided to the TLB <b>470</b> and is used to perform a lookup in the TLB <b>470</b> for the PPN address <b>480</b>. If the VPN address <b>410</b> and corresponding PPN address <b>480</b> are not present in the TLB <b>470</b> (a TLB \u201cmiss\u201d) then a second access to the virtually mapped third level of the page table is performed. The virtually mapped third level of the page table is implemented by incorporating subfields L<b>1</b><b>415</b> and L<b>2</b><b>420</b> into a new VPN address <b>401</b> and using subfield L<b>3</b><b>425</b> as the page offset <b>404</b>. The TLB <b>470</b> thus contains physical page number addresses corresponding to both the virtual page number address <b>410</b> and the new VPN address as described above. If the second access to the TLB <b>470</b> using the new VPN address <b>401</b> described above also results in a miss (the PPN address <b>480</b> is not present in TLB <b>470</b>) then a \u201cwalk\u201d of the three level page table <b>434</b> is performed using the VPN address field <b>410</b>. Level <b>1</b><b>435</b> of the three level page table <b>434</b> indexed by L<b>1</b> subfield <b>415</b> selects Level <b>2</b><b>440</b> of the page table. The Level <b>1</b> page table <b>435</b> contains all possible L<b>1</b> subfield values. Each entry of the Level <b>1</b> page table <b>435</b> contains an address for a particular Level <b>2</b> page table <b>440</b>. After selection of a particular Level <b>2</b> page table <b>440</b> based on the L<b>1</b> subfield <b>415</b>, the Level <b>2</b> page table <b>440</b> is then accessed by L<b>2</b> subfield <b>420</b> to select a Level <b>3</b> page table <b>460</b>. Each Level <b>2</b> page table <b>440</b> contains all possible L<b>2</b> subfield values <b>420</b>. Each entry of a Level <b>2</b> page table <b>440</b> contains an address for a particular Level <b>3</b> page table <b>460</b>. After selection of a particular Level <b>3</b> page table <b>460</b> based on the L<b>2</b> subfield <b>420</b>, the Level <b>3</b> page table <b>460</b> is then accessed by L<b>3</b> subfield <b>425</b> to determine the physical page number address. Each Level <b>3</b> page table <b>460</b> contains a unique set of physical page number (\u201cPPN\u201d) addresses corresponding to unique VPN addresses. In the preferred embodiment, L<b>1</b> subfield <b>415</b>, L<b>2</b> subfield <b>420</b> and L<b>3</b> subfield <b>425</b> accesses to the Level <b>1</b><b>435</b>, Level <b>2</b><b>440</b>, and Level <b>3</b><b>460</b> page tables occur as a sequential traversal of the page table levels. Thus, the L<b>2</b> subfield <b>420</b> is provided to a Level <b>2</b> page table <b>440</b> that has been selected by the L<b>1</b> subfield <b>415</b> indexing Level <b>1</b> page table <b>435</b>. Similarly, the L<b>3</b> subfield <b>425</b> is provided to a Level <b>3</b> page table <b>460</b> that has been selected by the L<b>2</b> subfield <b>420</b> indexing the Level <b>2</b> page table. The L<b>3</b> subfield <b>425</b> accessing the Level <b>3</b> page table <b>460</b> determines the PPN address <b>480</b>.</p><p>Once the PPN address <b>480</b> has been selected by a walk of the three level page table using the VPN Address <b>410</b> as described above, the PPN address <b>480</b> corresponding to the VPN address <b>410</b> is placed into the TLB <b>470</b>. The PPN address <b>480</b> corresponding to the VPN address <b>401</b> of the virtually mapped third level of the page table is also placed in the TLB <b>470</b>. TLB <b>470</b> is again accessed with the VPN address <b>410</b> to generate the recently loaded PPN address <b>480</b> at the TLB <b>470</b> output. The physical address <b>485</b> consisting of the physical page number address:page offset (PPN address:page offset) is then used to access physical main memory <b>490</b> of the computer system. The PPN address <b>480</b> determines the particular page and the page offset <b>430</b> determines the offset within the page that the memory access is to.</p><p>FIG. 5 of the preferred embodiment shows a virtual memory system that can perform virtual-to-physical address translations using a multilevel page table for multiple size pages. The preferred embodiment effectively eliminates one or more levels of the page table for a region of the address space. For virtual memory addresses in this region of the address space, many fewer page table entries are needed to translate the same amount of memory. The minimum page size in this region of the address space is a large page size, preferably 512 Megabytes or above, that corresponds to the elimination of the page table levels. Thus, the elimination of Level <b>3</b> of the page table of FIG. 5 would correspond to a large page size of 512 megabytes. The larger the page size, the fewer pages are required to describe physical main memory and thus the corresponding number of virtual memory address bits needed to differentiate the pages decreases. In the preferred embodiment, a wide range of page sizes may be used over the entire virtual address space, but only large pages above the minimum threshold (e.g., 512 Megabytes) can be used in the region of the address space where one or more levels of the page table has been eliminated.</p><p>The preferred embodiment of FIG. 5 allows efficient virtual address translation of different size pages without the disadvantages of the multilevel page table shown in FIG. <b>4</b>. The virtual address translation scheme shown in FIG. 5, unlike the scheme shown in FIG. 4, can eliminate one or more levels of the page table for a region of the virtual address space with minimum page sizes above a large page size threshold. To eliminate one or more levels of the page table for a region of the virtual address space, the preferred embodiment requires a combination of hardware and software to implement. The hardware modifies the virtual address if the memory access is to the region of the virtual address space using the larger page size. The software modifies the multilevel page table stored in physical main memory by eliminating one or more levels of the page table for the region of the virtual address space using the larger page size. The software also processes double TLB misses for the modified page table differently as described below.</p><p>Efficient virtual-to-physical translation of any size page is supported by the multilevel page table shown in FIG. <b>5</b>. Preferably, TLB <b>570</b> is organized such that each TLB entry contains a physical page number (\u201cPPN\u201d) address <b>580</b> for either a 64 Kilobyte physical memory page <b>592</b> or a 512 Megabyte physical memory page <b>597</b>. Each 64 Kilobyte physical page number address entry in the TLB corresponds to a VPN address consisting of subfields L<b>1</b>:L<b>2</b>:L<b>3</b><b>410</b> as shown in FIG. <b>4</b>. Each 512 Megabyte physical page number address entry in the TLB corresponds to a VPN address consisting of subfields L<b>1</b>:L<b>2</b><b>510</b> as shown in FIG. <b>5</b>. Thus, the L<b>3</b> subfield <b>525</b> is eliminated from the VPN address <b>510</b> for 512 Megabyte page entries in the TLB and the L<b>3</b> subfield bits <b>525</b> become part of the page offset <b>530</b>. In the preferred embodiment, the virtual memory system permits 512 Megabyte and larger pages preferably in only one region of virtual and corresponding physical address space. Therefore, the minimum page size in this region of address space must be 512 Megabytes. Memory intensive software applications that reference large amounts of memory can allocate virtual addresses in address space allowing 512 Megabyte pages or larger for faster memory access and reduced virtual-to-physical address translation times. Alternatively, the virtual memory system of the preferred embodiment in all other regions of virtual and corresponding physical address space includes a minimum page size of 64 Kilobytes. Thus, these regions of address space have pages of size between 64 Kilobytes and less than 512 Megabytes. Addresses in address space with a minimum page size of 64 Kilobytes use VPN address consisting of L<b>1</b>:L<b>2</b>:L<b>3</b> subfield and three level page table while addresses in address space with minimum 512 Megabyte page size use VPN address consisting of L<b>1</b>:L<b>2</b> subfield and two level page table.</p><p>As shown in FIG. 5, a virtual address for pages of size 512 Megabytes and larger can be subdivided into two subfields of bits, a VPN address field <b>510</b> and page offset field <b>530</b>. The VPN address field <b>510</b> can be further subdivided into L<b>1</b> subfield <b>515</b> and L<b>2</b> subfield <b>520</b>. The VPN address <b>510</b> consisting of subfields L<b>1</b>:L<b>2</b> is provided to the multilevel page table <b>534</b>. If the access is to pages of size less than 512 Megabytes, then the VPN address <b>510</b> consisting of subfields L<b>1</b>:L<b>2</b>:L<b>3</b> is provided to multilevel page table <b>534</b>. The VPN address <b>510</b> (L<b>1</b>:L<b>2</b> or L<b>1</b>:L<b>2</b>:L<b>3</b>) is also provided to the TLB <b>570</b> and is used to perform a lookup in the TLB <b>570</b> for the PPN address <b>580</b>. If the VPN address <b>510</b> and corresponding PPN address <b>580</b> are not present in the TLB <b>570</b> (a TLB \u201cmiss\u201d) then a second access to the virtually mapped Level <b>3</b> or Level <b>2</b> of the page table, depending on the size of the page, is performed. The virtually mapped Level <b>3</b> or Level <b>2</b> of the page table is implemented by incorporating subfield L<b>1</b><b>502</b> or subfields L<b>1</b><b>402</b> and L<b>2</b><b>403</b>, depending on the size of the page, into a new VPN address <b>401</b> or <b>501</b> and using subfield L<b>2</b><b>503</b> or subfield L<b>3</b><b>404</b>, depending on the size of the page, as the page offset. Virtual Page Table Base (\u201cVPTB\u201d) address field <b>406</b> or <b>506</b> is a constant set by the virtual memory system that permits the page tables to be mapped into a linear region of the virtual address space. The size of the page can quickly be determined by two upper virtual address bits <b>504</b>. If these bits <b>504</b> are set to 01 then the new VPN address for the virtually mapped bottom level should be interpreted as an access to pages of size 512 Megabytes or larger. The TLB <b>570</b> thus contains physical page number addresses corresponding to both the virtual page number address <b>510</b> and the new VPN address <b>401</b> or <b>501</b> as described above. If the second access to the TLB <b>570</b> using the new VPN address <b>401</b> or <b>501</b> described above also results in a miss (the PPN address <b>580</b> is not present in TLB <b>570</b>) then a \u201cwalk\u201d of the variable level page table <b>534</b> is performed using the appropriate VPN address field (L<b>1</b>:L<b>2</b>:L<b>3</b> or L<b>1</b>:L<b>2</b>) for the page size.</p><p>Level <b>1</b><b>535</b> of the variable level page table <b>534</b> indexed by L<b>1</b> subfield <b>515</b> selects the appropriate Level <b>2</b><b>540</b> of the page table as the first step in a walk of the variable level page table for accesses to memory regions with less than 512 Megabyte page sizes. The Level <b>1</b> page table <b>535</b> contains all possible L<b>1</b> subfield values. Each entry of the Level <b>1</b> page table <b>535</b> contains an address for a particular Level <b>2</b> page table <b>540</b>. After selection of a particular Level <b>2</b> page table <b>540</b> based on the L<b>1</b> subfield <b>515</b>, the Level <b>2</b> page table <b>540</b> is then accessed by L<b>2</b> subfield <b>420</b> to select a Level <b>3</b> page table <b>560</b>. Each Level <b>2</b> page table <b>540</b> contains all possible L<b>2</b> subfield values <b>520</b>. Each entry of a Level <b>2</b> page table <b>540</b> contains an address for a particular Level <b>3</b> page table <b>560</b>. After selection of a particular Level <b>3</b> page table <b>560</b> based on the L<b>2</b> subfield <b>520</b>, the Level <b>3</b> page table <b>560</b> is then accessed by L<b>3</b> subfield <b>525</b> to determine the physical page number address. Each Level <b>3</b> page table <b>560</b> contains a unique set of physical page number addresses corresponding to unique L<b>1</b>:L<b>2</b>:L<b>3</b> VPN addresses. In the preferred embodiment, L<b>1</b> subfield <b>515</b>, L<b>2</b> subfield <b>520</b> and L<b>3</b> subfield <b>525</b> accesses to the Level <b>1</b><b>535</b>, Level <b>2</b><b>540</b>, and Level <b>3</b><b>560</b> page tables occur as a sequential traversal of the page table levels. Thus, the L<b>2</b> subfield <b>520</b> is provided to a Level <b>2</b> page table <b>540</b> that has been selected by the L<b>1</b> subfield <b>515</b> indexing Level <b>1</b> page table <b>535</b>. Similarly, the L<b>3</b> subfield <b>525</b> is provided to a Level <b>3</b> page table <b>560</b> that has been selected by the L<b>2</b> subfield <b>520</b> indexing the Level <b>2</b> page table. The L<b>3</b> subfield <b>525</b> accessing the Level <b>3</b> page table <b>560</b> determines the PPN address <b>580</b>.</p><p>For accesses to memory regions supporting page sizes greater than 512 Megabytes, the first step in a walk of the variable level page table is Level <b>1</b><b>535</b> of the variable level page table <b>534</b> indexed by L<b>1</b> subfield <b>515</b> selecting the appropriate Level <b>2</b><b>540</b> of the page table. The Level <b>1</b> page table <b>535</b> contains all possible L<b>1</b> subfield values. Each entry of the Level <b>1</b> page table <b>535</b> contains an address for a particular Level <b>2</b> page table <b>540</b>. After selection of a particular Level <b>2</b> page table <b>540</b> based on the L<b>1</b> subfield <b>515</b>, the Level <b>2</b> page table <b>540</b> is then accessed by L<b>2</b> subfield <b>420</b> to determine the physical page number address. Each Level <b>2</b> page table <b>540</b> contains all possible L<b>2</b> subfield values <b>520</b>. Each Level <b>2</b> page table <b>560</b> contains a unique set of physical page number addresses <b>580</b> corresponding to unique L<b>1</b>:L<b>2</b> VPN addresses. Thus, unlike the Level <b>2</b> page table that support page sizes of less than 512 Megabytes and contain an address for a particular Level <b>3</b> page table, the Level <b>2</b> page tables supporting pages of 512 Megabytes and greater contain PPN addresses. For the preferred embodiment, the Level <b>2</b> page tables are loaded with either Level <b>3</b> page table addresses or PPN addresses by software during initialization of the page tables. In the preferred embodiment, L<b>1</b> subfield <b>515</b> and L<b>2</b> subfield <b>520</b> accesses to the Level <b>1</b><b>535</b> and Level <b>2</b><b>540</b> page tables occur as a sequential traversal of the page table levels. Thus, the L<b>2</b> subfield <b>520</b> is provided to a Level <b>2</b> page table <b>540</b> that has been selected by the L<b>1</b> subfield <b>515</b> indexing Level <b>1</b> page table <b>535</b>. The L<b>2</b> lookup of the Level <b>2</b> page table <b>540</b> then determines the PPN address <b>580</b>.</p><p>Once the PPN address <b>580</b> has been selected by a walk of the variable level page table <b>534</b> using the L<b>1</b>:L<b>2</b>:L<b>3</b> or L<b>1</b>:L<b>2</b> fields as described above, the PPN address <b>580</b> corresponding to the VPN address is stored in the TLB <b>570</b>. The PPN address <b>580</b> corresponding to the VPN address <b>401</b> or <b>501</b> of the virtually mapped final level of the page table is also placed in the TLB <b>570</b>. TLB <b>570</b> is again accessed with the VPN address to generate the recently loaded PPN address <b>580</b> at the TLB output <b>580</b>. The physical address <b>585</b> consisting of the physical page number address:page offset (PPN address:page offset) is then used to access physical main memory <b>590</b> of the computer system. The PPN address <b>580</b> determines the particular page and the page offset <b>530</b> determines the offset within the page that the memory access is to.</p><p>As mentioned above, the variable level page table permits a fixed size TLB to map much larger amounts of physical main memory. This is because duplication of TLB entries (i.e., virtual-to-physical address translations) for very large pages is tremendously reduced in the areas of memory space with minimum page size of 512 Megabytes. Since large sized pages consume many fewer duplicate entries, the number of TLB misses is greatly reduced. Furthermore, because duplication of TLB entries is reduced, duplication of pages in the data cache are also reduced and many more pages are likely to be found in the data cache. The variable level page table results in much more efficient virtual-to-physical address translation for software applications that reference a large amount of memory while still retaining the advantages of pages as small as 64 Kilobytes in size for all other software applications.</p><p>The above discussion is meant to be illustrative of the principles and various embodiments of the present invention. Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Richard E.", "last_name": "Kessler", "name": ""}, {"first_name": "Jeffrey G.", "last_name": "Wiedemeier", "name": ""}, {"first_name": "Eileen J.", "last_name": "Samberg", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "HEWLETT-PACKARD DEVELOPMENT COMPANY, L.P."}, {"first_name": "", "last_name": "HEWLETT PACKARD ENTERPRISE DEVELOPMENT LP", "name": ""}, {"first_name": "", "last_name": "HEWLETT-PACKARD DEVELOPMENT COMPANY, L.P.", "name": ""}, {"first_name": "", "last_name": "COMPAQ INFORMATION TECHNOLOGIES GROUP, L.P.", "name": ""}, {"first_name": "", "last_name": "COMPAQ COMPUTER CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/10"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/10        20060101A I20051008RMUS"}], "national_classes": [{"primary": true, "label": "711207"}, {"primary": false, "label": "711E12061"}, {"primary": false, "label": "711E12059"}], "ecla_classes": [{"label": "S06F212:652"}, {"label": "S06F212:651"}, {"label": "G06F  12/10D"}, {"label": "G06F  12/10L"}], "cpc_classes": [{"label": "G06F2212/652"}, {"label": "G06F2212/652"}, {"label": "G06F  12/1027"}, {"label": "G06F2212/651"}, {"label": "G06F  12/1009"}, {"label": "G06F  12/1027"}, {"label": "G06F2212/651"}, {"label": "G06F  12/1009"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "2000-08-31", "application_date": "2000-08-31", "family_members": [{"ucid": "US-6715057-B1", "titles": [{"lang": "EN", "text": "Efficient translation lookaside buffer miss processing in computer systems with a large range of page sizes"}]}]}