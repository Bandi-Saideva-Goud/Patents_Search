{"patent_number": "US-6427188-B1", "publication_id": 73081941, "family_id": 23993382, "publication_date": "2002-07-30", "titles": [{"lang": "EN", "text": "Method and system for early tag accesses for lower-level caches in parallel with first-level cache"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50354607\"><p>A system and method are disclosed which determine in parallel for multiple levels of a multi-level cache whether any one of such multiple levels is capable of satisfying a memory access request. Tags for multiple levels of a multi-level cache are accessed in parallel to determine whether the address for a memory access request is contained within any of the multiple levels. For instance, in a preferred embodiment, the tags for the first level of cache and the tags for the second level of cache are accessed in parallel. Also, additional levels of cache tags up to N levels may be accessed in parallel with the first-level cache tags. Thus, by the end of the access of the first-level cache tags it is known whether a memory access request can be satisfied by the first-level, second-level, or any additional N-levels of cache that are accessed in parallel. Additionally, in a preferred embodiment, the multi-level cache is arranged such that the data array of a level of cache is accessed only if it is determined that such level of cache is capable of satisfying a received memory access request. Additionally, in a preferred embodiment the multi-level cache is partitioned into N ways of associativity, and only a single way of a data array is accessed to satisfy a memory access request, thereby preserving the remaining ways of a data array to save power and resources that may be accessed to satisfy other instructions.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00001\" num=\"1\"><claim-text>1. A method of accessing a multi-level cache, said method comprising:</claim-text><claim-text>reciving a memory access request into a multi-level cache structure; </claim-text><claim-text>determining if said memory access request can be satisfied by a level of said multi-level cache structure, wherein said determining is performed in parallel for multiple levels of said multi-level cache structure; and </claim-text><claim-text>if determined that said memory access request can be satisfied by a level of said multi-level cache structure, then satisfying said memory access request by said level of said multi-level cache structure. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00002\" num=\"2\"><claim-text>2. The method of <claim-ref idref=\"US-6427188-B1-CLM-00001\">claim 1</claim-ref>, wherein said determining if said memory access request can be satisfied by a level of said multi-level cache structure further comprises:</claim-text><claim-text>determining whether a memory address requested to be accessed is included within a level of said multi-level cache structure. </claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00003\" num=\"3\"><claim-text>3. The method of <claim-ref idref=\"US-6427188-B1-CLM-00001\">claim 1</claim-ref>, further comprising:</claim-text><claim-text>accessing a data array of a level of cache only if it is determined that such level of cache is capable of satisfying said memory access request. </claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00004\" num=\"4\"><claim-text>4. The method of <claim-ref idref=\"US-6427188-B1-CLM-00003\">claim 3</claim-ref>, further comprising:</claim-text><claim-text>powering up said data array only if it is determined that said data array is capable of satisfying said memory access request. </claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00005\" num=\"5\"><claim-text>5. The method of <claim-ref idref=\"US-6427188-B1-CLM-00001\">claim 1</claim-ref>, wherein said multi-level cache is partitioned into multiple ways.</claim-text></claim>"}, {"num": 6, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00006\" num=\"6\"><claim-text>6. The method of <claim-ref idref=\"US-6427188-B1-CLM-00005\">claim 5</claim-ref>, further comprising:</claim-text><claim-text>accessing only a single way of a data array for a level of cache that has been determined to be capable of satisfying said memory access request. </claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00007\" num=\"7\"><claim-text>7. The method of <claim-ref idref=\"US-6427188-B1-CLM-00006\">claim 6</claim-ref>, further comprising:</claim-text><claim-text>accessing, in parallel with accessing said single way, a second way of said data array for said level of cache that has been determined to be capable of satisfying a second memory access request. </claim-text></claim>"}, {"num": 8, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00008\" num=\"8\"><claim-text>8. The method of <claim-ref idref=\"US-6427188-B1-CLM-00001\">claim 1</claim-ref>, further comprising:</claim-text><claim-text>detmining if said memory access request can be satisfied by a level of said multi-level cache structure for at least one level of cache in series with another level of said multi-level cache. </claim-text></claim>"}, {"num": 9, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00009\" num=\"9\"><claim-text>9. The method of <claim-ref idref=\"US-6427188-B1-CLM-00001\">claim 1</claim-ref>, wherein said determining if said memory access request can be satisfied by a level of said multi-level cache further comprises:</claim-text><claim-text>determining if said memory access request can be satisfied by a first level of said multi-level cache structure and determining if said memory access request can be satisfied by a second level of cache in parallel with said determining for said first level; and </claim-text><claim-text>queuing said memory access request for a data array of said second level only if it is determined that said first level cannot satisfy said memory access request and determined that said second level can satisfy said memory access request. </claim-text></claim>"}, {"num": 10, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00010\" num=\"10\"><claim-text>10. The method of <claim-ref idref=\"US-6427188-B1-CLM-00001\">claim 1</claim-ref>, wherein said memory access request is a request that only requires access to tags of a level of said cache, further comprising:</claim-text><claim-text>accessing said tags of said level of said cache without accessing a data array of said level of said cache to satisfy said memory access request. </claim-text></claim>"}, {"num": 11, "parent": 10, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00011\" num=\"11\"><claim-text>11. The method of <claim-ref idref=\"US-6427188-B1-CLM-00010\">claim 10</claim-ref>, wherein said memory access request is a snoop request.</claim-text></claim>"}, {"num": 12, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00012\" num=\"12\"><claim-text>12. A computer system comprising:</claim-text><claim-text>a processor that executes instructions; and </claim-text><claim-text>a multi-level cache structure accessible by said processor to satisfy memory access requests, wherein said multi-level cache structure is configured to receive a memory access request and determine in parallel for multiple levels of said multi-level cache structure whether one of said multiple levels is capable of satisfying said received memory access request. </claim-text></claim>"}, {"num": 13, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00013\" num=\"13\"><claim-text>13. The computer system of <claim-ref idref=\"US-6427188-B1-CLM-00012\">claim 12</claim-ref>, wherein said multi-level cache structure comprises:</claim-text><claim-text>a TLB that receives a virtual address for said memory access request and outputs a corresponding physical address. </claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00014\" num=\"14\"><claim-text>14. The computer system of <claim-ref idref=\"US-6427188-B1-CLM-00013\">claim 13</claim-ref>, wherein said multi-level cache structure further comprises:</claim-text><claim-text>a tag memory array for a first cache level that receives a virtual address index and outputs a corresponding physical address; and </claim-text><claim-text>a tag memory array for a second cache level that receives a virtual address index and outputs a corresponding physical address. </claim-text></claim>"}, {"num": 15, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00015\" num=\"15\"><claim-text>15. The computer system of <claim-ref idref=\"US-6427188-B1-CLM-00014\">claim 14</claim-ref>, wherein said multi-level cache structure further comprises:</claim-text><claim-text>first compare circuitry that compares the physical address output by said TLB with the physical address output by said tag memory array for said first cache level to determine if a match is achieved for said first level of cache; and </claim-text><claim-text>second compare circuitry that compares the physical address output by said TLB with the physical address output by said tag memory array for said second cache level to determine if a match is achieved for said second level of cache, wherein said first compare circuitry and said second compare circuitry determine in parallel whether a match is achieved for either of said first level of cache or said second level of cache. </claim-text></claim>"}, {"num": 16, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00016\" num=\"16\"><claim-text>16. The computer system of <claim-ref idref=\"US-6427188-B1-CLM-00012\">claim 12</claim-ref>, wherein said multi-level cache structure further comprises:</claim-text><claim-text>a data array structure for each level of said multi-level cache, wherein a data array structure for a particular level of said multi-level cache is capable of being accessed to satisfy said memory access request if said multi-level cache determines that said particular level of cache is capable of satisfying said memory access request. </claim-text></claim>"}, {"num": 17, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00017\" num=\"17\"><claim-text>17. A multi-level cache structure that is accessible to a processor to satisfy memory access requests for instructions being executed by said processor, said cache structure comprising:</claim-text><claim-text>a means for receiving a memory access request from a processor; and </claim-text><claim-text>a means for determining whether a level of said multi-level cache structure is capable of satisfying a received memory access request, wherein said means for determining is configured to determine in parallel for multiple levels of said multi-level cache structure whether one of said multiple levels is capable of satisfying said received memory access request. </claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00018\" num=\"18\"><claim-text>18. The multi-level cache structure of <claim-ref idref=\"US-6427188-B1-CLM-00017\">claim 17</claim-ref>, wherein said means for receiving a memory access includes a TLB that receives a virtual address and outputs a corresponding physical address.</claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00019\" num=\"19\"><claim-text>19. The multi-level cache structure of <claim-ref idref=\"US-6427188-B1-CLM-00018\">claim 18</claim-ref>, wherein said means for receiving a memory access includes a tag memory array for a first cache level that receives a virtual address index and outputs a corresponding physical address and a tag memory array for a second cache level that receives a virtual address index and outputs a corresponding physical address.</claim-text></claim>"}, {"num": 20, "parent": 19, "type": "dependent", "paragraph_markup": "<claim id=\"US-6427188-B1-CLM-00020\" num=\"20\"><claim-text>20. The multi-level cache structure of <claim-ref idref=\"US-6427188-B1-CLM-00019\">claim 19</claim-ref>, wherein said means for determining whether a level of said multi-level cache structure is capable of satisfying a received memory access request includes:</claim-text><claim-text>first compare circuitry that compares the physical address output by said TLB with the physical address output by said tag memory array to determine if a match is achieved for said first level of cache; and </claim-text><claim-text>second compare circuitry that compares the physical address output by said TLB with the physical address output by said tag memory array to determine if a match is achieved for said second level of cache, wherein said first compare circuitry and said second compare circuitry determine in parallel whether a match is achieved for either of said first level of cache or said second level of cache.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53605085\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND</h4><p>Prior art cache designs for processors typically implement one or two level caches. More recently, multi-level caches having three or more levels have been designed in the prior art. Of course, it is desirable to have the cache implemented in a manner that allows the processor to access the cache in an efficient manner. That is, it is desirable to have the cache implemented in a manner such that the processor is capable of accessing the cache (i.e., reading from or writing to the cache) quickly so that the processor may be capable of executing instructions quickly and so that dependent instructions can receive data from cache as soon as possible.</p><p>An example of a prior art, multi-level cache design is shown in FIG. <b>1</b>. The exemplary cache design of FIG. 1 has a three-level cache hierarchy, with the first level referred to as L<b>0</b>, the second level referred to as L<b>1</b>, and the third level referred to as L<b>2</b>. Accordingly, as used herein L<b>0</b> refers to the first-level cache, L<b>1</b> refers to the second-level cache, L<b>2</b> refers to the third-level cache, and so on. It should be understood that prior art implementations of multi-level cache design may include more than three levels of cache, and prior art implementations having any number of cache levels are typically implemented in a serial manner as illustrated in FIG. <b>1</b>. As discussed more fully hereafter, multi-level caches of the prior art are generally designed such that a processor accesses each level of cache in series until the desired address is found. For example, when an instruction requires access to an address, the processor typically accesses the first-level cache L<b>0</b> to try to satisfy the address request (i.e., to try to locate the desired address). If the address is not found in L<b>0</b>, the processor then accesses the second-level cache L<b>1</b> to try to satisfy the address request. If the address is not found in L<b>1</b>, the processor proceeds to access each successive level of cache in a serial manner until the requested address is found, and if the requested address is not found in any of the cache levels, the processor then sends a request to the system's main memory to try to satisfy the request.</p><p>Typically, when an instruction requires access to a particular address, a virtual address is provided from the processor to the cache system. As is well-known in the art, such virtual address typically contains an index field and a virtual page number field. The virtual address is input into a translation look-aside buffer (\u201cTLB\u201d) <b>10</b> for the L<b>0</b> cache. The TLB <b>10</b> provides a translation from a virtual address to a physical address. The virtual address index field is input into the L<b>0</b> tag memory array(s) <b>12</b>. As shown in FIG. 1, the L<b>0</b> tag memory array <b>12</b> may be duplicated N times within the L<b>0</b> cache for N \u201cways\u201d of associativity. As used herein, the term \u201cway\u201d refers to a partition of the lower-level cache. For example, the lower-level cache of a system may be partitioned into any number of ways. Lower-level caches are commonly partitioned into four ways. As shown in FIG. 1, the virtual address index is also input into the L<b>0</b> data array structure(s) (or \u201cmemory structure(s)\u201d) <b>14</b>, which may also be duplicated N times for N ways of associativity. The L<b>0</b> data array structure(s) <b>14</b> comprise the data stored within the L<b>0</b> cache, which may be partitioned into several ways.</p><p>The L<b>0</b> tag <b>12</b> outputs a physical address for each of the ways of associativity. That physical address is compared with the physical address output by the L<b>0</b> TLB <b>10</b>. These addresses are compared in compare circuit(s) <b>16</b>, which may also be duplicated N times for N ways of associativity. The compare circuit(s) <b>16</b> generate a \u201chit\u201d signal that indicates whether a match is made between the physical addresses. As used herein, a \u201chit\u201d means that the data associated with the address being requested by an instruction is contained within a particular cache. As an example, suppose an instruction requests an address for a particular data labeled \u201cA.\u201d The data label \u201cA\u201d would be contained within the tag (e.g., the L<b>0</b> tag <b>12</b>) for the particular cache (e.g., the L<b>0</b> cache), if any, that contains that particular data. That is, the tag for a cache level, such as the L<b>0</b> tag <b>12</b>, represents the data that is residing in the data array for that cache level. Therefore, the compare circuitry, such as compare circuitry <b>16</b>, basically determines whether the incoming request for data \u201cA\u201d matches the tag information contained within a particular cache level's tag (e.g., the L<b>0</b> tag <b>12</b>). If a match is made, indicating that the particular cache level contains the data labeled \u201cA,\u201d then a hit is achieved for that particular cache level.</p><p>Typically, the compare circuit(s) <b>16</b> generate a single signal for each of the ways, resulting in N signals for N ways of associativity, wherein such signal indicates whether a hit was achieved for each way. The hit signals (i.e., \u201cL<b>0</b> way hits\u201d) are used to select the data from the L<b>0</b> data array(s) <b>14</b>, typically through multiplexer (\u201cMUX\u201d) <b>18</b>. As a result, MUX <b>18</b> provides the cache data from the L<b>0</b> cache if a way hit is found in the L<b>0</b> tags. If the signals generated from the compare circuitry <b>16</b> are all zeros, meaning that they are no hits, then \u201cmiss\u201d logic <b>20</b> is used to generate a L<b>0</b> cache miss signal. Such L<b>0</b> cache miss signal then triggers control to send the memory instruction to the L<b>1</b> instruction queue <b>22</b>, which queues (or holds) memory instructions that are waiting to access the L<b>1</b> cache. Accordingly, if it is determined that the desired address is not contained within the L<b>0</b> cache, a request for the desired address is then made in a serial fashion to the L<b>1</b> cache.</p><p>In turn, the L<b>1</b> instruction queue <b>22</b> feeds the physical address index field for the desired address into the L<b>1</b> tag(s) <b>24</b>, which may be duplicated N times for N ways of associativity. The physical address index is also input to the L<b>1</b> data array(s) <b>26</b>, which may also be duplicated N times for N ways of associativity. The L<b>1</b> tag(s) <b>24</b> output a physical address for each of the ways of associativity to the L<b>1</b> compare circuit(s) <b>28</b>. The L<b>1</b> compare circuit(s) <b>28</b> compare the physical address output by L<b>1</b> tag(s) <b>24</b> with the physical address output by the L<b>1</b> instruction queue <b>22</b>. The L<b>1</b> compare circuit(s) <b>28</b> generate an L<b>1</b> hit signal(s) for each of the ways of associativity indicating whether a match between the physical addresses was made for any of the ways of L<b>1</b>. Such L<b>1</b> hit signals are used to select the data from the L<b>1</b> data array(s) <b>26</b> utilizing MUX <b>30</b>. That is, based on the L<b>1</b> hit signals input to MUX <b>30</b>, MUX <b>30</b> outputs the appropriate L<b>1</b> cache data from L<b>1</b> data array(s) <b>26</b> if a hit was found in the L<b>1</b> tag(s) <b>24</b>. If the L<b>1</b> way hits generated from the L<b>1</b> compare circuitry <b>28</b> are all zeros, indicating that there was no hit generated in the L<b>1</b> cache, then a miss signal is generated from the \u201cmiss\u201d logic <b>32</b>. Such a L<b>1</b> cache miss signal generates a request for the desired address to the L<b>2</b> cache structure <b>34</b>, which is typically implemented in a similar fashion as discussed above for the L<b>1</b> cache. Accordingly, if it is determined that the desired address is not contained within the L<b>1</b> cache, a request for the desired address is then made in a serial fashion to the L<b>2</b> cache. In the prior art, additional levels of hierarchy may be added after the L<b>2</b> cache, as desired, in a similar manner as discussed above for levels L<b>0</b> through L<b>2</b> (i.e., in a manner such that the processor accesses each level of the cache in series, until an address is found in one of the levels of cache). Finally, if a hit is not achieved in the last level of cache (e.g., L<b>2</b> of FIG. <b>1</b>), then the memory request is sent to the processor system bus to access the main memory of the system.</p><p>Multi-level cache designs of the prior art are problematic in that such designs require each level of cache to be accessed in series until a \u201chit\u201d is achieved. That is, when an address is requested, each level of cache is accessed in series until the requested address is found within the cache (or it is determined that the requested address does not reside within cache, wherein a request for the address is then made to the system's main memory). Accordingly, if a requested address is residing in the L<b>2</b> cache structure, the request must first be checked in the L<b>0</b> cache, and then next in the L<b>1</b> cache, in a serial manner, before it can begin the access into the L<b>2</b> cache. Therefore, the more levels of cache implemented within a design generally increases the amount of time required to access a higher-level cache (e.g., the third-level cache L<b>2</b> or higher) because of the serial nature of accessing each cache level one by one.</p><p>A further problem with prior art multi-level cache designs is that such prior art designs typically look up the tag and the data for a particular cache level in parallel in an attempt to improve the access time to that cache level. For example, in an attempt to improve the access time, a prior art implementation would typically perform a tag lookup for cache level L<b>1</b> utilizing the L<b>1</b> tag <b>24</b>, while also looking up the desired data in L<b>1</b> data array <b>26</b> in parallel. Accordingly, if the desired address is found in the L<b>1</b> tag <b>24</b>, the data from L<b>1</b> data array <b>26</b> may be readily available because the lookup in the L<b>1</b> data array <b>26</b> was performed in parallel with the tag lookup. However, with such prior art design, more of the data array (e.g., the L<b>1</b> data array <b>26</b>) is powered up than is necessary. For, example, assume that a four-way associative cache data structure is implemented. Prior art designs power up all four ways of the data array to lookup the desired data in parallel with performing the tag lookup. At best, only one of the four ways will need to be accessed for the desired address (assuming that the desired address is found within that level of cache), and possibly none of the four ways will need to be accessed for the desired address (if the desired address is not found within that level of cache). Accordingly, such prior art design wastes the power that is utilized to power up every way of a data array unnecessarily. Moreover, the resources of the data array are wasted in such prior art designs because each way of the data array is accessed without fully utilizing the resources of each way of the data array. That is, prior art designs typically access every way of a data array, thereby tying up the resources of every way of the data array (i.e., preventing those resources from being accessed by other instructions), while at best only utilizing one way of the data array and possibly not utilizing any of the ways of the data array (i.e., if the desired address is not found within that level of cache). Therefore, prior art designs tie up cache resources unnecessarily, thereby wasting resources that may potentially be used to satisfy other instructions.</p><p>Additionally, certain instructions encountered within a system only need to access the tags of the cache, without requiring access to the cache's data array. For example, snoops off of a system bus need to access the tags to find out if a certain cache line is resident in any of the levels of cache. As used herein a \u201csnoop\u201d is an inquiry from a first processor to a second processor as to whether a particular cache address is found within the second processor. A high percentage of the time, the tag access for a snoop will indicate that the cache line is not present, so no data access is necessary. However, as discussed above, prior art cache designs are typically implemented such that the cache data array is accessed in parallel with the tag lookup. Therefore, a snoop access of the tag typically wastes the resources of the data array because most of the time access to the data array is not needed. Furthermore, system bus snoops generally require a very quick response. Accordingly, the serial access design of prior art caches may result in a greater response time than is required by the system for responding to a snoop. Therefore, the multi-level cache designs of the prior art may negatively impact the time required to satisfy requests, such as snoop requests, that only require access to the cache's tags.</p><h4>SUMMARY OF THE INVENTION</h4><p>In view of the above a desire exists for a cache design that allows for upper-level caches (e.g., level two or higher) to be accessed in a timely manner. A further desire exists for a cache design that does not unnecessarily access the cache's data array, thus not wasting power or resources of the data array. Yet a further desire exists for a cache design that performs requests that require access only to the cache's tags, such as snoops, in a timely manner and in a manner that does not unnecessarily waste power or resources of the cache's data array.</p><p>These and other objects, features and technical advantges are achieved by a system and method which determine in parallel for multiple-levels of a multi-level cache whether any one of such multiple levels is capable of satisfying a memory access request. That is tags for multiple levels of a multi-level cache are accessed in parallel to determine whether the address for a memory access request is contained within any of the tiple levels. Thus, the tag accesses for multiple levels of a multi-level cache are performed early in the pipeline of the cache hierarchy to provide an early lookup of the tags. For instance, in a preferred embodiment the tags for the first level of cache and the tags for the second level of cache are accessed in parallel. Also, additional levels of cache tags up to N levels may be accessed in parallel with the first-level cache tags. As a result, as tags for the first-level cache are being accessed, tags for other levels of the cache are being accessed in parallel, such that by the end of the access of the first-level cache tags it is known whether a memory access request can be satisfied by the first-level, second-level, and any additional N-levels of cache that are accessed in parallel.</p><p>Additionally, in a preferred embodiment, the multi-level cache is arranged such that the data array of a level of cache is accessed only if it is determined that such level of cache is needed to satisfy a received memory access request. Accordingly, in a preferred embodiment, the data arrays of the multi-level cache are not unnecessarily accessed. For instance, in a preferred embodiment, the tag access is performed separate from the data array access for a level of cache, and the data array for that level of cache is not accessed if a hit is not achieved within that level's tags (i.e., the data is not present in that level of the cache). This has the advantage of saving power because the data arrays are not powered up unnecessarily. That is, unused memory banks (or data arrays) may not be powered up in a preferred embodiment, thereby reducing power consumption. Of course, in alternative embodiments, the unused memory banks may still be powered up, and any such embodiment is intended to be within the scope of the present invention. Also, it results in preserving the data array resources so that they may be used by other instructions, rather than unnecessarily wasting them by accessing them when the data is not present in that level of cache. Also, requests that require access only to the cache tags, such as snoop requests, do not cause the data array resources to be accessed because the tag access is separated from the data array access, in a preferred embodiment. Accordingly, in a preferred embodiment, requests that require access only to the cache's tags are performed in a timely manner and in a manner that preserves power and data array resources because the cache is implemented such that an access to the data array is not performed for such requests.</p><p>Additionally, in a preferred embodiment the multi-level cache is partitioned into N ways of associativity. Most preferably, the multi-level cache is partitioned into four ways. In a preferred embodiment, only a single way of a data array is accessed to satisfy a memory access request. That is, in a preferred embodiment, the cache tags for a level of cache are accessed to determine whether a requested memory address is found within a level of cache before such level's data array is accessed. Accordingly, when the data array for a level of cache is accessed, it is known in which way the desired data is residing. Therefore, only the one data array way in which the data resides is powered up and accessed to satisfy the access request. Because the other ways of the data array that are not capable of satisfying the request are not accessed, a saving in power and data array resources may be recognized.</p><p>In a most preferred embodiment, the multi-level cache may be implemented to allow maximum utilization of the cache data arrays. As discussed above, in a preferred embodiment, cache data arrays are not accessed unnecessarily. Accordingly, in a preferred embodiment, cache data arrays that are not capable of satisfying one request remain free to be accessed for another request. In a most preferred embodiment, the multi-level cache may be implemented such that multiple instructions may be satisfied by the cache data arrays in parallel. That is, the resources that remain free may be utilized to satisfy other memory access requests in an efficient manner.</p><p>It should be understood that in a preferred embodiment a multi-level cache structure is provided that is capable of receiving and satisfying any type of access request from an instruction, including a read request, write request, and read-modify-write request. It should be appreciated that a technical advantage of one aspect of the present invention is that a multi-level cache structure is implemented to allow for multiple levels of cache tags to be accessed in parallel, thereby allowing a determination to be made quickly and efficiently as to whether a memory access request can be satisfied by any one of such multiple levels of cache. By implementing the tag access of upper-level caches (e.g., level two or higher) early in the pipeline, fast access to higher-level cache or beyond cache (e.g., access to main memory) is achieved. A further technical advantage of one aspect of the present invention is that a multi-level cache design of a preferred embodiment does not unnecessarily access the cache's data array, thus preserving power and resources of the data array. Yet a further technical advantage of one aspect of the present invention is that a multi-level cache design of a preferred embodiment performs requests that require access only to the cache's tags, such as snoops, in a timely manner and in a manner that does not unnecessarily waste power or resources of the cache's data array.</p><p>The foregoing has outlined rather broadly the features and technical advantages of the present invention in order that the detailed description of the invention that follows may be better understood. Additional features and advantages of the invention will be described hereinafter which form the subject of the claims of the invention. It should be appreciated by those skilled in the art that the conception and specific embodiment disclosed may be readily utilized as a basis for modifying or designing other structures for carrying out the same purposes of the present invention. It should also be realized by those skilled in the art that such equivalent constructions do not depart from the spirit and scope of the invention as set forth in the appended claims.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWING</h4><p>For a more complete understanding of the present invention, and the advantages thereof, reference is now made to the following descriptions taken in conjunction with the accompanying drawing, in which:</p><p>FIG. 1 shows an exemplary multi-level cache design of the prior art; and</p><p>FIG. 2 shows a preferred embodiment for a multi-level cache design of the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION</h4><p>Turning to FIG. 2, a preferred embodiment of the present invention is shown. In a preferred embodiment, the first level cache structure L<b>0</b> is implemented in a manner as described for the prior art L<b>0</b> cache of FIG. <b>1</b>. More specifically, when an instruction requires access to a particular address, a virtual address is provided from the processor to the cache system. Such virtual address typically contains an index field and a virtual page number field. Most preferably, the virtual address is input into TLB <b>10</b> for the L<b>0</b> cache. Even though a preferred embodiment includes TLB <b>10</b> to translate a virtual address to a physical address, it should be understood that such a TLB implementation is not necessary to the present invention. Thus, while it is most preferable to implement the cache structure with a TLB <b>10</b>, in alternative embodiments the cache may be implemented without such a TLB <b>10</b>. For example, the cache structure may receive physical addresses rather than virtual addresses for memory access requests (and therefore not require a TLB), or the cache may be implemented in a manner to access the memory using virtual addresses received without requiring a translation of such virtual addresses to physical addresses. Furthermore, a structure other than a TLB <b>10</b> may be utilized to translate a received virtual address to a physical address.</p><p>In a preferred embodiment, the TLB <b>10</b> provides a translation from a virtual address to a physical address. The virtual address index field is input into the L<b>0</b> tag memory array(s) <b>12</b>. As shown in FIG. 2, the L<b>0</b> tag memory array <b>12</b> may be duplicated N times within the L<b>0</b> cache for N \u201cways\u201d of associativity. It should be understood that in a preferred embodiment the lower-level cache of a system may be partitioned into any number of ways. In a most preferred embodiment, the lower-level caches are partitioned into four ways. As shown in FIG. 2, the virtual address index is also input into the L<b>0</b> data array structure(s) (or \u201cmemory structure(s)\u201d) <b>14</b>, which may also be duplicated N times for N ways of associativity. It should be understood that the L<b>0</b> data array structure(s) <b>14</b> comprise the data stored within the L<b>0</b> cache, which may be partitioned into several ways.</p><p>The L<b>0</b> tag <b>12</b> outputs a physical address for each of the ways of associativity. That physical address is compared with the physical address output by the L<b>0</b> TLB <b>10</b> utilizing compare circuit(s) <b>16</b>, which may also be duplicated N times for N ways of associativity. As discussed in conjunction with FIG. 1, the compare circuit(s) <b>16</b> generate a \u201chit\u201d signal that indicates whether a match is made between the physical addresses. Most preferably, the compare circuit(s) <b>16</b> generate a single signal for each of the ways, resulting in N signals for N ways of associativity, wherein such signal indicates whether a hit was achieved for each way. The hit signals (i.e., \u201cL<b>0</b> way hits\u201d) are used to select the data from the L<b>0</b> data array(s) <b>14</b>, most preferably through a multiplexer (\u201cMUX\u201d) <b>18</b>. As a result, in a preferred embodiment, MUX <b>18</b> provides the cache data from the L<b>0</b> cache if a way hit is found in the L<b>0</b> tags. However, if the signals generated from the compare circuitry <b>16</b> are all zeros, meaning that they are no hits, then \u201cmiss\u201d logic <b>20</b> is used to generate a L<b>0</b> cache miss signal, in a preferred embodiment.</p><p>In a preferred embodiment, the L<b>1</b> tag memory array <b>24</b> is implemented early in the pipeline and in parallel with the first-level cache access. The L<b>1</b> tag memory array <b>24</b> is accessed having the index from the virtual address input thereto, and L<b>1</b> tag memory array <b>24</b> outputs a physical address, which is used by compare circuitry (comparator) <b>28</b> to compare the physical address generated from TLB <b>10</b> with the outputs of the L<b>1</b> tag memory array <b>24</b>. As a result, an early L<b>1</b> tag hit signal is generated in parallel with the first-level cache access. Accordingly, in a preferred embodiment, a hit signal is generated for both the L<b>0</b> tag <b>12</b> and the L<b>1</b> tag <b>24</b> in parallel. As shown in FIG. 2, if a hit is achieved in L<b>1</b>, then the request may be scheduled in the L<b>1</b> instruction queue <b>22</b> to be satisfied by the L<b>1</b> data array <b>26</b>.</p><p>In a preferred embodiment, if a hit is achieved in the L<b>0</b> cache, then the request is satisfied by the L<b>0</b> data array <b>14</b>, and such request is not scheduled in the L<b>1</b> instruction queue <b>22</b>. For instance, in a preferred embodiment, the output of miss logic <b>20</b> for the L<b>0</b> cache is combined with the L<b>1</b> hit signal output by compare circuitry <b>28</b>, such that only if a miss is indicated for the L<b>0</b> cache and a hit is achieved by the L<b>1</b> cache is the request inserted in the L<b>1</b> instruction queue <b>22</b>. Thus, a signal from miss logic <b>20</b> indicating that no hit was achieved in the L<b>0</b> cache triggers control to send the memory instruction to the L<b>1</b> instruction queue <b>22</b> if a hit is achieved in the L<b>1</b> tag <b>24</b>, as indicated by the output of compare circuitry <b>28</b>. Thus, in a preferred embodiment, if no way hit is achieved in the L<b>0</b> cache, but a way hit is achieved in the L<b>1</b> tag(s) <b>24</b>, then the request is loaded into the L<b>1</b> instruction queue <b>22</b> to access the L<b>1</b> data array(s) <b>26</b>.</p><p>In view of the above, in a preferred embodiment, the L<b>1</b> data array(s) <b>26</b> are only accessed if a hit is achieved within the L<b>1</b> tag(s) <b>24</b>. It should be understood, however, that alternative embodiments may be implemented such that the L<b>1</b> data array(s) <b>26</b> are accessed in parallel with the L<b>1</b> tag(s) <b>24</b>, and any such implementation is intended to be within the scope of the present invention. For example, in an alternative embodiment, the L<b>0</b> tag(s) <b>12</b> and L<b>1</b> tag(s) <b>24</b> may be accessed in parallel with each other, and the L<b>0</b> data array(s) <b>14</b> may be accessed in parallel with the L<b>0</b> tag(s) <b>12</b> and the L<b>1</b> data array(s) <b>26</b> may be accessed in parallel with the L<b>1</b> tag(s) <b>24</b>.</p><p>As discussed above, in a preferred embodiment, a memory access request is scheduled in the L<b>1</b> instruction queue <b>22</b> only after a determination has been made that a hit has been achieved in the L<b>1</b> tag(s) <b>24</b>. Additionally, in a preferred embodiment, information indicating the specific way to be accessed is included with a request scheduled in instruction queue <b>22</b> (i.e., the specific way for which a hit was achieved). That is, because the compare circuitry <b>28</b> determines if a hit has been achieved for the L<b>1</b> tag(s) <b>24</b> before the request is scheduled in L<b>1</b> queue <b>22</b>, the requests scheduled in L<b>1</b> queue <b>22</b> may include information specifying the way for which a hit was achieved. As a result, the L<b>1</b> instruction queue <b>22</b> issues the request to only the one data array <b>26</b> that needs to be accessed to satisfy the request, in a preferred embodiment. Thus, as shown in FIG. 2, if a hit is achieved within L<b>1</b> tag(s) <b>24</b> for a requested address, then only one of the data arrays <b>26</b>A-<b>26</b>D is accessed to satisfy the request. Accordingly, the remaining data arrays may be used to satisfy other instructions or may be powered down to save power on the processor. Thus, in a preferred embodiment, the L<b>1</b> instruction queue is provided information indicating the particular way of the data array that is needed to satisfy an instruction, which enables the instruction queue to issue instructions to the four data array structures <b>26</b>A through <b>26</b>D in an efficient manner. For example, suppose a first instruction requires access to L<b>1</b> data array way <b>0</b> (shown as <b>26</b>A in FIG. 2) and a second instruction requires access to L<b>1</b> data array way <b>1</b> (shown as <b>26</b>B in FIG. <b>2</b>). In a preferred embodiment, the first and second instructions could be issued in parallel with each other by the L<b>1</b> instruction queue <b>22</b>. Thus, it would be possible for N instructions to access N ways of a cache level's data array in parallel.</p><p>It will be recalled that prior art cache designs typically required an access to all ways of the data array, wherein only one of the ways, at best, was utilized to satisfy the request. Accordingly, a preferred embodiment provides a much more effective use of the cache resources by only powering up and accessing the data array way necessary to satisfy the request, leaving the remaining data array ways free to be powered down or accessed in parallel to satisfy other requests. Thus, in a preferred embodiment, while a data array of a first way is being utilized to satisfy a first request, one or more of the remaining data array ways may be utilized to satisfy a second request, thereby satisfying the second request more efficiently than if such second request were required to wait until after the first request had completed its access to the first way, as is typically required in prior art designs.</p><p>In view of the above, in a preferred embodiment, information is provided to enable only the data array way that is capable of satisfying the request to be accessed. It should be understood, however, that alternative embodiments may be implemented such that all ways of the data array for a level of cache for which a hit was achieved are accessed (e.g., all of data array(s) <b>26</b>A through <b>26</b>D), and any such implementation is intended to be within the scope of the present invention. For example, in an alternative embodiment, the L<b>0</b> tag(s) <b>12</b> and L<b>1</b> tag(s) <b>24</b> may be accessed in parallel with each other, and if a hit is achieved for the L<b>1</b> tag(s), all of the L<b>1</b> data array(s) <b>26</b>A through <b>26</b>D may be accessed in an attempt to satisfy the request.</p><p>As discussed above, in a preferred embodiment, the signals output from the compare circuitry <b>28</b> are combined with the output signals of the miss logic <b>20</b> of the L<b>0</b> cache. If the signals generated from the compare circuitry <b>28</b> are all zeros, meaning that they are not hits, and the signals generated from the miss logic <b>20</b> indicate that no hit is achieved for the L<b>0</b> cache, then the request cannot be satisfied by either the L<b>0</b> or the L<b>1</b> cache. Accordingly, in such case the request is not scheduled in the L<b>1</b> instruction queue <b>22</b>, which prevents an unnecessary access to the L<b>1</b> data array(s) <b>26</b>. Additionally, when no hit is made in either the L<b>0</b> or the L<b>1</b> cache, \u201cmiss\u201d logic <b>32</b> is used to generate a L<b>1</b> cache miss signal, in a preferred embodiment, which triggers control to send the request to the next sequential level of cache <b>50</b> or, if no further level of cache is present, to the system's main memory <b>60</b>.</p><p>Accordingly, by implementing the access of the L<b>1</b> tag <b>24</b> early in the pipeline to allow access to the L<b>1</b> tag <b>24</b> in parallel with access to the L<b>0</b> tag <b>12</b>, faster access of the L<b>2</b> cache is achieved, if necessary. That is, rather than accessing the L<b>0</b> and L<b>1</b> tags in series, as typically performed in the prior art, the L<b>0</b> and L<b>1</b> tags are accessed in parallel, resulting in faster access to the higher, L<b>2</b> cache level if the request cannot be satisfied by the lower L<b>0</b> or L<b>1</b> cache levels. Of course, it should be understood that any number of lower cache levels may be implemented in such a parallel fashion to allow faster access to a higher cache level, and any such implementation is intended to be within the scope of the present invention. For example, cache levels L<b>0</b>, L<b>1</b>, and L<b>2</b> may all be implemented in a parallel fashion, wherein the tags for each of such cache levels are accessed in parallel in an attempt to satisfy a request. Furthermore, higher-level caches may be implemented in a serial manner, such that the lower-level caches may be accessed in parallel and then higher-level caches accessed in a serial manner, and any such implementation is intended to be within the scope of the present invention. Accordingly, block <b>50</b> of FIG. 2 represents any additional levels of cache (e.g., cache levels L<b>2</b> through LN) that may be implemented within the cache design in either a parallel or serial manner. Of course, if a hit is not achieved within any of the cache levels, then a request may be made to the system's main memory <b>60</b> to satisfy the memory access request of an instruction.</p><p>In view of the above, in a preferred embodiment, the cache is implemented such that access is made to the lower-level cache tags in parallel (e.g., accessing L<b>0</b> tag <b>12</b> and L<b>1</b> tag <b>24</b> in parallel). For instance, access is made to the first-level cache tags <b>12</b> in parallel with one or more higher-level cache tags (e.g., L<b>1</b> tag <b>24</b>). Also, in a preferred embodiment, the cache is implemented such that if a hit is achieved for the first-level cache, then the request is satisfied by the L<b>0</b> cache data array <b>14</b> and the request is not scheduled in any higher-level instruction queue (e.g., the L<b>1</b> instruction queue <b>22</b>). However, if a hit is not achieved for the first-level cache and a hit is achieved for a higher-level cache (e.g., the L<b>1</b> cache), then the request is scheduled in such higher-level cache's instruction queue (e.g., the L<b>1</b> instruction queue <b>22</b>). Moreover, if a hit is not achieved for either the first-level cache or a higher-level cache (e.g., the L<b>1</b> cache) that is accessed in parallel with the first-level cache, then a request is not scheduled in an instruction queue for any of such higher-level caches, and a request is made to the next sequential level of cache (or to the system's main memory if no further levels of cache exist).</p><p>Additionally, in a preferred embodiment, instructions or activities that only require a tag access, such as a snoop request, do not cause a data access. For example, a snoop request maybe received by the cache structure of a first processor from the system bus from a second processor requesting whether a particular cache line is present within the first processor's cache. Such a snoop request may cause a physical or virtual address representing the desired address to be input to the cache system. In a preferred embodiment, that address is sent to the L<b>0</b> tag, as well as the L<b>1</b> tag in parallel. One advantage of accessing the first-level and second-level caches at the same time is that the compare results output by logic <b>16</b> and <b>28</b> may be sent to a snoop response block <b>36</b>, which then provides the result of whether the desired address is found within L<b>0</b> or L<b>1</b> of the processor's cache. Thus, a preferred embodiment provides a faster snoop response to the system bus than a cache implementation that requires an access of the L<b>0</b> cache first and then the L<b>1</b> cache in a serial manner. As a result, the snoop response may be achieved in a timely manner as a result of a parallel access of multiple levels of cache tags.</p><p>Even though a preferred embodiment has been shown and described herein as enabling parallel access to the first-level and second-level cache tags, it should be understood that the cache may be implemented such that any number of levels of cache tags may be accessed in parallel, and any such implementation is intended to be within the scope of the present invention. Furthermore, while multiple levels of cache tags are accessed in parallel in a preferred embodiment, the cache may be implemented such that other levels of cache are accessed in a serial manner (as in prior art implementations), and any such implementation is intended to be within the scope of the present invention. For instance, the first two levels of cache tags may be accessed in parallel, and if a hit is not achieved therein, access may be made to a third level of cache in a serial fashion.</p><p>It should also be understood that a preferred embodiment may be implemented for a cache having any number of ports. That is, any number of ports may be implemented within the cache levels. For example, the cache structure may be implemented as a four-ported cache, wherein four accesses are made to the L<b>0</b> cache tags and four are made to the L<b>1</b> cache tags in parallel (simultaneously). Thus, the cache may be implemented having a single port up to any number of ports. Also, the cache may be implemented such that the number of ports differs for the various levels of cache. For example, a first-level cache may be implemented having two ports and two accesses per cycle, and a second-level cache may be implemented to support four ports and four accesses per cycle.</p><p>It should be understood that in a preferred embodiment the cache structure is capable of receiving and satisfying any type of access request from an instruction, including a read request, write request, and read-modify-write request. It should also be understood that although a preferred embodiment has been described herein as including a TLB to translate a virtual address to a physical address, such a TLB implementation is not necessary to the invention. Thus, while it is most preferable to implement the cache with a TLB, the cache may be implemented without having such a TLB. For example, the cache may receive physical addresses for a memory access request (and therefore not require a TLB), the cache may be implemented in a manner to receive virtual addresses without requiring a TLB to translate such virtual addresses to a physical address, or a structure other than a TLB may be utilized to translate a virtual address to a physical address. Thus, the scope of the present invention is intended to encompass a cache implementation with or without a TLB. It should further be understood that a cache structure of the present invention may be implemented within any type of computer system having a processor, including but not limited to a personal computer (PC), laptop computer, and personal data assistant (e.g., a palmtop PC). It should further be understood that a cache structure of the present invention may be implemented for a network processor, such as a network processor that is implemented for Internet, Intranet, or other network applications.</p><p>Although the present invention and its advantages have been described in detail, it should be understood that various changes, substitutions and alterations can be made herein without departing from the spirit and scope of the invention as defined by the appended claims. Moreover, the scope of the present application is not intended to be limited to the particular embodiments of the process, machine, manufacture, composition of matter, means, methods and steps described in the specification. As one of ordinary skill in the art will readily appreciate from the disclosure of the present invention, processes, machines, manufacture, compositions of matter, means, methods, or steps, presently existing or later to be developed that perform substantially the same function or achieve substantially the same result as the corresponding embodiments described herein may be utilized according to the present invention. Accordingly, the appended claims are intended to include within their scope such processes, machines, manufacture, compositions of matter, means, methods, or steps.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Terry L", "last_name": "Lyon", "name": ""}, {"first_name": "Eric R", "last_name": "DeLano", "name": ""}, {"first_name": "Dean A.", "last_name": "Mulla", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "HEWLETT-PACKARD COMPANY"}, {"first_name": "", "last_name": "HEWLETT PACKARD ENTERPRISE DEVELOPMENT LP", "name": ""}, {"first_name": "", "last_name": "HEWLETT-PACKARD DEVELOPMENT COMPANY, L.P.", "name": ""}, {"first_name": "", "last_name": "HEWLETT-PACKARD COMPANY", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMFR"}], "national_classes": [{"primary": true, "label": "711122"}, {"primary": false, "label": "711E12043"}], "ecla_classes": [{"label": "S06F212:1028"}, {"label": "G06F  12/08B10"}, {"label": "S06F212:6082"}, {"label": "Y02B60:12F"}, {"label": "S06F12:08B16F"}, {"label": "G06F  12/08B22L"}], "cpc_classes": [{"label": "G06F  12/0884"}, {"label": "G06F  12/0884"}, {"label": "Y02D  10/00"}, {"label": "G06F2212/6082"}, {"label": "G06F  12/0897"}, {"label": "G06F  12/0897"}, {"label": "G06F2212/1028"}, {"label": "G06F2212/6082"}, {"label": "G06F2212/1028"}, {"label": "Y02D  10/00"}, {"label": "G06F  12/0864"}, {"label": "G06F  12/0864"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "2000-02-09", "application_date": "2000-02-09", "family_members": [{"ucid": "US-6427188-B1", "titles": [{"lang": "EN", "text": "Method and system for early tag accesses for lower-level caches in parallel with first-level cache"}]}, {"ucid": "FR-2804770-A1", "titles": [{"lang": "EN", "text": "METHOD AND SYSTEM OF EARLY ACCESS, IN PARALLEL, TO LABELS OF LOWER LEVELS AND FIRST-LEVEL HISTORY"}, {"lang": "FR", "text": "PROCEDE ET SYSTEME D'ACCES PRECOCE, EN PARALLELE, AUX ETIQUETTES DES ANTEMEMOIRES DE NIVEAUX INFERIEURS ET A L'ANTEMOIRE DE PREMIER NIVEAU"}]}, {"ucid": "FR-2804770-B1", "titles": [{"lang": "FR", "text": "PROCEDE ET SYSTEME D'ACCES PRECOCE, EN PARALLELE, AUX ETIQUETTES DES ANTEMEMOIRES DE NIVEAUX INFERIEURS ET A L'ANTEMOIRE DE PREMIER NIVEAU"}, {"lang": "EN", "text": "METHOD AND SYSTEM OF EARLY ACCESS, IN PARALLEL, TO LABELS OF LOWER LEVELS AND FIRST-LEVEL HISTORY"}]}]}