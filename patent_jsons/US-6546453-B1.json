{"patent_number": "US-6546453-B1", "publication_id": 73348826, "family_id": 24619472, "publication_date": "2003-04-08", "titles": [{"lang": "EN", "text": "Proprammable DRAM address mapping mechanism"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50473594\"><p>A computer system contains a processor that includes a software programmable memory mapper. The memory mapper maps an address generated by the processor into a device address for accessing physical main memory. The processor also includes a cache controller that maps the processor address into a cache address. The cache address places a block of data from main memory into a memory cache using an index subfield. The physical main memory contains RDRAM devices, each of the RDRAM devices containing a number of memory banks that store rows and columns of data. The memory mapper maps processor addresses to device addresses to increases memory system performance. The mapping minimizes memory access conflicts between the memory banks. Conflicts between memory banks are reduced by placing a number of bits corresponding to the bank subfield above the most significant boundary bit of the index subfield. This diminishes page misses caused by replacement of data blocks from the cache memory because the read of the new data block and write of the victim data block are not to the same memory bank. Adjacent memory bank conflicts are reduced for sequential accesses to memory banks by reversing the bit order of a bank number subfield within the bank subfield of the device address.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6546453-B1-CLM-00001\" num=\"1\"><claim-text>1. A computer system, comprising:</claim-text><claim-text>a processor including a memory mapper that is software programmable, wherein said memory mapper maps a processor address into a device address; </claim-text><claim-text>a cache controller in said processor, said cache controller mapping the processor address into a cache address; </claim-text><claim-text>a system memory coupled to said processor, said system memory containing a plurality of memory devices, each of said memory devices containing a plurality of memory banks; and </claim-text><claim-text>wherein said memory mapper reduces memory access conflicts between said plurality of memory banks; </claim-text><claim-text>wherein said cache address includes an index subfield containing a plurality of bit positions and said device address includes a bank subfield containing a plurality of bit positions, and said bank subfield includes a bank number subfield, said bank number subfield containing a plurality of bit positions; and </claim-text><claim-text>wherein said memory mapper reverses the order of the bank number subfield bit positions to prevent memory accesses simultaneously requiring open pages from adjacent memory banks. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6546453-B1-CLM-00002\" num=\"2\"><claim-text>2. The computer system of <claim-ref idref=\"US-6546453-B1-CLM-00001\">claim 1</claim-ref> wherein at least a number of said plurality of bank subfield bit positions do not have overlapping index subfield bit positions such that replacement of a data block in a cache memory in the computer system will result in fewer memory bank conflicts.</claim-text></claim>"}, {"num": 3, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6546453-B1-CLM-00003\" num=\"3\"><claim-text>3. A computer system, comprising:</claim-text><claim-text>a processor including a memory mapper that is software programmable, wherein said memory mapper maps a first address into a second address; </claim-text><claim-text>a cache controller in said processor, said cache controller mapping the first address into a third address; </claim-text><claim-text>a system memory coupled to said processor, said system memory containing a plurality of memory devices, each of said memory devices containing a plurality of memory banks; </claim-text><claim-text>wherein said memory mapper increases memory system performance by minimizing memory access conflicts between said plurality of memory banks; and </claim-text><claim-text>a disk drive coupled to said processor; </claim-text><claim-text>wherein said third address includes an index subfield containing a plurality of bit positions and said second address includes a bank subfield containing a plurality of bit positions, and said bank subfield includes a bank number subfield comprising a plurality of bit positions; and </claim-text><claim-text>wherein said memory mapper reverses the order of the bank number subfield bit positions to prevent memory accesses simultaneously requiring open pages from adjacent memory banks. </claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6546453-B1-CLM-00004\" num=\"4\"><claim-text>4. The computer system of <claim-ref idref=\"US-6546453-B1-CLM-00003\">claim 3</claim-ref> wherein at least a number of said plurality of bank subfield bit positions do not have overlapping index subfield bit positions such that replacement of a data block in a cache memory in the computer system will result in fewer memory bank conflicts.</claim-text></claim>"}, {"num": 5, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6546453-B1-CLM-00005\" num=\"5\"><claim-text>5. A processor adapted to access memory, comprising:</claim-text><claim-text>a cache controller which maps a processor address to a cache address; </claim-text><claim-text>a memory mapper that maps a processor address to a device address; </claim-text><claim-text>wherein said cache address includes an index subfield containing a plurality of bits and said device address includes a bank subfield containing a plurality of bits that encodes a multibit bank number subfield; and </claim-text><claim-text>wherein said memory mapper reverses the order of the bank number subfield bits to prevent memory accesses simultaneously requiring open pages from adjacent memory banks. </claim-text></claim>"}, {"num": 6, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6546453-B1-CLM-00006\" num=\"6\"><claim-text>6. The processor of <claim-ref idref=\"US-6546453-B1-CLM-00005\">claim 5</claim-ref> wherein at least a number of said plurality of bank subfield bits do not have overlapping index subfield bits such that replacement of a data block in a cache memory will result in fewer memory bank conflicts.</claim-text></claim>"}, {"num": 7, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6546453-B1-CLM-00007\" num=\"7\"><claim-text>7. A method of accessing memory, comprising:</claim-text><claim-text>mapping a processor address to a cache address which includes an index subfield; </claim-text><claim-text>mapping a processor address to a device address which includes a bank subfield containing a plurality of bits that encodes a multibit bank number subfield; </claim-text><claim-text>reversing the order of the bank number subfield bits to prevent memory accesses simultaneously requiring open pages from adjacent memory banks. </claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53903153\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>CROSS-REFERENCE TO RELATED APPLICATIONS</h4><p>This application relates to the following commonly assigned co-pending applications entitled:</p><p>\u201cApparatus And Method For Interfacing A High Speed Scan-Path With Slow-Speed Test Equipment,\u201d Ser. No. 09/653,642, filed Aug. 31, 2000, \u201cPriority Rules For Reducing Network Message Routing Latency,\u201d Ser. No. 09/652,322, filed Aug. 31, 2000, \u201cScalable Directory Based Cache Coherence Protocol,\u201d Ser. No. 09/652,703, filed Aug. 31, 2000, \u201cScalable Efficient I/O Port Protocol,\u201d Ser. No. 09/652,391, filed Aug. 31, 2000, \u201cEfficient Translation Lookaside Buffer Miss Processing In Computer Systems With A Large Range Of Page Sizes,\u201d Ser. No. 09/652,552, filed Aug. 31, 2000, \u201cFault Containment And Error Recovery Techniques In A Scalable Multiprocessor,\u201d Ser. No. 09/651,949, filed Aug. 31, 2000, \u201cSpeculative Directory Writes In A Directory Based Cache Coherent Nonuniform Memory Access Protocol,\u201d Ser. No. 09/652,834, filed Aug. 31, 2000, \u201cSpecial Encoding Of Known Bad Data,\u201d Ser. No. 09/652,314, filed Aug. 31, 2000, \u201cBroadcast Invalidate Scheme,\u201d Ser. No. 09/652,165, filed Aug. 31, 2000, \u201cMechanism To Track All Open Pages In A DRAM Memory System,\u201d Ser. No. 09/652,704, filed Aug. 31, 2000, \u201cComputer Architecture And System For Efficient Management Of Bi-Directional Bus,\u201d Ser. No. 09/652,323, filed Aug. 31, 2000, \u201cAn Efficient Address Interleaving With Simultaneous Multiple Locality Options,\u201d Ser. No. 09/652,452, filed Aug. 31, 2000, \u201cA High Performance Way Allocation Strategy For A Multi-Way Associative Cache System,\u201d Ser. No. 09/653,092, filed Aug. 31, 2000, \u201cMethod And System For Absorbing Defects In High Performance Microprocessor With A Large N-Way Set Associative Cache,\u201d Ser. No. 09/651,948, filed Aug. 31, 2000, \u201cA Method For Reducing Directory Writes And Latency In A High Performance, Directory-Based, Coherency Protocol,\u201d Ser. No. 09/652,324, filed Aug. 31, 2000, \u201cMechanism To Reorder Memory Read And Write Transactions For Reduced Latency And Increased Bandwidth,\u201d Ser. No. 09/653,094, filed Aug. 31, 2000, \u201cSystem For Minimizing Memory Bank Conflicts In A Computer System,\u201d Ser. No. 09/652,325, filed Aug. 31, 2000, \u201cComputer Resource Management And Allocation System,\u201d Ser. No. 09/651,945, filed Aug. 31, 2000, \u201cInput Data Recovery Scheme,\u201d Ser. No. 09/653,643, filed Aug. 31, 2000, \u201cFast Lane Prefetching,\u201d Ser. No. 09/652,451, filed Aug. 31, 2000, \u201cMechanism For Synchronizing Multiple Skewed Source-Synchronous Data Channels With Automatic Initialization Feature,\u201d Ser. No. 09/652,480, filed Aug. 31, 2000, \u201cMechanism To Control The Allocation Of An N-Source Shared Buffer,\u201d Ser. No. 09/651,924, filed Aug. 31, 2000, and \u201cChaining Directory Reads And Writes To Reduce DRAM Bandwidth In A Directory Based CC-NUMA Protocol,\u201d Ser. No. 09/652,315, filed Aug. 31, 2000, all of which are incorporated by reference herein.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT</h4><p>Not applicable.</p><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>The present invention generally relates to a computer system that includes one or more random access memory (\u201cRAM\u201d) devices for storing data. More particularly, the invention relates to a computer system with RAM devices in which multiple banks of storage can be accessed simultaneously to enhance the performance of the memory devices. Still more particularly, the present invention relates to a system for the mapping of processor addresses to memory device addresses that effectively minimizes simultaneous accesses to the same bank of memory to avoid access delays.</p><p>2. Background of the Invention</p><p>Superscalar processors achieve high performance by executing multiple instructions per clock cycle and by choosing the shortest possible clock cycle consistent with the design. On the other hand, superpipelined processor designs divide instruction execution into a large number of subtasks which can be performed quickly, and assign pipeline stages to each subtask. By overlapping the execution of many instructions within the pipeline, superpipelined processors attempt to achieve high performance.</p><p>Superscalar processors demand low main memory latency due to the number of instructions attempting concurrent execution and due to the increasing clock frequency (i.e., shortened clock cycle) employed by the processors. Many of the instructions include memory operations to fetch (\u201cread\u201d) and update (\u201cwrite\u201d) memory operands. The memory operands must be fetched from or conveyed to main memory, and each instruction must originally be fetched from main memory as well. Similarly, processors that are superpipelined demand low main memory latency because of the high clock frequency employed by these processors and the attempt to begin execution of a new instruction each clock cycle. It is noted that a given processor design may employ both superscalar and superpipelined techniques in an attempt to achieve the highest possible performance characteristics.</p><p>Processors are often configured into computer systems that have a relatively large and slow main memory. Typically, multiple random access memory (\u201cRAM\u201d) modules comprise the main memory system. The RAM modules may be Single Inline Memory Modules (\u201cSIMM\u201d), Double Inline Memory Modules (\u201cDIMM\u201d), or RAMbus\u2122 Inline Memory Modules (\u201cRIMM\u201d) that incorporate a number of Random Access Memory (\u201cRAM\u201d) devices (see \u201cRAMBUS Preliminary Information Direct RDRAM\u2122\u201d, Document DL0060 Version 1.01; \u201cDirect Rambus\u2122 RIMM\u2122 Module Specification Version 1.0\u201d, Document SL-0006-100; \u201cRambus\u00ae RIMM\u2122 Module (with 128/144 Mb RDRAMs)\u201d Document DL00084 Version 1.1, all of which are incorporated by reference herein). RAM devices may be Dynamic Random Access Memory (\u201cDRAM\u201d) devices, RAMbus\u2122 DRAM (\u201cRDRAM\u201d) or any of a number of other types of memory storage devices. Each RAM device consists of a DRAM core section containing memory banks organized into rows and columns, with each column containing a number of bytes (in the preferred embodiment 16 bytes). A large main memory provides storage for a large number of instructions and/or a large amount of data for use by the processor, providing faster access to the instructions and/or data than may be achieved for example from disk storage. However, the access times of modem RAMs are significantly longer than the clock cycle length of modem processors. The memory access time for each set of bytes being transferred to the processor is therefore long. Accordingly, the main memory system is not a low latency system. Processor performance may suffer due to high memory latency.</p><p>Many types of RAMs employ a \u201cpage mode\u201d which allows for memory latency to be decreased for transfers within the same \u201cpage\u201d. Generally, as explained above, RAMs comprise memory arranged into rows and columns of storage. A first portion of the address identifying the desired data/instructions is used to select one of the rows (the \u201crow address\u201d), and a second portion of the address is used to select one of the columns (the \u201ccolumn address\u201d). One or more bytes residing at the selected row and columns are provided as output of the RAM. Typically, the row address is provided to the RAM first, and the selected row is placed into a temporary sense amplifier buffer within the RAM. The row of data that is stored in the RAM's sense amplifier is referred to as a page. Thus, addresses having the same row address are said to be in the same page. Subsequent to the selected row being placed into the sense amplifier buffer, the column address is provided and the selected data is output from the RAM. A row/page hit occurs if the next address to access the RAM is within the same row/page stored in the sense amplifier buffer. Thus, the next access may be performed by providing the column portion of the address only, omitting the row address transmission. The next access to a different column may therefore be performed with lower latency, saving the time required for transmitting the row address because the page corresponding to the row has already been activated. The size of a row/page is dependent upon the number of columns within the row/page. The row/page stored in the sense amplifier within the RAM is referred to as an \u201copen page\u201d, since accesses within the open page can be performed by transmitting the column portion of the address only.</p><p>Unfortunately, the first access to a given row/page generally does not occur to an open row/page, thereby incurring a higher memory latency. Even further, the first access may experience a row/page miss. A row/page miss can occur if the sense amplifier has another particular row/page open, and the particular row/page must first be closed before opening the row/page containing the current access. A row/page miss can also occur if the sense amplifier is empty. Often, this first access is critical to maintaining performance in the processor within the computer system, as the data/instructions are immediately needed to satisfy a miss. Instruction execution may stall because of the row/page miss while the row/page containing the current access is being opened. The more often that instructions can access main memory using row/page hits, the lower the latency of memory access and the better the system performance. In a memory system containing many RAM devices and thus a large number of sense amplifier buffers, a large amount of memory can be accessed using row/page hits, resulting in an increased opportunity to maximize performance.</p><p>Software applications executing on the computer system frequently perform read or write operations that include a processor memory address mapped to a device address. The device address identifies a DRAM device, memory banks within the DRAM device, and rows and columns within each memory bank. The mapping of the processor memory address to the device address selects the DRAM device and row and column and manages memory bank conflicts. Memory bank conflicts are caused by attempts to perform a read or write to a memory bank within a DRAM device while another read or write is occurring to the same memory bank. Memory bank conflicts degrade memory system performance because memory transactions must be delayed while a previous memory transaction completes within the DRAM device. Thus, to increase system performance the mapping strategy implemented must reduce memory bank conflicts. Because memory configurations can vary widely in the number of DRAM devices present as well as the organization of the DRAM devices (i.e., number of memory banks, interface logic operation), it is highly desirable to permit a system programmer to program the mapping scheme for each particular configuration and software application to allow maximum system performance.</p><p>The mapping of processor memory addresses to device addresses for optimal performance must take into account read and write traffic patterns on main memory. One property of read/write memory traffic is referred to as locality of reference. Locality of reference means that if a memory address \u201cA\u201d is accessed, then it is likely that the next address \u201cB\u201d is near or adjacent to \u201cA.\u201d An address-mapping scheme should not result in memory bank conflicts from successive accesses to contiguous addresses in main memory. For example, assume that a software application is performing reads and writes to a large contiguous area of main memory that spans row/page boundaries. As long as memory is being accessed from the same row/page in the sense amplifier, no row/page misses occur and thus the page in the sense amplifier does not have to be replaced with a different row/page. However, when the end of the row/page in the sense amplifier is reached and the next row/page is required, a row/page close cycle is needed to store the old row/page and a row/page open cycle is required to open the new row/page. If the processor memory address to device address mapping scheme is such that the next required row containing the new page is in the same memory bank or an adjacent memory bank (for DRAM devices in which memory banks share sense amplifiers) as the row containing the previous page, opening the next row/page to perform reads and writes will be delayed while the closure of the previous row/page completes. It would be advantageous if successive reads and writes to contiguous rows/pages of memory resulted in accesses to different nonadjacent memory banks of the DRAM device.</p><p>Another common read/write traffic pattern occurs in processors that include cache memories. Processors use cache memory in memory systems to improve computer system performance. A cache memory holds a subset of the contents of main memory and is faster and smaller than main memory. An architecture common in the art provides a level one (\u201cL<b>1</b>\u201d) cache on the same integrated circuit as the microprocessor and a level two (\u201cL<b>2</b>\u201d) cache either on the same integrated circuit as the microprocessor or on the system board of the computer. The smallest unit of memory that can be loaded into a cache memory is known as a cache block. A set associative cache is divided up into sets with each set containing two or more block frames that store blocks of data from main memory. A block of data from main memory is first mapped into a set of the cache and then it can be placed anywhere within the set. The cache placement is called n-way set associative if there are n block frames in a set.</p><p>Read/write memory transactions in the computer system may result in the cache memory becoming full. A read or write request to a memory block not present in the cache would then result in the replacement of a existing memory block present in a set of the cache memory. If the cache memory is a writeback set associative cache, the new read or write requests can result in the replacement of modified data in a cache block that must be written back to main memory. Each processor address in a block of data from main memory is mapped to a cache address that includes an index subfield identifying the particular set in the cache that the data block would be placed into. Thus, the addresses of blocks of data in the block frames of a particular set in the cache have the same index subfield and other blocks of data in main memory may also have the same index subfield. A processor address to device address mapping scheme should advantageously seek to prevent memory bank conflicts from occurring by mapping the portion of the address that is not equal (i.e., fields other than index subfield) in such a manner so that the mapped memory banks selected are different. Despite the apparent performance advantages of such a mapping scheme, to date no such system allowing flexibility to maximize performance over all memory hardware configurations has been implemented.</p><h4>BRIEF SUMMARY OF THE INVENTION</h4><p>The problems noted above are solved in large part by the systems and techniques of the preferred embodiment of the present invention, which avoids delays resulting from memory bank conflicts. Preferably, a computer system contains a processor that includes a software programmable memory mapper. The memory mapper maps an address generated by the processor into a device address for accessing physical main memory. The processor also includes a cache controller that maps the processor address into a cache address. The cache address places a block of data from main memory into a memory cache using an index subfield. The physical main memory contains RDRAM devices, each of the RDRAM devices containing a number of memory banks that store rows and columns of data. The memory mapper maps processor addresses to device addresses to increases memory system performance. The mapping minimizes memory access conflicts between the memory banks.</p><p>Conflicts between memory banks are reduced by placing a number of bits corresponding to the bank subfield above the most significant boundary bit of the index subfield. This diminishes the likelihood of page misses resulting from the replacement of data blocks in the cache memory because the read of the new data block and write of the victim data block are not to the same memory bank.</p><p>Adjacent memory bank conflicts are reduced for sequential accesses to memory banks by reversing the bit order of a bank number subfield within the bank subfield of the device address.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>For a detailed description of the preferred embodiments of the invention, reference will now be made to the accompanying drawings in which:</p><p>FIG. 1 shows a system diagram of a plurality of microprocessors coupled together;</p><p>FIGS. 2<i>a </i>and <b>2</b><i>b </i>show a block diagram of the microprocessors of FIG. 1;</p><p>FIG. 3 is a diagram showing a RAMbus\u2122 Dynamic Random Access Memory device of the preferred embodiment;</p><p>FIG. 4 shows mappings of a processors address to a cache address and device addresses; and</p><p>FIG. 5 shows bit reversal of the bank number that minimizes adjacent bank conflicts for contiguous row/page accesses.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>NOTATION AND NOMENCLATURE</h4><p>Certain terms are used throughout the following description and claims to refer to particular system components. As one skilled in the art will appreciate, computer companies may refer to a component by different names. This document does not intend to distinguish between components that differ in name but not function. In the following discussion and in the claims, the terms \u201cincluding\u201d and \u201ccomprising\u201d are used in an open-ended fashion, and thus should be interpreted to mean \u201cincluding, but not limited to . . . \u201d. Also, the term \u201ccouple\u201d or \u201ccouples\u201d is intended to mean either an indirect or direct electrical connection. Thus, if a first device couples to a second device, that connection may be through a direct electrical connection, or through an indirect electrical connection via other devices and connections.</p><h4>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>Referring now to FIG. 1, in accordance with the preferred embodiment of the invention, computer system <b>90</b> constructed in accordance with the preferred embodiment comprises one or more processors <b>100</b> coupled to a memory <b>102</b> and an input/output (\u201cI/O\u201d) controller <b>104</b>. As shown in FIG. 1, computer system <b>90</b> includes twelve processors <b>100</b>, each processor coupled to a memory and an I/O controller. Although the computer system <b>90</b> is shown as a multiple processor system in FIG. 1, it should be understood that the present invention also may be implemented on a single processor system, and thus the following disclosure is intended to be illustrative of the preferred embodiment of practicing the invention, and is not intended to imply that the invention is limited to use in a multi-processor system.</p><p>According to the preferred embodiment, each processor preferably includes four ports for connection to adjacent processors. The inter-processor ports are designated \u201cnorth,\u201d \u201csouth,\u201d \u201ceast,\u201d and \u201cwest\u201d in accordance with the well-known Manhattan grid architecture. As such, each processor <b>100</b> can be connected to four other processors. The processors on both end of the system layout wrap around and connect to processors on the opposite side to implement a 2D torus-type connection. Although twelve processors <b>100</b> are shown in the exemplary embodiment of FIG. 1, any desired number of processors (e.g., 256) can be included. In the preferred embodiment, computer system <b>90</b> is designed to accommodate either 256 processors or 128 processors, depending on the size of the memory associated with the processors.</p><p>The I/O controller <b>104</b> provides an interface to various input/output devices such as disk drives <b>105</b> and <b>106</b> as shown. Data from the I/O devices thus enters the 2D torus via the I/O controllers.</p><p>In accordance with the preferred embodiment, the memory <b>102</b> preferably comprises RAMbus\u2122 memory devices, but other types of memory devices can be used if desired. The capacity of the memory devices <b>102</b> can be any suitable size. Furthermore, memory devices <b>102</b> preferably are coupled to the microprocessor through Rambus Interface Memory Modules (\u201cRIMMs\u201d).</p><p>In general, computer system <b>90</b> can be configured so that any microprocessor <b>100</b> can access its own memory <b>102</b> and I/O devices, as well as the memory and I/O devices of all other microprocessors in the network. Preferably, the computer system may have dedicated physical connections between each microprocessor resulting in low interprocessor communication times and improved memory and I/O device access reliability. If dedicated physical connections are not present between each pair of microprocessors, a pass-through or bypass path is preferably implemented in each microprocessor that permits accesses to a microprocessor's memory and I/O devices by another microprocessor through one or more pass-through microprocessors.</p><p>Referring now to FIGS. 2<i>a </i>and <b>2</b><i>b</i>, each microprocessor <b>100</b> preferably includes an instruction cache <b>110</b>, an instruction fetch, issue and retire unit (\u201cIbox\u201d) <b>120</b>, an integer execution unit (\u201cEbox\u201d) <b>130</b>, a floating-point execution unit (\u201cFbox\u201d) <b>140</b>, a memory reference unit (\u201cMbox\u201d) <b>150</b>, a data cache <b>160</b>, an L<b>2</b> instruction and data cache control unit (\u201cCbox\u201d) <b>170</b>, a level L<b>2</b> cache <b>180</b>, two memory controllers (\u201cZbox<b>0</b>\u201d and \u201cZbox<b>1</b>\u201d) <b>190</b>, and an interprocessor and I/O router unit (\u201cRbox\u201d) <b>200</b>. The following discussion describes each of these units.</p><p>Each of the various functional units <b>110</b>-<b>200</b> contains control logic that communicate with various other control logic as shown in FIGS. 2<i>a </i>and <b>2</b><i>b</i>. The instruction cache control logic <b>110</b> communicates with the Ibox <b>120</b>, Cbox <b>170</b>, and L<b>2</b> Cache <b>180</b>. In addition to communicating with the instruction cache <b>110</b>, the Ibox control logic <b>120</b> communicates with Ebox <b>130</b>, Fbox <b>140</b> and Cbox <b>170</b>. The Ebox <b>130</b> and Fbox <b>140</b> control logic both communicate with the Mbox <b>150</b>, which in turn communicates with the data cache <b>160</b> and Cbox <b>170</b>. The Cbox control logic also communicates with the L<b>2</b> cache <b>180</b>, Zboxes <b>190</b>, and Rbox <b>200</b>.</p><p>Referring still to FIGS. 2<i>a </i>and <b>2</b><i>b</i>, the Ibox <b>120</b> preferably includes a fetch unit <b>121</b> which contains a virtual program counter (\u201cVPC\u201d) <b>122</b>, a branch predictor <b>123</b>, an instruction-stream translation buffer (\u201cITB\u201d) <b>124</b>, an instruction predecoder <b>125</b>, a retire unit <b>126</b>, decode and rename registers <b>127</b>, an integer instruction queue <b>128</b>, and a floating point instruction queue <b>129</b>. Generally, the VPC <b>122</b> maintains virtual addresses for instructions that are in flight. An instruction is said to be \u201cin-flight\u201d from the time it is fetched until it retires or aborts. The Ibox <b>120</b> can accommodate as many as <b>80</b> instructions, in <b>20</b> successive fetch slots, in flight between the decode and rename registers <b>127</b> and the end of the pipeline. The VPC preferably includes a 20-entry table to store these fetched VPC addresses.</p><p>The Ibox <b>120</b> uses the branch predictor <b>123</b> to handle branch instructions. A branch instruction requires program execution either to continue with the instruction immediately following the branch instruction if a certain condition is met, or branch to a different instruction if the particular condition is not met. Accordingly, the outcome of a branch instruction is not known until the instruction is executed. In a pipelined architecture, a branch instruction (or any instruction for that matter) may not be executed for at least several, and perhaps many, clock cycles after the fetch unit in the microprocessor fetches the branch instruction. In order to keep the pipeline full, which is desirable for efficient operation, the microprocessor preferably includes branch prediction logic that predicts the outcome of a branch instruction before it is actually executed (also referred to as \u201cspeculating\u201d). The branch predictor <b>123</b>, which receives addresses from the VPC queue <b>122</b>, preferably bases its speculation on short and long-term history of prior instruction branches. As such, using branch prediction logic, a microprocessor's fetch unit can speculate the outcome of a branch instruction before it is actually executed. The speculation, however, may or may not turn out to be accurate. That is, the branch predictor logic may guess wrong regarding the direction of program execution following a branch instruction. If the speculation proves to have been accurate, which is determined when the microprocessor executes the branch instruction, then the next instructions to be executed have already been fetched and are working their way through the pipeline.</p><p>If, however, the branch speculation performed by the branch predictor <b>123</b> turns out to have been wrong (referred to as \u201cmisprediction\u201d or \u201cmisspeculation\u201d), many or all of the instructions behind the branch instruction may have to be flushed from the pipeline (i.e., not executed) because of the incorrect fork taken after the branch instruction. Branch predictor <b>123</b> uses any suitable branch prediction algorithm, however, that results in correct speculations more often than misspeculations, and the overall performance of the microprocessor is better (even in the face of some misspeculations) than if speculation was turned off.</p><p>The Instruction Translation Buffer (\u201cITB\u201d) <b>124</b> couples to the instruction cache <b>110</b> and the fetch unit <b>121</b>. The ITB <b>124</b> comprises a 128-entry, fully-associative instruction-stream translation buffer that is used to store recently used instruction-stream address translations and page protection information. Preferably, each of the entries in the ITB <b>124</b> may be 1, 8, 64 or 512 contiguous 8-kilobyte (\u201cKB\u201d) pages or 1, 32, 512, 8192 contiguous 64-kilobyte pages. The allocation scheme used for the ITB <b>124</b> is a round-robin scheme, although other schemes can be used as desired.</p><p>The predecoder <b>125</b> reads an octaword (16 contiguous bytes) from the instruction cache <b>110</b>. Each octaword read from instruction cache may contain up to four naturally aligned instructions per cycle. Branch prediction and line prediction bits accompany the four instructions fetched by the predecoder <b>125</b>. The branch prediction scheme implemented in branch predictor <b>123</b> generally works most efficiently when only one branch instruction is contained among the four fetched instructions. The predecoder <b>125</b> predicts the instruction cache line that the branch predictor <b>123</b> will generate. The predecoder <b>125</b> generates fetch requests for additional instruction cache lines and stores the instruction stream data in the instruction cache.</p><p>Referring still to FIGS. 2<i>a </i>and <b>2</b><i>b</i>, the retire unit <b>126</b> fetches instructions in program order, executes them out of order, and then retires (also called \u201ccommitting\u201d an instruction) them in order. The Ibox <b>120</b> logic maintains the architectural state of the microprocessor by retiring an instruction only if all previous instructions have executed without generating exceptions or branch mispredictions. An exception is any event that causes suspension of normal instruction execution. Retiring an instruction commits the microprocessor to any changes that the instruction may have made to the software accessible registers and memory. The microprocessor <b>100</b> preferably includes the following three machine code accessible hardware units: integer and floating-point registers, memory, and internal microprocessor registers. The retire unit <b>126</b> of the preferred embodiment can retire instructions at a sustained rate of eight instructions per cycle, and can retire as many as 11 instructions in a single cycle.</p><p>The decode and rename registers <b>127</b> contains logic that forwards instructions to the integer and floating-point instruction queues <b>128</b>, <b>129</b>. The decode and rename registers <b>127</b> preferably the following two functions. First, the decode and rename registers <b>127</b> eliminates register write-after-read (\u201cWAR\u201d) and write-after-write (\u201cWAW\u201d) data dependency while preserving true read-after-write (\u201cRAW\u201d) data dependencies. This permits instructions to be dynamically rescheduled. Second, the decode and rename registers <b>127</b> permits the microprocessor to speculatively execute instructions before the control flow previous to those instructions is resolved.</p><p>The logic in the decode and rename registers <b>127</b> preferably translates each instruction's operand register specifiers from the virtual register numbers in the instruction to the physical register numbers that hold the corresponding architecturally-correct values. The logic also renames each instruction destination register specifier from the virtual number in the instruction to a physical register number chosen from a list of free physical registers, and updates the register maps. The decode and rename register logic can process four instructions per cycle. Preferably, the logic in the decode and rename registers <b>127</b> does not return the physical register, which holds the old value of an instruction's virtual destination register, to the free list until the instruction has been retired, indicating that the control flow up to that instruction has been resolved.</p><p>If a branch misprediction or exception occurs, the register logic backs up the contents of the integer and floating-point rename registers to the state associated with the instruction that triggered the condition, and the fetch unit <b>121</b> restarts at the appropriate Virtual Program Counter (\u201cVPC\u201d). Preferably, as noted above, 20 valid fetch slots containing up to 80 instructions can be in flight between the registers <b>127</b> and the end of the microprocessor's pipeline, where control flow is finally resolved. The register <b>127</b> logic is capable of backing up the contents of the registers to the state associated with any of these 80 instructions in a single cycle. The register logic <b>127</b> preferably places instructions into the integer or floating-point issue queues <b>128</b>, <b>129</b>, from which they are later issued to functional units <b>130</b> or <b>136</b> for execution.</p><p>The integer instruction queue <b>128</b> preferably includes capacity for 20 integer instructions. The integer instruction queue <b>128</b> issues instructions at a maximum rate of four instructions per cycle. The specific types of instructions processed through queue <b>128</b> include: integer operate commands, integer conditional branches, unconditional branches (both displacement and memory formats), integer and floating-point load and store commands, Privileged Architecture Library (\u201cPAL\u201d) reserved instructions, integer-to-floating-point and floating-point-integer conversion commands.</p><p>Referring still to FIGS. 2<i>a </i>and <b>2</b><i>b</i>, the integer execution unit (\u201cEbox\u201d) <b>130</b> includes Arithmetic Logic Units (\u201cALUs\u201d) <b>131</b>, <b>132</b>, <b>133</b>, and <b>134</b> and two integer register files <b>135</b>. Ebox <b>130</b> preferably comprises a 4-path integer execution unit that is implemented as two functional-unit \u201cclusters\u201d labeled <b>0</b> and <b>1</b>. Each cluster contains a copy of an 80-entry, physical-register file and two subclusters, named upper (\u201cU\u201d) and lower (\u201cL\u201d). As such, the subclusters <b>131</b>-<b>134</b> are labeled U<b>0</b>, L<b>0</b>, U<b>1</b>, and L<b>1</b>. Bus <b>137</b> provides cross-cluster communication for moving integer result values between the clusters.</p><p>The subclusters <b>131</b>-<b>134</b> include various components that are not specifically shown in FIG. 2<i>a</i>. For example, the subclusters preferably include four 64-bit adders that are used to calculate results for integer add instructions, logic units, barrel shifters and associated byte logic, conditional branch logic, a pipelined multiplier for integer multiply operations, and other components known to those of ordinary skill in the art.</p><p>Each entry in the integer instruction queue <b>128</b> preferably asserts four request signals\u2014one for each of the Ebox <b>130</b> subclusters <b>131</b>, <b>132</b>, <b>133</b>, and <b>134</b>. A queue entry asserts a request when it contains an instruction that can be executed by the subcluster, if the instruction's operand register values are available within the subcluster. The integer instruction queue <b>128</b> includes two arbiters\u2014one for the upper subclusters <b>132</b> and <b>133</b> and another arbiter for the lower subclusters <b>131</b> and <b>134</b>. Each arbiter selects two of the possible 20 requesters for service each cycle. Preferably, the integer instruction queue <b>128</b> arbiters choose between simultaneous requesters of a subcluster based on the age of the request older requests are given priority over newer requests. If a given instruction requests both lower subclusters, and no older instruction requests a lower subcluster, then the arbiter preferably assigns subcluster <b>131</b> to the instruction. If a given instruction requests both upper subclusters, and no older instruction requests an upper subcluster, then the arbiter preferably assigns subcluster <b>133</b> to the instruction.</p><p>The floating-point instruction queue <b>129</b> preferably comprises a 15-entry queue and issues the following types of instructions: floating-point operates, floating-point conditional branches, floating-point stores, and floating-point register to integer register transfers. Each queue entry preferably includes three request lines-one for the add pipeline, one for the multiply pipeline, and one for the two store pipelines. The floating-point instruction queue <b>129</b> includes three arbiters\u2014one for each of the add, multiply, and store pipelines. The add and multiply arbiters select one requester per cycle, while the store pipeline arbiter selects two requesters per cycle, one for each store pipeline. As with the integer instruction queue <b>128</b> arbiters, the floating-point instruction queue arbiters select between simultaneous requesters of a pipeline based on the age of the request\u2014older request are given priority. Preferably, floating-point store instructions and floating-point register to integer register transfer instructions in even numbered queue entries arbitrate for one store port. Floating-point store instructions and floating-point register to integer register transfer instructions in odd numbered queue entries arbitrate for the second store port.</p><p>Floating-point store instructions and floating-point register to integer register transfer instructions are queued in both the integer and floating-point queues. These instructions wait in the floating-point queue until their operand register values are available from the floating-point execution unit (\u201cFbox\u201d) registers. The instructions subsequently request service from the store arbiter. Upon being issued from the floating-point queue <b>129</b>, the instructions signal the corresponding entry in the integer queue <b>128</b> to request service. Finally, upon being issued from the integer queue <b>128</b>, the operation is completed.</p><p>The integer registers <b>135</b>, <b>136</b> preferably contain storage for the microprocessor's integer registers, results written by instructions that have not yet been retired, and other information as desired. The two register files <b>135</b>, <b>136</b> preferably contain identical values. Each register file preferably includes four read ports and six write ports. The four read ports are used to source operands to each of the two subclusters within a cluster. The six write ports are used to write results generated within the cluster or another cluster and to write results from load instructions.</p><p>The floating-point execution queue (\u201cFbox\u201d) <b>129</b> contains a floating-point add, divide and square-root calculation unit <b>142</b>, a floating-point multiply unit <b>144</b> and a register file <b>146</b>. Floating-point add, divide and square root operations are handled by the floating-point add, divide and square root calculation unit <b>142</b> while floating-point operations are handled by the multiply unit <b>144</b>.</p><p>The register file <b>146</b> preferably provides storage for 72 entries including 31 floating-point registers and 41 values written by instructions that have not yet been retired. The Fbox register file <b>146</b> contains six read ports and four write ports (not specifically shown). Four read ports are used to source operands to the add and multiply pipelines, and two read ports are used to source data for store instructions. Two write ports are used to write results generated by the add and multiply pipelines, and two write ports are used to write results from floating-point load instructions. still to FIG. 2<i>a</i>, the Mbox <b>150</b> controls the L<b>1</b> data cache <b>160</b> and ensures architecturally correct behavior for load and store instructions. The Mbox <b>150</b> preferably contains a datastream translation buffer (\u201cDTB\u201d) <b>151</b>, a load queue (\u201cLQ\u201d) <b>152</b>, a store queue (\u201cSQ\u201d) <b>153</b>, and a miss address file (\u201cMAF\u201d) <b>154</b>. The DTB <b>151</b> preferably comprises a fully associative translation buffer that is used to store data stream address translations and page protection information. Each of the entries in the DTB <b>151</b> can map 1, 8, 64, or 512 contiguous 8-KB pages. The allocation scheme preferably is round robin, although other suitable schemes could also be used. The DTB <b>151</b> also supports an 8-bit Address Space Number (\u201cASN\u201d) and contains an Address Space Match (\u201cASM\u201d) bit. The ASN is an optionally implemented register used to reduce the need for invalidation of cached address translations for process-specific addresses when a context switch occurs.</p><p>The LQ <b>152</b> preferably is a reorder buffer used for load instructions. It contains 32 entries and maintains the state associated with load instructions that have been issued to the Mbox <b>150</b>, but for which results have not been delivered to the microprocessor and the instructions retired. The Mbox <b>150</b> assigns load instructions to LQ slots based on the order in which they were fetched from the instruction cache <b>110</b>, and then places them into the LQ <b>152</b> after they are issued by the integer instruction queue <b>128</b>. The LQ <b>152</b> also helps to ensure correct memory reference behavior for the microprocessor.</p><p>The SQ <b>153</b> preferably is a reorder buffer and graduation unit for store instructions. It contains 32 entries and maintains the state associated with store instructions that have been issued to the Mbox <b>150</b>, but for which data has not been written to the data cache <b>160</b> and the instruction retired. The Mbox <b>150</b> assigns store instructions to SQ slots based on the order in which they were fetched from the instruction cache <b>10</b> and places them into the SQ <b>153</b> after they are issued by the instruction cache <b>110</b>. The SQ <b>153</b> holds data associated with the store instructions issued from the integer instruction unit <b>128</b> until they are retired, at which point the store can be allowed to update the data cache <b>160</b>. The LQ <b>152</b> also helps to ensure correct memory reference behavior for the microprocessor.</p><p>The MAF <b>154</b> preferably comprises a 16-entry file that holds physical addresses associated with pending instruction cache <b>110</b> and data cache <b>160</b> fill requests and pending input/output (\u201cI/O\u201d) space read transactions.</p><p>Microprocessor <b>100</b> preferably includes two on-chip primary-level (\u201cL<b>1</b>\u201d) instruction and data caches <b>110</b> and <b>160</b>, and single secondary-level, unified instruction/data (\u201cL<b>2</b>\u201d) cache <b>180</b> (FIG. 2<i>b</i>). The L<b>1</b> instruction cache <b>110</b> preferably is a 64-KB virtual-addressed, two-way set-associative cache. Prediction is used to improve the performance of the two-way set-associative cache without slowing the cache access time. Each instruction cache block preferably contains a plurality (preferably 16) instructions, virtual tag bits, an address space number, an address space match bit, a one-bit PALcode bit to indicate physical addressing, a valid bit, data and tag parity bits, four access-check bits, and predecoded information to assist with instruction processing and fetch control.</p><p>The L<b>1</b> data cache <b>160</b> preferably is a 64-KB, two-way set associative, virtually indexed, physically tagged, write-back, read/write allocate cache with 64-byte cache blocks. During each cycle the data cache <b>160</b> preferably performs one of the following transactions: two quadword (or shorter) read transactions to arbitrary addresses, two quadword write transactions to the same aligned octaword, two non-overlapping less-than quadword writes to the same aligned quadword, one sequential read and write transaction from and to the same aligned octaword. Preferably, each data cache block contains 64 data bytes and associated quadword ECC bits, physical tag bits, valid, dirty shared, and modified bits, tag parity bit calculated across the tag, dirty, shared, and modified bits, and one bit to control round-robin set allocation. The data cache <b>160</b> is organized to contain two sets, each with <b>512</b> rows containing 64-byte blocks per row (i.e., 32-KB of data per set). The microprocessor <b>100</b> uses two additional bits of virtual address beyond the bits that specify an 8-KB page in order to specify the data cache row index. A given virtual address might be found in four unique locations in the data cache <b>160</b>, depending on the virtual-to-physical translation for those two bits. The microprocessor <b>100</b> prevents this aliasing by keeping only one of the four possible translated addresses in the cache at any time.</p><p>The L<b>2</b> cache <b>180</b> preferably is a 1.75-MB, seven-way set associative write-back mixed instruction and data cache. Preferably, the L<b>2</b> cache holds physical address data and coherence state bits for each block.</p><p>Referring now to FIG. 2<i>b</i>, the L<b>2</b> instruction and data cache control unit (\u201cCbox\u201d) <b>170</b> controls the L<b>2</b> instruction and data cache <b>190</b> and system ports. As shown, the Cbox <b>170</b> contains a fill buffer <b>171</b>, a data cache victim buffer <b>172</b>, a system victim buffer <b>173</b>, a cache miss address file (\u201cCMAF\u201d) <b>174</b>, a system victim address file (\u201cSVAF\u201d) <b>175</b>, a data victim address file (\u201cDVAF\u201d) <b>176</b>, a probe queue (\u201cPRBQ\u201d) <b>177</b>, a requester miss-address file (\u201cRMAF\u201d) <b>178</b>, a store to I/O space (\u201cSTIO\u201d) <b>179</b>, and an arbitration unit <b>181</b>.</p><p>The fill buffer <b>171</b> preferably in the Cbox is used to buffer data that comes from other functional units outside the Cbox. The data and instructions get written into the fill buffer and other logic units in the Cbox process the data and instructions before sending to another functional unit or the L<b>1</b> cache. The data cache victim buffer (\u201cVDF\u201d) <b>172</b> preferably stores data flushed from the L<b>1</b> cache or sent to the System Victim Data Buffer <b>173</b>. The System Victim Data Buffer (\u201cSVDB\u201d) <b>173</b> is used to send data flushed from the L<b>2</b> cache to other microprocessors in the system and to memory. Cbox Miss-Address File (\u201cCMAF\u201d) <b>174</b> preferably holds addresses of L<b>1</b> cache misses. CMAF updates and maintains the status of these addresses. The System Victim-Address File (\u201cSVAF\u201d) <b>175</b> in the Cbox preferably contains the addresses of all SVDB data entries. Data Victim-Address File (\u201cDVAF\u201d) <b>176</b> preferably contains the addresses of all data cache victim buffer (\u201cVDF\u201d) data entries.</p><p>The Probe Queue (\u201cPRBQ\u201d) <b>177</b> preferably comprises a 18-entry queue that holds pending system port cache probe commands and addresses. This queue includes 10 remote request entries, 8 forward entries, and lookup L<b>2</b> tags and requests from the PRBQ content addressable memory (\u201cCAM\u201d) against the RMAF, MAF, and SVAF. Requestor Miss-Address Files (\u201cRMAF\u201d) <b>178</b> in the Cbox preferably accepts requests and responds with data or instructions from the L<b>2</b> cache. Data accesses from other functional units in the microprocessor, other microprocessors in the computer system or any other devices that might need data out of the L<b>2</b> cache are sent to the RMAF for service. The Store Input/Output (\u201cSTIO\u201d) <b>179</b> preferably transfer data from the local microprocessor to I/O cards in the computer system. Finally, arbitration unit <b>181</b> in the Cbox preferably arbitrates between load and store accesses to the same memory location of the L<b>2</b> cache and informs other logic blocks in the Cbox and computer system functional units of the conflict.</p><p>Referring still to FIG. 2<i>b</i>, microprocessor <b>100</b> preferably includes dual, integrated Rambus memory controllers <b>190</b> (Zbox<b>0</b> and Zbox<b>1</b>). Each Zbox <b>190</b> controls 4 or 5 channels of information flow with the main memory <b>102</b> (FIG. <b>1</b>). Each Zbox preferably includes a front-end directory in-flight table (\u201cDIFT\u201d) <b>191</b>, a middle mapper <b>192</b>, and a back end <b>193</b>. The front-end DIFT <b>191</b> performs a number of functions such as managing the microprocessor's directory-based memory coherency protocol, processing request commands from the Cbox <b>170</b> and Rbox <b>200</b>, sending forward commands to the Rbox, sending response commands to and receiving packets from the Cbox and Rbox, and tracking up to 32 in-flight transactions. The front-end DIFT <b>191</b> also sends directory read and write requests to the Zbox and conditionally updates directory information based on request type, Local Probe Response (\u201cLPR\u201d) status and directory state.</p><p>The middle mapper <b>192</b> maps the physical address into Rambus device format by device, bank, row, and column. The middle mapper <b>192</b> also maintains an open-page table to track all open pages and to close pages on demand if bank conflicts arise. The mapper <b>192</b> also schedules Rambus transactions such as timer-base request queues. The Zbox back end <b>193</b> preferably packetizes the address, control, and data into Rambus format and provides the electrical interface to the Rambus devices themselves.</p><p>The Rbox <b>200</b> provides the interfaces to as many as four other microprocessors and one I/O controller <b>104</b> (FIG. <b>1</b>). The inter-processor interfaces are designated as North (\u201cN\u201d), South (\u201cS\u201d), East (\u201cE\u201d), and West (\u201cW\u201d) and provide two-way communication between adjacent microprocessors.</p><p>Referring now to FIG. 3, a RAMbus\u2122 DRAM (\u201cRDRAM\u201d) device <b>300</b> includes a DRAM core device <b>320</b> containing memory banks <b>325</b> and sense amplifiers <b>330</b> and RAMbus\u2122 interface logic <b>335</b> that permits an external control device to preferably access the DRAM core <b>320</b> at up to 1.6 gigabytes/second. A number of memory banks <b>325</b>, preferably sixty-four are shown in FIG. 3 although a DRAM core device <b>320</b> with 16, 32, 128, or a multiplier of 64 may be used in the preferred embodiment. Each DRAM core device <b>320</b> preferably contains 64 sense amplifiers <b>330</b>. Each sense amplifier <b>330</b> shared <b>335</b> between two adjacent banks <b>325</b> of the DRAM core device <b>320</b> (except for sense amplifiers <b>0</b>, <b>31</b>, <b>32</b>, and <b>63</b> that are not shared). The sense amplifiers <b>330</b> are connected through data paths DQA and DQB <b>350</b> that read and write data to RAMbus\u2122 interface logic <b>335</b> that is then output to the memory controller <b>190</b>. Control lines Precharge <b>360</b>, RowA <b>370</b>, and ColCRd/ColCWr <b>380</b>, respectively, causes a memory bank to close a page, activate a page, or read/write a page to the memory bank through DQA and DQB <b>350</b>.</p><p>In the preferred embodiment, the 64 Mbyte DRAM core device <b>320</b> of the RDRAM <b>300</b> is divided into 64 one-Mbyte banks <b>325</b>, each organized as <b>512</b> rows, with each row containing 128 columns and each column containing sixteen bytes. Thus, each row contains 2 Kilobytes of data (128*16=2 Kilobytes). A column is the smallest unit of data that can be addressed in a memory bank. The RDRAM <b>300</b> preferably contains 64 sense amplifier buffers <b>330</b>. Each sense amplifier buffer <b>330</b> is capable of storing 1024 bytes (<b>512</b> for DQA and <b>512</b> for DQB) and in the preferred embodiment can hold one-half of one row of a RDRAM memory bank <b>325</b>. The number of bytes that can be stored in two sense amplifiers <b>330</b> is called the page size of the RDRAM device <b>300</b> because each memory bank <b>325</b> has access to two sense amplifiers <b>330</b>. Thus, the page size for the preferred embodiment is 2048 bytes (2 Kilobytes). In other embodiments of the invention, a page can be 1 Kilobyte or 4 Kilobytes based on the storage capacity of the sense amplifier. A sense amplifier may hold any of the 512 half-rows of an associated memory bank. However, as mentioned above, each sense amplifier is shared <b>335</b> between two adjacent banks of the RDRAM. This introduces the restriction that adjacent banks <b>325</b> in the preferred embodiment may not be simultaneously accessed.</p><p>Control line Precharge <b>360</b> coupled to the DRAM core device <b>320</b> transmits a precharge command that, along with the (RDRAM device, memory bank) address, causes the selected memory bank <b>325</b> to release its two associated sense amplifiers <b>330</b>. This permits a different row in that memory bank to be activated, or permits adjacent memory banks to be activated. The RowA <b>370</b> control line coupled to the DRAM core device <b>320</b> transmits an Activate command that, along with the (RDRAM device, memory bank) and row address, causes the selected row of the selected bank to be loaded into its associated sense amplifiers <b>330</b> (two 512 byte sense amplifiers for DQA and two 512 byte sense amplifiers for DQB). The ColCRd <b>380</b> command is issued to a (RDRAM device, memory bank, column) to transfer a column of data (16 bytes) from one of the two sense amplifiers <b>330</b> shared by the memory bank <b>325</b> through the DQA/DQB <b>350</b> data paths to the RAMbus\u2122 interface logic <b>335</b>. The data is then output to the Zbox memory controller <b>190</b>. A ColCWr <b>380</b> command transfers a column of data from the Zbox memory controller <b>190</b> through the RAMbus\u2122 interface logic <b>335</b> and DQA/DQB data paths <b>350</b> to one of the two sense amplifiers <b>330</b> for the (RDRAM device, memory bank, column).</p><p>Turning now to FIG. 4, the processor in the computer system organizes main memory by allocating a processor address to each byte of main memory storage. For each read or write memory access to a byte of data, the cache memory interprets the 44 bit processor address of the byte as a cache address <b>400</b> to determine whether the memory block containing the byte is present. The cache address includes a tag subfield <b>410</b>, an index subfield <b>420</b> and an offset subfield <b>430</b>. As explained above, the index subfield <b>420</b> in a set associative cache memory identifies the particular set in the cache memory to which a block of data from main memory can be mapped. Each set in the cache memory contains a number of block frames to which a data block from main memory can be placed. A block in a block frame within a set is identified by its tag subfield from the cache address. The offset subfield <b>410</b> in the cache address determines the actual byte within the block of data in the cache memory.</p><p>Zbox memory controller <b>190</b> preferably interprets the processor address as a device address <b>440</b> that identifies a particular preferred RDRAM memory device, memory bank, row and column. In one embodiment, a ten-bit wide bank subfield <b>445</b> includes a five-bit wide bank number and a five-bit wide device number. Bits [<b>15</b>:<b>6</b>] corresponding to the bank subfield <b>445</b> in the device address <b>440</b> all correspond to bits [<b>15</b>:<b>6</b>] of the index subfield <b>420</b> of the cache address <b>400</b>. Row subfield <b>455</b> identifies a particular row or page within a memory bank for the device address <b>440</b>. Column subfield <b>450</b> preferably identifies a 16-byte column of data in the row <b>455</b> of the memory bank <b>445</b> for the device address <b>440</b>. Offset subfield <b>460</b> identifies a particular byte within a column <b>450</b> of data.</p><p>FIG. 4 shows the device address mapping of the preferred embodiment using a ten bit wide bank subfield <b>489</b> that includes a five bit wide bank number and a five bit wide device number. Bits [<b>24</b>:<b>15</b>] corresponding to the bank subfield <b>445</b> in the device address <b>440</b> do not completely fall within bits [<b>17</b>:<b>6</b>] corresponding to index subfield <b>420</b> of the cache address <b>400</b>. Thus, bank subfield <b>489</b> bits [<b>24</b>:<b>18</b>] are above the upper bit of index subfield <b>420</b> in the cache address <b>400</b>. The row subfield <b>487</b> and row bit <b>491</b> of the device address <b>480</b> identifies the row or page within a memory bank. In the preferred embodiment, the row identifier is divided into subfields <b>487</b> and <b>491</b> so that the bank subfield <b>489</b> can shift left and not overlap completely with the index subfield <b>420</b> of the cache address <b>400</b>. Column subfield <b>492</b> preferably identifies a 16-byte column of data in the row of the memory bank <b>445</b> for the device address <b>440</b>. Offset subfield <b>495</b> identifies a particular byte within a column <b>492</b> of data.</p><p>Device address mapping <b>440</b>, because bank subfield <b>445</b> bits each correspond to bits in the index subfield, results in memory bank conflicts whenever replacement of a block in cache memory occurs. A cache miss occurs when a needed block of data is not present in the cache memory. The cache requests the needed data block from main memory by performing a read to the memory block. If the block of data to be removed from cache memory, the victim block, has been modified, then this modified block of data must be written back to main memory. The requested data block from main memory is placed into a particular set of the cache memory based on its index subfield <b>420</b>. Since the victim block was also placed into a set based on its index subfield <b>420</b> and a set in the memory cache is identified by one index subfield <b>420</b>, both the requested data block and victim block must have the same index subfield <b>420</b>. In device address <b>440</b> bank subfield <b>445</b> bits each correspond to bits in index subfield <b>420</b>. Thus, the requested data block and victim block would both have identical bank subfields <b>445</b> and would both access the same RDRAM device and memory bank. The read resulting from the request for the data block would cause the opening of a row/page from the memory bank. Assuming the victim block is in a different row/page then the read request data block, the cache would then write the victim block to the different row/page of the same memory bank closing the previously opened row/page for the requested data block and opening the row/page for the victim block. Because of the sequential nature read and write and the opening and closing of multiple row/pages, memory system performance would be significantly reduced.</p><p>As mentioned above, the phenomenon of locality of reference makes it highly likely that contiguous blocks of data stored in a row/page will be accessed. Thus, the likelihood of subsequent cache misses and replacement of data blocks is highly probable. If the cache memory requires a different block of data and consequently must replace another victim block and these new memory requests require the same memory bank and row/page as the data blocks of the prior replacement request, then the request would result in the closing of the previous row/page in the memory bank for the victim block and opening of the row/page for the read resulting from the new request. Thus, if the row/page prior read request for the data block had not been closed, then the subsequent access to the same row/page would have resulted in a row/page hit. Similarly if the row/page for the prior victim block had not been closed, then the subsequent access to the same row/page would have resulted in a row/page hit.</p><p>Device address mapping <b>480</b> shows one preferred embodiment that overcomes the disadvantages of device address mapping <b>440</b> by changing the location of the bank <b>489</b>, column <b>492</b> and row <b>487</b> subfields relative to the cache address <b>400</b>. Bank subfield <b>489</b> bits [<b>24</b>:<b>18</b>] are above the upper bit of index subfield <b>420</b> in the cache address <b>400</b>. This results in selection of a different memory bank for the read request data block and victim data block.</p><p>Preferably, based on placement of the subfields across the 29 bits, a total of <b>122</b> different device address mappings are possible using software programming. Bank subfield <b>489</b> in device address <b>480</b> may be programmably shifted left or right by shifting the corresponding row subfield <b>487</b> and column subfield <b>492</b>. Shifting of the bank subfield <b>489</b> must follow a few general guidelines. Bank subfield placement must be such that the upper bits of the bank subfield are above index subfield <b>420</b> boundary bit [<b>17</b>]. Right shifting the bank subfield as much as possible while maintaining a few bits above the index subfield boundary bit will allow multiple pages to remain open. This will permit sequential accesses to reference many different memory banks and take advantage of the inherent parallelism exposed by the memory banks for the cache replacement scenario described above.</p><p>Maintaining the row subfield left of the bank subfield and the column subfield right of the bank subfield would also take advantage of the locality of reference sequential access phenomenon. A row/page miss would result in the missed row/page being opened and subsequent sequential access to the same row/page resulting in page hits. This is because sequential accesses will more than likely be to the same row/page but different columns within the row/page with the row subfield <b>487</b> and column subfield <b>492</b> locations shown. Thus, the placement of subfields shown in device address mapping <b>480</b> tries to advantageously balances both locality of reference and parallelism of multiple open rows/pages.</p><p>Referring now to FIG. 5, locality of reference for sequential accesses may lead to conflicts between adjacent banks that implement shared sense amplifier buffer DRAM core devices. Locality of reference improves performance for memory accesses within the same row/page because the row/page is already open. If sequential accesses result in open row/pages from adjacent banks, memory performance will suffer significantly because the shared sense amplifier buffer architecture does not permit two adjacent banks to concurrently have open rows/pages. In the preferred embodiment shown in FIG. 5, reordering of the ten bit bank subfield <b>489</b> in device address <b>480</b> advantageously solves the problem of memory accesses requiring open row/pages from adjacent banks. The bank subfield <b>489</b> includes bank number subfield <b>520</b> and device number subfield <b>530</b>. Bank number subfield <b>520</b> preferably specifies a memory bank out of the 32 possible memory banks in the RDRAM device. Device number subfield <b>530</b> preferably specifies a RDRAM device out of 32 possible RDRAM devices in the computer system. Thus, 1024 memory banks can be addressed by the 10-bit bank subfield <b>489</b>.</p><p>Bit ordering of bank subfield <b>489</b> is from right to left, with the least significant bit [<b>15</b>] on the right and most significant bit [<b>24</b>] on the left. Minimizing adjacent bank conflicts requires that the bank number subfield <b>520</b> bit order be reversed as shown in FIG. 5 with most significant bit [<b>24</b>] becoming the least significant bit [<b>0</b>] of bank number subfield <b>520</b> after the bit reordering. Thus, incrementing the bank number subfield by one from bank number subfield=10000 (<b>16</b>) to bank number subfield=10001 (<b>17</b>) after bit reversal would result in accesses to bank number=00001 (<b>1</b>) and bank number=10001 (<b>17</b>). Thus, bit order reversal of the bank number subfield <b>520</b> significantly minimizes accesses to rows/pages in adjacent banks using a straightforward solution that can be implemented quickly and simply in hardware.</p><p>The above discussion is meant to be illustrative of the principles and various embodiments of the present invention. Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Richard E.", "last_name": "Kessler", "name": ""}, {"first_name": "Maurice B.", "last_name": "Steinman", "name": ""}, {"first_name": "Peter J.", "last_name": "Bannon", "name": ""}, {"first_name": "Michael C.", "last_name": "Braganza", "name": ""}, {"first_name": "Gregg A.", "last_name": "Bouchard", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "COMPAQ INFORMATION TECHNOLOGIES GROUP, L.P."}, {"first_name": "", "last_name": "HEWLETT-PACKARD DEVELOPMENT COMPANY, L.P.", "name": ""}, {"first_name": "", "last_name": "COMPAQ INFORMATION TECHNOLOGIES GROUP, L.P.", "name": ""}, {"first_name": "", "last_name": "COMPAQ INFORMATION TECHNOLOGIES GROUP, L.P.", "name": ""}, {"first_name": "", "last_name": "COMPAQ COMPUTER CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/02        20060101A I20051008RMEP"}, {"label": "G06F  12/08        20060101A I20051008RMEP"}, {"label": "G06F  12/06        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711005"}, {"primary": false, "label": "711E12079"}, {"primary": false, "label": "711202"}, {"primary": false, "label": "711105"}, {"primary": false, "label": "711E12004"}, {"primary": false, "label": "711E12054"}, {"primary": false, "label": "711210"}], "ecla_classes": [{"label": "G06F  12/08B16D"}, {"label": "G06F  12/02C"}, {"label": "G06F  12/06A"}], "cpc_classes": [{"label": "G06F  12/0607"}, {"label": "G06F  12/0215"}, {"label": "G06F  12/0607"}, {"label": "G06F  12/0882"}, {"label": "G06F  12/0882"}, {"label": "G06F  12/0215"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "2000-08-31", "application_date": "2000-08-31", "family_members": [{"ucid": "US-6546453-B1", "titles": [{"lang": "EN", "text": "Proprammable DRAM address mapping mechanism"}]}]}