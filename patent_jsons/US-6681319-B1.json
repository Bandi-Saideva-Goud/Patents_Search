{"patent_number": "US-6681319-B1", "publication_id": 73677770, "family_id": 30001905, "publication_date": "2004-01-20", "titles": [{"lang": "EN", "text": "Dual access instruction and compound memory access instruction with compatible address fields"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11624637\" source=\"national office\"><p>A processing engine 10 includes an instruction buffer 502 operable to buffer single and compound instructions pending execution. A decode mechanism is configured to decode instructions from the instruction buffer. The decode mechanism is arranged to respond to a predetermined tag in a tag field of an instruction, which predetermined tag is representative of the instruction being a compound instruction formed from separate programmed memory instructions. The decode mechanism is operable in response to the predetermined tag to decode at least first data flow control for a first programmed instruction and second data flow control for a second programmed instruction. The use of compound instructions enables effective use of the bandwidth available within the processing engine. A soft dual memory instruction can be compiled from separate first and second programmed memory instructions. A compound address field of the predetermined compound instruction can be arranged at the same bit positions as the address field for a hard compound memory instruction, that is a compound instruction which is programmed. In this case the decoding of the addresses can be started before the operation code of the instructions have been decoded. To reduce the number of bits in the compound instruction, addressing can be restricted to indirect addressing and the operation codes for at least the first instruction can be reduced in size. In this way, the compound instruction can be arranged to have the same number of bits in total as the sum of the bits of the separate programmed instructions.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50609766\"><p>A processing engine <b>10 </b>includes an instruction buffer <b>502 </b>operable to buffer single and compound instructions pending execution. A decode mechanism is configured to decode instructions from the instruction buffer. The decode mechanism is arranged to respond to a predetermined tag in a tag field of an instruction, which predetermined tag is representative of the instruction being a compound instruction formed from separate programmed memory instructions. The decode mechanism is operable in response to the predetermined tag to decode at least first data flow control for a first programmed instruction and second data flow control for a second programmed instruction. The use of compound instructions enables effective use of the bandwidth available within the processing engine. A soft dual memory instruction can be compiled from separate first and second programmed memory instructions. A compound address field of the predetermined compound instruction can be arranged at the same bit positions as the address field for a hard compound memory instruction, that is a compound instruction which is programmed. In this case the decoding of the addresses can be started before the operation code of the instructions have been decoded. To reduce the number of bits in the compound instruction, addressing can be restricted to indirect addressing and the operation codes for at least the first instruction can be reduced in size. In this way, the compound instruction can be arranged to have the same number of bits in total as the sum of the bits of the separate programmed instructions.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00001\" num=\"1\"><claim-text>1. A digital system comprising a processing engine, wherein the processing engine comprises:</claim-text><claim-text>an instruction buffer operable to buffer single and compound instructions pending execution thereof; </claim-text><claim-text>a decode mechanism configured, to decode instructions from the instruction buffer, the decode mechanism being responsive to a predetermined tag in an instruction, the predetermined tag being representative of the instruction being a compound instruction formed from separate programmed memory instructions, to decode at least first data flow control for a first programmed instruction and at least second data flow control for a second programmed instruction; and </claim-text><claim-text>wherein the compound instruction is a compound memory access instruction formed by combining separate first and second programmed memory access instructions such that a compound address field of the compound instruction is formed at the same bit positions as an address field for a hard programmed dual memory instruction and wherein the compound address field is decoded into first and second memory addresses for first and second memory address instructions respectively. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00002\" num=\"2\"><claim-text>2. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00001\">claim 1</claim-ref>, wherein the decode mechanism is operable to decode a first memory address for a first programmed memory address instruction and a second memory address for a second programmed memory instruction from a compound memory address field in the compound instruction.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00003\" num=\"3\"><claim-text>3. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00001\">claim 1</claim-ref>, wherein the memory addresses in the compound address field of the compound instruction are indirect addresses, the decode mechanism being operable to decode the indirect addresses.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00004\" num=\"4\"><claim-text>4. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00001\">claim 1</claim-ref>, wherein the compound instruction comprises a split operation code field for a first programmed instruction of the compound instruction.</claim-text></claim>"}, {"num": 5, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00005\" num=\"5\"><claim-text>5. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00004\">claim 4</claim-ref>, wherein the decode mechanism is responsive to the predetermined tag to decode a split operation code for the first programmed instruction of the compound instruction.</claim-text></claim>"}, {"num": 6, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00006\" num=\"6\"><claim-text>6. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00005\">claim 5</claim-ref>, wherein the compound instruction comprises an operation code field for a first programmed instruction of the compound instruction, which operation code field comprises less bits than the operation code field of the first programmed instruction.</claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00007\" num=\"7\"><claim-text>7. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00006\">claim 6</claim-ref>, wherein the decode mechanism is responsive to the predetermined tag to decode a reduced size operation code for the first programmed instruction of the compound instruction.</claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00008\" num=\"8\"><claim-text>8. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00007\">claim 7</claim-ref>, wherein the compound instruction has the same number of bits in total as the sum of the bits of the separate programmed instructions.</claim-text></claim>"}, {"num": 9, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00009\" num=\"9\"><claim-text>9. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00001\">claim 1</claim-ref>, wherein the compound instruction has a combined data address generation (DAGEN) field formed from DAGEN fields of the first and second programmed memory instructions.</claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00010\" num=\"10\"><claim-text>10. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00009\">claim 9</claim-ref>, wherein the combined DAGEN field forms part of a combined address field.</claim-text></claim>"}, {"num": 11, "parent": 10, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00011\" num=\"11\"><claim-text>11. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00010\">claim 10</claim-ref>, wherein the decode mechanism is responsive to a predetermined DAGEN tag to decode the combined DAGEN field.</claim-text></claim>"}, {"num": 12, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00012\" num=\"12\"><claim-text>12. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00001\">claim 1</claim-ref>, comprising a fetch controller operable to fetch in parallel first and second operands from addresses identified by the first and second memory addresses, respectively.</claim-text></claim>"}, {"num": 13, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00013\" num=\"13\"><claim-text>13. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00012\">claim 12</claim-ref>, comprising a write controller operable to write in parallel the result of first and second data flow operations for the first and second programmed instructions, respectively.</claim-text></claim>"}, {"num": 14, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00014\" num=\"14\"><claim-text>14. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00001\">claim 1</claim-ref>, wherein the decode mechanism is further operable to interpret a single memory access instruction as implicitly capable of parallel execution, whereby the single memory access instruction does not including a parallel enable field.</claim-text></claim>"}, {"num": 15, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00015\" num=\"15\"><claim-text>15. The processing engine according to <claim-ref idref=\"US-6681319-B1-CLM-00014\">claim 14</claim-ref>, wherein the single memory access instruction is constrained to be a first programmed instruction of a pair of instructions in the instruction buffer.</claim-text></claim>"}, {"num": 16, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00016\" num=\"16\"><claim-text>16. The digital system of <claim-ref idref=\"US-6681319-B1-CLM-00001\">claim 1</claim-ref> being a cellular telephone, further comprising:</claim-text><claim-text>an integrated keyboard connected to the processor via a keyboard adapter; </claim-text><claim-text>a display, connected to the processor via a display adapter; </claim-text><claim-text>radio frequency (RF) circuitry connected to the processor; and </claim-text><claim-text>an aerial connected to the RF circuitry. </claim-text></claim>"}, {"num": 17, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00017\" num=\"17\"><claim-text>17. The digital system of <claim-ref idref=\"US-6681319-B1-CLM-00001\">claim 1</claim-ref>, further comprising an instruction preprocessing means for preparing instructions for execution, the instruction preprocessing means being operable to combine separate programmed memory instructions to form a compound memory instruction.</claim-text></claim>"}, {"num": 18, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00018\" num=\"18\"><claim-text>18. A method of improving the performance of a processing engine, the method comprising the steps of:</claim-text><claim-text>combining separate first and second programmed memory instructions to form a compound instruction such that a compound address field of the compound instruction is formed at the same bit positions as an address field for a hard programmed dual memory instruction and wherein the compound address field is decoded into first and second memory addresses for first and second memory address instructions respectively, the compound instruction including a tag field containing a predetermined compound instruction tag; </claim-text><claim-text>storing the compound instruction in an instruction buffer; and </claim-text><claim-text>responding to the predetermined compound instruction tag in the tag field of an instruction in the instruction buffer to decode, from the compound instruction, at least first data control for a first programmed instruction and second data flow control for a second programmed instruction. </claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00019\" num=\"19\"><claim-text>19. The method according to <claim-ref idref=\"US-6681319-B1-CLM-00018\">claim 18</claim-ref>, further comprising the step of decoding at least a first memory address for the first programmed memory instruction and a second memory address for the second programmed memory instruction from a compound address field of the compound instruction.</claim-text></claim>"}, {"num": 20, "parent": 19, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00020\" num=\"20\"><claim-text>20. The method according to <claim-ref idref=\"US-6681319-B1-CLM-00019\">claim 19</claim-ref>, further comprising the step of decoding the compound address field of the compound instruction from the same bit positions as for the address field for a hard programmed dual memory instruction.</claim-text></claim>"}, {"num": 21, "parent": 19, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00021\" num=\"21\"><claim-text>21. The method according to <claim-ref idref=\"US-6681319-B1-CLM-00019\">claim 19</claim-ref>, further comprising the step of fetching in parallel first and second operands from addresses identified by first and second memory addresses, respectively.</claim-text></claim>"}, {"num": 22, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00022\" num=\"22\"><claim-text>22. The method according to <claim-ref idref=\"US-6681319-B1-CLM-00021\">claim 21</claim-ref>, comprising writing in parallel the result of first and second data flow operations for first and second programmed instructions, respectively, of the compound instruction.</claim-text></claim>"}, {"num": 23, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00023\" num=\"23\"><claim-text>23. The method according to <claim-ref idref=\"US-6681319-B1-CLM-00018\">claim 18</claim-ref>, further comprising the step of decoding a split operation code for a first instruction of the compound instruction.</claim-text></claim>"}, {"num": 24, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00024\" num=\"24\"><claim-text>24. The method according to <claim-ref idref=\"US-6681319-B1-CLM-00018\">claim 18</claim-ref>, further comprising decoding a reduced size operation code for the first instruction of the compound instruction.</claim-text></claim>"}, {"num": 25, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00025\" num=\"25\"><claim-text>25. The method according to <claim-ref idref=\"US-6681319-B1-CLM-00018\">claim 18</claim-ref>, wherein the step of responding comprises decoding a combined data address generation (DAGEN) field formed from DAGEN fields of the first and second programmed memory instructions.</claim-text></claim>"}, {"num": 26, "parent": 25, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00026\" num=\"26\"><claim-text>26. The method according to <claim-ref idref=\"US-6681319-B1-CLM-00025\">claim 25</claim-ref>, wherein the combined DAGEN field forms part of a combined address field.</claim-text></claim>"}, {"num": 27, "parent": 25, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00027\" num=\"27\"><claim-text>27. The method according to <claim-ref idref=\"US-6681319-B1-CLM-00025\">claim 25</claim-ref>, wherein the decode mechanism is responsive to a predetermined DAGEN tag to decode the combined DAGEN field.</claim-text></claim>"}, {"num": 28, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00028\" num=\"28\"><claim-text>28. The method according to <claim-ref idref=\"US-6681319-B1-CLM-00018\">claim 18</claim-ref>, wherein the step of combining comprises determining whether the separate programmed memory instructions may be combined prior to assembly of the compound instruction.</claim-text></claim>"}, {"num": 29, "parent": 28, "type": "dependent", "paragraph_markup": "<claim id=\"US-6681319-B1-CLM-00029\" num=\"29\"><claim-text>29. The method according to <claim-ref idref=\"US-6681319-B1-CLM-00028\">claim 28</claim-ref>, wherein the step of combining further comprises:</claim-text><claim-text>determining programmed memory instructions capable of being combined; and </claim-text><claim-text>combining the determined programmed memory instructions to form a compound memory instruction. </claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54118867\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><p>This application claims priority to S.N. 98402456.2, filed in Europe on Oct. 6, 1998 (TI-27685EU) and S.N. 98402455.4, filed in Europe on Oct. 6, 1998 (TI-28433EU).</p><h4>FIELD OF THE INVENTION</h4><p>The present invention relates to processing engines, and to the parallel execution of instructions in such processing engines.</p><h4>BACKGROUND OF THE INVENTION</h4><p>It is known to provide for parallel execution of instructions in microprocessors using multiple instruction execution units. Many different architectures are known to provide for such parallel execution. Providing parallel execution increases the overall processing speed. Typically, multiple instructions are provided in parallel in an instruction buffer and these are then decoded in parallel and are dispatched to the execution units. Microprocessors are general purpose processing engines which require high instruction throughputs in order to execute software running thereon, which can have a wide range of processing requirements depending on the particular software applications involved. Moreover, in order to support parallelism, complex operating systems have been necessary to control the scheduling of the instructions for parallel execution.</p><p>Many different types of processing engines are known, of which microprocessors are but one example. For example, Digital Signal Processors (DSPs) are widely used, in particular for specific applications. DSPs are typically configured to optimize the performance of the applications concerned and to achieve this they employ more specialized execution units and instruction sets.</p><p>The present invention is directed to improving the performance of processing engines such as for example, but not exclusively, digital signal processors.</p><h4>SUMMERY OF THE INVENTION</h4><p>Particular and preferred aspects of the invention are set out in the accompanying independent and dependent claims. Combinations of features from the dependent claims may be combined with features of the independent claims as appropriate and not merely as explicitly set out in the claims.</p><p>In accordance with a first aspect of the invention, there is provided a processing engine comprising an instruction buffer operable to buffer single and compound instructions pending execution thereof, and a decode mechanism configured to decode instructions from the instruction buffer. The decode mechanism is configured to be responsive to a predetermined tag in a tag field of an instruction, which predetermined tag is representative of the instruction being a compound instruction formed from separate programmed memory instructions. The decode mechanism is operable in response to the predetermined tag to decode at least a first data flow control for a first programmed instruction and a second data flow control for a second programmed instruction.</p><p>Thus, an embodiment of the invention provides a decode mechanism responsive to compound instructions formed (e.g., assembled or compiled) by combining separate programmed instructions. In this manner, it is possible to optimize the use of the bandwidth available within the processing engine. Appropriate programmed instructions, such as suitable memory instructions, can thus be assembled, or compiled, to form a compound instruction. By generating a separate control flow for each of the constituent programmed instructions from the compound instruction, those instructions can be performed wholly or partially in parallel with a positive effect on the overall throughput of the processing engine. The control flow generated by the decode mechanism for each of the programmed instructions can be the same as that which would have been generated for the programmed instructions if they had been held as single instructions in the instruction buffer.</p><p>A compact and efficient encoding can be enabled in an embodiment of the invention. For example by ensuring that a memory instruction can only be a first of a pair of instructions in the instruction buffer in the form of a predetermined compound instruction, parallelism of memory access instructions can be provided with efficient encoding, efficient use of real estate and reduced power consumption.</p><p>In an embodiment of the invention, the compound instruction is defined as a soft compound memory instruction formed by combining (e.g. using an instruction preprocessing mechanism such as a compiler or an assembler) from separate programmed memory instructions. In a particular example, the compound instruction is a soft dual memory instruction, that is a dual memory instruction assembled from separate first and second programmed memory instructions, although in other examples more than two instructions can be assembled into a compound instruction.</p><p>Preferably, the decode mechanism is operable to decode a first memory address for a first programmed memory address instruction and a second memory address for a second programmed memory instruction from a compound memory address field in the compound instruction. Particularly, where the compound address field of the compound instruction is at the same bit positions as the address field for a hard programmed dual memory instruction, this can have a positive effect on instruction throughput. In this case the decoding of the addresses can be started before the operation code of the instructions have been decoded regardless of the format of first and second instructions of a dual instruction.</p><p>In order to reduce the number of bits required for the compound instruction, the memory addresses in the compound address field of the compound instruction can be arranged to be indirect addresses, whereby the decode mechanism needs only to be operable to decode indirect addresses for such instructions. As dual instructions support less options than single instructions, the size of a post modification field for the addresses can be reduced, thereby reducing the number of bits required for the addresses themselves and also to dispense with an indirect/direct indicator bit.</p><p>A memory access instruction can be constrained to be a first instruction of a pair of instructions in the instruction buffer. In this case a soft dual instruction effectively provides an encoding corresponding to two memory instructions. As a result, the need for a parallel enable field can be avoided, any memory instruction being implicitly capable of parallelism. This also provides further advantages of providing a reduction of an application code size, with optimization of external interface bandwidth and a reduction of cache misses.</p><p>The decoder for the second instruction of an instruction pair can also be made as a subset of the decoder for the first instruction resulting in a reduction in the integrated circuit real estate required and a reduction in power consumption for the processing engine.</p><p>In order to provide a compact instruction format and to enable the address field to be located at the same position as for a hard compound instruction, the compound instruction can comprise a split operation code field for a first instruction of the predetermined compound instruction. The operation code can be spilt either side of the address field, for example. The decoder can be response to detection of the appropriate tag field to decode the split operation code for the first instruction of the compound instruction.</p><p>In order to further reduce the number of bits, the compound instruction can comprise a reduced operation code field for at least the first instruction of the predetermined compound instruction such that the operation code field comprises fewer bits that the operation code field of the first programmed instruction. By restricting the range of operation codes for memory instructions to be within a certain range or ranges, the number of bits which need to be provided for the first operation code can be reduced. The decode mechanism can be arranged to be responsive to the predetermined tag to decode a reduced size operation code for the first instruction of the compound instruction.</p><p>With the various measures mentioned above, the predetermined compound instruction can be arranged to have the same number of bits in total as the sum of the bits of the separate programmed instructions. Reorganization of the fields from the programmed instructions can lead to the predetermined compound instruction having a common overall format with other instructions.</p><p>Where each programmed instruction has a data address generation (DAGEN) code field, the individual DAGEN codes of the individual programmed instructions could be combined into a combined DAGEN code field within the compound instruction. This could provide more rapid decoding and execution of the compound instruction. The combined DAGEN code field could form part of a combined address field. Where a combined DAGEN code field is provided, the decode mechanism can be operable to respond to a predetermined DAGEN tag to decode the combined DAGEN field.</p><p>The processing engine can be provided with a data fetch controller operable to fetch, in parallel, first and second operands from addresses identified by the first and second memory addresses, respectively. A data write controller can also be operable to write in parallel the result of first and second data flow operations for the first and second instructions, respectively. Also, dual read/write operations can be provided.</p><p>In an embodiment of the invention, assembler syntax can differentiate between hard compound and soft compound syntax to provide visibility for available slots for parallelism. A hard compound instruction can be executed in parallel with a non-memory instruction such as a control flow or register instruction as indicated by a parallel enable bit and as long as there are no bus/operator resource conflicts.</p><p>In accordance with another aspect of the invention, there is provided a processor, for example, but not necessarily, a digital signal processor, comprising a processing engine as described above. The processor can be implemented as an integrated circuit, for example as an Application Specific Integrated Circuit (ASIC).</p><p>A digital signal processing system comprising a processing engine as described above can also be provided with an instruction preprocessing mechanism operable to combine separate programmed memory instructions to form a compound memory instruction. The instruction preprocessor can be in the form of a compiler, assembler, etc., which is operable to compile or assemble compound instructions from programmed instructions. The mechanism can be configured to be operable to determine whether the separate programmed memory instructions may be combined prior to assembly of the compound instruction.</p><p>In accordance with a further aspect of the invention, there is provided an instruction preprocessor for a digital signal processing system, the instruction preprocessor being configured to be operable:</p><p>to determine programmed memory instructions capable of being combined; and</p><p>to assemble a compound memory instruction from said determined programmed memory instructions.</p><p>It should be understood that in the present context the term \u201cinstruction preprocessor\u201d is to be understood broadly to cover any mechanism for preprocessing instructions, that is compiling and/or assembling instructions, including compilers, assemblers, etc.</p><p>The instruction preprocessor may be provided separately, for example on a carrier medium such as a data storage medium (a disc, solid state memory, a data transmission medium such as an electrical, optical or other electromagnetic (e.g. wireless transmission medium)).</p><p>In accordance with another aspect of the invention, there is provided a method of improving the performance of a processing engine. The method includes:</p><p>buffering a compound instruction assembled from separate programmed memory instructions, the compound instruction including a tag field containing a predetermined compound instruction tag; and</p><p>responding to the predetermined compound instruction tag in the tag field of an instruction in the instruction buffer to decode, from the compound instruction, at least first data flow control for a first programmed instruction and second data flow control for a second programmed instruction.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Particular embodiments in accordance with the invention will now be described, by way of example only, and with reference to the accompanying drawings in which like reference signs are used to denote like parts, unless otherwise stated, and in which:</p><p>FIG. 1 is a schematic block diagram of a processor in accordance with an embodiment of the invention;</p><p>FIG. 2 is a schematic diagram of a core of the processor of FIG. 1;</p><p>FIG. 3 is a more detailed schematic block diagram of various execution units of the core of the processor of FIG. 1;</p><p>FIG. 4 is a schematic diagram of an instruction buffer queue and an instruction decoder controller of the processor of FIG. 1;</p><p>FIG. 5 is a representation of pipeline phases of the processor of FIG. 1;</p><p>FIG. 6 is a diagrammatic illustration of an example of the operation of a pipeline in the processor of FIG. 1;</p><p>FIG. 7 is a schematic representation of the core of the processor for explaining the operation of the pipeline of the processor of FIG. 1;</p><p>FIG. 8 illustrates examples of instruction pairs;</p><p>FIG. 9 illustrates the relative timing of bus cycles for various instructions;</p><p>FIG. 10 illustrates an example of the execution of a soft dual instruction;</p><p>FIG. 11 is a schematic diagram illustrating the generation of a soft dual instruction.</p><p>FIG. 12 is a flow diagram of the generation of a soft dual instruction;</p><p>FIG. 13 is a block diagram of a structure for executing a soft dual instruction;</p><p>FIG. 14 illustrates memory bus interfacing for a soft dual instruction operation;</p><p>FIG. 15 is a table illustrating operand fetch control for a soft dual instruction.</p><p>FIG. 16 is a schematic representation of an integrated circuit incorporating the processor of FIG. 1; and</p><p>FIG. 17 is a schematic representation of a telecommunications device incorporating the processor of FIG. <b>1</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF PARTICULAR EMBODIMENTS</h4><p>Although the invention finds particular application to Digital Signal Processors (DSPs), implemented for example in an Application Specific Integrated Circuit (ASIC), it also finds application to other forms of processing engines.</p><p>FIG. 1 is a block diagram of a microprocessor <b>10</b> which has an embodiment of the present invention. Microprocessor <b>10</b> is a digital signal processor (\u201cDSP\u201d). In the interest of clarity, FIG. 1 only shows those portions of microprocessor <b>10</b> that are relevant to an understanding of an embodiment of the present invention. Details of general construction for DSPs are well known, and may be found readily elsewhere. For example, U.S. Pat. No. 5,072,418 issued to Frederick Boutaud, et al, describes a DSP in detail and is incorporated herein by reference. U.S. Pat. No. 5,329,471 issued to Gary Swoboda, et al, describes in detail how to test and emulate a DSP and is incorporated herein by reference. Details of portions of microprocessor <b>10</b> relevant to an embodiment of the present invention are explained in sufficient detail hereinbelow, so as to enable one of ordinary skill in the microprocessor art to make and use the invention.</p><p>Several example systems which can benefit from aspects of the present invention are described in U.S. Pat. No. 5,072,418, which was incorporated by reference herein, particularly with reference to FIGS. 2-18 of U.S. Pat. No. 5,072,418. A microprocessor incorporating an aspect of the present invention to improve performance or reduce cost can be used to further improve the systems described in U.S. Pat. No. 5,072,418. Such systems include, but are not limited to, industrial process controls, automotive vehicle systems, motor controls, robotic control systems, satellite telecommunication systems, echo canceling systems, modems, video imaging systems, speech recognition systems, vocoder-modem systems with encryption, and such.</p><p>A description of various architectural features and a description of a complete set of instructions of the microprocessor of FIG. 1 is provided in co-assigned application Ser. No. 09/410,977 (TI-28433), which is incorporated herein by reference.</p><p>The basic architecture of an example of a processor according to the invention will now be described.</p><p>FIG. 1 is a schematic overview of a processor <b>10</b> forming an exemplary embodiment of the present invention. The processor <b>10</b> includes a processing engine <b>100</b> and a processor backplane <b>20</b>. In the present embodiment, the processor is a Digital Signal Processor <b>10</b> implemented in an Application Specific Integrated Circuit (ASIC).</p><p>As shown in FIG. 1, the processing engine <b>100</b> forms a central processing unit (CPU) with a processing core <b>102</b> and a memory interface, or management, unit <b>104</b> for interfacing the processing core <b>102</b> with memory units external to the processor core <b>102</b>.</p><p>The processor backplane <b>20</b> comprises a backplane bus <b>22</b>, to which the memory management unit <b>104</b> of the processing engine is connected. Also connected to the backplane bus <b>22</b> is an instruction cache memory <b>24</b>, peripheral devices <b>26</b> and an external interface <b>28</b>.</p><p>It will be appreciated that in other embodiments, the invention could be implemented using different configurations and/or different technologies. For example, the processing engine <b>100</b> could form the processor <b>10</b>, with the processor backplane <b>20</b> being separate therefrom. The processing engine <b>100</b> could, for example be a DSP separate from and mounted on a backplane <b>20</b> supporting a backplane bus <b>22</b>, peripheral and external interfaces. The processing engine <b>100</b> could, for example, be a microprocessor rather than a DSP and could be implemented in technologies other than ASIC technology. The processing engine, or a processor including the processing engine, could be implemented in one or more integrated circuits.</p><p>FIG. 2 illustrates the basic structure of an embodiment of the processing core <b>102</b>. As illustrated, the processing core <b>102</b> includes four elements, namely an Instruction Buffer Unit (I Unit) <b>106</b> and three execution units. The execution units are a Program Flow Unit (P Unit) <b>108</b>, Address Data Flow Unit (A Unit) <b>110</b> and a Data Computation Unit (D Unit) <b>112</b> for executing instructions decoded from the Instruction Buffer Unit (I Unit) <b>106</b> and for controlling and monitoring program flow.</p><p>FIG. 3 illustrates the P Unit <b>108</b>, A Unit <b>110</b> and D Unit <b>112</b> of the processing core <b>102</b> in more detail and shows the bus structure connecting the various elements of the processing core <b>102</b>. The P Unit <b>108</b> includes, for example, loop control circuitry, GoTo/Branch control circuitry and various registers for controlling and monitoring program flow such as repeat counter registers and interrupt mask, flag or vector registers. The P Unit <b>108</b> is coupled to general purpose Data Write busses (EB, FB) <b>130</b>, <b>132</b>, Data Read busses (CB, DB) <b>134</b>, <b>136</b> and an address constant bus (KAB) <b>142</b>. Additionally, the P Unit <b>108</b> is coupled to sub-units within the A Unit <b>110</b> and D Unit <b>112</b> via various busses labeled CSR, ACB and RGD.</p><p>As illustrated in FIG. 3, in the present embodiment the A Unit <b>110</b> includes a register file <b>30</b>, a data address generation sub-unit (DAGEN) <b>32</b> and an Arithmetic and Logic Unit (ALU) <b>34</b>. The A Unit register file <b>30</b> includes various registers, among which are 16 bit pointer registers (AR<b>0</b>-AR<b>7</b>) and data registers (DR<b>0</b>-DR<b>3</b>) which may also be used for data flow as well as address generation. Additionally, the register file includes 16 bit circular buffer registers and 7 bit data page registers. As well as the general purpose busses (EB, FB, CB, DB) <b>130</b>, <b>132</b>, <b>134</b>, <b>136</b>, a data constant bus <b>140</b> and address constant bus <b>142</b> are coupled to the A Unit register file <b>30</b>. The A Unit register file <b>30</b> is coupled to the A Unit DAGEN unit <b>32</b> by unidirectional busses <b>144</b> and <b>146</b> respectively operating in opposite directions. The DAGEN unit <b>32</b> includes 16 bit X/Y registers and coefficient and stack pointer registers, for example for controlling and monitoring address generation within the processing engine <b>100</b>.</p><p>The A Unit <b>110</b> also comprises the ALU <b>34</b> which includes a shifter function as well as the functions typically associated with an ALU such as addition, subtraction, and AND, OR and XOR logical operators. The ALU <b>34</b> is also coupled to the general-purpose busses (EB, DB) <b>130</b>, <b>136</b> and an instruction constant data bus (KDB) <b>140</b>. The A Unit ALU is coupled to the P Unit <b>108</b> by a PDA bus for receiving register content from the P Unit <b>108</b> register file. The ALU <b>34</b> is also coupled to the A Unit register file <b>30</b> by busses RGA and RGB for receiving address and data register contents and by a bus RGD for forwarding address and data registers in the register file <b>30</b>.</p><p>As illustrated, the D Unit <b>112</b> includes a D Unit register file <b>36</b>, a D Unit ALU <b>38</b>, a D Unit shifter <b>40</b> and two multiply and accumulate units (MAC<b>1</b>, MAC<b>2</b>) <b>42</b> and <b>44</b>. The D Unit register file <b>36</b>, D Unit ALU <b>38</b> and D Unit shifter <b>40</b> are coupled to busses (EB, FB, CB, DB and KDB) <b>130</b>, <b>132</b>, <b>134</b>, <b>136</b> and <b>140</b>, and the MAC units <b>42</b> and <b>44</b> are coupled to the busses (CB, DB, KDB) <b>134</b>, <b>136</b>, <b>140</b> and data read bus (BB) <b>144</b>. The D Unit register file <b>36</b> includes 40-bit accumulators (AC<b>0</b>-AC<b>3</b>) and a 16-bit transition register. The D Unit <b>112</b> can also utilize the 16 bit pointer and data registers in the A Unit <b>110</b> as source or destination registers in addition to the 40-bit accumulators. The D Unit register file <b>36</b> receives data from the D Unit ALU <b>38</b> and MACs <b>1</b>&amp;<b>2</b><b>42</b>, <b>44</b> over accumulator write busses (ACW<b>0</b>, ACW<b>1</b>) <b>146</b>, <b>148</b>, and from the D Unit shifter <b>40</b> over accumulator write bus (ACW<b>1</b>) <b>148</b>. Data is read from the D Unit register file accumulators to the D Unit ALU <b>38</b>, D Unit shifter <b>40</b> and MACs <b>1</b>&amp;<b>2</b><b>42</b>, <b>44</b> over accumulator read busses (ACR<b>0</b>, ACR<b>1</b>) <b>150</b>, <b>152</b>. The D Unit ALU <b>38</b> and D Unit shifter <b>40</b> are also coupled to sub-units of the A Unit <b>108</b> via various busses labeled EFC, DRB, DR<b>2</b> and ACB.</p><p>Referring now to FIG. 4, there is illustrated an instruction buffer unit <b>106</b> comprising a 32 word instruction buffer queue (IBQ) <b>502</b>. The IBQ <b>502</b> comprises 32\u00d716 bit registers <b>504</b>, logically divided into 8 bit bytes <b>506</b>. Instructions arrive at the IBQ <b>502</b> via the 32-bit program bus (PB) <b>122</b>. The instructions are fetched in a 32-bit cycle into the location pointed to by the Local Write Program Counter (LWPC) <b>532</b>. The LWPC <b>532</b> is contained in a register located in the P Unit <b>108</b>. The P Unit <b>108</b> also includes the Local Read Program Counter (LRPC) <b>536</b> register, and the Write Program Counter (WPC) <b>530</b> and Read Program Counter (RPC) <b>534</b> registers. LRPC <b>536</b> points to the location in the IBQ <b>502</b> of the next instruction or instructions to be loaded into the instruction decoder(s) <b>512</b> and <b>514</b>. That is to say, the LRPC <b>534</b> points to the location in the IBQ <b>502</b> of the instruction currently being dispatched to the decoders <b>512</b>, <b>514</b>. The WPC points to the address in program memory of the start of the next 4 bytes of instruction code for the pipeline. For each fetch into the IBQ, the next 4 bytes from the program memory are fetched regardless of instruction boundaries. The RPC <b>534</b> points to the address in program memory of the instruction currently being dispatched to the decoder(s) <b>512</b> and <b>514</b>.</p><p>The instructions are formed into a 48-bit word and are loaded into the instruction decoders <b>512</b>, <b>514</b> over a 48-bit bus <b>516</b> via multiplexors <b>520</b> and <b>521</b>. It will be apparent to a person of ordinary skill in the art that the instructions may be formed into words comprising other than 48-bits, and that the present invention is not limited to the specific embodiment described above.</p><p>The bus <b>516</b> can load a maximum of two instructions, one per decoder, during any one instruction cycle. The combination of instructions may be in any combination of formats, 8, 16, 24, 32, 40 and 48 bits, which will fit across the 48-bit bus. Decoder <b>1</b>, <b>512</b>, is loaded in preference to decoder <b>2</b>, <b>514</b>, if only one instruction can be loaded during a cycle. The respective instructions are then forwarded on to the respective function units in order to execute them and to access the data for which the instruction or operation is to be performed. Prior to being passed to the instruction decoders, the instructions are aligned on byte boundaries. The alignment is done based on the format derived for the previous instruction during decoding thereof. The multiplexing associated with the alignment of instructions with byte boundaries is performed in multiplexors <b>520</b> and <b>521</b>.</p><p>The processor core <b>102</b> executes instructions through a 7 stage pipeline, the respective stages of which will now be described with reference to FIG. <b>5</b>.</p><p>The first stage of the pipeline is a PRE-FETCH (P<b>0</b>) stage <b>202</b>, during which stage a next program memory location is addressed by asserting an address on the address bus (PAB) <b>118</b> of a memory interface, or memory management unit <b>104</b>.</p><p>In the next stage, FETCH (P<b>1</b>) stage <b>204</b>, the program memory is read and the I Unit <b>106</b> is filled via the PB bus <b>122</b> from the memory management unit <b>104</b>.</p><p>The PRE-FETCH and FETCH stages are separate from the rest of the pipeline stages in that the pipeline can be interrupted during the PRE-FETCH and FETCH stages to break the sequential program flow and point to other instructions in the program memory, for example for a Branch instruction.</p><p>The next instruction in the instruction buffer is then dispatched to the decoder/s <b>512</b>/<b>514</b> in the third stage, DECODE (P<b>2</b>) <b>206</b>, where the instruction is decoded and dispatched to the execution unit for executing that instruction, for example to the P Unit <b>108</b>, the A Unit <b>110</b> or the D Unit <b>112</b>. The decode stage <b>206</b> includes decoding at least part of an instruction including a first part indicating the class of the instruction, a second part indicating the format of the instruction and a third part indicating an addressing mode for the instruction.</p><p>The next stage is an ADDRESS (P<b>3</b>) stage <b>208</b>, in which the address of the data to be used in the instruction is computed, or a new program address is computed should the instruction require a program branch or jump. Respective computations take place in the A Unit <b>110</b> or the P Unit <b>108</b> respectively.</p><p>In an ACCESS (P<b>4</b>) stage <b>210</b> the address of a read operand is output and the memory operand, the address of which has been generated in a DAGEN X operator with an Xmem indirect addressing mode, is then READ from indirectly addressed X memory (Xmem).</p><p>The next stage of the pipeline is the READ (P<b>5</b>) stage <b>212</b> in which a memory operand, the address of which has been generated in a DAGEN Y operator with an Ymem indirect addressing mode or in a DAGEN C operator with coefficient address mode, is READ. The address of the memory location to which the result of the instruction is to be written is output.</p><p>In the case of dual access, read operands can also be generated in the Y path, and write operands in the X path.</p><p>Finally, there is an execution EXEC (P<b>6</b>) stage <b>214</b> in which the instruction is executed in either the A Unit <b>110</b> or the D Unit <b>112</b>. The result is then stored in a data register or accumulator, or written to memory for Read/Modify/Write or store instructions. Additionally, shift operations are performed on data in accumulators during the EXEC stage.</p><p>The basic principle of operation for a pipeline processor will now be described with reference to FIG. <b>6</b>. As can be seen from FIG. 6, for a first instruction <b>302</b>, the successive pipeline stages take place over time periods T<sub>1</sub>-T<sub>7</sub>. Each time period is a clock cycle for the processor machine clock. A second instruction <b>304</b>, can enter the pipeline in period T<sub>2</sub>, since the previous instruction has now moved on to the next pipeline stage. For instruction <b>3</b>, <b>306</b>, the PRE-FETCH stage <b>202</b> occurs in time period T<sub>3</sub>. As can be seen from FIG. <b>6</b>. for a seven stage pipeline a total of 7 instructions may be processed simultaneously. For all 7 instructions <b>302</b>, <b>304</b>, <b>306</b>, <b>308</b>, <b>310</b>, <b>312</b>, <b>314</b>, FIG. 6 shows them all under process in time period T<sub>7</sub>. Such a structure adds a form of parallelism to the processing of instructions. As shown in FIG. 7, the present embodiment of the invention includes a memory management unit <b>104</b> which is coupled to external memory units (not shown) via a 24 bit address bus <b>114</b> and a bi-directional 16 bit data bus <b>116</b>. Additionally, the memory management unit <b>104</b> is coupled to program storage memory (not shown) via a 24 bit address bus <b>118</b> and a 32 bit bi-directional data bus <b>120</b>. The memory management unit <b>104</b> is also coupled to the I Unit <b>106</b> of the machine processor core <b>102</b> via a 32 bit program read bus (PB) <b>122</b>. The P Unit <b>108</b>, A Unit <b>110</b> and D Unit <b>112</b> are coupled to the memory management unit <b>104</b> via data read and data write busses and corresponding address busses. The P Unit <b>108</b> is further coupled to a program address bus <b>128</b>.</p><p>More particularly, the P Unit <b>108</b> is coupled to the memory management unit <b>104</b> by a 24 bit program address bus <b>128</b>, the two 16 bit data write busses (EB, FB) <b>130</b>, <b>132</b>, and the two 16 bit data read busses (CB, DB) <b>134</b>, <b>136</b>. The A Unit <b>110</b> is coupled to the memory management unit <b>104</b> via two 24 bit data write address busses (EAB, FAB) <b>160</b>, <b>162</b>, the two 16 bit data write busses (EB, FB) <b>130</b>, <b>132</b>, the three data read address busses (BAB, CAB, DAB) <b>164</b>, <b>166</b>, <b>168</b> and the two 16 bit data read busses (CB, DB) <b>134</b>, <b>136</b>. The D Unit <b>112</b> is coupled to the memory management unit <b>104</b> via the two data write busses (EB, FB) <b>130</b>, <b>132</b> and three data read busses (BB, CB, DB) <b>144</b>, <b>134</b>, <b>136</b>.</p><p>FIG. 7 represents the passing of instructions from the I Unit <b>106</b> to the P Unit <b>108</b> at <b>124</b>, for forwarding branch instructions for example. Additionally, FIG. 7 represents the passing of data from the I Unit <b>106</b> to the A Unit <b>110</b> and the D Unit <b>112</b> at <b>126</b> and <b>128</b> respectively.</p><p>In a particular embodiment of the invention, the processing engine <b>100</b> is responsive to machine instructions in a number of formats. Examples of such instructions in different formats are illustrated in the following.</p><h4>8 Bit instruction: OOOO OOOO</h4><p>This represents an eight bit instruction, for example a memory map qualifier (MMAP( )) or a read port qualifier (readport( )). Such a qualifier comprises merely an eight bit opcode (OOOO OOOO). In such a case parallelism is implicit.</p><h4>16 Bit Instruction: OOOO OOOE FSSS FDDD</h4><p>This represents an example of a sixteen bit instruction, for example an instruction where the content of a destination register (e.g., dst) becomes the sum of the prior content of that register (dst) and the content of a source register (src), that is:</p><p><maths><formula-text><i>dst=dst+src</i></formula-text></maths></p><p>Such an instruction comprises a seven bit opcode (OOOO OOO) with a one bit parallel enable field (E), a four bit source register identifier (FSSS) and a four bit destination register identifier (FDDD).</p><h4>16 Bit Instruction: OOOO FDDD PPPM MMMI</h4><p>This represents another example of a sixteen bit instruction, for example where the content of a destination register (e.g., dst) becomes the content of a memory location (Smem), that is:</p><p><maths><formula-text><i>dst=Smem</i></formula-text></maths></p><p>Such an instruction comprises a four bit opcode (OOOO), a four bit destination register identifier (FDDD), a three bit pointer address (PPP), a four bit address modifier (MMMM) and a direct/indirect address indicator (I).</p><h4>24 Bit Instruction: OOOO OOOE LLLL LLLL oCCC CCCC</h4><p>This represents an example of a twenty four bit instruction, for example a conditional instruction for a branch to and offset (L<b>8</b>) where a condition is met, that is:</p><p><maths><formula-text>if(cond) goto L<b>8</b></formula-text></maths></p><p>Such an instruction comprises a seven bit opcode (OOOO OOO) with a one bit parallel enable field (E), an eight bit branch offset (LLLL LLLL), a one bit opcode extension (o) and a seven bit condition field (CCC CCCC).</p><h4>24 Bit Instruction: OOOO OOOO PPPM MMMI SSDD ooU%</h4><p>This is another example of a twenty-four bit instruction, for example a single memory operand instruction where the content of an accumulator (AC<sub>y</sub>) becomes the result of rounding the sum of the content of another accumulator (AC<sub>x</sub>) and the square of the content of a memory location (with optional rounding), and optionally the content of a data register (DR<b>3</b>) can become the content of the memory location, that is:</p><p><maths><formula-text><i>AC</i><sub>y</sub><i>=rnd</i>(<i>AC</i><sub>x</sub><i>*Smem*Smem</i>),<i>DR</i><b>3</b>=<i>Smem</i></formula-text></maths></p><p>Such an instruction comprises an eight bit opcode (OOOO OOOO), a three bit pointer address (PPP), a four bit address modifier (MMMM), a one bit direct/indirect address indicator field (I), a two bit source accumulator identifier (SS), a two bit destination accumulator identifier (DD), a two bit opcode extension (oo), an update condition field (u), and a one bit rounding option field (%).</p><h4>32 Bit Instruction: OOOO OOOO PPPM MMMI KKKK KKKK KKKK KKKK</h4><p>This is an example of a thirty-two bit instruction, for example an instruction where the content of a test register (TC<b>1</b>) is set to 1 or 0 depending on the sign comparison of a memory location (Smem) to a constant value (K<b>16</b>), that is:</p><p><maths><formula-text><i>TC</i><b>1</b>=(<i>Smem==K</i><b>16</b>)</formula-text></maths></p><p>Such an instruction comprises an eight bit opcode (OOOO OOOO), a three bit pointer address (PPP), a four bit address modifier (MMMM), a one bit direct/indirect address indicator field (I) and a sixteen bit constant field (KKKK KKKK KKKK KKKK).</p><h4>Hard Dual Instruction: OOOO OOOO XXXM MMYY YMMM SSDD ooox ssU%</h4><p>This is an example of a 32 bit dual access instruction, which could be termed a \u201chard dual access instruction\u201d, or a hard programmed dual memory instruction, that is a dual instruction which has been programmed as such, for example, by a programmer. Such an instruction requires two DAGEN operators. A second instruction can be executed in parallel. This is typically a register or control instruction. Memory stack instructions can also be executed in parallel as long as there are no bus conflicts. An example of such an instruction is:</p><p><maths><formula-text><i>C</i><sub>y</sub><i>=rnd</i>(<i>DR</i><sub>x</sub><i>*Xmem</i>),</formula-text></maths></p><p><maths><formula-text><i>Ymem=HI</i>(<i>AC</i><sub>x</sub><i>&lt;&lt;DR</i><b>2</b>)</formula-text></maths></p><p><maths><formula-text><i>DR</i><b>3</b>=<i>Xmem</i></formula-text></maths></p><p>This instruction comprises an eight bit opcode (OOOO OOOO), a three bit Xmem pointer address (XXX) with a four bit address modifier (MMMM), a three bit Ymem pointer address (YYY) with a four bit address modifier (MMMM), a two bit source accumulator (AC<sub>x</sub>) identifier (SS), a two bit destination accumulator (AC<sub>y</sub>) identifier (DD), a three bit opcode extension (ooo), a don't care bit (x), a two bit source accumulator identifier (ss), a one bit optional DR<b>3</b> update field (U) and a one bit optional rounding field (%).</p><p>FIG. 8 is a table illustrating combinations of instructions forming instruction pairs and also a soft dual instruction. In such instruction pairs, the first instruction of the pair is always a memory operation. It will be noted that where the second instruction is also a memory instruction, then this is configured as a soft dual instruction, that is a compound instruction.</p><p>Instructions which may be located in a second position of an instruction pair (i.e. for the higher program address of the pair) include a parallel enable field (E bit) to indicate whether the instruction can be performed in parallel with the first of a pair of instructions. The parallel enable bit is located at a predetermined offset from the instruction format boundary between the instructions. The decoder is arranged to be responsive to the \u2018E\u2019 bit in order to control instruction execution.</p><p>The reason for having a memory operation first in an instruction pair is that at the entry to the address decode stage of the processor pipeline, the decoder does not know the format of the instruction, or even where the format boundary is located. Memory address decoding is one of the critical stages of the pipeline to ensure good instruction throughput. Accordingly, it is necessary to be able reliably to know the location and size of the address bits for a memory instruction to be decoded in order that the decoding can commence even before the exact nature of the instruction is determined.</p><p>A further advantage which results from constraining a memory instruction to be located as the first instruction in an instruction pair is that it is then not necessary for a memory instruction to include a field indicating whether parallel operation is permitted. This makes the instruction set more efficient and allows improved code size.</p><p>Yet a further advantage is that the hardware necessary for decoding a second instruction of an instruction pair need only be a subset of the hardware for decoding the first instruction of the instruction pair. The first instruction is the instruction of the instruction pair with a lower program address than the second instruction of the instruction pair. Thus, the decode hardware for the instruction with a higher program address of an instruction pair can be a subset of the decode hardware for the instruction with a lower program address of an instruction pair. This enables a reduction in the silicon area and power consumption required for implementing and operating the decode hardware.</p><p>Where two instructions of an instruction pair can be performed in parallel, this takes place in respective decoding and execution stages. However, due to physical bus timing constraints, bus transfers can be staggered.</p><p>FIG. 9 illustrates the pipeline stage in which memory access takes place for different types of instructions, including dual instructions. It should be noted, as for FIG. 4, that the pipeline stages shown are for illustrative purposes only. In practice, the prefetch and fetch stages form a flow separate from that of the remaining stages.</p><p>Comparing FIG. 9 with FIG. 5, P<b>1</b> represents the fetch stage, P<b>2</b> the decode stage, P<b>3</b> the address computation stage, P<b>4</b> the access stage, P<b>5</b> the read stage and P<b>6</b> the execute stage. B represents a coefficient read access from a register via the B bus. C and D represent memory read accesses via the C and D busses respectively. E and F represent write accesses via the E and F busses respectively. In order that the read and write accesses can be performed at the required cycles without causing a bubble (or stall) on the pipeline, decoding is performed as early as possible.</p><p>FIG. 10 illustrates a particular form of dual memory access instruction. It is effectively formed from two merged programmed instructions which have implied parallelism. The dual memory instruction of FIG. 10 is termed a soft dual instruction, or also a compound instruction herein. It is formed by combining two programmed single memory access instructions in an instruction preprocessor, for example in a compiler or an assembler. In other words, this compound instruction is not programmed, or pre-programmed, as a dual instruction by a programmer. This provision of this form of compound instruction enables improved memory access performance by permitting parallel operation, with both instructions being executed in the same cycle. In a particular example described in the following, the soft dual instruction is restricted to indirect addressing with dual modifier options. As a result, it is possible to encode the soft dual instruction to achieve increased performance through parallel operation with no size penalty in respect of the combined instruction size.</p><p>The soft dual instruction is qualified by a five bit tag field <b>701</b>, with individual following instruction fields organized as illustrated in FIG. <b>10</b>. The size of the tag field results from constraints relating to the particular implementation, namely:</p><p>that the total encoding format is constrained not be greater than the sum of the encoding formats of the two constituent programmed instructions;</p><p>that the total instruction format size is a multiple of <b>8</b>; and</p><p>the availability of opcodes with respect to other single instructions.</p><p>Following the tag field <b>701</b> are:</p><p>part <b>702</b> of the operation code field for a first instruction;</p><p>a compound address field <b>703</b>/<b>704</b> including an indirect memory address (XXXMMM) <b>703</b> for the first instruction and an indirect memory address (YYYMMM) <b>704</b> for a second instruction;</p><p>the remainder of the operation code field <b>705</b> for the first instruction;</p><p>a data flow field <b>706</b> for the first instruction;</p><p>an operation code field <b>707</b> for the operation code of the second instruction; and</p><p>a data flow field <b>708</b> for the second instruction.</p><p>It can be seen, therefore, that the combined address portion for the soft dual instruction is held at the same location in the soft dual instruction as for any other dual instruction. This provides the advantage of rapid address decoding as a result of being able to commence address decoding without knowledge of the instruction type involved. It will be seen that in order to achieve this, some reorganization of the bits in the soft dual instruction is necessary, for example as described above.</p><p>In addition to the modifications described above, where two programmed instructions each comprise a data address generation (DAGEN) field, these could be combined to form a combined DAGEN field in the soft dual instruction. The provision of a combined DAGEN field can facilitate and speed subsequent execution of the soft dual instruction.</p><p>FIG. 11 illustrates various steps in transforming two independent instructions into a soft dual instruction.</p><p>Two independent instructions <b>721</b> and <b>722</b> are represented at stage <b>720</b>.</p><p>As shown at <b>723</b>, a first 24 bit instruction <b>721</b> includes an eight bit operation code <b>724</b> in the first byte, a single memory (Smem) address <b>725</b> in the next byte and data flow bits <b>726</b> in the next byte. A second 24 bit instruction <b>722</b> includes an eight bit operation code <b>727</b> in the first byte, a single memory address <b>728</b> in the next byte and data flow bits <b>729</b> in the next byte. At <b>730</b>, the eight operation code bits are each labeled \u2018O\u2019 in the operation code bytes <b>724</b> and <b>727</b> of each of the instructions. The single memory addresses <b>725</b> and <b>728</b> are each shown to comprise 7 address bits \u2018A\u2019 plus an indirect/direct indicator bit \u2018I\u2019. This is because addresses for the standard memory accesses can be either direct or indirect. In the example shown, the granularity is based on bytes. However, in other examples a granularity based on other than 8 bits may be employed.</p><p>At stage <b>735</b>, the operation code <b>724</b> of the first instruction is split into two parts. Only seven of the eight bits of the operation code <b>724</b> need to be considered. This is as a result of memory code mapping which can ensure that this is redundant in the case of a soft dual instruction. (e.g., by ensuring that all memory instructions have operation codes within a determined range, for example, 80-FF in hexadecimal notation, for a soft dual instruction). As can be seen later in stages <b>736</b> and <b>740</b>, and also in FIG. 10 the operation code for the first instruction is split. Three bits of the operation code for the first instruction are placed between a. soft dual instruction tag <b>737</b> and the combined addresses <b>738</b> for the first and second instructions and four bits are placed after the combined addresses <b>738</b>.</p><p>At stage <b>736</b>, the insertion of a soft dual instruction tag <b>737</b> is shown. This as a tag which can be interpreted by the decoder as representing a soft dual instruction. Also shown is the merging of the single memory fields <b>725</b> and <b>728</b>. This can be achieved because all soft dual instructions are restricted to indirect addresses, whereby an indirect/direct flag is not needed. The indirect addresses are indicated by a three bit base address XXX or YYY, for the first and second instructions, respectively, and a three bit modifier (MMM). Stage <b>736</b> further illustrates the moving of the data flow for the first instruction to the first byte position of the second instruction, with the operation code for the second instruction being moved to the second byte position of that instruction.</p><p>As a result, the format of the soft dual instruction represented in FIG. 10 is achieved. It is to be noted that there is no code size penalty for a soft dual instruction versus two single memory access instructions. By replacing two single memory (Smem) instructions by an Xmem, Ymem, enough bits are freed up to insert the \u2018soft dual\u2019 tag <b>701</b>/<b>737</b>. The soft dual tag by itself allows the decoder to detect that it should decode the pair of instructions as memory instructions. Instruction set mapping can be used to ensure that memory instructions are encoded within a window 80-FF, whereby the most significant bit (bit 7) of the first operation code <b>724</b> can be discarded when effecting the dual field encoding.</p><p>In the example shown, the various stages illustrated in FIG. 11 are performed by an instruction preprocessor, for example a compiler or an assembler, when preparing instructions for execution. The steps performed by the instruction preprocessor are represented in a flow diagram shown in FIG. <b>12</b>.</p><p>In step S<b>1</b>, the instruction preprocessor detects the presence of two instructions which might potentially be combined into a soft dual instruction. In order for this to be possible, the instructions will need to be such that they may be performed in parallel and do not result in data or control flow anomalies. Each instruction within the instruction set is qualified by DAGEN variables in a DAGEN tag, which define the address generator resources and the type of memory access involved to support the instruction.</p><p>Accordingly, in step S<b>2</b>, the instruction preprocessor performs a first step in determining the feasibility of merging two standalone memory instructions into a soft dual instruction by analyzing the DAGEN variables. Assuming this checks out, then the instruction preprocessor is operable to analyze potential bus and operator conflicts and to establish whether there is a potential bar to the combining of the first and second instructions.</p><p>In step S<b>3</b>, the instruction preprocessor then applies the soft dual instruction tag <b>737</b> and modifies the operation codes and address indications, as well as the field positions as illustrated in FIG. <b>11</b>.</p><p>In step S<b>4</b>, the soft dual instruction is output by the instruction preprocessor.</p><p>FIG. 13 is a schematic block diagram illustrating the decoding process for a soft dual instruction. FIG. 13 illustrates the decoding of a 48 bit instruction word <b>800</b> from the instruction buffer unit <b>106</b>.</p><p>From the operation code (opcode), which is located at the left of the instruction word as shown in FIG. 13, logic <b>802</b>, <b>804</b> in the opcode decoding circuitry is able rapidly to detect whether a built in dual or soft dual instruction is to be decoded. The detection of a soft dual tag by tag decoding logic <b>804</b> controls a multiplexor <b>808</b> to select either an \u201cE\u201d bit or the soft dual opcodes to be passed from format logic <b>806</b> to instruction #<b>2</b> alignment and. remapping logic <b>818</b>. Single addressing logic <b>810</b> and dual addressing logic <b>812</b> are operable. in parallel to commence decoding of the address fields, which are always located at a determined offset from the left hand end of the instruction. Outputs of dual decoding logic <b>802</b> and soft dual tag field decoding logic <b>804</b> are combined by logic <b>814</b> and form a control input to a multiplexor <b>816</b>. Thus, when a dual instruction is detected, the output of dual addressing logic <b>812</b> is passed to the DAGEN control, otherwise the output of single addressing logic <b>810</b> is passed to DAGEN control.</p><p>As mentioned above, in an alternative form, a compound instruction can comprise a combined DAGEN code field replacing the separate DAGEN codes of the pair of instructions forming the compound instruction. A DAGEN tag in the compound instruction could identify the presence of the combined DAGEN code field, with the decoder being configured to be responsive to the DAGEN tag to decode the combined DAGEN code field. The combined DAGEN code field could form part of the combined address field. The provision of a combined DAGEN field can provide advantages in execution speed.</p><p>If the instruction is a soft dual instruction, then remapping is necessary before decoding can be performed. Accordingly, instruction field remapping logic <b>824</b> is responsive to the output of the soft dual tag decoding logic <b>804</b> to cause the remapping of the information relating to the first instruction of the pair before passing the remapped operation information to decode logic <b>826</b> for the first instruction. Similarly, instruction alignment and remapping logic <b>818</b> for a second instruction of the instruction pair is responsive to the output of the soft dual tag decoding logic <b>804</b> to cause remapping of the information relating to the second memory instruction prior to passing the information to the decode logic <b>822</b> for the second instruction. The instruction alignment and field remapping logic <b>818</b> is also operable to realign the second instruction dependent upon the format of the first instruction, according to the instruction boundary at bit <b>16</b>, bit <b>24</b>, bit <b>32</b> or bit <b>40</b>, as appropriate.</p><p>With reference to FIGS. 10 and 13, it can be seen that the decode mechanism shown in FIG. 13 is configured to decode instructions from the instruction buffer. The decode mechanism is responsive to a predetermined tag in a tag field of a soft dual instruction as shown in FIG. 10 to decode a first memory addresses for a first memory instruction and a second memory address for a second memory instruction from a compound address field in the predetermined soft dual instruction.</p><p>Parallel enable bit decoding logic <b>820</b> is operable to validate whether the second instruction may be decoded and executed in parallel with the first instruction. As a soft dual instruction does not include a parallel enable (\u201cE\u201d) bit, this logic <b>820</b> is disabled when a soft dual instruction is detected.</p><p>FIG. 14 is a schematic block diagram illustrating aspects of the memory bus interfacing for a soft dual instruction, and FIG. 15 is a table summarizing the operand fetch control for a soft dual instruction.</p><p>FIG. 14 illustrates the C bus <b>750</b>, the D bus <b>752</b>, the E bus <b>760</b> and the F bus <b>762</b>, which busses were referenced earlier, but were not individually identified.</p><p>A soft dual fetch controller <b>754</b> forms part of the instruction control functions of the processor core <b>102</b>. This is operable to control operand fetch mechanisms <b>756</b> and <b>782</b> to fetch X and Y operands <b>758</b> and <b>780</b> for a first data flow path <b>790</b>, and X and Y operands <b>784</b> and <b>786</b> for a second data flow path <b>792</b>, respectively, via the C and D busses <b>750</b> and <b>752</b>. A soft dual write controller <b>755</b>, which also forms part of the instruction control functions of the processor core <b>102</b>, is operable to control memory write interfaces <b>794</b> and <b>796</b> to control the writing of operands from the first data flow path <b>790</b> and the second data flow path <b>792</b>, respectively to the E and F busses <b>760</b> and <b>762</b>.</p><p>The table which forms FIG. 15 illustrates the open and fetch control operations performed by the soft dual fetch controller <b>754</b>. This illustrates the changes to the operand fetch flow for a soft dual memory instruction compared to a single memory instruction performed standalone. Thus, when a single memory instruction is executed standalone, the operand register is loaded from the D bus, whereby the memory request is a D request, thereby requiring two cycles. However, when a soft dual instruction is executed, the fetch controller changes the operand fetch flow for the Ymem path, such that the request is re-directed to a C request and the operand is fetched from the C bus instead of the D bus as indicated at <b>1500</b>. Advantageously, operand #<b>1</b> and operand #<b>2</b> are fetched in parallel in the same cycle. The same mechanism applies to the write interface. For example, an E bus request can be redirected to an F bus request.</p><p>FIG. 16 is a schematic representation of an integrated circuit <b>40</b> incorporating the processor <b>10</b> of FIG. <b>1</b>. The integrated circuit can be implemented using application specific integrated circuit (ASIC) technology. As shown, the integrated circuit includes a plurality of contacts <b>42</b> for surface mounting. However, the integrated circuit could include other configurations, for example a plurality of pins on a lower surface of the circuit for mounting in a zero insertion force socket, or indeed any other suitable configuration.</p><p>One application for a processing engine such as the processor <b>10</b>, for example as incorporated in an integrated circuit as in FIG. 16, is in a telecommunications device, for example a mobile wireless telecommunications device. FIG. 17 illustrates one example of such telecommunications device. In the specific example illustrated in FIG. 17, the telecommunications device is a mobile telephone <b>11</b> with integrated user input device such as a keypad, or keyboard <b>12</b> and a display <b>14</b> in housing <b>15</b>. The display could be implemented using appropriate technology, as, for example, a liquid crystal display or a TFT display. The processor <b>10</b> is connected to the keypad <b>12</b>, where appropriate via a keyboard adapter (not shown), to the display <b>14</b>, where appropriate via a display adapter (not shown), and to a telecommunications interface or transceiver <b>16</b>, for example a wireless telecommunications interface including radio frequency (RF) circuitry. The radio frequency circuitry could be incorporated into, or separate from, an integrated circuit <b>40</b> comprising the processor <b>10</b>. The RF circuitry <b>16</b> is connected to an aerial <b>18</b>.</p><p>Thus, there has been described a processing engine which provides for execution of soft encoded dual memory access instructions. The soft dual instruction mechanism enables execution of two memory access instructions in parallel with high encoding efficiency. Due to increased parallelism, power consumption can be reduced. Also, a decoder for a second instruction can be a subset of the decoder for a first instruction resulting in efficient use of silicon real estate and providing further opportunities for a reduction in power consumption.</p><p>As used herein, the terms \u201capplied,\u201d \u201cconnected,\u201d and \u201cconnection\u201d mean electrically connected, including where additional elements may be in the electrical connection path.</p><p>While the invention has been described with reference to illustrative embodiments, this description is not intended to be construed in a limiting sense. Various other embodiments of the invention will be apparent to persons skilled in the art upon reference to this description. It is therefore contemplated that the appended claims will cover any such modifications of the embodiments as fall within the true scope and spirit of the invention.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Karim", "last_name": "Djafarian", "name": ""}, {"first_name": "Gilbert", "last_name": "Laurenti", "name": ""}, {"first_name": "Herve", "last_name": "Catan", "name": ""}, {"first_name": "Vincent", "last_name": "Gillet", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "TEXAS INSTRUMENTS INCORPORATED"}, {"first_name": "", "last_name": "TEXAS INSTRUMENTS INCORPORATED", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F   9/30"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   5/01        20060101A I20051008RMEP"}, {"label": "G06F   7/60        20060101A I20051008RMEP"}, {"label": "G06F   9/308       20060101A I20051008RMEP"}, {"label": "G06F   9/318       20060101A I20051008RMEP"}, {"label": "G06F   7/76        20060101A I20051008RMEP"}, {"label": "G06F   9/32        20060101A I20051008RMEP"}, {"label": "H04M   1/73        20060101A N20051008RMEP"}, {"label": "G06F   7/74        20060101A I20051008RMEP"}, {"label": "G06F   9/315       20060101A I20051008RMEP"}, {"label": "G06F   9/355       20060101A I20051008RMEP"}, {"label": "G06F   9/45        20060101A I20051008RMEP"}, {"label": "G06F   9/38        20060101A I20051008RMEP"}, {"label": "G06F   9/30        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "712208"}, {"primary": false, "label": "712E09072"}, {"primary": false, "label": "712E0903"}, {"primary": false, "label": "712E09043"}, {"primary": false, "label": "712E09067"}, {"primary": false, "label": "712E09074"}, {"primary": false, "label": "712E09035"}, {"primary": false, "label": "712E09071"}, {"primary": false, "label": "712E09049"}, {"primary": false, "label": "712E09054"}, {"primary": false, "label": "712204"}, {"primary": false, "label": "712E09078"}, {"primary": false, "label": "712E09034"}, {"primary": false, "label": "712205"}, {"primary": false, "label": "712E09032"}, {"primary": false, "label": "712E09019"}, {"primary": false, "label": "712E09027"}, {"primary": false, "label": "712E09073"}, {"primary": false, "label": "712E09029"}, {"primary": false, "label": "712E09062"}], "ecla_classes": [{"label": "G06F   9/38T6C"}, {"label": "G06F   9/30X"}, {"label": "G06F   9/38E1"}, {"label": "G06F   9/38C4"}, {"label": "G06F   9/30A1B"}, {"label": "G06F   9/30A1M"}, {"label": "G06F   9/355B"}, {"label": "G06F   9/38E1R"}, {"label": "G06F   9/38S1"}, {"label": "G06F   9/30T4T"}, {"label": "G06F   9/30R5D"}, {"label": "G06F   9/30R5S"}, {"label": "T05K999:99"}, {"label": "G06F   9/30T2"}, {"label": "G06F   9/38E"}, {"label": "G06F   9/32A"}, {"label": "G06F   9/30T4"}, {"label": "G06F   9/38T"}, {"label": "G06F   7/76L"}, {"label": "G06F   9/32"}, {"label": "G06F   9/38P"}, {"label": "G06F   5/01"}, {"label": "G06F   7/74"}, {"label": "G06F   7/60P"}, {"label": "G06F   7/76M"}, {"label": "G06F   9/38E6"}], "cpc_classes": [{"label": "G06F   9/30134"}, {"label": "G06F   9/30149"}, {"label": "G06F   9/3885"}, {"label": "G06F   9/3822"}, {"label": "G06F   7/74"}, {"label": "G06F   9/3838"}, {"label": "G06F   9/30134"}, {"label": "G06F   9/3885"}, {"label": "G06F   9/30167"}, {"label": "G06F   7/607"}, {"label": "G06F   9/3016"}, {"label": "G06F   9/30181"}, {"label": "G06F   7/74"}, {"label": "G06F   9/3838"}, {"label": "G06F   7/607"}, {"label": "G06F   7/764"}, {"label": "G06F   9/3016"}, {"label": "G06F   9/32"}, {"label": "G06F   9/384"}, {"label": "G06F   9/30032"}, {"label": "G06F   9/321"}, {"label": "G06F   9/30018"}, {"label": "G06F   9/32"}, {"label": "G06F   5/01"}, {"label": "G06F   9/3879"}, {"label": "G06F   9/30149"}, {"label": "G06F   9/3013"}, {"label": "G06F   9/3891"}, {"label": "G06F   9/321"}, {"label": "G06F   9/384"}, {"label": "G06F   7/762"}, {"label": "G06F   9/30181"}, {"label": "G06F   9/3867"}, {"label": "G06F   9/3853"}, {"label": "G06F   9/3836"}, {"label": "G06F   7/762"}, {"label": "G06F   9/30032"}, {"label": "G06F   9/3552"}, {"label": "G06F   9/3853"}, {"label": "G06F   7/764"}, {"label": "G06F   9/3836"}, {"label": "G06F   9/30167"}, {"label": "G06F   9/3822"}, {"label": "G06F   9/3891"}, {"label": "G06F   9/3552"}, {"label": "G06F   9/3867"}, {"label": "G06F   9/30018"}, {"label": "G06F   9/3879"}, {"label": "G06F   9/3013"}, {"label": "G06F   5/01"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1998-10-06", "application_date": "1999-10-01", "family_members": [{"ucid": "US-6681319-B1", "titles": [{"lang": "EN", "text": "Dual access instruction and compound memory access instruction with compatible address fields"}]}]}