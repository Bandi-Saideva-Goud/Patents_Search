{"patent_number": "US-6460117-B1", "publication_id": 73154158, "family_id": 23328879, "publication_date": "2002-10-01", "titles": [{"lang": "EN", "text": "Set-associative cache memory having a mechanism for migrating a most recently used set"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50386844\"><p>A set-associative cache memory having a mechanism for migrating a most recently used set is disclosed. The cache memory has multiple congruence classes of cache lines. Each congruence class includes a number of sets organized in a set-associative manner. The cache memory further includes a migration means for directing the information from a cache \u201chit\u201d to a predetermined set of the cache memory.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00001\" num=\"1\"><claim-text>1. A cache memory, comprising:</claim-text><claim-text>a plurality of congruence classes of cache lines, wherein each of said congruence classes includes a plurality of sets organized in a set-associative manner, wherein each of said plurality of sets within said cache memory has an access time dependent on a relative location of each of said plurality of sets to an output of said cache memory; and </claim-text><claim-text>a migration means for directing information from a cache match to a predetermined one of said plurality of sets of said cache memory. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00002\" num=\"2\"><claim-text>2. The cache memory according to <claim-ref idref=\"US-6460117-B1-CLM-00001\">claim 1</claim-ref>, wherein said predetermined set is closest to said output of said cache memory.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00003\" num=\"3\"><claim-text>3. The cache memory according to <claim-ref idref=\"US-6460117-B1-CLM-00002\">claim 2</claim-ref>, wherein said migration means places information from a set with said cache match to said predetermined set closest to said output of said cache memory.</claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00004\" num=\"4\"><claim-text>4. The cache memory according to <claim-ref idref=\"US-6460117-B1-CLM-00003\">claim 3</claim-ref>, wherein said migration means places information from said set with said cache match to said predetermined set closest to said output of said cache memory after said migration means migrates information from said predetermined set closest to said output of said cache memory to an immediate adjacent set.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00005\" num=\"5\"><claim-text>5. The cache memory according to <claim-ref idref=\"US-6460117-B1-CLM-00001\">claim 1</claim-ref>, wherein said cache memory further includes fanout wiring incrementally distributed between an address decoder and each of said plurality of sets.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00006\" num=\"6\"><claim-text>6. The cache memory according to <claim-ref idref=\"US-6460117-B1-CLM-00001\">claim 1</claim-ref>, wherein said cache memory further includes output wiring incrementally distributed between each of said plurality of sets and an output.</claim-text></claim>"}, {"num": 7, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00007\" num=\"7\"><claim-text>7. The cache memory according to <claim-ref idref=\"US-6460117-B1-CLM-00001\">claim 1</claim-ref>, wherein said cache memory is a primary cache.</claim-text></claim>"}, {"num": 8, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00008\" num=\"8\"><claim-text>8. The cache memory according to <claim-ref idref=\"US-6460117-B1-CLM-00001\">claim 1</claim-ref>, wherein said cache memory is a secondary cache.</claim-text></claim>"}, {"num": 9, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00009\" num=\"9\"><claim-text>9. A processor, comprising:</claim-text><claim-text>a central processing unit; and </claim-text><claim-text>a cache memory coupled to said central processing unit, wherein said cache memory includes: </claim-text><claim-text>a plurality of congruence classes of cache lines, wherein each of said congruence classes includes a plurality of sets organized in a set-associative manner, wherein each of said plurality of sets within said cache memory has an access time dependent on a relative location of each of said plurality of sets to an output of said cache memory; and </claim-text><claim-text>a migration means for directing information from a cache match to a predetermined one of said plurality of sets of said cache memory. </claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00010\" num=\"10\"><claim-text>10. The processor according to <claim-ref idref=\"US-6460117-B1-CLM-00009\">claim 9</claim-ref>, wherein said predetermined set is closest to said output of said cache memory.</claim-text></claim>"}, {"num": 11, "parent": 10, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00011\" num=\"11\"><claim-text>11. The processor according to <claim-ref idref=\"US-6460117-B1-CLM-00010\">claim 10</claim-ref>, wherein said migration means places information from a set with said cache match to said predetermined set closest to said output of said cache memory.</claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00012\" num=\"12\"><claim-text>12. The processor according to <claim-ref idref=\"US-6460117-B1-CLM-00011\">claim 11</claim-ref>, wherein said migration means places information from said set with said cache match to said predetermined set closest to said output of said cache memory after said migration means migrates information from said predetermined set closest to said output of said cache memory to an immediate adjacent set.</claim-text></claim>"}, {"num": 13, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00013\" num=\"13\"><claim-text>13. The processor according to <claim-ref idref=\"US-6460117-B1-CLM-00009\">claim 9</claim-ref>, wherein said cache memory further includes fanout wiring incrementally distributed between an address decoder and each of said plurality of sets.</claim-text></claim>"}, {"num": 14, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00014\" num=\"14\"><claim-text>14. The processor according to <claim-ref idref=\"US-6460117-B1-CLM-00009\">claim 9</claim-ref>, wherein said cache memory further includes output wiring incrementally distributed between each of said plurality of sets and an output.</claim-text></claim>"}, {"num": 15, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00015\" num=\"15\"><claim-text>15. The processor according to <claim-ref idref=\"US-6460117-B1-CLM-00009\">claim 9</claim-ref>, wherein said cache memory is a primary cache.</claim-text></claim>"}, {"num": 16, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6460117-B1-CLM-00016\" num=\"16\"><claim-text>16. The processor according to <claim-ref idref=\"US-6460117-B1-CLM-00009\">claim 9</claim-ref>, wherein said cache memory is a secondary cache.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53642871\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>RELATED PATENT APPLICATION</h4><p>The present patent application is related to copending application U.S. Ser. No. 09/339,410, filed on even date.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Technical Field</p><p>The present invention relates to cache memories in general, and in particular to set-associative cache memories. Still more particularly, the present invention relates to a set-associative cache memory having a mechanism for migrating a most recently used set.</p><p>2. Description of the Prior Art</p><p>In order to increase the speed of access to data stored within a main memory, modern data processing systems generally maintain the most recently used data in a high-speed memory known as a cache memory. This cache memory has multiple cache lines, with several bytes per cache line for storing information in contiguous addresses within the main memory. In addition, each cache line has an associated tag that typically identifies a partial address of a corresponding page of the main memory. Because the information within each cache line may come from different pages of the main memory, the tag provides a convenient way identify to which page of the main memory the information within a cache line belongs.</p><p>In a typical cache memory implementation, information is stored in one or several memory arrays. In addition, the corresponding tags for each cache line are stored in a structure known as a directory or tag array. Usually, an additional structure, called a translation lookaside buffer (TLB), is utilized to facilitate the translation of an effective address to a real address during a cache memory access.</p><p>In order to access a byte in a cache memory with an effective address, the mid-order bits, for example, of the effective address are utilized to select a cache line from the memory array along with a corresponding tag from the directory. The low-order bits, for example, of the effective address are then utilized to choose the indicated byte from the selected cache line. At the same time, the high-order bits, for example, of the effective address are translated via the translation lookaside buffer to determine a real page number. If the real page number obtained by this translation matches the real address tag stored within the directory, then the data read from the selected cache line is the data actually sought by a processing unit. This is commonly referred to as a cache \u201chit,\u201d meaning the requested data was found in the cache memory. If the real address tag and translated real page number do not agree, a cache \u201cmiss\u201d occurs, meaning that the requested data was not stored in the cache memory. Accordingly, the requested data have to be subsequently retrieved from the main memory or elsewhere within the memory hierarchy.</p><p>With a direct-mapped cache, only one of the group of corresponding lines from all pages in a real memory page can be stored in the cache memory at a time; but in order to achieve a better \u201chit\u201d ratio, sometimes a set-associative cache is utilized instead. For example, with an N-way set associative cache, corresponding lines from N different pages may be stored. Since all entries can be distinguished by their associated tags, it is always possible to resolve which of the N lines having the same line number contains the requested information. The resolution requires comparison of the translated real page number to the N tags associated with a given line number. Each comparison generates an input to an N-to-1 multiplexor to select an appropriate cache line from among the N possibilities. In order to achieve high parallelism and uniformity within the cache design, according to the prior art cache architecture, each set within the N-way set-associative cache is identical in size. Furthermore, the layout and wiring of the N-way set-associative cache are fashioned in such a manner that the access time to each set within the cache is identical.</p><h4>SUMMARY OF THE INVENTION</h4><p>A cache memory has multiple congruence classes of cache lines. Each congruence class includes a number of sets organized in a set-associative manner. In accordance with a preferred embodiment of the present invention, the cache memory further includes a migration means for directing information from a cache \u201chit\u201d to a predetermined set of the cache memory.</p><p>All objects, features, and advantages of the present invention will become apparent in the following detailed written description.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The invention itself, as well as a preferred mode of use, further objects, and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein:</p><p>FIG. 1 is a block diagram of a general structure of a processor in accordance with a preferred embodiment of the invention;</p><p>FIG. 2 is a detailed block diagram of the processor from FIG. 1, in accordance with a preferred embodiment of the present invention;</p><p>FIG. 3 is a block diagram of a set-associative cache memory having a mechanism for migrating a most recently used set, in accordance with a preferred embodiment of the present invention; and</p><p>FIG. 4 is a detailed block diagram of the migration mechanism from FIG. 3, in accordance with a preferred embodiment of the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF A PREFERRED EMBODIMENT</h4><p>The present invention may be implemented in a variety of processors having a cache memory. The cache memory may be, for example, a primary cache, a secondary cache, or a tertiary cache.</p><p>Referring now to the drawings and in particular to FIG. 1, there is depicted a block diagram of a general structure of a processor in accordance with a preferred embodiment of the invention. As shown, processor <b>10</b> includes a central processing unit (CPU) <b>11</b>, an instruction cache <b>12</b>, and a data cache <b>13</b>. CPU <b>11</b> is preferably connected to instruction cache <b>12</b> and data cache <b>13</b> via respective high bandwidth buses. Processor <b>10</b> is also coupled to a main memory <b>14</b>. Both instruction cache <b>12</b> and data cache <b>13</b> are high speed set-associative caches which enable processor <b>10</b> to achieve a relatively fast access time to a subset of instructions or data previously transferred from main memory <b>14</b>.</p><p>With reference now to FIG. 2, there is depicted a detailed block diagram of processor <b>10</b> in accordance with a preferred embodiment of the present invention. Within processor <b>10</b>, a bus interface unit <b>21</b> is coupled to instruction cache <b>12</b> and data cache <b>13</b>. Instruction cache <b>12</b> is further coupled to an instruction unit <b>22</b> which fetches instructions from instruction cache <b>12</b> during each execution cycle.</p><p>Processor <b>10</b> also includes at least three execution units, namely, an integer unit <b>15</b>, a load/store unit <b>16</b>, and a floating-point unit <b>17</b>. These three execution units are collectively known as CPU <b>11</b> as depicted in FIG. <b>1</b>. Each of execution units <b>15</b>-<b>17</b> can execute one or more classes of instructions, and all execution units <b>15</b>-<b>17</b> can operate concurrently during each processor cycle. After execution of an instruction has terminated, any of execution units <b>15</b>-<b>17</b> stores data results to a respective rename buffer, depending upon the instruction type. Then, any one of execution units <b>15</b>-<b>17</b> may signal a completion unit <b>20</b> that the execution of an instruction has finished. Finally, each instruction is completed in program order, and the result data are transferred from a respective rename buffer to a general purpose register <b>18</b> or a floating-point register <b>19</b>, accordingly.</p><p>Referring now to FIG. 3, there is depicted a block diagram of a set-associative cache memory in accordance with a preferred embodiment of the present invention. As shown, a set-associative cache memory <b>30</b> includes memory array <b>31</b> along with a directory <b>32</b>. Set-associative cache memory <b>30</b> may be an instruction cache, such as instruction cache <b>12</b> in FIG. 2, or a data cache, such as data cache <b>13</b> in FIG. <b>2</b>. Each cache line in memory array <b>31</b> has a corresponding row in directory <b>32</b>. The data or instructions portion of a cache line is maintained in memory array <b>31</b> while the tag portion of the same cache line is maintained in directory <b>32</b>. Cache memory <b>30</b> also includes a TLB <b>33</b> for translating an effective address to a corresponding real address.</p><p>For the purpose of illustration, cache memory <b>30</b> is an eight-way set-associative cache memory. As a preferred embodiment of the present invention, fanout wiring <b>36</b> from a decoder <b>37</b> for accessing memory array <b>31</b> is incrementally distributed (i.e., distance from decoder <b>37</b> incrementally increase) across all eight sets within memory array <b>31</b> which may require a number of latch stages to maintain an adequate cycle time. Within memory array <b>31</b>, the distance between each set and output wiring <b>38</b> also incrementally increases across the sets. For example, as shown in memory array <b>31</b> of FIG. 3, set <b>0</b> is proportionally closer to address decoder <b>37</b> (1 cycle) than set <b>1</b>, set <b>1</b> is proportionally closer to address decoder <b>37</b> (2 cycles) than set <b>2</b>, set <b>2</b> is proportionally closer to address decoder <b>37</b> (3 cycles) than set <b>3</b>, etc. Similarly, set 0 is proportionally closer to output wiring <b>38</b> (1 cycle) than set <b>1</b>, set <b>1</b> is proportionally closer to output wiring <b>38</b> (2 cycles) than set <b>2</b>, set <b>2</b> is proportionally closer to output wiring <b>38</b> (3 cycles) and than set <b>3</b>, etc. Hence, signal propagation latency from address decoder <b>37</b> to each set within memory array <b>31</b> is directly proportional to the distance (i.e., number of latch stages) between address decoder <b>37</b> and the set. Also, the signal propagation latency from each set within memory array <b>31</b> to the output of memory array <b>31</b> is directly proportional to the distance between the set and the output of memory array <b>31</b>. As a result, the latency for a cache access is different for each of the eight sets within memory array <b>31</b>, depending on the set in which the instruction or data is being stored. In other words, there is an access latency varies incrementally among sets, from the fastest set <b>0</b> to the slowest set <b>7</b>.</p><p>The information stored in memory array <b>31</b> may be accessed by an effective address <b>35</b>. Effective address <b>35</b> includes a byte field, an index field, and a page number field. The index field of effective address <b>35</b> is utilized to select a specific congruence class within memory array <b>31</b> and the byte field of effective address <b>35</b> is utilized to index a specific byte within the selected cache line. In addition, the page number field of effective address <b>35</b> is sent to TLB <b>33</b> to be translated to a corresponding real page number. This real page number is utilized for comparison with a tag of the selected cache line from directory <b>32</b> via comparators <b>34</b> in order to determine whether there is a cache \u201chit\u201d or \u201cmiss \u201d Incidentally, a match between a tag from one of eight ways in directory <b>32</b> and the real page number implies a cache \u201chit.\u201d</p><p>After a cache \u201chit,\u201d the cache information is sent to the CPU, and the cache information and its associated directory information are also transferred to the first set (i.e., set <b>0</b>) of memory array <b>31</b> and directory <b>32</b>, respectively, via a migration mechanism. As shown in FIG. 3, a migration controller <b>39</b> is part of the migration mechanism and controls the migration of the \u201chit\u201d information within memory array <b>31</b> and directory <b>32</b>.</p><p>With reference now to FIG. 4, there is illustrated a detailed block diagram of the migration mechanism from FIG. 3, in accordance with a preferred embodiment of the present invention. For simplicity sake, only the migration mechanism for memory array <b>31</b> (of FIG. 3) is illustrated in FIG. 4, and it is understood by those skilled in the relevant art that the same principle can be applied to the migration mechanism for directory <b>32</b> (of FIG. <b>3</b>). As shown, a migration mechanism <b>40</b> includes migration controller <b>39</b> and multiple multiplexors <b>41</b>. Each of multiplexors <b>41</b> is associated with one of the sets within memory array <b>31</b>. Latches <b>42</b> represent the latch stage delay among the sets for the data to propagate to multiplexor <b>43</b>.</p><p>Migration controller <b>39</b> receives a \u201chit\u201d signal (from FIG. <b>3</b>), sel_<b>0</b> through sel_<b>7</b> signals (from FIG. <b>3</b>), and data signal from multiplexor <b>44</b> as inputs. These inputs are then utilized by migration controller <b>39</b> to generate write enable signals for set <b>0</b> through set <b>7</b> and data signal (same as the input data signal). The write enable signals from migration controller <b>39</b> control a write enable input of a respective multiplexor for each of the sets in a manner as described below. Migration controller <b>39</b> can be implemented as a state machine as it is understood by those skilled in the relevant art.</p><p>After a cache \u201chit\u201d signal is received, migration controller <b>39</b> simultaneously directs the cache \u201chit\u201d information to set <b>0</b> of memory array <b>31</b> while the information in each set of the respective arrays is shifted towards the set in which the \u201chit\u201d information originally resided. For example, if the \u201chit\u201d information originally resides in set <b>3</b>, after the \u201chit\u201d information has been sent to multiplexors <b>43</b> and <b>44</b>, the information stored in set <b>3</b> will be replaced with the information stored in set <b>2</b>. Subsequently, the information stored in set <b>2</b> will be replaced with the information stored in set <b>1</b>, and the information stored in set <b>1</b> will be replaced with the information stored in set <b>0</b>. Information stored in sets <b>4</b>-<b>7</b> remain unchanged, in this example, via a non-assertion of a write enable signal to their respective associated multiplexors <b>41</b>. Finally, the information stored in set <b>0</b> will be replaced by the \u201chit\u201d information (originally stored from set <b>3</b>). As a result, the most recently used (MRU) cache line is automatically placed in set <b>0</b> of memory array <b>31</b> (which is the set having the shortest access time).</p><p>As has been described, the present invention provides a set associative cache memory having a set associative cache memory having a mechanism for migrating a most recently used set in order to reduce the average access time of the cache. Although eight ways are shown in the present disclosure, it is understood by those skilled in the art that the number of ways can be any number higher than one. In addition, the incremental latency of the memory array in the present disclosure is assumed to be one cycle between adjacent sets, though it is understood by those skilled in the art that this incremental latency can be any number of cycles. For example, set <b>0</b>-<b>3</b> may not require any latch stage delays while sets <b>4</b>-<b>7</b> may only require one latch stage delay. With the present invention, speed improvements in cache access latency can be expected because the MRU, which is the set most likely to be accessed again in a short time, is migrated to the set that is closest to the output of the memory array, which has the shortest access time among sets.</p><p>For some software applications, the MRU set may not be the most likely set to be accessed again immediately. Thus, it is understood by those skilled in the art that the present invention can be modified to migrate the \u201chit\u201d information to any one of the sets within the cache memory array, as desired by the specific application, in order to provide an optimal speed performance.</p><p>While the invention has been particularly shown and described with reference to a preferred embodiment, it will be understood by those skilled in the art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Ravi Kumar", "last_name": "Arimilli", "name": ""}, {"first_name": "Lakshminarayana Baba", "last_name": "Arimilli", "name": ""}, {"first_name": "John Steven", "last_name": "Dodson", "name": ""}, {"first_name": "James Stephen", "last_name": "Fields, Jr.", "name": ""}, {"first_name": "Guy Lynn", "last_name": "Guthrie", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTERNATIONAL BUSINESS MACHINES CORPORATION"}, {"first_name": "", "last_name": "INTERNATIONAL BUSINESS MACHINES CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/12        20060101A N20051008RMEP"}, {"label": "G06F  12/08        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711128"}, {"primary": false, "label": "711E12041"}, {"primary": false, "label": "711122"}, {"primary": false, "label": "3650491"}, {"primary": false, "label": "711E12018"}], "ecla_classes": [{"label": "S06F12:12B4"}, {"label": "G06F  12/08B10"}, {"label": "G06F  12/08B22"}], "cpc_classes": [{"label": "G06F  12/0893"}, {"label": "G06F  12/0864"}, {"label": "G06F  12/0864"}, {"label": "G06F  12/123"}, {"label": "G06F  12/123"}, {"label": "G06F  12/0893"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1999-06-24", "application_date": "1999-06-24", "family_members": [{"ucid": "US-6460117-B1", "titles": [{"lang": "EN", "text": "Set-associative cache memory having a mechanism for migrating a most recently used set"}]}]}