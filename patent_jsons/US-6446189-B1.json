{"patent_number": "US-6446189-B1", "publication_id": 73133883, "family_id": 23258696, "publication_date": "2002-09-03", "titles": [{"lang": "EN", "text": "Computer system including a novel address translation mechanism"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11394113\" source=\"national office\"><p>A processor is presented including a cache unit coupled to a bus interface unit (BIU). Address signal selection and masking functions are performed by circuitry within the BIU rather than within the cache unit, and physical addresses produced by the BIU are stored within the TLB. As a result, address signal selection and masking circuitry (e.g., a multiplexer and gating logic) are eliminated from a critical speed path within the cache unit, allowing the operational speed of the cache unit to be increased. The cache unit stores data items, and produces a data item corresponding to a received linear address. A translation lookaside buffer (TLB) within the cache unit stores multiple linear addresses and corresponding physical addresses. When a physical address corresponding to the received linear address is not found within the TLB, the cache unit passes the linear address to the BIU. The BIU includes address translation circuitry, a multiplexer, and gating logic, and returns the physical address corresponding to the linear address to the cache unit. The cache unit stores the physical address and the linear address within the TLB. The processor may also include a programmable control register and a microexecution unit. Upon detecting a change in state of an external masking signal, the microexecution unit may flush the contents of the TLB and modify a masking bit within the control register to reflect a new state of the masking signal.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50372938\"><p>A processor is presented including a cache unit coupled to a bus interface unit (BIU). Address signal selection and masking functions are performed by circuitry within the BIU rather than within the cache unit, and physical addresses produced by the BIU are stored within the TLB. As a result, address signal selection and masking circuitry (e.g., a multiplexer and gating logic) are eliminated from a critical speed path within the cache unit, allowing the operational speed of the cache unit to be increased. The cache unit stores data items, and produces a data item corresponding to a received linear address. A translation lookaside buffer (TLB) within the cache unit stores multiple linear addresses and corresponding physical addresses. When a physical address corresponding to the received linear address is not found within the TLB, the cache unit passes the linear address to the BIU. The BIU includes address translation circuitry, a multiplexer, and gating logic, and returns the physical address corresponding to the linear address to the cache unit. The cache unit stores the physical address and the linear address within the TLB. The processor may also include a programmable control register and a microexecution unit. Upon detecting a change in state of an external masking signal, the microexecution unit may flush the contents of the TLB and modify a masking bit within the control register to reflect a new state of the masking signal.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00001\" num=\"1\"><claim-text>1. A processor, comprising:</claim-text><claim-text>a cache unit for storing data items, wherein said cache unit is coupled to receive a linear address and comprises a translation lookaside buffer (TLB) for storing a plurality of linear addresses and corresponding physical addresses, and wherein said cache unit is configured to provide said linear address when a physical address corresponding to said linear address is not found within said TLB; </claim-text><claim-text>a bus interface unit coupled to receive said linear address from said cache unit and configured to provide a physical address corresponding to said linear address to said cache unit; </claim-text><claim-text>wherein said linear address comprises a first plurality of address signals, and wherein the bus interface unit comprises: </claim-text><claim-text>address translation circuitry coupled to receive said first plurality of address signals, and wherein said address translation circuitry is configured to produce a second plurality of address signals from said first plurality of address signals; </claim-text><claim-text>a multiplexer coupled to receive said first and second plurality of address signals and a paging signal, wherein said multiplexer is configured to produce a third plurality of address signals, and wherein said third plurality of address signals is either said first plurality of address signals or said second plurality of address signals dependent upon said paging signal; and </claim-text><claim-text>gating logic coupled to receive at least one of said third plurality of address signals and a first masking signal, and wherein said gating logic is configured to either pass the at least one of said third plurality of address signals or to mask the at least one of said third plurality of address signals dependent upon said first masking signal; and </claim-text><claim-text>wherein said bus interface unit provides the third plurality of address signals acted upon by said gating logic to said cache unit as the physical address corresponding to said linear address. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00002\" num=\"2\"><claim-text>2. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00001\">claim 1</claim-ref>, further comprising a control register including a masking bit and a paging bit, wherein said first masking signal is a value of said masking bit, and wherein said paging signal is a value of said paging bit.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00003\" num=\"3\"><claim-text>3. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00002\">claim 2</claim-ref>, further comprising a microexecution unit coupled to receive a second masking signal generated external to said processor, wherein said microexecution unit is configured to: (i) flush the contents of said TLB dependent upon said second masking signal, and (ii) modify the value of said masking bit within said control register dependent upon said second masking signal.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00004\" num=\"4\"><claim-text>4. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00001\">claim 1</claim-ref>, wherein said bus interface unit is coupled to receive said paging signal, and wherein said paging signal is asserted when a paged addressing mode is enabled, and wherein said address translation circuitry is configured to produce said second plurality of address signals from said first plurality of address signals when said paging signal is asserted.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00005\" num=\"5\"><claim-text>5. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00001\">claim 1</claim-ref>, wherein said first plurality of address signals comprise a virtual address when said paging signal is asserted, and wherein said second plurality of address signals comprise a physical address when said paging signal is asserted such that said address translation circuitry performs a virtual-to-physical address translation when said paging signal is asserted.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00006\" num=\"6\"><claim-text>6. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00001\">claim 1</claim-ref>, wherein said gating logic produces the at least one of said third plurality of address signals when said gating logic passes the at least one of said third plurality of address signals.</claim-text></claim>"}, {"num": 7, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00007\" num=\"7\"><claim-text>7. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00001\">claim 1</claim-ref>, wherein said gating logic produces logic \u201c0\u201d signals as the at least one of said third plurality of address signals when said gating logic masks the at least one of said third plurality of address signals.</claim-text></claim>"}, {"num": 8, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00008\" num=\"8\"><claim-text>8. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00001\">claim 1</claim-ref>, wherein said bus interface unit is coupled to a main memory, and wherein said main memory is used to store virtual memory system information, and wherein said address translation circuitry uses said virtual memory system information to produce said second plurality of address signals.</claim-text></claim>"}, {"num": 9, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00009\" num=\"9\"><claim-text>9. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00007\">claim 7</claim-ref>, wherein said virtual memory system information comprises a page directory and a page table.</claim-text></claim>"}, {"num": 10, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00010\" num=\"10\"><claim-text>10. A method for performing address translation, comprising:</claim-text><claim-text>providing a translation lookaside buffer (TLB) for storing a plurality of linear addresses and corresponding physical addresses; </claim-text><claim-text>performing the following upon detecting a change in state of a masking signal from an old state to a new state: </claim-text><claim-text>flushing the TLB; and </claim-text><claim-text>saving the new state of the masking signal; </claim-text><claim-text>producing a physical address from a linear address when said linear address is not found within said TLB, wherein said physical address comprises a plurality of physical address signals; </claim-text><claim-text>masking at least one of said physical address signals dependent upon the saved state of the masking signal; and </claim-text><claim-text>storing said linear address and said physical address within said TLB. </claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00011\" num=\"11\"><claim-text>11. A processor comprising:</claim-text><claim-text>a control register configured to store a paging indication indicative of whether or not virtual to physical address translation is enabled; and </claim-text><claim-text>a translation lookaside buffer (TLB) coupled to receive a first address, wherein the TLB is configured to map the first address to a second address stored in the TLB responsive to the first address hitting in the TLB, the TLB outputting the second address and a hit indication indicative of whether or not the first address hits in the TLB; </claim-text><claim-text>wherein the TLB is configured to detect the hit independent of whether or not virtual to physical address translation is enabled, and wherein the TLB is configured to map the first address to the second address independent of whether or not virtual to physical address translation is enabled. </claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00012\" num=\"12\"><claim-text>12. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00011\">claim 11</claim-ref> wherein, if the first address is a miss in the TLB, the processor passes the first address to circuitry configured to map the first address to the second address, wherein the circuitry is configured to selectively mask at least one bit of the second address prior to storage in the TLB, the selective mask performed in response to a masking signal.</claim-text></claim>"}, {"num": 13, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00013\" num=\"13\"><claim-text>13. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00012\">claim 12</claim-ref> wherein, if the masking signal is in a first state, the at least one bit is masked to a zero value.</claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00014\" num=\"14\"><claim-text>14. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00013\">claim 13</claim-ref> wherein, if the masking signal is in a second state, the at least one bit is passed through unchanged.</claim-text></claim>"}, {"num": 15, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00015\" num=\"15\"><claim-text>15. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00012\">claim 12</claim-ref> wherein the masking signal is sourced by the control register.</claim-text></claim>"}, {"num": 16, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00016\" num=\"16\"><claim-text>16. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00015\">claim 15</claim-ref> wherein the processor is configured to determine the masking signal from a second masking signal input to the processor, and wherein the processor, in response to detecting a change in the second masking signal, is configured to generate an exception on an instruction to update the control register.</claim-text></claim>"}, {"num": 17, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00017\" num=\"17\"><claim-text>17. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00012\">claim 12</claim-ref> wherein the circuitry is coupled to the control register, and wherein, in response to virtual to physical address translation not being enabled, the circuitry is configured to map the first address to a second address which equals the first address prior to the selective masking.</claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00018\" num=\"18\"><claim-text>18. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00017\">claim 17</claim-ref> wherein, if the masking signal indicates that masking is to be performed, the second address equals the first address except in the at least one bit, which is masked to zero.</claim-text></claim>"}, {"num": 19, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00019\" num=\"19\"><claim-text>19. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00017\">claim 17</claim-ref> wherein, in response to virtual to physical address translation being enabled, the circuitry is configured to map the first address to the second address using translation tables stored in a main memory to which the processor has access.</claim-text></claim>"}, {"num": 20, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00020\" num=\"20\"><claim-text>20. The processor as recited in <claim-ref idref=\"US-6446189-B1-CLM-00011\">claim 11</claim-ref> further comprising cache hit circuitry configured to determining if the first address is a hit in a cache, the cache hit circuitry coupled to receive the second address and the hit indication from the TLB, wherein the cache hit circuitry is configured to detect a hit in the cache by comparing the second address to tags from the cache responsive to the hit indication from the TLB indicating a hit and independent of whether or not virtual to physical address translation is enabled.</claim-text></claim>"}, {"num": 21, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00021\" num=\"21\"><claim-text>21. A method comprising:</claim-text><claim-text>detecting that a first address is a hit in a TLB, wherein the detecting is independent of whether or not virtual to physical address translation is enabled; and </claim-text><claim-text>mapping the first address to a second address stored in the TLB responsive to detecting that the first address is a hit in the TLB, wherein the mapping is independent of whether or not virtual to physical address translation is enabled. </claim-text></claim>"}, {"num": 22, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00022\" num=\"22\"><claim-text>22. The method as recited in <claim-ref idref=\"US-6446189-B1-CLM-00021\">claim 21</claim-ref> further comprising:</claim-text><claim-text>detecting that a third ad dress is a miss in the TLB; </claim-text><claim-text>mapping the third address to a fourth address; and </claim-text><claim-text>selectively masking at least one bit in the fourth address prior to storing the fourth address in the TLB responsive to a masking signal. </claim-text></claim>"}, {"num": 23, "parent": 22, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00023\" num=\"23\"><claim-text>23. The method as recited in <claim-ref idref=\"US-6446189-B1-CLM-00022\">claim 22</claim-ref> wherein the selectively masking comprises:</claim-text><claim-text>passing the at least one bit of the fourth address unchanged in response to the masking signal having a first state; and </claim-text><claim-text>masking the at least one bit of the fourth address to zero in response to the masking signal having a second state. </claim-text></claim>"}, {"num": 24, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6446189-B1-CLM-00024\" num=\"24\"><claim-text>24. The method as recited in <claim-ref idref=\"US-6446189-B1-CLM-00021\">claim 21</claim-ref> further comprising detecting a cache hit in response to comparing the second address to one or more cache tags and independent of whether or not virtual to physical address translation is enabled.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53627606\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>This invention relates to processors and computer systems, and more particularly to address translation mechanisms used within computer systems and processors.</p><p>2. Description of the Related Art</p><p>A typical computer system includes a processor which reads and executes instructions of software programs stored within a memory system. In order to maximize the performance of the processor, the memory system must supply the instructions to the processor such that the processor never waits for needed instructions. There are many different types of memory from which the memory system may be formed, and the cost associated with each type of memory is typically directly proportional to the speed of the memory. Most modern computer systems employ multiple types of memory. Smaller amounts of faster (and more expensive) memory are positioned closer to the processor, and larger amounts of slower (and less expensive) memory are positioned farther from the processor. By keeping the smaller amounts of faster memory filled with instructions (and data) needed by the processor, the speed of the memory system approaches that of the faster memory, while the cost of the memory system approaches that of the less expensive memory.</p><p>Most modern computer systems also employ a memory management technique called \u201cvirtual\u201d memory which allocates memory to software programs upon request. This automatic memory allocation effectively hides the memory hierarchy described above, making the many different types of memory within a typical memory system (e.g., random access memory, magnetic hard disk storage, etc.) appear as one large memory. Virtual memory also provides for isolation between different programs by allocating different physical memory locations to different programs running concurrently.</p><p>Early x86 (e.g., 8086/88) processors used a segmented addressing scheme in which a 16-bit segment value is combined with a 16-bit offset value to form a 20-bit physical address. In a shift-and-add operation, the 16-bit segment portion of the address is first shifted left four bit positions to form a segment base address. The 16-bit offset portion is then added to the segment base address, producing the 20-bit physical address. In the early x86 processors, when the shift-and-add operation resulted in a physical address having a value greater than FFFFFh, the physical address value \u201cwrapped around\u201d and started at 00000h. Programmers developing software for the early x86 processors began to rely upon this address wrap-around \u201cfeature\u201d. In order to facilitate software compatibility, later x86 processors included an address bit <b>20</b> \u201cmasking\u201d feature controlled by an \u201cA20M\u201d input pin. By asserting an A20M signal coupled to the A20M pin, address bit <b>20</b> is produced having a logic value of \u201c0\u201d. As a result, address values greater than FFFFFh appear to wrap around and start at 00000h, emulating the behavior of the early x86 processors.</p><p>Many modem processors, including x86 processors, support a form of virtual memory called \u201cpaging\u201d. Paging divides a physical address space, defined by the number of address signals generated by the processor, into fixed-sized blocks of contiguous memory called \u201cpages\u201d. If paging is enabled, a \u201cvirtual\u201d address is translated or \u201cmapped\u201d to a physical address. For example, in an x86 processor with paging enabled, a paging unit within the processor translates a \u201clinear\u201d address produced by a segmentation unit to a physical address. If an accessed page is not located within the main memory unit, paging support constructs (e.g., operating system software) load the accessed page from secondary memory (e.g., magnetic disk) into main memory. In x86 processors, two different tables stored within the main memory unit, namely a page directory and a page table, are used to store information needed by the paging unit to perform the linear-to-physical address translations.</p><p>Accesses to the main memory unit require relatively large amounts of time. In order to reduce the number of required main memory unit accesses to retrieve information from the page directory and page table, a small cache memory system called a translation lookaside buffer (TLB) is typically used to store the most recently used address translations. As the amount of time required to access an address translation in the TLB is relatively small, overall processor performance is increased as needed address translations are often found in the readily accessible TLB.</p><p>A typical modem processor includes a cache memory unit coupled between an execution unit and a bus interface unit. The execution unit executes software instructions. The cache memory unit includes a relatively small amount of memory which can be accessed very quickly. The cache memory unit is used to store instructions and data (i.e. data items) recently used by the execution unit, along with data items which have a high probability of being needed by the execution unit in the near future. Searched first, the cache memory unit makes needed information readily available to the execution unit. When needed information is not found in the cache memory unit, the bus interface unit is used to fetch the needed information from a main memory unit located external to the processor. The overall performance of the processor is improved when needed information is often found within the cache memory unit, eliminating the need for time-consuming accesses to the main memory unit.</p><p>FIG. 1 is a block diagram illustrating an address translation mechanism of an exemplary modem x86 computer system. A cache unit <b>10</b> within an x86 processor may be used to store instructions and/or data (i.e., data items) recently used or likely to be needed by an execution unit coupled to cache unit <b>10</b>. Cache unit <b>10</b> includes a TLB <b>12</b> used to store the most recently used address translations, a multiplexer <b>14</b>, and gating logic <b>16</b>.</p><p>TLB <b>12</b> receives a linear address provided to cache unit <b>10</b> and produces a stored physical address corresponding to the linear address. Multiplexer <b>14</b> receives the linear address provided to cache unit <b>10</b> and the physical address produced by TLB <b>12</b>. Multiplexer <b>14</b> produces either the physical address or the linear address dependent upon a PAGING signal. When paging is disabled, the linear address provided to cache unit <b>10</b> is a physical address, and address translation by TLB <b>12</b> is unnecessary. In this case, the PAGING signal is deasserted, and multiplexer <b>14</b> produces the linear address. When paging is enabled, the linear address provided to cache unit <b>10</b> is a virtual address, and translation of the virtual address to a physical address is necessary. In this case, the PAGING signal is asserted, and multiplexer <b>14</b> produces the physical address produced by TLB <b>12</b>. If a stored physical address corresponding to the linear address is found within TLB <b>12</b>, TLB <b>12</b> asserts a TLB HIT signal. Otherwise, the TLB hit signal is deasserted.</p><p>Gating logic <b>16</b> receives address bit <b>20</b> (i.e., signal A<b>20</b>) of the physical address produced by multiplexer <b>14</b>, and the A20M signal. Gating logic <b>16</b> produces a new signal A<b>20</b> dependent upon the A20M signal. When the A20M signal is deasserted, gating logic produces the new signal A<b>20</b> such that the new signal A<b>20</b> has the same value as the signal A<b>20</b> of the physical address produced by multiplexer <b>14</b>. In other words, when signal A20M is deasserted, gating logic \u201cpasses\u201d the signal A<b>20</b> of the physical address produced by multiplexer <b>14</b>. On the other hand, when the A20M signal is asserted, gating logic produces the new signal A<b>20</b> with a logic value of \u201c0\u201d. In other words, when signal A20M is asserted, gating logic \u201cmasks\u201d or \u201cclears\u201d the signal A<b>20</b> of the physical address produced by multiplexer <b>14</b>.</p><p>In addition to TLB <b>12</b>, cache unit <b>10</b> includes a cache memory <b>18</b> for storing the data items recently used or likely to be needed by the execution unit coupled to cache unit <b>10</b>. Cache memory <b>14</b> includes a tag array <b>20</b> for storing physical address \u201ctags\u201d,and a data array <b>22</b> for storing the data items. Each data item stored in data array <b>22</b> has a corresponding physical address \u201ctag\u201d stored in tag array <b>20</b>.</p><p>When the linear address is provided to TLB <b>12</b>, a least-significant or lower ordered \u201cindex\u201d portion of the linear address is simultaneously provided to tag array <b>20</b> and data array <b>22</b> of cache memory <b>18</b>. In the embodiment of FIG. 1, cache memory <b>18</b> is a two-way set associative cache structure. The index portion of the linear address is used as an index into tag array <b>20</b>. As a result, tag array <b>20</b> produces two physical address \u201ctags\u201d. One of the two physical address \u201ctags\u201d is provided to a comparator (CO) <b>24</b><i>a</i>, and the other physical address \u201ctag\u201d is provided to a comparator <b>24</b><i>b</i>. The index portion of the linear address is also used as an index into data array <b>22</b>. As a result, data array <b>22</b> produces two data items. The two data items are provided to different inputs of a multiplexor (MUX) <b>26</b>.</p><p>After passing through multiplexer <b>14</b> and gating logic <b>16</b>, the physical address is provided to comparators <b>24</b><i>a -b</i>. If the physical address matches one of the physical address \u201ctags\u201d provided by tag array <b>20</b>, the corresponding comparator <b>24</b> asserts an output signal. The output signals produced by comparators <b>24</b><i>a-b </i>are provided to a control unit <b>28</b> which controls the operations of cache unit <b>10</b>. The output signal produced by comparator <b>24</b><i>b </i>is also provided to a control input of multiplexor <b>26</b>. Multiplexer <b>26</b> produces an output DATA signal in response to the output signal produced by comparator <b>24</b><i>b</i>. The output DATA signal may include the data item from data array <b>22</b> corresponding to the physical address \u201ctag\u201d which matches the physical address provided to comparators <b>24</b><i>a-b</i>. Control unit <b>82</b> uses the TLB HIT signal and the output signals produced by comparators <b>24</b><i>a-b </i>to determine when the DATA signal produced by multiplexor <b>80</b> is \u201cvalid\u201d. When the DATA signal produced by multiplexor <b>80</b> is valid, control unit <b>82</b> asserts an output DATA VALID signal. Control unit <b>82</b> also produces an output CACHE HIT signal which is asserted when the data item corresponding to the provided linear address was found in cache memory <b>18</b>.</p><p>Cache unit <b>10</b> is coupled to a bus interface unit (BIU) <b>30</b> within the x86 processor, and BIU <b>30</b> is coupled to a main memory <b>32</b> located external to the x86 processor. When the PAGING signal is asserted and TLB <b>12</b> does not contain the physical address corresponding to the linear address (i.e., the TLB HIT signal is deasserted), control unit <b>28</b> provides the linear address (i.e., virtual address) to BIU <b>30</b>. BIU <b>30</b> may include address translation circuitry to perform the virtual-to-physical address translation. The address translation circuitry within BIU <b>30</b> may access virtual memory system information (e.g., the page directory and the page table) stored within main memory <b>32</b> in order to perform the virtual-to-physical address translation. BIU <b>30</b> may provide the resulting physical address to control unit <b>28</b>, and control unit <b>28</b> may provide the physical address to TLB <b>12</b>. TLB <b>12</b> may store the linear address (i.e., virtual address) and corresponding physical address, assert the TLB HIT signal, and provide the physical address to comparators <b>24</b><i>a-b. </i></p><p>If the physical address does not match one of the physical address \u201ctags\u201d provided by tag array <b>20</b>, control unit <b>28</b> may submit a read request to BIU <b>30</b>, providing the physical address to BIU <b>30</b>. BIU <b>30</b> may then read the data item from main memory <b>32</b>, and forward the data item directly to cache memory <b>18</b> as indicated in FIG. <b>1</b>. Cache memory <b>18</b> may store the physical address within tag array <b>20</b>, and store the corresponding data item retrieved from main memory <b>32</b> within data array <b>22</b>. Cache memory <b>18</b> may also forward the stored physical address to either comparator <b>24</b><i>a </i>or <b>24</b><i>b</i>, and forward the stored data item to an input of multiplexor <b>26</b>. As a result, the comparator to which the stored physical address is provided asserts the output signal, multiplexor <b>26</b> produces the DATA signal including the stored data item, and control unit <b>28</b> asserts the CACHE HIT signal.</p><p>Multiplexer <b>14</b> and gating logic <b>16</b> exist along a critical speed path within cache unit <b>10</b>, and thus limit the maximum speed at which cache unit <b>10</b> may operate. It would thus be desirable to have a processor including a cache unit which does not include multiplexer <b>14</b> and gating logic <b>16</b> coupled as shown in FIG. 1 such that the operational speed of the cache unit may be increased.</p><h4>SUMMARY OF THE INVENTION</h4><p>The problems outlined above are in large part solved by a computer system implementing a novel address translation mechanism. The computer system includes a processor which executes instructions. The present processor includes a cache unit coupled to a bus interface unit (BIU). Address signal selection and masking functions are performed by circuitry within the BIU rather than within the cache unit, and physical addresses produced by the BIU are stored within the TLB. As a result, address signal selection and masking circuitry (e.g., a multiplexer and gating logic) are eliminated from a critical speed path within the cache unit, allowing the operational speed of the cache unit of the present processor to be increased.</p><p>The cache unit stores data items, and produces a data item corresponding to a received linear address. The cache unit includes a translation lookaside buffer (TLB) for storing multiple linear addresses and corresponding physical addresses. When a physical address corresponding to the received linear address is not found within the TLB, the cache unit passes the linear address to the BIU. The BIU returns the physical address corresponding to the linear address to the cache unit. The linear address includes multiple linear address signals, and the physical address includes multiple physical address signals.</p><p>The BIU includes address translation circuitry, a multiplexer, and gating logic. The address translation circuitry receives the multiple linear address signals and produces multiple physical address signals from the multiple linear address signals. The multiplexer receives the multiple linear and physical address signals and a paging signal, wherein the paging signal may be asserted when a paged addressing mode is enabled. When the paging signal is deasserted, the multiplexer may produce the linear address signals as physical address signals at an output. On the other hand, the multiplexer may produce the multiple physical address signals at the output when the paging signal is asserted.</p><p>The gating logic receives one or more of the physical address signals produced by the multiplexer. The gating logic either passes the one or more physical address signals or masks the one or more physical address signals dependent upon a first masking signal. When the first masking signal is deasserted, the gating logic may produce the one or more physical address signals unchanged at an output. On the other hand, the gating logic may produce constant logic value signals (e.g., logic \u201c0\u201d signals) in place of the one or more physical address signals at the output when the first masking signal is asserted, thus masking the one or more physical address signals when the first masking signal is asserted. The BIU may provide the physical address signals acted upon by the gating logic to the cache unit as the physical address corresponding to the linear address. The cache unit may store the physical address and the linear address within the TLB.</p><p>The present processor may also include a microexecution unit and a programmable control register. The control register may include a masking bit and a paging bit. The first masking signal may be a value of the masking bit, and the paging signal may be a value of the paging bit. The microexecution unit may receive a second masking signal generated external to the processor. Upon detecting a change in state of the second masking signal from an old state to a new state (e.g., a transition from a logic low or \u201c0\u201d voltage level to a logic high or \u201c1\u201d voltage level), the microexecution unit may: (i) flush the contents of the TLB, and (ii) modify the value of the masking bit within the control register to reflect the new state of the second masking signal. Such actions may be delayed after detecting the change in state of the second masking signal to allow a certain number of instructions (e.g., 2) to be executed in the context of the old state of the second masking signal before the masking bit within the control register is changed.</p><p>The BIU may receive the paging signal (i.e., the value of the paging bit) from the control register. As described above, the paging signal may be asserted when the paged addressing mode is enabled. When the paging signal is asserted, the multiple linear address signals may form a virtual address. The address translation circuitry within the BIU may produce the multiple physical address signals from the multiple linear address signals when the paging signal is asserted. In other words, the address translation circuitry may perform a virtual-to-physical address translation when the paging signal is asserted.</p><p>The BIU may be coupled to a main memory located external to the processor. The main memory may be used to store virtual memory system information (e.g., a page directory and a page table). The address translation circuitry may use the virtual memory system information stored within the main memory to produce the multiple physical address signals.</p><p>The present processor implements a novel address translation method. This method may include providing a translation lookaside buffer (TLB) for storing multiple linear addresses and corresponding physical addresses. Upon detecting a change in state of a masking signal (e.g., the externally generated second masking signal described above) from the old state to the new state, the TLB may be flushed, and the new state of the masking signal may be saved. When a linear address is not found within the TLB, a physical address including multiple physical address signals may be produced from the linear address. One or more of the physical address signals may be masked dependent upon the saved state of the second masking signal. The linear address and the physical address may then be saved within the TLB.</p><p>A computer system is described which includes the present processor. The computer system may also include a bus coupled to the processor, and a peripheral device coupled to the bus. The bus may be a peripheral component interconnect (PCI) bus, and the peripheral device may be, for example, a network interface card, a video accelerator, an audio card, a hard disk drive, or a floppy disk drive. Alternately, the bus may be an extended industry standard architecture (EISA)/industry standard architecture (ISA) bus, and the peripheral device may be, for example, a modem, a sound card, or a data acquisition card.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Other objects and advantages of the invention will become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p><p>FIG. 1 is a block diagram illustrating an address translation mechanism of an exemplary modern x86 computer system;</p><p>FIG. 2 is a block diagram of a computer system including an address translation mechanism in accordance with the present invention;</p><p>FIG. 3 is a flow chart of a method for configuring address translation hardware within the computer system of FIG. 2 to reflect a change in state of an external A20M address masking signal;</p><p>FIG. 4 is a block diagram of one embodiment of a processor incorporating the present address translation mechanism; and</p><p>FIG. 5 is a block diagram of one embodiment of a computer system including the processor of FIG. <b>4</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><p>While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><h4>DETAILED DESCRIPTION OF THE INVENTION</h4><p>FIG. 2 is a block diagram of a computer system <b>40</b> including an address translation mechanism in accordance with the present invention. Computer system <b>40</b> includes a processor <b>42</b> coupled to a main memory <b>44</b>. Processor <b>42</b> is configured to execute instructions (e.g., x86 instructions). Main memory <b>44</b> is configured to store data items including instructions.</p><p>Processor <b>42</b> includes a cache unit <b>46</b> coupled to a bus interface unit (BIU) <b>48</b>, and a microexecution unit <b>50</b>. Cache unit <b>46</b> may be used to store instructions or data recently used or likely to be needed by an execution unit coupled to cache unit <b>46</b>. BIU <b>48</b> is used to transfer data between processor <b>42</b> and devices connected to a bus external to processor <b>42</b>. For example, main memory <b>44</b> may be coupled to the bus, and BIU <b>48</b> may handle data transfers between processor <b>42</b> and main memory <b>44</b>.</p><p>Cache unit <b>46</b> includes a translation lookaside buffer (TLB) <b>52</b> used to store the most recently used address translations. TLB <b>52</b> may be a fully associative TLB including, for example, <b>32</b> entry locations for storing linear-to-physical (i.e., virtual-to-physical) address translations. TLB <b>52</b> receives a linear address provided to cache unit <b>46</b> (e.g., by the execution unit) and produces a stored physical address corresponding to the linear address. When a stored physical address corresponding to the linear address is found within TLB <b>52</b>, TLB <b>52</b> asserts a TLB HIT signal. Otherwise, the TLB HIT signal is deasserted.</p><p>Cache unit <b>46</b> also includes a cache memory <b>54</b> for storing the data items recently used or likely to be needed by the execution unit. Cache memory <b>54</b> includes a tag array <b>56</b> for storing the physical address \u201ctags\u201d,and a data array <b>58</b> for storing the data items. Each data item stored in data array <b>58</b> has a corresponding physical address \u201ctag\u201d stored in tag array <b>56</b>.</p><p>When the linear address is provided to TLB <b>52</b>, a least-significant or lower ordered \u201cindex\u201d portion of the linear address is simultaneously provided to tag array <b>56</b> and data array <b>58</b> of cache memory <b>54</b>. In the embodiment of FIG. 2, cache memory <b>54</b> is a two way set associative structure. The index portion of the linear address is used as an index into tag array <b>56</b>. As a result, tag array <b>56</b> produces two physical address \u201ctags\u201d. One of the two physical address \u201ctags\u201d is provided to a comparator (CO) <b>60</b><i>a</i>, and other physical address tag is provided to a comparator (CO) <b>60</b><i>b</i>. The index portion of the linear address is also used as an index into data array <b>58</b>. As a result, data array <b>58</b> produces two data items. The two data items are provided to different inputs of a multiplexer <b>62</b>.</p><p>Rather than using multiplexer <b>14</b> to bypass TLB <b>12</b> when paging is enabled as shown in FIG. 1, TLB <b>52</b> of FIG. 2 is used to provide the physical address when paging is both enabled and disabled. As will be described in detail below, the physical address stored within TLB <b>52</b> accounts for the state of the A20M signal described above. As a result, multiplexer <b>14</b> and gating logic <b>16</b> of FIG. 1 are eliminated from the speed-sensitive physical address input paths to comparators <b>60</b><i>a-b </i>of FIG. <b>2</b>.</p><p>If the physical address produced by TLB <b>52</b> matches one of the physical address \u201ctags\u201d provided by tag array <b>56</b>, the corresponding comparator <b>60</b> produces an asserted TAG HIT signal. The TAG HIT signals are provided to a control unit <b>64</b> which controls the operations of cache unit <b>46</b>. One of the TAG HIT signals is also provided to a control input of multiplexer <b>62</b>. In FIG. 2, the TAG HIT signal produced by comparator <b>60</b><i>b </i>is provided to the control input of multiplexer <b>62</b>. multiplexer <b>62</b> produces the data corresponding to address tag provided to comparator <b>60</b><i>b </i>if the TAG HIT signal is asserted, and produces the data corresponding to address tag provided to comparator <b>60</b><i>a </i>if the TAG HIT signal is deasserted. Control unit <b>64</b> produces a DATA VALID signal and a CACHE HIT signal dependent upon the TLB HIT signal and the TAG HIT signals. When the data provided by multiplexer <b>62</b> is valid, control unit <b>64</b> asserts the DATA VALID signal. Control unit <b>64</b> asserts the CACHE HIT signal when the data item corresponding to the provided linear address was found within cache memory <b>54</b>.</p><p>In the embodiment of FIG. 2, processor <b>42</b> includes a programmable control register (CR) <b>72</b>. Control register <b>72</b> stores multiple control bits which determine the functions of various elements of processor <b>42</b>. Control register <b>72</b> includes a PAGING bit and an A20M bit. The value of the PAGING bit is dependent upon whether paging is enabled. For example, when paging is disabled, the PAGING bit may be a logic \u201c0\u201d,and the PAGING bit may be a logic \u201c1\u201d when paging is enabled. The value of the A20M bit is dependent upon whether address bit <b>20</b> (A<b>20</b>) of the address signal is to be masked (i.e., cleared). For example, when A<b>20</b> is not to be masked, the A20M bit may be a logic \u201c1\u201d,and the A20M bit may be a logic \u201c0\u201d when A<b>20</b> is to be masked.</p><p>Microexecution unit <b>50</b> executes stored instructions called microinstructions. Microexecution unit <b>50</b> receives the external A20M signal, e.g., from hardware coupled to processor <b>42</b>. In x86 processors, such hardware coupled to processor <b>42</b> is often triggered to assert the A20M signal when processor <b>42</b> executes an x86 \u201cOUT\u201d instruction. Execution of the \u201cOUT\u201d instruction by processor <b>42</b> causes processor <b>42</b> to write a value to a selected address (e.g., port \u201c92h\u201d) during an I/O bus cycle. The hardware coupled to processor <b>42</b> may be configured to detect the writing of the value to the selected address during an I/O bus cycle and to assert the A20M signal in response.</p><p>Any sampled edge transition of the received A20M signal produces a microinterrupt within microexecution unit <b>50</b>. The microinterrupt causes microexecution unit <b>50</b> to suspend execution of a microinstruction stream and to execute instructions of a microinterrupt handler. The instructions of the microinterrupt handler cause microexecution unit <b>50</b> to clear or \u201cflush\u201d the contents of TLB <b>52</b> and to change the state of the A20M bit in control register <b>72</b>. The flushing of TLB <b>52</b> clears out \u201cstale\u201d address mappings stored in the context of the old state of the A20M pin of control register <b>72</b>. After executing the instructions of the microinterrupt handler, microcontroller <b>50</b> resumes execution of the microinstruction stream. As TLB <b>52</b> will contain no valid address mappings after being flushed, the instruction executed by processor <b>42</b> following the flushing of TLB <b>52</b> will \u201cmiss\u201d in TLB <b>52</b>. As a result, control unit <b>64</b> will request an address mapping from BIU <b>48</b>. BIU <b>48</b> will return an address mapping in the context of the new state of the A20M pin of control register <b>72</b>.</p><p>When TLB <b>52</b> does not contain the physical address corresponding to the linear address, control unit <b>64</b> provides the linear address to BIU <b>48</b>. BIU <b>48</b> includes a multiplexer <b>66</b>, gating logic <b>68</b>, and address translation circuitry (ATC) <b>70</b>. ATC <b>70</b> is used to perform virtual-to-physical address translation when paging is enabled. ATC <b>70</b> may perform the virtual to physical address translation by accessing virtual memory system information (e.g. the page directory and the page table) stored within main memory <b>44</b>.</p><p>Multiplexer <b>66</b> receives the linear address provided by control unit <b>64</b> and the physical address produced by ATC <b>70</b> at data inputs, and the value of the PAGING bit of control register <b>72</b> at a control input. Multiplexer <b>66</b> produces either the physical address or the linear address dependent upon the value of the PAGING bit. When paging is disabled, the linear address is a physical address as described above, and address translation by ATC <b>70</b> is unnecessary. In this case, the PAGING bit is inactive, and multiplexer <b>66</b> produces the linear address. When paging is enabled, the linear address is a virtual address as described above, and translation of the virtual address to a physical address by ATC <b>70</b> is necessary. In this case, the PAGING bit is active, and multiplexer <b>66</b> produces the physical address produced by ATC <b>70</b>.</p><p>Gating logic <b>68</b> receives address bit <b>20</b> (i.e., signal A<b>20</b>) of the physical address produced by multiplexer <b>66</b>, and the value of the A20M bit of control register <b>72</b>. Gating logic <b>68</b> produces a new signal A<b>20</b> dependent upon the value of the A20M bit. When the A20M bit is inactive, gating logic <b>68</b> produces the new signal A<b>20</b> such that the new signal A<b>20</b> has the same value as the signal A<b>20</b> of the physical address produced by multiplexer <b>66</b>. In other words, when the A20M bit is inactive, gating logic \u201cpasses\u201d the signal A<b>20</b> of the physical address produced by multiplexer <b>66</b>. On the other hand, when the A20M bit is active, gating logic produces the new signal A<b>20</b> with a logic value of \u201c0\u201d. In other words, when the A20M bit is active, gating logic \u201cmasks\u201d or \u201cclears\u201d the signal A<b>20</b> of the physical address produced by multiplexer <b>66</b>.</p><p>After passing through multiplexer <b>66</b> and gating logic <b>68</b>, the physical address produced by BIU <b>48</b> is provided to TLB <b>52</b>. TLB <b>52</b> stores the linear address and corresponding physical address, asserts the TLB HIT signal and provides the physical address to the comparators <b>60</b><i>a-b. </i></p><p>If the physical address does not match on of the physical address \u201ctags\u201d provided by the tag array, control unit <b>64</b> may submit a read request to BIU <b>48</b>, providing the physical address to BIU <b>48</b>. BIU <b>48</b> may then read the data item from main memory <b>44</b> and forward the data item directly to cache memory <b>54</b> as indicated in FIG. <b>2</b>. Cache memory <b>54</b> may store the physical address within tag array <b>56</b>, and store the corresponding data item retrieved from main memory <b>44</b> within data array <b>58</b>. Cache memory <b>54</b> may also forward the stored physical address to an input of comparator <b>60</b><i>a </i>or <b>60</b><i>b</i>, and may forward the data item to an input of multiplexer <b>62</b>. As a result, the comparator <b>60</b> to which the stored physical address is forwarded may assert the TAG HIT output signal, multiplexer <b>62</b> may produce the data item, and control unit <b>64</b> may assert the DATA VALID and CACHE HIT signals.</p><p>The external A20M signal received by microexecution unit <b>50</b> may be active low, meaning that the signal is asserted when the signal voltage is low, and is deasserted when the signal voltage is high. In this case, a transition in the signal voltage of the received A20M signal from high to low causes a microinterrupt within microexecution unit <b>50</b>. During handling of the microinterrupt, microexecution unit <b>50</b> samples the low voltage level of the A20M signal and changes the value of the A20M bit in control register <b>72</b> from an inactive state to an active state (e.g., from logic \u201c1\u201d to logic \u201c0\u201d). Gating logic <b>68</b> may be, for example, a two input AND gate receiving signal A<b>20</b> of the physical address produced by multiplexer <b>66</b> at a first input and the A20M bit of control register <b>72</b> at the second input. In response to the active logic \u201c0\u201d state of the A20M bit, gating logic <b>68</b> produces a \u201cmasked\u201d or \u201ccleared\u201d signal A<b>20</b> at an output. BIU <b>48</b> thus returns an address mapping in the context of the new state of the A20M pin of control register <b>72</b>.</p><p>A transition in the voltage level of the received A20M signal from low to high also causes a microinterrupt within microexecution unit <b>50</b>. During handling of the microinterrupt, microexecution unit <b>50</b> samples the high voltage level of the A20M signal and changes the value of the A20M bit in control register <b>72</b> from the active state the inactive state (e.g., from logic \u201c0\u201d to logic \u201c1\u201d). Gating logic <b>68</b> receives the signal A<b>20</b> of the physical address produced by multiplexer <b>66</b> and the A20M bit of control register <b>72</b>. In response to the inactive logic \u201c1\u201d state of the A20M bit, gating logic <b>68</b> \u201cpasses\u201d the signal A<b>20</b> of the physical address produced by multiplexer <b>66</b>. Again, BIU <b>48</b> returns an address mapping in the context of the new state of the A20M pin of control register <b>72</b>.</p><p>When processor <b>42</b> is an x86 processor and the A20M signal is asserted via execution of an \u201cOUT\u201d instruction, handling of the microinterrupt may be deferred until processor <b>42</b> completes the instruction following the \u201cOUT\u201d instruction. This allows the \u201cOUT\u201d instruction and the target instruction to be completed before the state of the A20M bit in control register <b>72</b> is changed. For clarity, the instruction following the \u201cOUT\u201d instruction will be referred to as the \u201ctarget\u201d instruction. Such microinterrupt completion deferral may be accomplished by associating one or more \u201cstatus\u201d bits with instructions executed by processor <b>42</b>. One of the status bits associated with the target instruction may be used to signal that a deferred microinterrupt must be completed after execution of the target instruction is completed.</p><p>FIG. 3 is a flow chart of a method for configuring address translation hardware to reflect a change in state of the A20M signal. A change in state of the A20M signal is detected in a step <b>74</b>. This detecting may be efficiently accomplished via an interrupt (e.g., a microinterrupt) as described above. Completion of the execution of the target instruction is determined during a step <b>76</b>. As described above, the target instruction is a certain number of instructions following the instruction during which the change in state of the A20M signal is detected. As described above, the target instruction may be an instruction following an \u201cOUT\u201d instruction which causes the change in state of the A20M signal. Once execution of the target instruction is completed, the TLB (e.g., TLB <b>52</b>) is flushed during a step <b>78</b>. For example, microexecution unit <b>50</b> may issue a signal, either to control unit <b>64</b> or directly to TLB <b>52</b>, which causes TLB <b>52</b> to be flushed. The new state of the A20M signal is saved during a step <b>80</b>. Microexecution unit <b>50</b> may, for example, sample the new state of the A20M signal as described above and change the value of the A20M bit in control register <b>72</b> to reflect the new state of the A20M signal. It is noted that the method of FIG. 3 may be carried out in software or in hardware.</p><p>FIG. 4 is a block diagram of one embodiment of a processor <b>90</b> incorporating the address translation mechanism described above. Processor <b>90</b> includes an instruction cache <b>92</b> and a data cache <b>94</b>. Both instruction cache <b>92</b> and data cache <b>94</b> may be instances of cache unit <b>46</b> shown in FIG. <b>2</b> and described above. Other embodiments of processor <b>90</b> are possible and contemplated. Processor <b>90</b> also includes BIU <b>84</b>, a prefetch/predecode unit <b>96</b>, a branch prediction unit <b>98</b>, an instruction alignment unit <b>100</b>, multiple decode units <b>102</b>A-<b>102</b>C, reservation stations <b>104</b>A-<b>104</b>C, functional units <b>106</b>A-<b>106</b>C, a load/store unit <b>108</b>, a register file <b>110</b>, a reorder buffer <b>112</b>, a microcode read only memory (MROM) unit <b>114</b>, result buses <b>116</b>, and a control register (CR) <b>118</b>. BIU <b>84</b> may be an instance of BIU <b>48</b>, MROM unit <b>114</b> may be an instance of microexecution unit <b>50</b>, and control register <b>118</b> may be an instance of control register <b>72</b> as shown in FIG. <b>2</b> and described above. Elements referred to herein with a particular reference number followed by a letter will be collectively referred to by the reference number alone. For example, decode units <b>102</b>A-<b>102</b>C will be collectively referred to as decode units <b>102</b>.</p><p>Both instruction cache <b>92</b> and data cache <b>94</b> may include one or more TLBs (e.g., TLB <b>52</b> of FIG. <b>2</b>). MROM unit <b>114</b> may receive the A20M signal generated external to processor <b>90</b>, and may monitor the state of the A20M signal. Upon detecting a change in state of the A20M signal (e.g., via a microinterrupt), MROM <b>114</b> may determine and \u201ctag\u201d a target instruction. Such tagging may be accomplished via status bits associated with the target instruction as described above. Once execution of the target instruction is completed, the one or more TLBs within instruction cache <b>92</b> and data cache <b>94</b> may be flushed. MROM unit <b>114</b> may sample the new state of the A20M signal as described above and change the value of the A20M bit within control register <b>118</b> to reflect the new state of the A20M signal. The address translation hardware within BIU <b>84</b> may translate linear addresses to physical addresses in accordance with the A20M bit of control register <b>118</b>, and forward the physical addresses to the one or more TLBs within instruction cache <b>92</b> and data cache <b>94</b>.</p><p>Prefetch/predecode unit <b>96</b> is coupled to BIU <b>84</b>, instruction cache <b>92</b>, and branch prediction unit <b>98</b>. Branch prediction unit <b>98</b> is coupled to instruction cache <b>92</b>, decode units <b>102</b>, and functional units <b>106</b>. Instruction cache <b>92</b> is further coupled to MROM unit <b>114</b> and instruction alignment unit <b>100</b>. MROM unit <b>114</b> is coupled to decode units <b>102</b>. Instruction alignment unit <b>100</b> is in turn coupled to decode units <b>102</b>. Each decode unit <b>102</b>A-<b>102</b>C is coupled to load/store unit <b>108</b> and to respective reservation stations <b>104</b>A-<b>104</b>C. Reservation stations <b>104</b>A-<b>104</b>C are further coupled to respective functional units <b>106</b>A-<b>106</b>C. Additionally, decode units <b>102</b> and reservation stations <b>104</b> are coupled to register file <b>110</b> and reorder buffer <b>112</b>. Functional units <b>106</b> are coupled to load/store unit <b>108</b>, register file <b>110</b>, and reorder buffer <b>112</b>. Data cache <b>94</b> is coupled to load/store unit <b>108</b> and BIU <b>84</b>. BIU <b>84</b> is coupled to a level <b>2</b> (L2) cache and a bus. Main memory <b>44</b> (see FIG. 2) may be coupled to the bus, and thus to BIU <b>84</b> via the bus.</p><p>Prefetch/predecode unit <b>96</b> prefetches instructions (i.e., fetches instructions before they are needed) from either the L2 cache or main memory <b>44</b> via BIU <b>84</b>, and stores the prefetched instructions within instruction cache <b>92</b>. Instruction cache <b>92</b> is a high speed cache memory for storing a relatively small number of instructions. Instructions stored within instruction cache <b>92</b> are fetched by instruction alignment unit <b>100</b> and dispatched to decode units <b>102</b>. In one embodiment, instruction cache <b>92</b> is configured to store up to 64 kilobytes of instructions in a two-way set associative structure having multiple lines with 32 bytes in each line, wherein each byte includes 8 bits. Alternatively, any other desired configuration and size may be employed. For example, it is noted that instruction cache <b>92</b> may be implemented as a fully associative, set associative, or direct mapped configuration.</p><p>Prefetch/predecode unit <b>96</b> may employ a variety of prefetch schemes. As prefetch/predecode unit <b>96</b> stores prefetched instructions within instruction cache <b>92</b>, prefetch/predecode unit <b>96</b> may generate three predecode bits for each byte of the instructions: a start bit, an end bit, and a functional bit. The predecode bits may form tags indicative of the boundaries of each instruction. The predecode tags may also convey additional information such as whether a given instruction can be decoded directly by decode units <b>102</b> or whether the instruction is executed by invoking a microcode procedure controlled by MROM unit <b>114</b>. Prefetch/predecode unit <b>96</b> may be configured to detect branch instructions and to store branch prediction information corresponding to the branch instructions within branch prediction unit <b>98</b>. Other embodiments may employ any suitable predecode scheme.</p><p>Processor <b>90</b> may execute instructions from a variable byte length instruction set. A variable byte length instruction set is an instruction set in which different instructions may occupy differing numbers of bytes. An exemplary variable byte length instruction set is the x86 instruction set.</p><p>In an exemplary predecode encoding of instructions from a variable byte length instruction set, the start bit for a first byte of an instruction is set, and the end bit for a last byte of the instruction is also set. Instructions which may be directly decoded by decode units <b>102</b> will be referred to as \u201cfast path\u201d instructions, and the remaining x86 instructions will be referred to as MROM instructions. For fast path instructions, the functional bit is set for each prefix byte included in the instruction, and is cleared for other bytes. For MROM instructions, the functional bit is cleared for each prefix byte and set for other bytes. Accordingly, if the functional bit corresponding to the end byte is clear, the instruction is a fast path instruction. Conversely, if the functional bit corresponding to the end byte is set, the instruction is an MROM instruction. The opcode of a fast path instruction may thereby be located within an instruction as the byte associated with the first clear functional bit in the instruction. For example, a fast path instruction including two prefix bytes, a Mod RIM byte, and an immediate byte would have start, end, and functional bits as follows:</p><p><tables id=\"TABLE-US-00001\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"offset\" colwidth=\"49pt\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"91pt\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"77pt\"></colspec><thead><row><entry></entry><entry align=\"center\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\"></entry></row></thead><tbody valign=\"top\"><row><entry></entry><entry>Start bits</entry><entry>10000</entry></row><row><entry></entry><entry>Bnd bits</entry><entry>00001</entry></row><row><entry></entry><entry>Functional bits</entry><entry>11000</entry></row><row><entry></entry><entry align=\"center\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\"></entry></row></tbody></tgroup></table></tables></p><p>According to one particular embodiment, early identification of an instruction that includes a scale-index-base (SIB) byte is advantageous for MROM unit <b>114</b>. For such an embodiment, if an instruction includes at least two bytes after the opcode byte, the functional bit for the Mod R/M byte indicates the presence of an SIB byte. If the functional bit for the Mod R/M byte is set, then an SIB byte is present. Alternatively, if the functional bit for the Mod R/M byte is clear, then an SIB byte is not present.</p><p>MROM instructions are instructions which are determined to be too complex for decode by decode units <b>102</b>. MROM instructions are executed by invoking MROM unit <b>114</b>. More specifically, when an MROM instruction is encountered, MROM unit <b>114</b> parses and issues the instruction into a subset of defined fast path instructions to effectuate the desired operation. MROM unit <b>114</b> dispatches the subset of fast path instructions to decode units <b>102</b>.</p><p>Processor <b>90</b> employs branch prediction in order to speculatively fetch instructions subsequent to conditional branch instructions. Branch prediction unit <b>98</b> is included to perform branch prediction operations. In one embodiment, branch prediction unit <b>98</b> employs a branch target buffer which stores up to two branch target addresses and corresponding taken/not taken predictions per 16-byte portion of a cache line in instruction cache <b>92</b>. The branch target buffer may, for example, comprise 2048 entries or any other suitable number of entries.</p><p>Prefetch/predecode unit <b>96</b> may determine initial branch targets when a particular line is predecoded. Subsequent updates to the branch targets corresponding to a cache line may occur due to the execution of instructions within the cache line. Instruction cache <b>92</b> may provide an indication of the instruction address being fetched, so that branch prediction unit <b>98</b> may determine which branch target addresses to select for forming a branch prediction. Decode units <b>102</b> and functional units <b>106</b> may provide update information to branch prediction unit <b>98</b>. Decode units <b>102</b> may detect branch instructions which were not predicted by branch prediction unit <b>98</b>. Functional units <b>106</b> may execute the branch instructions and determine if the predicted branch direction is incorrect. The branch direction may be \u201ctaken\u201d,in which subsequent instructions are fetched from the target address of the branch instruction. Conversely, the branch direction may be \u201cnot taken\u201d, in which subsequent instructions are fetched from memory locations consecutive to the branch instruction.</p><p>When a mispredicted branch instruction is detected, instructions subsequent to the mispredicted branch may be discarded from the various units of processor <b>90</b>. In an alternative configuration, branch prediction unit <b>98</b> may be coupled to reorder buffer <b>112</b> instead of decode units <b>102</b> and functional units <b>106</b>, and may receive branch misprediction information from reorder buffer <b>112</b>. A variety of suitable branch prediction algorithms may be employed by branch prediction unit <b>98</b>.</p><p>As instruction alignment unit <b>100</b> fetches instructions cache <b>92</b>, the corresponding predecode data may be scanned to provide information to instruction alignment unit <b>100</b> (and to MROM unit <b>114</b>) regarding the instructions being fetched. Instruction alignment unit <b>100</b> may utilize the scanning data to align an instruction to each of decode units <b>102</b>. In one embodiment, instruction alignment unit <b>100</b> may align instructions from three sets of eight instruction bytes to decode units <b>102</b>. Decode unit <b>102</b>A may receive an instruction which is prior to instructions concurrently received by decode units <b>102</b>B and <b>102</b>C (in program order). Similarly, decode unit <b>102</b>B may receive an instruction which is prior to the instruction concurrently received by decode unit <b>102</b>C in program order.</p><p>Decode units <b>102</b> are configured to decode instructions received from instruction alignment unit <b>100</b>. Register operand information may be detected and routed to register file <b>110</b> and reorder buffer <b>112</b>. Additionally, if the instructions require one or more memory operations to be performed, decode units <b>102</b> may dispatch the memory operations to load/store unit <b>108</b>. Each instruction is decoded into a set of \u201ccontrol values\u201d for functional units <b>106</b>, and these control values are dispatched to reservation stations <b>104</b>. Operand address information and displacement or immediate data which may be included with the instruction may be forwarded to reservation stations <b>104</b> along with the control values. In one particular embodiment, each instruction is decoded into a maximum of two operations which may be separately executed by functional units <b>106</b>A-<b>106</b>C.</p><p>Processor <b>90</b> supports out of order instruction execution. Reorder buffer <b>112</b> is used to keep track of the original program sequence for register read and write operations, to implement register renaming, to allow for speculative instruction execution and branch misprediction recovery, and to facilitate precise exceptions. A temporary storage location within reorder buffer <b>112</b> may be reserved upon decode of an instruction that involves the update of a register to thereby store speculative register states. If a branch prediction is incorrect, the results of speculatively-executed instructions along the mispredicted path may be invalidated in the buffer before they are written to register file <b>110</b>. Similarly, if a particular instruction causes an exception, instructions subsequent to the particular instruction may be discarded. In this manner, exceptions are \u201cprecise\u201d (i.e. instructions subsequent to the particular instruction causing the exception are not completed prior to the exception). It is noted that a particular instruction is speculatively executed if it is executed prior to instructions which precede the particular instruction in program order. Preceding instructions may be a branch instruction or an exception-causing instruction, in which case the speculative results may be discarded by reorder buffer <b>112</b>.</p><p>The instruction control values and immediate or displacement data provided at the outputs of decode units <b>102</b> may be routed directly to respective reservation stations <b>104</b>. In one embodiment, each reservation station <b>104</b> is capable of holding instruction information (i.e., instruction control values as well as operand values, operand tags and/or immediate data) for up to five pending instructions awaiting issue to the corresponding functional unit. In the embodiment of FIG. 4, each reservation station <b>104</b> is associated with a dedicated functional unit <b>106</b>. Accordingly, three dedicated \u201cissue positions\u201d are formed by reservation stations <b>104</b> and functional units <b>106</b>. In other words, issue position <b>0</b> is formed by reservation station <b>104</b>A and functional unit <b>106</b>A. Instructions aligned and dispatched to reservation station <b>104</b>A are executed by functional unit <b>106</b>A. Similarly, issue position <b>1</b> is formed by reservation station <b>1</b><b>04</b>B and functional unit <b>106</b>B; and issue position <b>2</b> is formed by reservation station <b>104</b>C and functional unit <b>106</b>C.</p><p>Upon decode of a particular instruction, if a required operand is a register location, register address information is routed to reorder buffer <b>112</b> and register file <b>110</b> simultaneously. It is well known that the x86 register file includes eight 32-bit real registers (i.e., typically referred to as EAX, EBX, ECX, EDX, EBP, ESI, EDI and ESP). In embodiments of processor <b>90</b> which employ the x86 processor architecture, register file <b>110</b> may comprise storage locations for each of the 32-bit real registers. Additional storage locations may be included within register file <b>110</b> for use by MROM unit <b>114</b>.</p><p>Reorder buffer <b>112</b> may contain temporary storage locations for results which change the contents of the real registers to thereby allow out of order instruction execution. A temporary storage location of reorder buffer <b>112</b> may be reserved for each instruction which, upon decode, is determined to modify the contents of one of the real registers. Therefore, at various points during execution of a particular program, reorder buffer <b>112</b> may have one or more locations which contain the speculatively executed contents of a given register.</p><p>If, following decode of a given instruction, it is determined that reorder buffer <b>112</b> has a previous location or locations assigned to a register used as an operand in the given instruction, reorder buffer <b>112</b> may forward to the corresponding reservation station either: <b>1</b>) the value in the most recently assigned location, or <b>2</b>) a tag for the most recently assigned location if the value has not yet been produced by the functional unit that will eventually execute the previous instruction. If reorder buffer <b>112</b> has a location reserved for a given register, the operand value (or reorder buffer tag) may be provided from reorder buffer <b>112</b> rather than from register file <b>110</b>. If there is no location reserved for a required register in reorder buffer <b>112</b>, the value may be taken directly from register file <b>110</b>. If the operand corresponds to a memory location, the operand value may be provided to the reservation station through load/store unit <b>108</b>.</p><p>In one particular embodiment, reorder buffer <b>112</b> is configured to store and manipulate concurrently decoded instructions as a unit. This configuration will be referred to herein as \u201cline-oriented\u201d. By manipulating several instructions together, the hardware employed within reorder buffer <b>112</b> may be simplified. For example, a line-oriented reorder buffer may be included in the present embodiment which allocates storage sufficient for instruction information pertaining to three instructions (one from each decode unit <b>102</b>) whenever one or more instructions are dispatched by decode units <b>102</b>. By contrast, a variable amount of storage may be allocated in conventional reorder buffers, dependent upon the number of instructions actually dispatched. A comparatively larger number of logic gates may be required to allocate the variable amount of storage.</p><p>When each of the concurrently decoded instructions has executed, the instruction results may be stored into register file <b>110</b> simultaneously. The storage is then free for allocation to another set of concurrently decoded instructions. Additionally, the amount of control logic circuitry employed per instruction may be reduced as the control logic is amortized over several concurrently decoded instructions. A reorder buffer tag identifying a particular instruction may be divided into two fields: a line tag and an offset tag. The line tag may identify the set of concurrently decoded instructions including the particular instruction, and the offset tag may identify which instruction within the set corresponds to the particular instruction. Storing instruction results into register file <b>110</b> and freeing the corresponding storage is referred to as \u201cretiring\u201d the instructions. It is noted that any reorder buffer configuration may be employed in various embodiments of processor <b>90</b>.</p><p>As described above, reservation stations <b>104</b> store instructions until the instructions are executed by the corresponding functional unit <b>106</b>. An instruction may be selected for execution if: (i) the operands of the instruction have been provided; and (ii) the operands have not yet been provided for instructions which are within the same reservation station <b>104</b>A-<b>104</b>C and which are prior to the instruction in program order. It is noted that when an instruction is executed by one of the functional units <b>106</b>, the result of that instruction may be passed directly to any reservation stations <b>104</b> that are waiting for that result at the same time the result is passed to update reorder buffer <b>112</b> (this technique is commonly referred to as \u201cresult forwarding\u201d). An instruction may be selected for execution and passed to a functional unit <b>106</b>A-<b>106</b>C during the clock cycle that the associated result is forwarded. Reservation stations <b>104</b> may route the forwarded result to the functional unit <b>106</b> in this case. In embodiments in which instructions may be decoded into multiple operations to be executed by functional units <b>106</b>, the operations may be scheduled separately.</p><p>In one embodiment, each of the functional units <b>106</b> is configured to perform integer arithmetic operations of addition and subtraction, as well as shifts, rotates, logical operations, and branch operations. The operations are performed in response to the control values decoded for a particular instruction by decode units <b>102</b>. It is noted that a floating point unit (not shown) may also be employed to accommodate floating point operations. The floating point unit may be operated as a coprocessor, receiving instructions from MROM unit <b>114</b> or reorder buffer <b>112</b> and subsequently communicating with reorder buffer <b>112</b> to complete the instructions. Additionally, functional units <b>106</b> may be configured to perform address generation for load and store memory operations performed by load/store unit <b>108</b>. In one particular embodiment, each functional unit <b>106</b> may comprise an address generation unit for generating addresses and an execute unit for performing the remaining functions. The two units may operate independently upon different instructions or operations during a clock cycle.</p><p>Each of the functional units <b>106</b> may also provide information regarding the execution of conditional branch instructions to the branch prediction unit <b>98</b>. If a branch prediction was incorrect, branch prediction unit <b>98</b> may flush instructions subsequent to the mispredicted branch that have entered the instruction processing pipeline, and initiate the fetching of required instructions from instruction cache <b>92</b> or main memory. It is noted that in such situations, results of instructions in the original program sequence which occur after the mispredicted branch instruction may be discarded, including those which were speculatively executed and temporarily stored in load/store unit <b>108</b> and reorder buffer <b>112</b>. It is further noted that branch execution results may be provided by functional units <b>106</b> to reorder buffer <b>112</b>, which may indicate branch mispredictions to functional units <b>106</b>.</p><p>Results produced by functional units <b>106</b> may be sent to reorder buffer <b>112</b> if a register value is being updated, and to load/store unit <b>108</b> if the contents of a memory location are changed. If the result is to be stored in a register, reorder buffer <b>112</b> may store the result in the location reserved for the value of the register when the instruction was decoded. Multiple result buses <b>116</b> are included for forwarding of results from functional units <b>106</b> and load/store unit <b>108</b>. Result buses <b>116</b> convey the result generated, as well as the reorder buffer tag identifying the instruction being executed.</p><p>Load/store unit <b>108</b> provides an interface between functional units <b>106</b> and data cache <b>94</b>. In one embodiment, load/store unit <b>108</b> is configured with a first load/store buffer having storage locations for data and address information for pending loads or stores which have not accessed data cache <b>94</b> and a second load/store buffer having storage locations for data and address information for loads and stores which have access data cache <b>94</b>. For example, the first buffer may comprise <b>12</b> locations and the second buffer may comprise <b>32</b> locations. Decode units <b>102</b> may arbitrate for access to load/store unit <b>108</b>. When the first buffer is full, a decode unit may wait until load/store unit <b>108</b> has room for the pending load or store request information.</p><p>Load/store unit <b>108</b> may also perform dependency checking for load memory operations against pending store memory operations to ensure that data coherency is maintained. A memory operation is a transfer of data between processor <b>90</b> and the L2 cache or main memory <b>44</b> via BIU <b>84</b>. Memory operations may be the result of an instruction which utilizes an operand stored in memory, or may be the result of a load/store instruction which causes the data transfer but no other operation. Additionally, load/store unit <b>108</b> may include a special register storage for special registers such as the segment registers and other registers related to the address translation mechanism defined by the x86 processor architecture.</p><p>Data cache <b>94</b> is a high speed cache memory provided to temporarily store data being transferred between load/store unit <b>108</b> and the L2 cache or main memory <b>44</b>. In one embodiment, data cache <b>94</b> has a capacity of storing up to 64 kilobytes of data in an two-way set associative structure. It is understood that data cache <b>94</b> may be implemented in a variety of specific memory configurations, including a set associative configuration, a fully associative configuration, a direct-mapped configuration, and any suitable size of any other configuration.</p><p>Instruction cache <b>92</b> may be an embodiment of cache unit <b>70</b> shown in FIG. <b>4</b> and described above for storing instructions, and data cache <b>94</b> may be an embodiment of cache unit <b>70</b> for storing data. In embodiments of processor <b>90</b> employing the x86 processor architecture, instruction cache <b>92</b> and data cache <b>94</b> may be linearly addressed and physically tagged. A linear address is a virtual address as described above, and may be provided to instruction cache <b>92</b> or data cache <b>94</b>. TLB <b>10</b> within instruction cache <b>92</b> and data cache <b>94</b> may be used to store a relatively small number of virtual-to-physical address translations as described above. TLB <b>10</b> may provide the physical address corresponding to the linear address to a physically-tagged cache memory (e.g., cache memory <b>72</b>) within instruction cache <b>92</b> or data cache <b>94</b>. Instruction cache <b>92</b> and data cache <b>94</b> may translate linear addresses to physical addresses for accessing either the L2 cache or main memory <b>44</b> via BIU <b>84</b>.</p><p>BIU <b>84</b> is coupled to the bus, and is configured to communicate between processor <b>90</b> and other components also coupled to the bus via the bus. For example, the bus may be compatible with the EV-<b>6</b> bus developed by Digital Equipment Corporation. Alternatively, any suitable interconnect structure may be used including packet-based, unidirectional or bi-directional links, etc. An optional L2 cache interface may be included within BIU <b>84</b> for interfacing to the L2 cache.</p><p>FIG. 5 is a block diagram of one embodiment of a computer system <b>200</b> including processor <b>90</b> of FIG. <b>4</b>. Processor <b>90</b> is coupled to a variety of system components through a bus bridge <b>202</b>. Other embodiments of computer system <b>200</b> are possible and contemplated.</p><p>In the embodiment of FIG. 5, main memory <b>44</b> is coupled to bus bridge <b>202</b> through a memory bus <b>206</b>, and a graphics controller <b>208</b> is coupled to bus bridge <b>202</b> through an AGP bus <b>210</b>. Finally, a plurality of PCI devices <b>212</b>A-<b>212</b>B are coupled to bus bridge <b>202</b> through a peripheral component interconnect (PCI) bus <b>214</b>. A secondary bus bridge <b>216</b> may further be provided to accommodate an electrical interface to one or more EISA or ISA devices <b>218</b> through an extended industry standard architecture (EISA)/industry standard architecture (ISA) bus <b>220</b>. Processor <b>90</b> is coupled to bus bridge <b>202</b> through a CPU bus <b>224</b> and to an optional L2 cache <b>228</b>.</p><p>Bus bridge <b>202</b> provides an interface between processor <b>90</b>, main memory <b>204</b>, graphics controller <b>208</b>, and devices attached to PCI bus <b>214</b>. When an operation is received from one of the devices connected to bus bridge <b>202</b>, bus bridge <b>202</b> identifies the target of the operation (e.g. a particular device or, in the case of PCI bus <b>214</b>, that the target is on PCI bus <b>214</b>). Bus bridge <b>202</b> routes the operation to the targeted device. Bus bridge <b>202</b> generally translates an operation from the protocol used by the source device or bus to the protocol used by the target device or bus.</p><p>In addition to providing an interface to an ISA/EISA bus for PCI bus <b>214</b>, secondary bus bridge <b>216</b> may further incorporate additional functionality, as desired. An input/output controller (not shown), either external from or integrated with secondary bus bridge <b>216</b>, may also be included within computer system <b>200</b> to provide operational support for a keyboard and mouse <b>222</b> and for various serial and parallel ports, as desired. An external cache unit (not shown) may further be coupled to CPU bus <b>224</b> between processor <b>90</b> and bus bridge <b>202</b> in other embodiments. Alternatively, the external cache may be coupled to bus bridge <b>202</b> and cache control logic for the external cache may be integrated into bus bridge <b>202</b>. L2 cache <b>228</b> is further shown in a backside configuration to processor <b>90</b>. It is noted that L2 cache <b>228</b> may be separate from processor <b>90</b>, integrated into a cartridge (e.g. slot <b>1</b> or slot A) with processor <b>90</b>, or even integrated onto a semiconductor substrate with processor <b>90</b>.</p><p>Main memory <b>44</b> is used to store software instructions and data as described above. A suitable main memory <b>44</b> comprises dynamic random access memory (DRAM). For example, a plurality of banks of synchronous DRAM (SDRAM) or Rambus DRAM (RDRAM) may be suitable.</p><p>PCI devices <b>212</b>A-<b>212</b>B are illustrative of a variety of peripheral devices such as, for example, network interface cards, video accelerators, audio cards, hard or floppy disk drives or drive controllers, small computer systems interface (SCSI) adapters and telephony cards. Similarly, ISA device <b>218</b> is illustrative of various types of peripheral devices, such as a modem, a sound card, and a variety of data acquisition cards such as GPIB or field bus interface cards.</p><p>Graphics controller <b>208</b> is provided to control the rendering of text and images on a display <b>226</b>. Graphics controller <b>208</b> may embody a typical graphics accelerator generally known in the art to render three-dimensional data structures which can be effectively shifted into and from main memory <b>204</b>. Graphics controller <b>208</b> may therefore be a master of AGP bus <b>210</b> in that it can request and receive access to a target interface within bus bridge <b>202</b> to thereby obtain access to main memory <b>204</b>. A dedicated graphics bus accommodates rapid retrieval of data from main memory <b>204</b>. For certain operations, graphics controller <b>208</b> may further be configured to generate PCI protocol transactions on AGP bus <b>210</b>. The AGP interface of bus bridge <b>202</b> may thus include functionality to support both AGP protocol transactions as well as PCI protocol target and initiator transactions. Display <b>226</b> is any electronic display upon which an image or text can be presented. A suitable display <b>226</b> includes a cathode ray tube (\u201cCRT\u201d), a liquid crystal display (\u201cLCD\u201d), etc.</p><p>It is noted that, while the PCI, AGP, and EISA/ISA buses have been used as examples in the above description, any bus architectures may be substituted as desired. It is further noted that computer system <b>200</b> may be a multiprocessing computer system including additional processors (e.g. processor <b>90</b><i>a </i>shown as an optional component of computer system <b>200</b>). Processor <b>90</b><i>a </i>may be similar to processor <b>90</b>, or processor <b>90</b><i>a </i>may be an identical copy of processor <b>90</b>. Processor <b>90</b><i>a </i>may be connected to bus bridge <b>202</b> via an independent bus (as shown in FIG. 5) or may share CPU bus <b>224</b> with processor <b>90</b>. Furthermore, processor <b>90</b><i>a </i>may be coupled to an optional L2 cache <b>228</b><i>a </i>similar to L2 cache <b>228</b>.</p><p>It is noted that while certain embodiments have been described above as employing the x86 instruction set, any other instruction set architecture which employs virtual-to-physical address translation may employ the above described features.</p><p>Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Gerald D.", "last_name": "Zuraski, Jr.", "name": ""}, {"first_name": "Frederick D.", "last_name": "Weber", "name": ""}, {"first_name": "William A.", "last_name": "Hughes", "name": ""}, {"first_name": "William K.", "last_name": "Lewchuk", "name": ""}, {"first_name": "Scott A.", "last_name": "White", "name": ""}, {"first_name": "Michael T.", "last_name": "Clark", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "ADVANCED MICRO DEVICES, INC."}, {"first_name": "", "last_name": "GLOBALFOUNDRIES U.S. INC.", "name": ""}, {"first_name": "", "last_name": "GLOBALFOUNDRIES INC.", "name": ""}, {"first_name": "", "last_name": "WILMINGTON TRUST, NATIONAL ASSOCIATION", "name": ""}, {"first_name": "", "last_name": "GLOBALFOUNDRIES INC.", "name": ""}, {"first_name": "", "last_name": "ADVANCED MICRO DEVICES, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}, {"primary": false, "label": "G06F  12/10"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/10        20060101A I20051008RMUS"}], "national_classes": [{"primary": true, "label": "711207"}, {"primary": false, "label": "711E12063"}, {"primary": false, "label": "710049"}, {"primary": false, "label": "711209"}], "ecla_classes": [{"label": "G06F  12/10L4P"}], "cpc_classes": [{"label": "G06F  12/1054"}, {"label": "G06F  12/1054"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1999-06-01", "application_date": "1999-06-01", "family_members": [{"ucid": "US-6446189-B1", "titles": [{"lang": "EN", "text": "Computer system including a novel address translation mechanism"}]}]}