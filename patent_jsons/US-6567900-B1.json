{"patent_number": "US-6567900-B1", "publication_id": 73397312, "family_id": 24616881, "publication_date": "2003-05-20", "titles": [{"lang": "EN", "text": "Efficient address interleaving with simultaneous multiple locality options"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11506547\" source=\"national office\"><p>A computer system includes multiple processors, each of which includes an associated memory. Each of the processors is capable of accessing the memory of all other processors. Memory can be stored and accessed using different addressing schemes. For data that will only be used by the local processor, data is stored in memory using processor contiguous addressing, so that data is stored in the local memory. For data that may be accessed by multiple processors, data is stored using striping among a local processor set. A stripe control register in the memory controller of each memory comprises a mask that indicates which memory blocks should be accessed using processor contiguous addressing and which should be accessed by using striped addressing. For both striped and contiguous addressing, the address space includes a processor identification field to identify the processor where the associated memory resides, together with an offset indicating where in memory the address is located. The processor identification field for striped addressing includes two bits located in low order address space identifying a four processor local stripe set. The other processor identification bits define which four processors comprise each stripe set.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50494683\"><p>A computer system includes multiple processors, each of which includes an associated memory. Each of the processors is capable of accessing the memory of all other processors. Memory can be stored and accessed using different addressing schemes. For data that will only be used by the local processor, data is stored in memory using processor contiguous addressing, so that data is stored in the local memory. For data that may be accessed by multiple processors, data is stored using striping among a local processor set. A stripe control register in the memory controller of each memory comprises a mask that indicates which memory blocks should be accessed using processor contiguous addressing and which should be accessed by using striped addressing. For both striped and contiguous addressing, the address space includes a processor identification field to identify the processor where the associated memory resides, together with an offset indicating where in memory the address is located. The processor identification field for striped addressing includes two bits located in low order address space identifying a four processor local stripe set. The other processor identification bits define which four processors comprise each stripe set.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00001\" num=\"1\"><claim-text>1. A computer system, comprising:</claim-text><claim-text>a plurality of processors that are coupled together; </claim-text><claim-text>a memory associated with each of said plurality of processors, wherein each of said plurality of processors is capable of accessing the memory associated with any other processor; </claim-text><claim-text>wherein, in accordance with a stripe bit in an address signal, data is stored in any of the memories associated with said plurality of processors on either a processor contiguous basis, or by striping across multiple processors in a stripe set; and </claim-text><claim-text>wherein said address signal includes a field when striping across processors, said field includes a first sub-field that specifies the processors that comprise a stripe set and a second sub-field that specifies the number of processors in the stripe set. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00002\" num=\"2\"><claim-text>2. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00001\">claim 1</claim-ref>, wherein memory is accessed using address command signals whose coding differs depending on whether memory is stored on a processor contiguous basis or a striped basis.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00003\" num=\"3\"><claim-text>3. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00002\">claim 2</claim-ref>, wherein the address command signal includes a processor identification bit field that identifies the target of a memory access and contains the first and second sub-fields.</claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00004\" num=\"4\"><claim-text>4. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00003\">claim 3</claim-ref>, wherein the processor identification bit field includes n bits identifying 2<sup>n </sup>processors, and a portion of said n bits is located in low order address space for a striped memory access.</claim-text></claim>"}, {"num": 5, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00005\" num=\"5\"><claim-text>5. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00004\">claim 4</claim-ref>, wherein said low order address space resides in the lower two bytes of address space.</claim-text></claim>"}, {"num": 6, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00006\" num=\"6\"><claim-text>6. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00005\">claim 5</claim-ref>, wherein said low order address space resides in the lowest byte of address space.</claim-text></claim>"}, {"num": 7, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00007\" num=\"7\"><claim-text>7. The computer system as in <claim-ref idref=\"US-6567900-B1-CLM-00004\">claim 4</claim-ref>, wherein said portion of said n bits comprises two bits to identify a four processor striped set across which data is striped.</claim-text></claim>"}, {"num": 8, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00008\" num=\"8\"><claim-text>8. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00004\">claim 4</claim-ref>, wherein said address command signal includes bits representing a memory offset, and said low order space resides in bits that are less significant than at least a portion of the memory offset bits.</claim-text></claim>"}, {"num": 9, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00009\" num=\"9\"><claim-text>9. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00001\">claim 1</claim-ref>, wherein at least one of said plurality of said processors includes a first and second memory port, and said memory associated with said at least one processor comprises a first memory sub-system and a second memory sub-system, which are respectively coupled to said first and second memory port.</claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00010\" num=\"10\"><claim-text>10. The computer system as in <claim-ref idref=\"US-6567900-B1-CLM-00009\">claim 9</claim-ref>, wherein said first memory sub-system comprises DRAM memory, and said second memory sub-system comprises cache memory.</claim-text></claim>"}, {"num": 11, "parent": 10, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00011\" num=\"11\"><claim-text>11. The computer system as in <claim-ref idref=\"US-6567900-B1-CLM-00010\">claim 10</claim-ref>, wherein said at least one of said plurality of said processors includes an associated memory controller for each memory port.</claim-text></claim>"}, {"num": 12, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00012\" num=\"12\"><claim-text>12. The computer system as in <claim-ref idref=\"US-6567900-B1-CLM-00001\">claim 1</claim-ref>, wherein said plurality of said processors include a memory controller that interfaces said processor to said associated memory.</claim-text></claim>"}, {"num": 13, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00013\" num=\"13\"><claim-text>13. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00001\">claim 1</claim-ref>, wherein said plurality of said processors include a stripe control register that includes a mask for identifying which memory blocks are to be accessed with striped addressing and which memory blocks are to be accessed with processor contiguous addressing.</claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00014\" num=\"14\"><claim-text>14. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00013\">claim 13</claim-ref>, wherein said stripe control register comprises an internal processor register.</claim-text></claim>"}, {"num": 15, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00015\" num=\"15\"><claim-text>15. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00004\">claim 4</claim-ref>, wherein the processor identification bit field includes n bits identifying 2<sup>n </sup>processors, and said n bits are located in high order address space for a processor contiguous address.</claim-text></claim>"}, {"num": 16, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00016\" num=\"16\"><claim-text>16. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00001\">claim 1</claim-ref>, where data includes instructions.</claim-text></claim>"}, {"num": 17, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00017\" num=\"17\"><claim-text>17. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00001\">claim 1</claim-ref>, wherein said processors are grouped into local stripe sets based upon the lowermost bits in a processor identification bit field.</claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00018\" num=\"18\"><claim-text>18. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00017\">claim 17</claim-ref>, wherein said local stripe sets includes four processors that are determined by the two lowermost bits in said processor identification field.</claim-text></claim>"}, {"num": 19, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00019\" num=\"19\"><claim-text>19. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00001\">claim 1</claim-ref>, wherein memory accesses using striped addressing and memory accesses using processor contiguous addressing occur simultaneously in said computer system.</claim-text></claim>"}, {"num": 20, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00020\" num=\"20\"><claim-text>20. A computer system, comprising:</claim-text><claim-text>a plurality of processors that are coupled together; </claim-text><claim-text>a memory associated with each of said plurality of processors, wherein each of said plurality of processors is capable of accessing the memory associated with any other processor on either a processor contiguous basis, or on a stripe basis across multiple processors in a stripe set; and </claim-text><claim-text>an I/O controller, associated with each of said plurality of processors, capable of interfacing with I/O devices, wherein each of said plurality of processors is capable of accessing I/O devices associated with any other processor; </claim-text><claim-text>wherein, in accordance with a stripe bit in an address command signal, data is stored in any of the memories associated with said plurality of processors on either a processor contiguous basis, or by striping across multiple processors in a stripe set; and </claim-text><claim-text>wherein said address command signal includes a field when striping across processors, said field includes a first sub-field that specifies the processors that comprise a stripe set and a second sub-field that specifies the number of processors in the stripe set. </claim-text></claim>"}, {"num": 21, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00021\" num=\"21\"><claim-text>21. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00020\">claim 20</claim-ref>, wherein memory accesses include an address command signal that differs depending on whether memory is accessed on a processor contiguous basis or a striped basis, and the address command signal includes a processor identification bit field that identifies the target of a memory access and which includes said first and second sub-fields.</claim-text></claim>"}, {"num": 22, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00022\" num=\"22\"><claim-text>22. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00021\">claim 21</claim-ref>, wherein the processor identification bit field for a stripe memory access includes n bits identifying 2<sup>n </sup>processors, and wherein said first sub-field is located in high order address space, and said second sub-field is located in low order address space.</claim-text></claim>"}, {"num": 23, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00023\" num=\"23\"><claim-text>23. A computer system, comprising:</claim-text><claim-text>a plurality of processors that are coupled together; </claim-text><claim-text>a memory associated with each of said plurality of processors, wherein each of said plurality of processors is capable of accessing the memory associated with any other processor on either a processor contiguous basis, or on a stripe basis across multiple processors in a stripe set; and </claim-text><claim-text>an I/O controller, associated with each of said plurality of processors, capable of interfacing with I/O devices, wherein each of said plurality of processors is capable of accessing I/O devices associated with any other processor; </claim-text><claim-text>wherein memory accesses include an address command signal that differs depending on whether memory is accessed on a processor contiguous basis or a striped basis, and the address command signal includes a processor identification bit field that identifies the target of a memory access; </claim-text><claim-text>wherein the address command signal includes a stripe bit that indicates if the address command signal is a striped memory access or a processor contiguous memory access; </claim-text><claim-text>wherein the processor identification bit field for a stripe memory access includes n bits identifying 2<sup>n </sup>processors, and said processor identification bit field includes a first portion y and a second portion x, and wherein said first portion y is located in high order address space, and said second portion x is located in low order address space; and </claim-text><claim-text>wherein the first portion y includes a bit field that defines the processors that comprise the stripe set, and the second portion x includes a bit field that defines the number of processors in said stripe set. </claim-text></claim>"}, {"num": 24, "parent": 23, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00024\" num=\"24\"><claim-text>24. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00023\">claim 23</claim-ref>, wherein said low order address space resides in the lowest byte of address space and said high order space resides in the highest byte of address space.</claim-text></claim>"}, {"num": 25, "parent": 24, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00025\" num=\"25\"><claim-text>25. The computer system as in <claim-ref idref=\"US-6567900-B1-CLM-00024\">claim 24</claim-ref>, wherein said processor identification bit field a comprises at least seven bits, and the second portion x comprises two bits identifying four processors within each stripe set.</claim-text></claim>"}, {"num": 26, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00026\" num=\"26\"><claim-text>26. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00021\">claim 21</claim-ref>, wherein the address command signal includes a bit that identifies if an access targets memory or an I/O controller.</claim-text></claim>"}, {"num": 27, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00027\" num=\"27\"><claim-text>27. The computer system as in <claim-ref idref=\"US-6567900-B1-CLM-00020\">claim 20</claim-ref>, wherein said plurality of said processors include a memory controller that interfaces said processor to said associated memory.</claim-text></claim>"}, {"num": 28, "parent": 27, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00028\" num=\"28\"><claim-text>28. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00027\">claim 27</claim-ref>, wherein said memory controller includes a stripe control register that includes a mask for identifying which memory blocks in said associated memory are to be accessed with striped addressing and which memory blocks in said associated memory are to be accessed with processor contiguous addressing.</claim-text></claim>"}, {"num": 29, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6567900-B1-CLM-00029\" num=\"29\"><claim-text>29. The computer system of <claim-ref idref=\"US-6567900-B1-CLM-00020\">claim 20</claim-ref>, wherein memory accesses using striped addressing and memory accesses using processor contiguous addressing occur simultaneously in said computer system to different memory blocks.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53926111\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>CROSS-REFERENCE TO RELATED APPLICATIONS</h4><p>This application relates to the following commonly assigned co-pending applications entitled:</p><p>\u201cApparatus And Method For Interfacing A High Speed Scan-Path With Slow-Speed Test Equipment,\u201d Ser. No. 09/653,642, filed Aug. 31, 2000, \u201cPriority Rules For Reducing Network Message Routing Latency,\u201d Ser. No. 09/652,322, filed Aug. 31, 2000, \u201cScalable Directory Based Cache Coherence Protocol,\u201d Ser. No. 09/652,703, filed Aug. 31, 2000, \u201cScalable Efficient I/O Port Protocol,\u201d Ser. No. 09/652,391, filed Aug. 31, 2000, \u201cEfficient Translation Lookaside Buffer Miss Processing In Computer Systems With A Large Range Of Page Sizes,\u201d Ser. No. 09/652,552, filed Aug. 31, 2000,: \u201cFault Containment And Error Recovery Techniques In A Scalable Multiprocessor,\u201d Ser. No. 09/651,949, filed Aug. 31, 2000, \u201cSpeculative Directory Writes In A Directory Based Cache Coherent Nonuniform Memory Access Protocol,\u201d Ser. No. 09/652,834, filed Aug. 31, 2000, \u201cSpecial Encoding Of Known Bad Data,\u201d Ser. No. 09/652,314, filed: Aug. 31, 2000, \u201cBroadcast Invalidate Scheme,\u201d Ser. No. 09/652,165, filed Aug. 31, 2000, \u201cMechanism To Track All Open Pages In A DRAM Memory System,\u201d Ser. No. 09/652,704, filed Aug. 31, 2000, \u201cProgrammable DRAM Address Mapping Mechanism,\u201d Ser. No. 09/653,093, filed Aug. 31, 2000, \u201cComputer Architecture And System For Efficient Management Of Bi-Directional Bus,\u201d Ser. No. 09/652,323, filed Aug. 31, 2000, \u201cA High Performance Way Allocation Strategy For A Multi-Way Associative Cache System,\u201d Ser. No. 09/653,092, filed Aug. 31, 2000, \u201cMethod And System For Absorbing Defects In High Performance Microprocessor With A Large N-Way Set Associative Cache,\u201d Ser. No. 09/651,948, filed Aug. 31, 2000, \u201cA Method For Reducing Directory Writes And Latency In A High Performance, Directory-Based, Coherency Protocol,\u201d Ser. No. 09/652,324, filed Aug. 31, 2000, \u201cMechanism To Reorder Memory Read And Write Transactions For Reduced Latency And Increased Bandwidth,\u201d Ser. No. 09/653,094, filed Aug. 31, 2000, \u201cSystem For Minimizing Memory Bank Conflicts In A Computer System,\u201d Ser. No. 09/652,325, filed Aug. 31, 2000, \u201cComputer Resource Management And Allocation System,\u201d Ser. No. 09/651,945, filed Aug. 31, 2000, \u201cInput Data Recovery Scheme,\u201d Ser. No. 09/653,643, filed Aug. 31, 2000, \u201cFast Lane Prefetching,\u201d Ser. No. 09/652,451, filed Aug. 31, 2000, \u201cMechanism For Synchronizing Multiple Skewed Source-Synchronous Data Channels With Automatic Initialization. Feature,\u201d Ser. No. 09/652,480, filed Aug. 31, 2000, \u201cMechanism To Control The Allocation Of An N-Source Shared Buffer,\u201d Ser. No. 09/651,924, filed Aug. 31, 2000, and \u201cChaining Directory Reads And Writes To Reduce DRAM Bandwidth In A Directory Based CC-NUMA Protocol,\u201d Ser. No. 09/652,315, filed Aug. 31, 2000, all of which are incorporated by reference herein.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT</h4><p>Not applicable.</p><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>The present invention generally relates to a computer system that includes a plurality of microprocessors. More particularly, the invention relates to a multiple processor computer system with distributed memory sub-systems accessible by the processors in the system. Still more particularly, the present invention relates to an improved system and method that supports multiple address interleaving techniques that can be active simultaneously to reduce latency and increase memory bandwidth.</p><p>2. Background of the Invention</p><p>One of the basic issues in any computer system is determining the most efficient technique to address the various memory devices that are present in the system. The memory in a computer system stores data and instructions for subsequent retrieval and use by the processor and other components in the computer system. To facilitate the storage, retrieval and subsequent use of they data and instructions, the processor and other computer system components must be able to identify the address of the stored data. Typically, the computer system implements a defined protocol for assigning addresses to stored data. Whenever data is written or read from memory, the component requesting the transaction transmits an address signal or command to the memory identifying where the data should be written, or conversely, from where the data should be read. The memory typically has an associated memory controller that includes an address decoder that decodes the bits in the address signal to determine the location within memory being accessed. In a conventional memory system, this includes identifying the page of memory, and within the page, the row and column of the data being written or read. The particular coding in the address signal or command typically identifies the starting address of a particular memory device, while other bits identify the offset within the memory device where the particular access is targeted.</p><p>When data is written into memory, typically continuous memory addresses are used to identify contiguous memory locations. Thus, for example, the address <b>8001</b> will be followed by address <b>8002</b> (both of which would be written in binary format) to identify adjacent memory locations within a page of memory. More recently, it has become commonplace to include banks of memory within a computer system, so that a conventional personal computer system may include a single processor with multiple memory banks accessible via different memory ports. Some or all of the memory banks may be populated with some form of dynamic random access memory (\u201cDRAM\u201d). In systems with multiple memory banks, it has become common to implement some form of interleaving to more efficiently distribute the data within the memory banks. Thus, for example, each continuous address of memory may be distributed among different memory banks, instead of within a single memory bank. The advantage of such an interleaving scheme is that it may increase memory bandwidth, because it permits the higher speed processor to conduct overlapping memory transactions to the slower speed memory banks via the different memory ports.</p><p>To implement an interleaving scheme in a single processor system, certain bits in the address command are selected to identify the memory bank being accessed. Thus, for example, if eight memory banks are available in the system, three of the address bits might be used to identify a specific memory bank. If these three address bits are the low order bits in the address command, then consecutive memory addresses are distributed across the memory banks automatically by the system hardware. In such a system, the address <b>8000</b> might correspond to an address location in memory bank <b>1</b>, while address <b>8001</b> might correspond to an address location in memory bank <b>2</b>. Thus, by using the low order address bits to define the memory bank, the system will interleave data among memory banks as the operating system increments through the address space.</p><p>If conversely, the three address bits identifying the memory bank are high order bits (above the bits identifying the virtual page size), then address interleaving typically is performed as part of the software translation from the virtual address to the physical address. Thus, in this type of system, the interleaving is determined by software page placement policy choices typically programmed into the operating system.</p><p>In a distributed memory, multi-processor computer system, the memory is distributed throughout the computer system, and is not located in one finite location. In particular, one technique for implementing such a system is to associate memory with each processor in the computer system. Each of the processors within the system may be capable of accessing the memory associated with any other processor by properly transmitting a command coupled with the desired memory address to the appropriate memory location. Identifying an address within any particular memory location requires selecting the processor associated with the memory.</p><p>Because memory is distributed throughout the computer system, and multiple processors exist that may each simultaneously seek to access the same memory device or even the same memory data, special steps must be implemented to insure coherency of the data, while still maximizing the speed of memory accesses to minimize system latency. In an attempt to reduce latency (or \u201cwaiting\u201d) caused by coincident accesses to the same memory location, memory may be distributed within a particular processor sub-system by including multiple memory ports supporting separate memory banks. This adds yet another level of detail that must be identified in the address coding scheme. Thus, in addition to the processor identification, the address command must also identify the memory bank and the memory offset for that particular memory bank.</p><p>The conventional technique for addressing memory in a distributed memory computer system is to have the operating system assign continuous address references to contiguous locations on the same processor. Thus, typically the high order bits in the address define the processor, and the lower order bits define the offset in the memory associated with that processor. Thus, as the operating system increments through the address space, the processor being accessed does not change, as the lower order address bits are incremented. Thus, incrementing address space means that the data transactions occur locally on a given processor. Such a situation may be advantageous if the local processor is the source of the data transactions because it reduces the latency of the memory transactions by avoiding the necessity of transmitting commands to another processor to obtain the requested data. In other instances, however, this addressing scheme may be unfavorable. If, for example, multiple processors are referencing the same contiguous piece of memory associated with a different processor, a bottleneck may occur as each requesting processor tries to simultaneously communicate with the processor that controls the targeted memory.</p><p>Because the processor identification occurs in the high order bits of the address signal, typically the interleaving of data among processors is performed through software. Thus, in high order interleaving systems that are used with multiple processing systems, the task of distributing addresses is made at a page granularity level by the system software when it determines the virtual-to-physical page translation. Such software implementations, however, require involvement of the processor, and thus may act as a drag on system performance. In addition, simultaneous software interleaving can be very expensive since it requires many operations to convert addresses to a canonical form necessary for the hardware. Software interleaving also can be difficult to implement, and may require additional clock cycles for each memory transaction performed. It would be advantageous to develop a hardware address scheme that permits simultaneous interleaving without the attendant problems caused by software interleaving.</p><h4>BRIEF SUMMARY OF THE INVENTION</h4><p>The problems noted above are solved in large part by the system and techniques of the present invention, which permit multiple different address interleavings to be active simultaneously. In particular, unstriped addresses are used to interleave across processors using high order address bits. This allows instructions to be copied locally to all processors in the system, ensuring that all instructions are transmitted with low latency. Striped addresses, conversely, interleave across four processor sets at the low order, and the rest of the processors at high order. This makes a group of four processors the striped local set, with data references distributed to all memory ports of the four processor set. The striping of addresses within a four processor set reduces bottlenecks that may occur when other processors request data associated with memory of a different processor. The simultaneous use of striped and unstriped addresses can improve system performance, without the attendant deficiencies of software implemented systems.</p><p>The interleave scheme implemented in the preferred embodiment of the present invention uses an address bit to distinguish between two different types of address interleaving\u2014striped and unstriped. Preferably each processor includes two memory ports, with an entire cache block assigned to a single memory port. In both striped and unstriped interleavings, the lowest order address bits (<b>0</b>-<b>5</b>) indicate the cache alignment, and address bit <b>6</b> indicates the port within a processor. The unstriped interleave identifies the cache block within a port in address bits <b>7</b>-<b>33</b>, and the lower two processor bits in address bits <b>34</b> and <b>35</b>. The striped interleave has the lower two processor bits in address bits <b>7</b> and <b>8</b>, and the cache block in address bits <b>37</b>-<b>43</b> (for a system with up to 256 processors, each of which can have 16 GB of memory distributed across 2 ports).</p><p>In accordance with the preferred embodiment, the present invention is implemented in hardware. In response to a memory access that results in a cache miss, the hardware converts the address into a single canonical form which has the port, offset, and processor fields in fixed positions. These address bits are then transferred along with bit <b>36</b>, which comprises the stripe bit, to the port. The port returns the cache block in response. If necessary, the port may re-convert the address into its original form using the stripe bit if it needs to extract the block from another processor's cache. After conversion to the canonical form, the hardware manages the interleaving uniformly for each case by forwarding the reference to the appropriate memory port.</p><p>According to the preferred embodiment, the striped interleave is used for data that is more likely to be accessed by other processors, while unstriped interleaves is used for data that is likely to only be accessed by the local processor.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>For a detailed description of the preferred embodiments of the invention, reference will now be made to the accompanying drawings in which:</p><p>FIG. 1 shows a system level diagram of a multiple processor system coupled together in accordance with the preferred embodiment of the present invention;</p><p>FIGS. 2<i>a </i>and <b>2</b><i>b </i>show a block diagram of one of the processors depicted in the preferred embodiment of FIG. 1;</p><p>FIG. 3 shows sixteen processors, with associated memory ports, grouped into four local striped sets in accordance with an exemplary embodiment of the present invention; and</p><p>FIGS. 4<i>a </i>and <b>4</b><i>b </i>illustrate exemplary address command signals for an unstriped address interleave and for a striped address interleave, respectively.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>NOTATION AND NOMENCLATURE</h4><p>Certain terms are used throughout the following description and claims to refer to particular system components. As one skilled in the art will appreciate, computer companies may refer to a component by different names. This document does not intend to distinguish between components that differ in name but not function. In the following discussion and in the claims, the terms \u201cincluding\u201d and \u201ccomprising\u201d are used in an open-ended fashion, and thus should be interpreted to mean \u201cincluding, but not limited to . . . \u201d. Also, the term \u201ccouple\u201d or \u201ccouples\u201d is intended to mean either an indirect or direct electrical connection. Thus, if a first device couples to a second device, that connection may be through a direct electrical connection, or through an indirect electrical connection via other devices and connections. To the extent that any term is not specially defined in this specification, the intent is that the term is to be given it's plain and ordinary meaning.</p><h4>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>Referring now to FIG. 1, in accordance with the preferred embodiment of the invention, computer system <b>90</b> comprises one or more processors <b>100</b> coupled to a memory sub-system <b>102</b> and an input/output (\u201cI/O\u201d) controller <b>104</b>. As shown in FIG. 1, computer system <b>90</b> includes multiple processors <b>100</b> (twelve such processors are shown for purposes of illustration), with each processor coupled to an associated memory sub-system <b>102</b> and an I/O controller <b>104</b>. Each processor <b>100</b> preferably includes four ports for connection to adjacent processors. The inter-processor ports are designated \u201cnorth,\u201d \u201csouth,\u201d \u201ceast,\u201d and \u201cwest\u201d in accordance with the well-known Manhattan grid architecture. As such, each processor <b>100</b> can be connected to four other processors. The processors on both end of the system layout preferably wrap around and connect to processors on the opposite side to implement a 2D torus-type connection. Although twelve processors <b>100</b> are shown in the exemplary embodiment of FIG. 1, any desired number of processors can be included. In the preferred embodiment, computer system <b>90</b> is designed to accommodate either <b>256</b> processors or <b>128</b> processors, depending on the size of the memory associated with the processors.</p><p>The I/O controller <b>104</b> provides an interface to various input/output devices, such as disk drives <b>105</b> and <b>106</b>, as shown in the lower left-hand side of FIG. <b>1</b>. Data from the I/O devices thus enters the 2D torus via the I/O controllers associated with the various processors. In addition to disk drives, other input/output devices also may be connected to the I/O controllers, including for example, keyboards, mice, CD-ROMs, DVD-ROMs, PCMCIA drives, and the like.</p><p>In accordance with the preferred embodiment, the memory <b>102</b> preferably comprises RAMbus\u2122 memory devices, but other types of memory devices can be used if desired. The capacity of the memory devices <b>102</b> may be of any suitable size. The memory devices <b>102</b> preferably are implemented as Rambus Interface Memory Modules (\u201cRIMMS\u201d).</p><p>In general, computer system <b>90</b> can be configured so that any processor <b>100</b> can access its own memory <b>102</b> and I/O devices as well as the memory and I/O devices of all other processors in the system. Preferably, the computer system may have physical connections between each processor resulting in low interprocessor communication times and improved memory and I/O device access reliability. If physical connections are not present between each pair of processors, a pass-through or bypass path preferably is available for each processor to access the memory and I/O devices of any other processor through one or more intermediary processors, as graphically depicted in FIG. <b>1</b>.</p><p>The processors may be implemented with any suitable microprocessor architecture, although the Alpha processor is used in the preferred embodiment. Therefore, to aid in understanding the preferred embodiment of the present invention, details regarding the preferred processor architecture will be described with reference to FIGS. 2<i>a </i>and <b>2</b><i>b, </i>with the understanding that this architecture is not a mandatory requirement to practice the present invention. After discussing the preferred processor architecture with reference to FIGS. 2<i>a </i>and <b>2</b><i>b, </i>the present invention will be addressed in further detail with reference to FIGS. 3, <b>4</b><i>a </i>and <b>4</b><i>b. </i></p><p>Referring now to FIGS. 2<i>a </i>and <b>2</b><i>b, </i>each processor <b>100</b> preferably includes an instruction cache <b>110</b>, an instruction fetch, issue and retire unit (\u201cIbox\u201d) <b>120</b>, an integer execution unit (\u201cEbox\u201d) <b>130</b>, a floating-point execution unit (\u201cFbox\u201d) <b>140</b>, a memory reference unit (\u201cMbox\u201d) <b>150</b>, a data cache <b>160</b>, an L2 instruction and data cache control unit (\u201cCbox\u201d) <b>170</b>, a level L2 cache <b>180</b>, two memory controllers (\u201cZbox0\u201d and \u201cZbox1\u201d) <b>190</b>, and an interprocessor and I/O router unit (\u201cRbox\u201d) <b>200</b>. The following discussion describes each of these units in more detail.</p><p>Each of the various functional units <b>110</b>-<b>200</b> contains control logic that communicates with the control logic of other functional units, as shown in FIGS. 2<i>a </i>and <b>2</b><i>b. </i>Thus, referring still to FIGS. 2<i>a </i>and <b>2</b><i>b, </i>the instruction cache control logic <b>110</b> communicates with the Ibox <b>120</b>, Cbox <b>170</b>, and L2 Cache <b>180</b>. In addition to the control logic communicating with the instruction cache <b>110</b>, the Ibox control logic <b>120</b> communicates with Ebox <b>130</b>, Fbox <b>140</b> and Cbox <b>170</b>. The Ebox <b>130</b> and Fbox <b>140</b> control logic both communicate with the Mbox <b>150</b>, which in turn communicates with the data cache <b>160</b> and Cbox <b>170</b>. The Cbox control logic also communicates with the L2 cache <b>180</b>, Zboxes <b>190</b>, and Rbox <b>200</b>.</p><p>Referring still to FIGS. 2<i>a </i>and <b>2</b><i>b, </i>the Ibox <b>120</b> preferably includes a fetch unit <b>121</b> which contains a virtual program counter (\u201cVPC\u201d) <b>122</b>, a branch predictor <b>123</b>, an instruction-stream translation buffer <b>124</b>, an instruction predecoder <b>125</b>, a retire unit <b>126</b>, decode and rename registers <b>127</b>, an integer instruction queue <b>128</b>, and a floating point instruction queue <b>129</b>. Generally, the VPC <b>122</b> maintains virtual addresses for instructions that are in-flight. An instruction is said to be \u201cin-flight\u201d from the time it is fetched until it retires or aborts. The Ibox <b>120</b> can accommodate as many as 80 instructions, in 20 successive fetch slots, in-flight between the decode and rename registers <b>127</b> and the end of the pipeline. The VPC <b>122</b> preferably includes a 20-entry queue to store the fetched VPC addresses.</p><p>The branch predictor <b>123</b> is used by the Ibox <b>120</b> for predicting the outcome of branch instructions. A branch instruction requires program execution either to continue with the instruction immediately following the branch instruction if a certain condition is met, or branch to a different instruction if the particular condition is not met. Accordingly, the outcome of a branch instruction is not known until the instruction is executed. In a pipelined architecture, a branch instruction (or any instruction for that matter) may not be executed for at least several, and perhaps many, clock cycles after the fetch unit in the processor fetches the branch instruction. In order to keep the pipeline full, which is desirable for efficient operation, the processor preferably includes branch prediction logic that predicts the outcome of a branch instruction before it is actually executed (also referred to as \u201cspeculating\u201d). The branch predictor <b>123</b>, which receives addresses from the VPC queue <b>122</b>, preferably bases its speculation on short and long-term history of prior instruction branches. As such, using branch prediction logic, the fetch unit can speculate the outcome of a branch instruction before it is actually executed. The speculation, however, may or may not turn out to be accurate. Branch predictor <b>123</b> uses any suitable branch prediction algorithm that results in correct speculations more often than misspeculations, enhancing the overall performance of the processor.</p><p>The instruction translation buffer (\u201cITB\u201d) <b>124</b> couples to the instruction cache <b>110</b> and the fetch unit <b>121</b>. The ITB <b>124</b> comprises a 128-entry, fully-associative instruction-stream translation buffer that is used to store recently used instruction-stream address translations and page protection information. Preferably, each of the entries in the ITB <b>124</b> may be 1, 8, 64 or 512 contiguous 8-kilobyte (\u201cKB\u201d) pages or 1, 32, 512, 8192 contiguous 64-kilobyte pages. The allocation scheme used for the ITB <b>124</b> is a round-robin scheme, although other schemes can be used as desired.</p><p>The predecode logic <b>125</b> reads an octaword (16 contiguous bytes) from the instruction cache <b>110</b>. Each octaword read from the instruction cache <b>110</b> may contain up to four naturally aligned instructions per cycle. Branch prediction and line prediction bits accompany the four instructions fetched by the predecoder <b>125</b>. The branch prediction scheme implemented in branch predictor <b>123</b> generally works most efficiently when only one branch instruction is contained among the four fetched instructions. The predecoder <b>125</b> predicts the instruction cache line that the branch predictor <b>123</b> will generate. The predecoder <b>125</b> generates fetch requests for additional instruction cache lines and stores the instruction stream data in the instruction cache.</p><p>Referring still to FIGS. 2<i>a </i>and <b>2</b><i>b, </i>the retire unit <b>126</b> fetches instructions in program order, executes them out of order, and then retires (also called \u201ccommitting\u201d an instruction) them in order. The Ibox <b>120</b> logic maintains the architectural state of the processor by retiring an instruction only if all previous instructions have executed without generating exceptions or branch mispredictions. An exception is any event that causes suspension of normal instruction execution. Retiring an instruction commits the processor to any changes that the instruction may have made to the software accessible registers and memory. The processor <b>100</b> preferably includes the following three machine code accessible hardware: integer and floating-point registers, memory, and internal processor registers. With respect to the present invention, one of the internal process registers for the Cbox <b>170</b> is the Cbox stripe control register (with machine code mnemonic CBOX_STP_CTL).</p><p>The decode and rename registers <b>127</b> contain logic that forwards instructions to the integer and floating-point instruction queues <b>128</b>, <b>129</b>. The decode and rename registers <b>127</b> preferably eliminate register write-after-read (\u201cWAR\u201d) and write-after-write (\u201cWAW\u201d) data dependency while preserving true read-after-write (\u201cRAW\u201d) data dependencies. This permits instructions to be dynamically rescheduled. In addition, the decode and rename registers <b>127</b> permit the processor to speculatively execute instructions before the control flow preceding those instructions is resolved.</p><p>The logic in the decode and rename registers <b>127</b> preferably translates each instruction's operand register specifiers from the virtual register numbers in the instruction to the physical register numbers that hold the corresponding architecturally-correct values. The logic also renames each instruction destination register specifier from the virtual number in the instruction to a physical register number chosen from a list of free physical registers, and updates the register maps. The decode and rename register logic <b>127</b> can process four instructions per cycle. Preferably, the logic in the decode and rename registers <b>127</b> does not return the physical register, which holds the old value of an instruction's virtual destination register, to the free list until the instruction has been retired, indicating that the control flow up to that instruction has been resolved.</p><p>If a branch misprediction or exception occurs, the register logic backs up the contents of the integer and floating-point rename registers to the state associated with the instruction that triggered the condition, and the fetch unit <b>121</b> restarts at the appropriate Virtual Program Counter (\u201cVPC\u201d). Preferably, as noted above, 20 valid fetch slots containing up to 80 instructions can be in flight between the registers <b>127</b> and the end of the processor's pipeline, where control flow is finally resolved. The register <b>127</b> logic is capable of backing up the contents of the registers to the state associated with any of these 80 instructions in a single cycle. The register logic <b>127</b> preferably places instructions into the integer or floating-point issue queues <b>128</b>, <b>129</b>, from which they are later issued to functional units <b>130</b> or <b>136</b> for execution.</p><p>The integer instruction queue <b>128</b> preferably includes capacity for 20 integer instructions. The integer instruction queue <b>128</b> issues instructions at a maximum rate of four instructions per cycle. The specific types of instructions processed through queue <b>128</b> include: integer operate commands, integer conditional branches, unconditional branches (both displacement and memory formats), integer and floating-point load and store commands, Privileged Architecture Library (\u201cPAL\u201d) reserved instructions, integer-to-floating-point and floating-point-integer conversion commands.</p><p>Referring still to FIGS. 2<i>a </i>and <b>2</b><i>b, </i>the integer execution unit (Ebox) <b>130</b> includes arithmetic logic units (\u201cALUs\u201d) <b>131</b>, <b>132</b>, <b>133</b>, and <b>134</b> and two integer register files <b>135</b>. Ebox <b>130</b> preferably comprises a 4-path integer execution unit that is implemented as two functional-unit \u201cclusters\u201d labeled <b>0</b> and <b>1</b>. Each cluster contains a copy of an 80-entry, physical-register file and two subclusters, named upper (\u201cU\u201d) and lower (\u201cL\u201d). As such, the subclusters <b>131</b>-<b>134</b> are labeled U<b>0</b>, L<b>0</b>, U<b>1</b>, and L<b>1</b>. Bus <b>137</b> provides cross-cluster communication for moving integer result values between the clusters.</p><p>The subclusters <b>131</b>-<b>134</b> include various components that are not specifically shown in FIG. 2<i>a. </i>For example, the subclusters preferably include four 64-bit adders that are used to calculate results for integer add instructions, logic units, barrel shifters and associated byte logic, conditional branch logic, a pipelined multiplier for integer multiply operations, and other components known to those of ordinary skill in the art.</p><p>Each entry in the integer instruction queue <b>128</b> preferably asserts four request signals\u2014one for each of the Ebox <b>130</b> subclusters <b>131</b>, <b>132</b>, <b>133</b>, and <b>134</b>. A queue entry asserts a request when it contains an instruction that can be executed by the subcluster, if the instruction's operand register values are available within the subdluster. The integer instruction queue <b>128</b> includes two arbiters\u2014one for the upper subclusters <b>132</b> and <b>133</b> and another arbiter for the lower subclusters <b>131</b> and <b>134</b>. Each arbiter selects two of the possible 20 requesters for service each cycle. Preferably, the integer instruction queue <b>128</b> arbiters choose between simultaneous requesters of a subcluster based on the age of the request\u2014older requests are given priority over newer requests. If a given instruction requests both lower subclusters, and no older instruction requests a lower subcluster, then the arbiter preferably assigns subcluster <b>131</b> to the instruction. If a given instuction requests both upper subclusters, and no older instruction requests an upper subcluster, then the arbiter preferably assigns subcluster <b>133</b> to the instruction.</p><p>The floating-point instruction queue <b>129</b> preferably comprises a 15-entry queue and issues the following types of instructions: floating-point operates, floating-point conditional branches, floating-point stores, and floating-point register to integer register transfers. Each queue entry preferably includes three request lines\u2014one for the add pipeline, one for the multiply pipeline, and one for the two store pipelines. The floating-point instruction queue <b>129</b> includes three arbiters\u2014one for each of the add, multiply, and store pipelines. The add and multiply arbiters select one requester per cycle, while the store pipeline arbiter selects two requesters per cycle, one for each store pipeline. As with the integer instruction queue <b>128</b> arbiters, the floating-point instruction queue arbiters select between simultaneous requesters of a pipeline based on the age of the request\u2014older request are given priority. Preferably, floating-point store instructions and floating-point register to integer register transfer instructions in even numbered queue entries arbitrate for one store port. Floating-point store instructions and floating-point register to integer register transfer instructions in odd numbered queue entries arbitrate for the second store port.</p><p>Floating-point store instructions and floating-point register to integer register transfer instructions are queued in both the integer and floating-point queues. These instructions wait in the floating-point queue until their operand register values are available from the floating-point execution unit (\u201cFbox\u201d) registers. The instructions subsequently request service from the store arbiter. Upon being issued from the floating-point queue <b>129</b>, the instructions signal the corresponding entry in the integer queue <b>128</b> to request service. Finally, upon being issued from the integer queue <b>128</b>, the operation is completed.</p><p>The integer registers <b>135</b>, <b>136</b> preferably contain storage for the processor's integer registers, results written by instructions that have not yet been retired, and other information as desired. The two register files <b>135</b>, <b>136</b> preferably contain identical values. Each register file preferably includes four read ports and six write ports. The four read ports are used to source operands to each of the two subclusters within a cluster. The six write ports are used to write results generated within the cluster or another cluster and to write results from load instructions.</p><p>The floating-point execution queue (\u201cFbox\u201d) <b>129</b> contains a floating-point add, divide and square-root calculation unit <b>142</b>, a floating-point multiply unit <b>144</b> and a register file <b>146</b>. Floating-point add, divide and square root operations are handled by the floating-point add, divide and square root calculation unit <b>142</b> while floating-point operations are handled by the multiply unit <b>144</b>.</p><p>The register file <b>146</b> preferably provides storage for <b>72</b> entries including <b>31</b> floating-point registers and <b>41</b> values written by instructions that have not yet been retired. The Fbox register file <b>146</b> contains six read ports and four write ports (not specifically shown). Four read ports are used to source operands to the add and multiply pipelines, and two read ports are used to source data for store instructions. Two write ports are used to write results generated by the add and multiply pipelines, and two write ports are used to write results from floating-point load instructions.</p><p>Referring still to FIG. 2<i>a, </i>the Mbox <b>150</b> controls the L1 data cache <b>160</b> and ensures architecturally correct behavior for load and store instructions. The Mbox <b>150</b> preferably contains a datastream translation buffer (\u201cDTB\u201d) <b>151</b>, a load queue (\u201cLQ\u201d) <b>152</b>, a store queue (\u201cSQ\u201d) <b>153</b>, and a miss address file (\u201cMAF\u201d) <b>154</b>. The DTB <b>151</b> preferably comprises a fully associative translation buffer that is used to store data stream address translations and page protection information. Each of the entries in the DTB <b>151</b> can map 1, 8, 64, or 512 contiguous 8-KB pages. The allocation scheme preferably is round robin, although other suitable schemes could also be used. The DTB <b>151</b> also supports an 8-bit Address Space Number (\u201cASN\u201d) and contains an Address Space Match (\u201cASM\u201d) bit. The ASN is an optionally implemented register used to reduce the need for invalidation of cached address translations for process-specific addresses when a context switch occurs.</p><p>The LQ <b>152</b> preferably comprises a reorder buffer used for load instructions. It contains <b>32</b> entries and maintains the state associated with load instructions that have been issued to the Mbox <b>150</b>, but for which results have not been delivered to the processor and the instructions retired. The Mbox <b>150</b> assigns load instructions to LQ slots based on the order in which they were fetched from the instruction cache <b>110</b>, and then places them into the LQ <b>152</b> after they are issued by the integer instruction queue <b>128</b>. The LQ <b>152</b> also helps to ensure correct memory reference behavior for the processor.</p><p>The SQ <b>153</b> preferably is a reorder buffer and graduation unit for store instructions. It contains 32 entries and maintains the state associated with store instructions that have been issued to the Mbox <b>150</b>, but for which data has not been written to the data cache <b>160</b>. The Mbox <b>150</b> assigns store instructions to SQ slots based on the order in which they were fetched from the instruction cache <b>110</b> and places them into the SQ <b>153</b> after they are issued by the instruction cache <b>110</b>. The SQ <b>153</b> holds data associated with the store instructions issued from the integer instruction unit <b>128</b> until they are retired, at which point the store can be allowed to update the data cache <b>160</b>. The LQ <b>152</b> also helps to ensure correct memory reference behavior for the processor.</p><p>The MAF <b>154</b> preferably comprises a 16-entry file that holds physical addresses associated with pending instruction cache <b>110</b> and data cache <b>160</b> fill requests and pending input/output (\u201cI/O\u201d) space read transactions.</p><p>Processor <b>100</b> preferably includes two on-chip primary-level (\u201cL1\u201d) instruction and data caches <b>110</b> and <b>160</b>, and a single secondary-level, unified instruction/data (\u201cL2\u201d) cache <b>180</b> (FIG. 2<i>b</i>). The L1 instruction cache <b>110</b> preferably comprises a 64-KB virtual-addressed, two-way set-associative cache. Prediction logic improves the performance of the two-way set-associative cache without slowing the cache access time. Each instruction cache block preferably contains a plurality (preferably 16) instructions, virtual tag bits, an address space number, an address space match bit, a one-bit PALcode bit to indicate physical addressing, a valid bit, data and tag parity bits, four access-check bits, and predecoded information to assist with instruction processing and fetch control.</p><p>The L1 data cache <b>160</b> preferably comprises a 64 KB, two-way set associative, virtually indexed, physically tagged, write-back, read/write allocate cache with 64-byte cache blocks. During each cycle the data cache <b>160</b> preferably performs one of the following transactions: two quadword (or shorter) read transactions to arbitrary addresses, two quadword write transactions to the same aligned octaword, two non-overlapping less-than quadword writes to the same aligned quadword, one sequential read and write transaction from and to the same aligned octaword. Preferably, each data cache block contains 64 data bytes and associated quadword ECC bits, physical tag bits, valid, dirty, shared, and modified bits, tag parity bit calculated across the tag, dirty, shared, and modified bits, and one bit to control round-robin set allocation. The data cache <b>160</b> is organized to contain two sets, each with 512 rows containing 64-byte blocks per row (i.e., 32-KB of data per set). The processor <b>100</b> uses two additional bits of virtual address beyond the bits that specify an 8-KB page in order to specify the data cache row index. A given virtual address might be found in four unique locations in the data cache <b>160</b>, depending on the virtual-to-physical translation for those two bits. The processor <b>100</b> prevents this aliasing by keeping only one of the four possible translated addresses in the cache at any time.</p><p>The L2 cache <b>180</b> preferably comprises a 1.75-MB, seven-way set associative write-back mixed instruction and data cache. Preferably, the L2 cache holds physical address data and coherence state bits for each block.</p><p>Referring now to FIG. 2<i>b, </i>the L2 instruction and data cache control unit (\u201cCbox\u201d) <b>170</b> controls the L2 instruction and data cache <b>190</b> and system ports. As shown, the Cbox <b>170</b> contains a fill buffer <b>171</b>, a data cache victim buffer <b>172</b>, a system victim buffer <b>173</b>, a cache miss address file (\u201cCMAF\u201d) <b>174</b>, a system victim address file (\u201cSVAF\u201d) <b>175</b>, a data victim address file (\u201cDVAF\u201d) <b>176</b>, a probe queue (\u201cPRBQ\u201d) <b>177</b>, a requester miss-address file (\u201cRMAF\u201d) <b>178</b>, a store to I/O space (\u201cSTIO\u201d) <b>179</b>, and an arbitration unit <b>181</b>. In addition, the Cbox <b>170</b> also preferably includes a stripe control register <b>183</b> that functions as a mask for memory blocks in the associated memory, indicating whether each memory block may be accessed with striped addressing techniques, as disclosed herein.</p><p>The fill buffer <b>171</b> preferably buffers data received from other functional units external to the Cbox. The data and instructions are written into the fill buffer <b>171</b>, and other logic units in the Cbox process the data and instructions before relaying to other fuinctional units or the L1 cache. The data cache victim buffer (\u201cVDF\u201d) <b>172</b> preferably stores data flushed from the L1 cache or sent: to the System Victim Data Buffer <b>173</b>. The System Victim Data Buffer (\u201cSVDB\u201d) <b>173</b> is used to send data flushed from the L2 cache to other processors in the system and to memory. Cbox Miss-Address File (\u201cCMAF\u201d) <b>174</b> preferably holds addresses of any transaction that results in an L1 cache miss. CMAF updates and maintains the status of these addresses. The System Victim-Address File (\u201cSVAF\u201d) <b>175</b> in the Cbox preferably contains the addresses of all SVDB data entries. The Data Victim-Address File (\u201cDVAF\u201d) <b>176</b> preferably contains the addresses of all data cache victim buffer (\u201cVDF\u201d) data entries.</p><p>The Probe Queue (\u201cPRBQ\u201d) <b>177</b> preferably comprises an 18-entry queue that holds pending system port cache probe commands and addresses. This queue includes 10 remote request entries, 8 forward entries, and lookup L2 tags and requests from the PRBQ content addressable memory (\u201cCAM\u201d) against the RMAF, CMAF and SVAF. Requestor Miss-Address Files (\u201cRMAF\u201d) <b>178</b> in the Cbox preferably accepts requests and responds with data or instructions from the L2 cache. Data accesses from other functional units in the processor, other processors in the computer system or any other devices that might need data out of the L2 cache are sent to the RMAF for service. The Store Input/Output (\u201cSTIO\u201d) <b>179</b> preferably transfer data from the local processor to I/O cards in the computer system. Finally, arbitration unit <b>181</b> in the Cbox preferably arbitrates between load and store accesses to the same memory location of the L2 cache and informs other logic blocks in the Cbox and other computer system functional units of any conflict.</p><p>The stripe control register <b>183</b> preferably comprises a 64 bit register that serves as a mask representing memory blocks in the memory sub-system associated with each processor. Each bit in the stripe control register <b>183</b> represents either 256 MB or 512 MB of memory. Thus, the full 64 bits represent the maximum 16 GB or 32 GB of memory associated with a particular processor. If the corresponding mask bit is clear, the memory block must be addressed without striping, using processor contiguous addressing. Conversely, if the corresponding mask bit is set, the memory block must be referenced with address striping. The determination of whether a memory block must be addressed by striping may be made in many ways, as will be apparent to one skilled in the art. In the preferred embodiment, this determination is based on whether the data stored in the memory block will be accessed solely by the local processor, or whether other processors also may access the data block. This determination may be historically based, depending on the prior access history of similar type data. Other predictive logic also may be used, if desired, to set or clear the corresponding mask bit for a memory block.</p><p>The stripe control register may preferably be used to enable the associated computer to issue references to a memory location even though the location does not exist. Such a memory reference may be referred to as a non-existent memory reference (\u201cNXM\u201d). In the preferred embodiment, a correctly functioning processor may generate NXMs during normal operation because it can create speculative memory references. If a processor was not permitted to generate speculative memory references, then the software could be used to insure that only correct addresses were used. If one were to consider a memory location A in (\u201cDRAM\u201d) memory, this location could be referenced when a processor used either the address A\u2032 (unstriped) or A\u2033 (striped). Only one of A\u2032 and A\u2033 are valid addresses. The stripe control register <b>183</b> guarantees that only one of A\u2032 and A\u2033 exist at the same time. If A\u2032 is the legitimate address, then any reference to A using the A\u2033 address will be a NXM reference\u2014the reference is nulled and the address A\u2033 is not allowed to be loaded into the cache.</p><p>Referring still to FIG. 2<i>b, </i>processor <b>100</b> preferably includes dual, integrated RAMbus memory controllers <b>190</b> (identified as Zbox<b>0</b> and Zbox<b>1</b>). Thus, each processor preferably includes two memory ports (referred to herein as port <b>0</b> and port <b>1</b>). Each Zbox controller <b>190</b> controls 4 or 5 channels of information flow with the main memory <b>102</b> (FIG. <b>1</b>). Each Zbox preferably includes a front-end directory in-flight table (\u201cDIFT\u201d) <b>191</b>, a middle mapper <b>192</b>, and a back end <b>193</b>. The front-end DIFT <b>191</b> performs a number of functions such as managing the processor's directory-based memory coherency protocol, processing request commands from the Cbox <b>170</b> and Rbox <b>200</b>, sending forward commands to the Rbox, sending response commands to and receiving packets from the Cbox and Rbox, and tracking up to 32 in-flight transactions. The front-end DIFT <b>191</b> also sends directory read and write requests to the Zbox and conditionally updates directory information based on request type, Local Probe Response (\u201cLPR\u201d) status and directory state.</p><p>The middle mapper <b>192</b> maps the physical address into RAMbus device format by device, bank, row, and column. The middle mapper <b>192</b> also maintains an open-page table to track all open pages and to close pages on demand if bank conflicts arise. The mapper <b>192</b> also schedules RAMbus transactions such as timer-base request queues. The Zbox back end <b>193</b> preferably packetizes the address, control, and data into RAMbus format and provides the electrical interface to the RAMbus devices themselves.</p><p>The Rbox <b>200</b> provides the interfaces to as many as four other processors and one I/O controller <b>104</b> (FIG. <b>1</b>). The inter-processor interfaces are designated as North (\u201cN\u201d), South (\u201cS\u201d), East (\u201cE\u201d), and West (\u201cW\u201d) and provide two-way communication between adjacent processors.</p><p>According to the preferred embodiment, the present invention includes the capability of striping data across a local set of processors, or of using processor contiguous addressing, depending on the status of the mask bits in the Cbox stripe control register <b>183</b>. Thus, if the mask bit is set in stripe control register <b>183</b> for a memory block, references to that memory block must implement stripe addressing. Consequently, the present invention supports the ability to perform both striped addressing among a local processor set, and the ability to perform convention processor contiguous addressing within the memory block of a particular processor.</p><p>In the preferred embodiment, the address command signal used for striped memory addressing differs from that used for processor contiguous addressing. Referring now to FIGS. <b>4</b><i>a </i>and <b>4</b><i>b, </i>the core address space preferably comprises 44 physical bits. The most significant bit, bit <b>43</b>, selects between I/O space and memory cacheable space. The 44 bit address space supports up to 256 processors and 256 I/O ASICs configured with up to 16 GB of associated memory. Larger memory sizes are also possible (such as 32 GB), by proportionally reducing the number of processors that are supported.</p><p>The memory address space preferably is defined as a processor-contiguous address or a striped address based on the status of stripe bit <b>36</b>. Thus, a given memory block may be accessed using either processor-contiguous or striped addressing, though preferably only one addressing mechanism is used at any given time to access a particular memory block. The stripe bit <b>36</b> must be carried with the physical address offset so that conversion between a core address and a network address can be performed, as required.</p><p>Referring now particularly to FIG. 4A (and assuming 256 processors with 16 GB of memory storage capability per processor), bit <b>43</b> is set to zero to indicate a memory-cacheable address. The particular processor being accessed is identified by the processor identification (\u201cPID\u201d) bits <b>7</b>:<b>0</b>. The eight processor identification provide bits provide identification of which of the 256 different processors is selected for a given memory access. Bits <b>42</b>-<b>37</b> of the address space: identify the upper six bits (PID bits <b>7</b>:<b>2</b>) of the processor identification number where the memory access is targeted. For processor contiguous addressing, the lower two bits of the processor identification number (PID bits <b>1</b>:<b>0</b>) are located in bits <b>35</b>-<b>34</b> of the address space. Positioned between the lower and upper PID bits is the stripe bit, which comprises bit <b>36</b> of the address space. In the case of processor-contiguous memory space addressing, bit <b>36</b> is set to \u201c0\u201d, as shown in FIG. 4<i>a. </i>Bits <b>33</b>-<b>0</b>, which support up to 16 GB of memory per processor, determine the memory offset at the targeted processor. If larger memory sizes are desired on a per processor basis, then bit <b>42</b> may be used as another memory offset bit, instead of as a processor ID bit, thus limiting the number of processors supported to 128. As shown in FIG. 4<i>a, </i>bits <b>5</b>-<b>0</b> specify the offset within the cache block, and bit <b>6</b> preferably represents the port bit identifying the memory port being addressed.</p><p>Referring now to FIG. 4<i>b, </i>the striped memory space also is partitioned with bit <b>43</b> identifying a memory cacheable address (with a logic \u201c0\u201d) or an I/O access, and bits <b>43</b>-<b>37</b> specifying the upper six bits of the processor ID (PID <b>7</b>:<b>2</b>). Bit <b>36</b>, which is the stripe bit, is set to \u201c1\u201d to indicate a striped memory address. Address space bits <b>35</b>-<b>9</b> and <b>6</b>-<b>0</b> identify the memory offset within the selected processor, thereby supporting up to 16 GB of memory per processor. For striped addressing, address bits <b>7</b>-<b>8</b> identify the lower two bits of the processor ID (PID <b>1</b>:<b>0</b>). In addition, in accordance with the preferred embodiment, bit <b>6</b> identifies the memory port of the processor. Bits <b>5</b>-<b>0</b> identify the low order address offset at that processor. If a larger memory size is desired, then bit <b>42</b> may be used as a high order offset bit instead of as a processor ID bit. In the preferred embodiment, striping is only allowed or disallowed in blocks or chunks of memory. Thus, according to the preferred embodiment, striping is determined with a 256 MB granularity in 16 GB per processor configurations. In 32 GB per processor configurations, striping can only be allowed or disallowed with a granularity of 512 MB.</p><p>The present invention includes the capability to select between processor-contiguous addressing and striping based upon the type of data and instructions to be stored in memory. Some applications have instruction streams and data structures that are only used by a single processor. Storing in contiguous memory space reduces memory latency for that data when accessed by the local processor exclusively. Striped addressing, conversely, makes memory latency more uniform for data and instructions that are used by more than one processor, because it permits portions of the data to be retrieved from different processors.</p><p>When processor contiguous addressing is used, data and instructions are stored within the memory of a single processor, in accordance with normal convention. In the preferred embodiment, the processor contiguous memory space ranges are pre-assigned based on the following Table I.</p><p><tables id=\"TABLE-US-00001\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"4\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"63pt\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"35pt\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"56pt\"></colspec><colspec align=\"center\" colname=\"4\" colwidth=\"63pt\"></colspec><thead><row><entry nameend=\"4\" namest=\"1\" rowsep=\"1\">TABLE I</entry></row><row><entry align=\"center\" nameend=\"4\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry>PROCESSOR #</entry><entry>PID</entry><entry>LOWER RANGE</entry><entry>UPPER RANGE</entry></row><row><entry align=\"center\" nameend=\"4\" namest=\"1\" rowsep=\"1\"></entry></row></thead><tbody valign=\"top\"><row><entry>0</entry><entry>0000000</entry><entry>000.0000.0000</entry><entry>003.FFFF.FFFF</entry></row><row><entry>1</entry><entry>0000001</entry><entry>004.0000.0000</entry><entry>007.FFFF.FFFF</entry></row><row><entry>2</entry><entry>0000010</entry><entry>008.0000.0000</entry><entry>00B.FFFF.FFFF</entry></row><row><entry>3</entry><entry>0000011</entry><entry>00C.0000.0000</entry><entry>00F.FFFF.FFFF</entry></row><row><entry>4</entry><entry>0000100</entry><entry>020.0000.0000</entry><entry>023.FFFF.FFFF</entry></row><row><entry>5</entry><entry>0000101</entry><entry>024.0000.0000</entry><entry>027.FFFF.FFFF</entry></row><row><entry>6</entry><entry>0000110</entry><entry>028.0000.0000</entry><entry>02b.FFFF.FFFF</entry></row><row><entry>7</entry><entry>0000111</entry><entry>02C.0000.0000</entry><entry>02F.FFFF.FFFF</entry></row><row><entry>*</entry><entry>*</entry><entry>*</entry><entry>*</entry></row><row><entry>127\u2003</entry><entry>1111111</entry><entry>3EC.0000.0000</entry><entry>3EF.FFFF.FFFF</entry></row><row><entry align=\"center\" nameend=\"4\" namest=\"1\" rowsep=\"1\"></entry></row></tbody></tgroup></table></tables></p><p>The striped memory space ranges preferably are pre-assigned based on the following Table II.</p><p><tables id=\"TABLE-US-00002\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"4\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"42pt\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"63pt\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"56pt\"></colspec><colspec align=\"center\" colname=\"4\" colwidth=\"56pt\"></colspec><thead><row><entry nameend=\"4\" namest=\"1\" rowsep=\"1\">TABLE II</entry></row><row><entry align=\"center\" nameend=\"4\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry>PRO-</entry><entry></entry><entry></entry><entry></entry></row><row><entry>CESSOR</entry></row><row><entry>#</entry><entry>PID</entry><entry>LOWER RANGE</entry><entry>UPPER RANGE</entry></row><row><entry align=\"center\" nameend=\"4\" namest=\"1\" rowsep=\"1\"></entry></row></thead><tbody valign=\"top\"><row><entry>0-3</entry><entry>0000000-0000011</entry><entry>010.0000.0000</entry><entry>01F.FFFF.FFFF</entry></row><row><entry>4-7</entry><entry>0000100-0000111</entry><entry>030.0000.0000</entry><entry>03F.FFFF.FFFF</entry></row><row><entry>\u20028-11</entry><entry>0001000-0000111</entry><entry>050.0000.0000</entry><entry>05F.FFFF.FFFF</entry></row><row><entry>12-15</entry><entry>0001000-0001111</entry><entry>070.0000.0000</entry><entry>07F.FFFF.FFFF</entry></row><row><entry>16-19</entry><entry>\u2002001000-0010011</entry><entry>090.0000.0000</entry><entry>09F.FFFF.FFFF</entry></row><row><entry>20-23</entry><entry>0010100-0010111</entry><entry>0B0.0000.0000</entry><entry>0BF.FFFF.FFFF</entry></row><row><entry>24-27</entry><entry>0011000-0011011</entry><entry>0D0.0000.0000</entry><entry>0DF.FFFF.FFFF</entry></row><row><entry>28-31</entry><entry>0011100-0011111</entry><entry>0F0.0000.0000</entry><entry>0FF.FFFF.FFFF</entry></row><row><entry>*</entry><entry>*</entry><entry>*</entry><entry>*</entry></row><row><entry>124-127</entry><entry>1111100-1111111</entry><entry>3F0.0000.0000</entry><entry>3FF.FFFF.FFFF</entry></row><row><entry align=\"center\" nameend=\"4\" namest=\"1\" rowsep=\"1\"></entry></row></tbody></tgroup></table></tables></p><p>Referring now to FIG. 3, a portion of an exemplary multiple processor computer system is shown with 16 processors that are identified as <b>100</b><i>a</i>-<b>100</b><i>p. </i>Each processor preferably includes two memory ports (port <b>0</b> and port <b>1</b>) which connect to two memory banks <b>302</b>, <b>304</b>. According to one exemplary embodiment, one of the two ports may be configured with cache memory, while the other comprises standard DRAM memory. Also, as noted above, address space bit <b>6</b> identifies the port being addressed for a given processor.</p><p>As shown in FIGS. 3 and 4<i>b, </i>the processors are split into local striped sets based upon processor identification numbers. Thus, the local striped processor set 325 comprises all that have the PID bits <b>000000</b>xx. Put differently, local striped set comprises all processors with the first six PID bits of <b>000000</b>. As shown in FIG. 3, these six bits would be represented in address space bits <b>42</b>-<b>37</b>. The last two PID bits, which are found in address space bits <b>8</b>-<b>7</b>, identify which of the four processors is referenced. Thus, if the last two PID bits are 00, that indicates that processor <b>100</b><i>a </i>is targeted (for a complete PID of <b>00000000</b>). Similarly, the last two PID bits of processor <b>100</b><i>b </i>are 01, while processors <b>100</b><i>c </i>and <b>100</b><i>d </i>have the last two PID bits of 10 and 11.</p><p>The second local striped processor set <b>350</b> are those processors <b>100</b><i>e</i>-<b>100</b><i>h </i>with the first six PID bits of <b>000001</b> in address space bits <b>42</b>-<b>37</b>. The specific processor <b>100</b><i>e</i>-<b>100</b><i>h </i>is identified by the four possible bit combinations present in address space bits <b>7</b>-<b>8</b>, which are the two least significant PID bits. To further continue the example, the third local striped processor set <b>375</b> are processors <b>100</b><i>i</i>-<b>100</b><i>l </i>with the first six PID bits of <b>000010</b> in address space bits <b>42</b>-<b>37</b>. The specific processor <b>100</b><i>i</i>-<b>100</b><i>l </i>is identified by the four possible bit combinations present in address space bits <b>7</b>-<b>8</b>, which are the two least significant PID bits.</p><p>Because the lower two PID bits are identified in the low order bits of the address space for striped addressing, incrementing of addresses causes the data to be striped between the four processors in the local striped processor set. Thus, as data or instructions are stored, the operating system will increment the value in the address space. If striped addressing is indicated, this process automatically causes the data to be stored in an interleave fashion among the local four processor set. This then permits the data to be accessed in parallel from the four processors.</p><p>It should be understood that the local striped processor sets does not necessarily mean that these sets can only be referenced by striped addressing. As indicated above, in the preferred embodiment either striped or processor contiguous addressing may be used for all system memory. The determination of whether to use striped addressing for a memory block preferably is based on the mask in the stripe control register <b>183</b> (FIG. 2<i>b</i>).</p><p>Moreover, it should also be understood that the system is capable of translating between processor contiguous addressing and striped addressing, as required. In addition, a special addressing command may be used to transfer data requests between processors. Thus, in the preferred embodiment, a source processor identifies the memory block it needs, and sends a request in canonical form to the local processor that is acting as the memory controller for that data. If the data is stored in striped form (as indicated by the mask in the Cbox stripe control register), local processor must translate the canonical form of the address to the striped address to retrieve the requested data. Moreover, the address may need to be relayed to another processor that has the data in its cache, in which case the address must be re-converted back to canonical form for sending to the other processor who has what may be a dirty copy of the data in its cache.</p><p>The above discussion is meant to be illustrative of the principles and various embodiments of the present invention. Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. Thus, for example, although only two addressing schemes are disclosed in the preferred embodiment (which are striped and processor contiguous addressing), other addressing schemes may also be used simultaneously in addition to or as an alternative to these two addressing schemes. To implement addition addressing schemes, more than one bit would be used for the stripe bit. Thus, for example, if two bits were dedicated to the address type, four addressing schemes could be used simultaneously. In addition, the bit field positions and widths of the addressing space could be varied for each addressing scheme without departing from the principles of the present invention. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Richard E.", "last_name": "Kessler", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "HEWLETT-PACKARD DEVELOPMENT COMPANY, L.P."}, {"first_name": "", "last_name": "HEWLETT PACKARD ENTERPRISE DEVELOPMENT LP", "name": ""}, {"first_name": "", "last_name": "HEWLETT-PACKARD DEVELOPMENT COMPANY, L.P.", "name": ""}, {"first_name": "", "last_name": "COMPAQ INFORMATION TECHNOLOGIES GROUP, L.P.", "name": ""}, {"first_name": "", "last_name": "COMPAQ COMPUTER CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711157"}, {"primary": false, "label": "711220"}, {"primary": false, "label": "711E12033"}, {"primary": false, "label": "711217"}, {"primary": false, "label": "711202"}, {"primary": false, "label": "711148"}], "ecla_classes": [{"label": "G06F  12/08B4P4"}], "cpc_classes": [{"label": "G06F  12/0831"}, {"label": "G06F  12/0831"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "2000-08-31", "application_date": "2000-08-31", "family_members": [{"ucid": "US-6567900-B1", "titles": [{"lang": "EN", "text": "Efficient address interleaving with simultaneous multiple locality options"}]}]}