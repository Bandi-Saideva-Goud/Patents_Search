{"patent_number": "US-6629206-B1", "publication_id": 73549160, "family_id": 23890213, "publication_date": "2003-09-30", "titles": [{"lang": "EN", "text": "Set-associative cache-management using parallel reads and serial reads initiated during a wait state"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50556826\"><p>A Harvard-architecture computer system includes a processor, an instruction cache, a data cache, and a write buffer. The caches are both set-associative in that they each have plural memories; both caches perform parallel reads by default. In a parallel read, all cache-memory locations of the selected cache corresponding to the set ID and word position bits of a requested read address are accessed in parallel while it is determined whether or not one of these locations has a tag matching the tag portion of the requested read address. If there is a \u201chit\u201d (match), then an output multiplexer selects the appropriate cache memory for providing its data to the processor. The parallel read thus achieves faster reads, but expends extra power in accessing non-matching sets. A cache receiving a read request while the processor is waited performs a serial read instead of a parallel read. In a serial read, the tag match is performed before the data is accessed. Accordingly, a cache memory is accessed only if a match is found, achieving a power savings relative to a parallel read. There is no latency penalty since the parallel read cannot be completed during the wait. Thus, the power savings is achieved without impairing performance.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00001\" num=\"1\"><claim-text>1. A cache-management method for managing a set-associative cache with plural cache memories, said method comprising the steps of:</claim-text><claim-text>in response to a read request issued while a wait is asserted, </claim-text><claim-text>performing a serial tag-match operation using at least one cache memory, and </claim-text><claim-text>once said wait is released, accessing at most one of the memories of said cache for providing data responsive to the serial tag-match operation; and </claim-text><claim-text>in response to a read request issued while a wait is not asserted, </claim-text><claim-text>concurrently accessing a plurality of the cache memories while performing an parallel tag-match operation using the plurality of cache memories, and </claim-text><claim-text>selecting at most one of said cache memories for providing data responsive to the parallel tag-match operation. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00002\" num=\"2\"><claim-text>2. A method as recited in <claim-ref idref=\"US-6629206-B1-CLM-00001\">claim 1</claim-ref> wherein exactly one of said cache memories is accessed after said wait is released when said serial tag-match operation results in a hit.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00003\" num=\"3\"><claim-text>3. A method as recited in <claim-ref idref=\"US-6629206-B1-CLM-00001\">claim 1</claim-ref> wherein none of said cache memories is accessed after said wait is released when said serial tag-match operation results in a miss.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00004\" num=\"4\"><claim-text>4. A method as recited in <claim-ref idref=\"US-6629206-B1-CLM-00001\">claim 1</claim-ref> wherein data is provided from exactly one of said cache memories when said parallel tag-match operation results in a hit.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00005\" num=\"5\"><claim-text>5. A method as recited in <claim-ref idref=\"US-6629206-B1-CLM-00001\">claim 1</claim-ref> wherein data is not provided from any of the cache memories when said parallel tag-match operation results in a miss.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00006\" num=\"6\"><claim-text>6. A method as recited in <claim-ref idref=\"US-6629206-B1-CLM-00001\">claim 1</claim-ref> wherein said steps performed in response to said read request issued while a wait is asserted collectively consume more system clock cycles than are collectively consumed by said steps performed in response to said read request issued while a wait is not asserted.</claim-text></claim>"}, {"num": 7, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00007\" num=\"7\"><claim-text>7. A method as recited in <claim-ref idref=\"US-6629206-B1-CLM-00001\">claim 1</claim-ref>, wherein the data is provided to a processor.</claim-text></claim>"}, {"num": 8, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00008\" num=\"8\"><claim-text>8. A set-associative cache comprising:</claim-text><claim-text>plural cache memories; </claim-text><claim-text>a cache control; </claim-text><claim-text>wherein</claim-text><claim-text>in response to a read request issued while a wait is asserted,</claim-text><claim-text>the cache control performs a serial tag-match operation using at least one cache memory, and once said wait is released, the memory control enables access at most one of the memories of said cache for providing data responsive to the serial tag-match operation, and </claim-text><claim-text>in response to a read request issued while a wait is not asserted,</claim-text><claim-text>the cache control enables concurrently accessing a plurality of the cache memories while performing a parallel tag-match operation using the plurality of cache memories, and the cache control enables selecting one of said cache memories for providing data responsive to the parallel tag-match operation. </claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00009\" num=\"9\"><claim-text>9. A set-associative cache as recited in <claim-ref idref=\"US-6629206-B1-CLM-00008\">claim 8</claim-ref> wherein exactly one of said cache memories is accessed after said wait is released when said serial tag-match operation results in a hit.</claim-text></claim>"}, {"num": 10, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00010\" num=\"10\"><claim-text>10. A set-associative cache as recited in <claim-ref idref=\"US-6629206-B1-CLM-00008\">claim 8</claim-ref> wherein none of said cache memories is accessed after said wait is released when said serial tag-match operation results in a miss.</claim-text></claim>"}, {"num": 11, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00011\" num=\"11\"><claim-text>11. A set-associative cache as recited in <claim-ref idref=\"US-6629206-B1-CLM-00008\">claim 8</claim-ref> wherein data is provided from exactly one of said cache memories when said parallel tag-match operation results in a hit.</claim-text></claim>"}, {"num": 12, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00012\" num=\"12\"><claim-text>12. A set-associative cache as recited in <claim-ref idref=\"US-6629206-B1-CLM-00008\">claim 8</claim-ref> wherein data is not provided from any of the cache memories when said parallel tag-match operation results in a miss.</claim-text></claim>"}, {"num": 13, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00013\" num=\"13\"><claim-text>13. A set-associative cache as recited in <claim-ref idref=\"US-6629206-B1-CLM-00008\">claim 8</claim-ref> wherein said steps performed in response to said read request issued while a wait is asserted collectively consume more system clock cycles than are collectively consumed by said steps performed in response to said read request issued while a wait is not asserted.</claim-text></claim>"}, {"num": 14, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00014\" num=\"14\"><claim-text>14. A set-associative cache as recited in <claim-ref idref=\"US-6629206-B1-CLM-00008\">claim 8</claim-ref>, wherein the data is provided to a processor.</claim-text></claim>"}, {"num": 15, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6629206-B1-CLM-00015\" num=\"15\"><claim-text>15. A system comprising:</claim-text><claim-text>a processor; </claim-text><claim-text>a main memory; and </claim-text><claim-text>a set-associative cache memory comprising: </claim-text><claim-text>plural cache memories; </claim-text><claim-text>a cache control; </claim-text><claim-text>wherein</claim-text><claim-text>in response to a read request from the processor issued while a wait is asserted,</claim-text><claim-text>the cache control performs a serial tag-match operation using at least one cache memory, and once said wait is released, the memory control enables access at most one of the memories of said cache for providing data responsive to the serial tag-match operation; and </claim-text><claim-text>in response to a read request issued from the processor while a wait is not asserted,</claim-text><claim-text>the cache control enables concurrently accessing a plurality of the cache memories while performing a parallel tag-match operation using the plurality of cache memories, and the cache control enables selecting one of said cache memories for providing data responsive to the parallel tag-match operation.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53993580\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>The present invention relates to computers and, more particularly, to a method for managing a set-associative cache. A major objective of the present invention is to reduce the average power consumed during read operations in a set-associative cache that employs parallel reads.</p><p>Much of modern progress is associated with the increasing prevalence of computers. In a conventional computer architecture, a data processor manipulates data in accordance with program instructions. The data and instructions are read from, written to, and stored in the computer's \u201cmain\u201d memory. Typically, main memory is in the form of random-access memory (RAM) modules.</p><p>A processor accesses main memory by asserting an address associated with a memory location. For example, a 32-bit address can select any one of up to 2<sup>32 </sup>address locations. In this example, each location holds eight bits, i.e., one \u201cbyte\u201d of data, arranged in \u201cwords\u201d of four bytes each, arranged in \u201clines\u201d of four words each. In all, there are 2<sup>30 </sup>word locations, and 2<sup>28 </sup>line locations.</p><p>Accessing main memory tends to be much faster than accessing disk and tape-based memories; nonetheless, main-memory accesses can leave a processor idling while it waits for a request to be fulfilled. To minimize such latencies, a cache can intercept processor requests to main memory and attempt to fulfill them faster than main memory can.</p><p>To fulfill processor requests to main memory, caches must contain copies of data stored in main memory. In part to optimize access times, a cache is typically much less capacious than main memory. Accordingly, it can represent only a small fraction of main-memory contents at any given time. To optimize the performance gain achievable by a cache, this small fraction must be selected strategically.</p><p>In the event of a cache \u201cmiss\u201d, i.e., when a request cannot be fulfilled by a cache, the cache fetches an entire line of main memory including the memory location requested by the processor. Addresses near a requested address are more likely than average to be requested in the near future. By fetching and storing an entire line, the cache acquires not only the contents of the requested main-memory location, but also the contents of the main-memory locations that are relatively likely to be requested in the near future.</p><p>Where the fetched line is stored within the cache depends on the cache type. A fully-associative cache can store the fetched line in any cache-storage location. Typically, any location not containing valid data is given priority as a target storage location for a fetched line. If all cache locations have valid data, the location with the data least likely to be requested in the near term can be selected as the target storage location. For example, the fetched line might be stored in the location with the least recently used data.</p><p>The fully-associative cache stores not only the data in the line, but also stores the line-address (the most-significant 28 bits) of the address as a \u201ctag\u201d in association with the line of data. The next time the processor asserts a main-memory address, the cache compares that address with all the tags stored in the cache. If a match is found, the requested data is provided to the processor from the cache.</p><p>In a fully-associative cache, every cache-memory location must be checked for a tag match. Such an exhaustive match checking process can be time consuming, making it hard to achieve the access speed gains desired of a cache. Another problem with a fully-associative cache is that the tags consume a relatively large percentage of cache capacity, which is limited to ensure high-speed accesses.</p><p>In a direct-mapped cache, each cache storage location is given an index that, for example, might correspond to the least-significant line-address bits. For example, in the 32-bit address example, a six-bit index might correspond to address bits <b>23</b>-<b>28</b>. A restriction is imposed that a line fetched from main memory can only be stored at the cache location with an index that matches bits <b>23</b>-<b>28</b> of the requested address. Since those six bits are known, only the first 22 bits are needed as a tag. Thus, less cache capacity is devoted to tags. Also, when the processor asserts an address, only one cache location (the one with an index matching the corresponding bits of the address asserted by the processor) needs to be examined to determine whether or not the request can be fulfilled from the cache.</p><p>In a direct-mapped cache, a line fetched in response to a cache miss must be stored at the one location having an index matching the index portion of the read address. Previously written data at that location is overwritten. If the overwritten data is subsequently requested, it must be fetched from main memory. Thus, a directed-mapped cache can force the overwriting of data that may be likely to be requested in the near future. The lack of flexibility in choosing the data to be overwritten limits the effectiveness of a direct-mapped cache.</p><p>A set-associative cache is divided into two or more direct-mapped memories. A set identification value (\u201cset ID\u201d), corresponding to an index for the direct-mapped cache, is associated with one memory location in each set. Thus, in a four-way set associative cache, there are four cache locations with the same set ID, and thus, four choices of locations to overwrite when a line is stored in the cache. This allows more optimal replacement strategies than are available for direct-mapped caches. Still, the number of locations that must be checked, e.g., one per memory, to determine whether a requested location is represented in the cache is quite limited. Also, the number of bits that need to be compared is reduced by the length of the set ID. Thus, set-associative caches combine some of the replacement strategy flexibility of a fully-associative cache with much of the speed advantage of a direct-mapped cache.</p><p>The portion of an asserted address corresponding to the set ID identifies one cache-line location within each cache memory. The tag portion of the asserted address can be compared with the tags at the identified cache-memory line locations to determine whether there is a hit (i.e., tag match) and, if so, in which cache memory the hit occurs. If there is a hit, the least-significant address bits are checked for the requested location within the line; the data at that location is then provided to the processor to fulfill the read request.</p><p>A read operation can be hastened by starting the data access before a tag match is determined. While checking the relevant tags for a match, the data locations with the proper set ID within each cache memory are accessed in parallel. By the time a match is determined, data from all four memories are ready for transmission. The match is used, e.g., as the control input to a multiplexer, to select the data actually transmitted. If there is no match, none of the data is transmitted.</p><p>The parallel-read operation is much faster since the data is accessed at the same time as the match operation is conducted rather than after. For example, a parallel \u201ctag-and-data\u201d read operation might consume only one memory cycle, while a serial \u201ctag-then-data\u201d read operation might require two cycles. Alternatively, if the serial read operation consumes only one cycle, the parallel read operation permits a shorter cycle, allowing for more processor operations per unit of time.</p><p>The gains of the parallel tag-and-data reads are not without some cost. The data accesses to the sets that do not provide the requested data consume additional power that can tax power sources and dissipate extra heat. The heat can fatigue, impair, and damage the incorporating integrated circuit and proximal components. Accordingly, larger batteries or power supplies and more substantial heat removal provisions may be required. What is needed is a cache-management method that achieves the speed advantages of parallel reads but with reduced power consumption.</p><h4>SUMMARY OF THE INVENTION</h4><p>In a context in which parallel reads are performed by default to achieve a performance advantage, the present invention provides for initiating a serial tag-match-then-access read during a wait state. The tag-match is performed during the wait. When the wait is released, the data to be read can be accessed as determined by the tag-match operation. Further tag-then-access reads can be performed in a pipelined fashion.</p><p>For at least some processors, the assertion of a wait does not preclude the processor from requesting data. Instead, the wait prevents the processor from recognizing a clock transition that would indicate when the requested read data is valid. The read request cannot be fulfilled while the wait is asserted. However, the tag matching can be performed. By the time the wait is released, the tag match is completed. The tag match data is thus available by the time the data is needed by the processor. Accordingly, only a cache memory having the requested data needs to be accessed. The other cache memories do not need to be accessed. Thus, the power associated with those superfluous accesses can be saved.</p><p>For example, consider the case in which a parallel read consumes one system cycle and a serial read consumes two cycles\u2014the first of which is devoted to the tag-match operation, and the second is devoted to accessing the data as indicated by the tag match. If a wait is asserted for one cycle, a parallel read cannot be implemented until the cycle following the wait. In the case of a serial read, the tag-match can be completed during the wait. In either case, (assuming a cache hit) the read is fulfilled in the second cycle.</p><p>Many caches do not provide for initiating a read operation while they are asserting a wait. However, many computer systems have multiple devices that can cause a wait to be asserted. For example, in a Harvard architecture, there can be separate data and iuction caches. A suitable processor can issue a read to one cache while waited due to an incomplete operation involving the other cache. Thus, the power savings afforded by the invention can be especially significant in Harvard and other architectes in which there are multiple devices that can be the cause of a wait being asserted.</p><p>The present invention provides for power savings without impairing performance. A parallel read initiated during a wait cannot be completed until the wait is removed. Thus, while a wait is asserted, there is no latency advantage to asserting a parallel read instead of a serial read. Therefore, in such a circumstance, the power savings associated with a serial tag-match-then-access read is achieved without a performance penalty. Further power savings can be achieved by pipelining subsequent read operations. These and other features and advantages are apparent from the description below with reference to the following drawings.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>FIG. 1 is a block diagram of a computer system employing the method of the invention.</p><p>FIG. 2 is a block diagram of an instruction cache of the system of FIG. <b>1</b>.</p><p>FIG. 3 is a flow chart of the method of the invention as implemented in the computer system of FIG. <b>1</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>A Harvard-architecture computer system AP<b>1</b> comprises a processor core <b>11</b>, a system bus <b>13</b>, and main memory <b>15</b>, as shown in FIG. <b>1</b>. Processor core <b>11</b> includes a data processor <b>21</b>, a write buffer <b>23</b>, a data cache <b>25</b>, a insection cache <b>27</b>, and an internal-bus interface unit <b>29</b>. Harvard-architecture processor <b>21</b>, an Arm <b>9</b> (manufactured by VLSI Technology, Inc.), supports both a data bus <b>31</b> and an instruction bus <b>33</b>. Data bus <b>31</b> couples processor <b>21</b>, write buffer <b>23</b>, data cache <b>25</b>, and interface <b>29</b>. Instruction bus <b>33</b> couples processor <b>21</b>, instrction cache <b>27</b>, and interface <b>29</b>. Interface <b>29</b> can issue a \u201cwait\u201d signal along lines <b>35</b> that can be received by processor <b>21</b>, write buffer <b>23</b>, data cache <b>25</b>, and instruction cache <b>27</b>. Data cache <b>25</b> can issue a \u201chit\u201d indication to interface <b>29</b> via line <b>37</b>; instruction cache <b>27</b> can issue a \u201chit\u201d indication to interface <b>29</b> via line <b>39</b>.</p><p>Four-way set-associative instruction cache <b>27</b> comprises cache memories <b>41</b>, a cache controller <b>43</b>, and a 4:1 output multiplexer <b>45</b>, as shown in FIG. <b>2</b>. Cache memories <b>41</b> include four memories SE<b>1</b>, SE<b>2</b>, SE<b>3</b>, and SE<b>4</b>. Each cache memory has sixty-four cache-line locations. Each cache-line location of each memory has a corresponding six-bit set ID. Each six-bit set ID thus corresponds to four cache-line locations. Each cache-line location stores a line of four 32-bit words and a 22-bit tag. The four 32-bit words are a copy of the four 32-bit words stored in main memory at a line location corresponding to an address that is the concatenation of the tag and the set ID associated with the cache-line location at which that tag is stored.</p><p>Data cache <b>25</b> is a read-write cache, but is otherwise functionally similar to instruction cache <b>27</b>. Data-write operations are directed to both main memory <b>15</b> and to data cache <b>25</b>. The writes to main memory are buffered by write buffer <b>23</b>, which is four write-requests deep. The operation of cache <b>25</b> is otherwise apparent from the detailed description of the operation of cache <b>27</b> below with reference to FIG. <b>2</b>.</p><p>By default, when it receives a read request along the control and address lines <b>47</b> of instruction bus <b>33</b>, instruction cache <b>27</b> implements a parallel-read operation. Cache controller <b>43</b> compares the four tags stored at the four cache-line locations having a set ID matching the <b>23</b>-<b>28</b><sup>th </sup>most-significant bits of the read address. Concurrently, cache controller <b>43</b> accesses the four cache-line locations so that the data stored at the requested word location within the line is provided to the four inputs of output multiplexer <b>45</b>. In the event that the tag-match operation indicates a hit, the input of multiplexer <b>45</b> corresponding to the cache memory in which the match was found is selected to provide the requested data to processor <b>21</b> via the data lines <b>49</b> of instruction bus <b>33</b>. The hit is indicated to interface <b>29</b> so that the read request is not forwarded to main memory <b>15</b>.</p><p>In the event of a cache miss, instruction cache <b>27</b> does not couple any of the cache memories SE<b>1</b>-SE<b>4</b> to instruction bus <b>33</b>. Also, instruction cache <b>27</b> does not indicate a hit to interface <b>29</b>. Accordingly, interface <b>29</b> forwards the read request to main memory <b>15</b> via system bus <b>13</b>. An entire line of instructions is fetched from main memory <b>15</b> and stored at an appropriate location of instruction cache <b>27</b>. The requested instruction is forwarded to processor <b>21</b>.</p><p>In system AP<b>1</b>, a read from main memory can consume ten to twenty system bus cycles. During this read operation, the system bus cannot handle another request. Accordingly, interface <b>29</b> issues a \u201cwait\u201d command to processor <b>21</b> until the read request is fulfilled.</p><p>The effect of the wait command is to gate a system clock signal as received by processor <b>21</b> along line <b>51</b> (FIG. <b>1</b>). The system-clock signal has two phases, a first phase in which it is high and a second phase in which it is low. The wait signal transitions only during the second phase. When it is asserted, the system clock signal as received by processor <b>21</b> does not undergo a positive transition normally associate with the first phase of the next clock signal. Processor <b>21</b> waits for the next upward clock transition to indicate when requested instruction is available.</p><p>Interface <b>29</b> issues \u201cwait\u201d commands not only in the event of an instruction-cache miss, but also in the event of a data-cache miss, and in the event of a write operation. If processor <b>21</b> does not need the results of a first read request first, it can issue a second read request even while it is waited. However, this read request cannot be fulfilled until the wait signal associated with the first read is released.</p><p>If an instruction read request is made while processor <b>21</b> is waited due to a previous data-read miss, instruction cache <b>27</b> can still receive the request. Since the request is received, the tag-matching can be implemented. Since processor <b>21</b> cannot receive the results of the instruction read request, there is no advantage to placing the requested instruction on instruction bus <b>33</b> immediately. Accordingly, the data access can be delayed until the wait is released. Since the tag match is completed, either only one cache memory (in the case of a hit) or no cache memory (in the case of a miss) needs to be accessed. The power required is only that for a serial tag-match, yet, from the perspective of processor <b>21</b>, there is no delay in obtaining the results from the instruction read request relative to what would be achievable using a parallel read.</p><p>Of course, it does not really matter why the wait was asserted. A power savings can be achieved by initiating a serial read instead of a parallel read in the event a wait is asserted, for examples, due to a write operation, or to a read miss by the other cache. In systems where other devices can assert wait signals, the caches can similarly take advantage of the power-savings of the serial tag-match-then-access read.</p><p>A method M<b>1</b> of the invention as implemented by caches <b>25</b> and <b>27</b> of system AP<b>1</b> is flow charted in FIG. 3. A read request is detected at step S<b>1</b>. A determination of whether a \u201cwait\u201d is asserted is made at step S<b>2</b>. If a wait is asserted, serial read is performed at step S<b>3</b>. The serial read involves two substeps: 1) a tag match is performed at step S<b>3</b>A; and 2) then data is accessed and provided to processor <b>21</b> at step S<b>3</b>B. Substeps S<b>3</b>A and S<b>3</b>B are performed during different system-clock cycles. In the event of a hit, only the cache memory having the match is accessed. In the event of a miss, no cache memories are accessed, but main memory <b>15</b> is accessed.</p><p>If at step S<b>2</b>, it is determined that a wait is not asserted, a parallel read is performed at step S<b>4</b>. Parallel read step S<b>4</b> involves two substeps: 1) tag-matching and accessing all cache memories at substep S<b>4</b>A; and 2) selecting a multiplexer input (or no input) at substep S<b>4</b>B. In the event of a hit, step S<b>4</b> is completed in one system-clock cycle so that the data can be read at the beginning of the next cycle.</p><p>The invention provides for cases in which a serial read is performed even when a wait is not asserted. For example, if a read is requested while a prior read is being fulfilled from a cache, the latter read can be pipelined. The tag-match for the latter read can be performed during the same cycle that the prior read data is being transferred to the processor. In addition, a serial read can follow a write operation without costing any processor clock cycles. Furthermore, the invention is compatible with other methods for saving power during read requests; such methods include determining whether a current address is from the same line as the previous address and, if so, omitting the superfluous tag-match operation. These and other variations upon and modifications to the described embodiments are provided for by the present invention, the scope of which is defined by the following claims. In the claims, words introduced in quotes are labels and not words of limitation.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Mark W.", "last_name": "Johnson", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "KONINKLIJKE PHILIPS ELECTRONICS N.V."}, {"first_name": "", "last_name": "PHILIPS SEMICONDUCTORS VLSI INC.", "name": ""}, {"first_name": "", "last_name": "PHILIPS SEMICONDUCTORS INC.", "name": ""}, {"first_name": "", "last_name": "VLSI TECHNOLOGY, INC.", "name": ""}, {"first_name": "", "last_name": "CALLAHAN CELLULAR L.L.C.", "name": ""}, {"first_name": "", "last_name": "NXP B.V.", "name": ""}, {"first_name": "", "last_name": "NXP B.V.", "name": ""}, {"first_name": "", "last_name": "KONINKLIJKE PHILIPS ELECTRONICS N.V.", "name": ""}, {"first_name": "", "last_name": "PHILIPS SEMICONDUCTOR", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/08"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMUS"}], "national_classes": [{"primary": true, "label": "711123"}, {"primary": false, "label": "711120"}, {"primary": false, "label": "711144"}, {"primary": false, "label": "711E12018"}, {"primary": false, "label": "711145"}, {"primary": false, "label": "711131"}], "ecla_classes": [{"label": "S06F212:1028"}, {"label": "Y02B60:12F"}, {"label": "G06F  12/08B10"}], "cpc_classes": [{"label": "G06F2212/1028"}, {"label": "G06F  12/0864"}, {"label": "G06F  12/08"}, {"label": "Y02D  10/00"}, {"label": "Y02D  10/00"}, {"label": "G06F  12/0864"}, {"label": "G06F2212/1028"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1999-12-31", "application_date": "1999-12-31", "family_members": [{"ucid": "EP-1163588-A1", "titles": [{"lang": "FR", "text": "PROCEDE DE GESTION DU CACHE ASSOCIATIF UTILISANT DES LECTURES PARALLELES ET DES LECTURES SERIELLES LANCEES PENDANT L'ATTENTE DU PROCESSEUR"}, {"lang": "EN", "text": "SET-ASSOCIATIVE CACHE-MANAGEMENT METHOD USING PARALLEL READS AND SERIAL READS INITIATED WHILE PROCESSOR IS WAITED"}, {"lang": "DE", "text": "SET-ASSOZIATIVES CACHESPEICHER-VERWALTUNGSVERFAHREN MIT PARALLELEN LESEZUGRIFFEN UND SEQUENTIELLEN LESEZUGRIFFEN W\u00c4HREND EINES PROZESSOR-WARTEZUSTANDS"}]}, {"ucid": "WO-2001050273-A1", "titles": [{"lang": "EN", "text": "SET-ASSOCIATIVE CACHE-MANAGEMENT METHOD USING PARALLEL READS AND SERIAL READS INITIATED WHILE PROCESSOR IS WAITED"}, {"lang": "FR", "text": "PROCEDE DE GESTION DU CACHE ASSOCIATIF UTILISANT DES LECTURES PARALLELES ET DES LECTURES SERIELLES LANCEES PENDANT L'ATTENTE DU PROCESSEUR"}]}, {"ucid": "KR-100710922-B1", "titles": [{"lang": "KO", "text": "\uce90\uc2dc \uad00\ub9ac \ubc29\ubc95"}, {"lang": "EN", "text": "SET-ASSOCIATIVE CACHE-MANAGEMENT METHOD USING PARALLEL READS AND SERIAL READS INITIATED WHILE PROCESSOR IS WAITED"}]}, {"ucid": "DE-60044336-D1", "titles": [{"lang": "DE", "text": "SET-ASSOZIATIVES CACHESPEICHER-VERWALTUNGSVERFAHREN MIT PARALLELEN LESEZUGRIFFEN UND SEQUENTIELLEN LESEZUGRIFFEN W\u00c4HREND EINES PROZESSOR-WARTEZUSTANDS"}, {"lang": "EN", "text": "SET ASSOCIATIVE CACHE MEMORY MANAGEMENT PROCESS WITH PARALLEL READING AND SEQUENTIAL READING DURING A PROCESSOR WAIT STATE"}]}, {"ucid": "US-6629206-B1", "titles": [{"lang": "EN", "text": "Set-associative cache-management using parallel reads and serial reads initiated during a wait state"}]}, {"ucid": "KR-20010102441-A", "titles": [{"lang": "KO", "text": "\ud504\ub85c\uc138\uc11c\uac00 \ub300\uae30\ub418\ub294 \ub3d9\uc548 \uac1c\uc2dc\ub41c \ubcd1\ub82c \ud310\ub3c5 \ubc0f \uc9c1\ub82c\ud310\ub3c5\uc744 \uc774\uc6a9\ud558\ub294 \uc138\ud2b8 \uc5f0\uad00 \uce90\uc2dc \uad00\ub9ac \ubc29\ubc95"}, {"lang": "EN", "text": "SET-ASSOCIATIVE CACHE-MANAGEMENT METHOD USING PARALLEL READS AND SERIAL READS INITIATED WHILE PROCESSOR IS WAITED"}]}, {"ucid": "JP-2003519836-A", "titles": [{"lang": "JA", "text": "\u30d7\u30ed\u30bb\u30c3\u30b5\u304c\u30a6\u30a8\u30a4\u30c8\u72b6\u614b\u306e\u9593\u306b\u958b\u59cb\u3055\u308c\u305f\u4e26\u5217\u8aad\u307f\u51fa\u3057\u3068\u76f4\u5217\u8aad\u307f\u51fa\u3057\u3092\u7528\u3044\u3066\u30bb\u30c3\u30c8\u30fb\u30a2\u30bd\u30b7\u30a8\u30a4\u30c6\u30a3\u30d6\u30fb\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u7ba1\u7406\u3059\u308b\u65b9\u6cd5"}, {"lang": "EN", "text": "A method for managing a set associative cache using parallel and serial reads initiated while a processor is in a wait state"}]}, {"ucid": "EP-1163588-B1", "titles": [{"lang": "FR", "text": "PROCEDE DE GESTION DU CACHE ASSOCIATIF UTILISANT DES LECTURES PARALLELES ET DES LECTURES SERIELLES LANCEES PENDANT L'ATTENTE DU PROCESSEUR"}, {"lang": "EN", "text": "SET-ASSOCIATIVE CACHE-MANAGEMENT METHOD USING PARALLEL READS AND SERIAL READS INITIATED WHILE PROCESSOR IS WAITED"}, {"lang": "DE", "text": "SET-ASSOZIATIVES CACHESPEICHER-VERWALTUNGSVERFAHREN MIT PARALLELEN LESEZUGRIFFEN UND SEQUENTIELLEN LESEZUGRIFFEN W\u00c4HREND EINES PROZESSOR-WARTEZUSTANDS"}]}]}