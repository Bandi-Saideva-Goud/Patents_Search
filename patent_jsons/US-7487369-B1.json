{"patent_number": "US-7487369-B1", "publication_id": 92544156, "family_id": 40298172, "publication_date": "2009-02-03", "titles": [{"lang": "EN", "text": "Low-power cache system and method"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA51419479\"><p num=\"p-0001\">The invention provides a cache architecture that selectively powered-up a portion of data array in a pipelined cache architecture. A tag array is first powered-up, but the data array is not powered-up during this time, to determine whether there is a tag hit from the decoded index address comparing to the tag compare data. If there is a tag hit, during a later time, a data array is then powered-up at that time to enable a cache line which corresponds with the tag hit for placing onto a data bus. The power consumed by the tag represents a fraction of the power consumed by the data array. A significant power is conserved during the time in which the tag array is assessing whether a tag hit occurs while the data array is not powered-on at this point.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"CLM-00001\" num=\"00001\"><claim-text>1. A scalable pipelined cache architecture with at least a first one-way of associativity, comprising:\n<claim-text>a first tag array, a first data array and a first address decoder shared by both the first tag array and the first data array, the first address decoder having an input for receiving the index address, and having an output coupled directly to the first tag array;</claim-text>\n<claim-text>during a first time unit, said first tag array being powered-up for finding a first tag that corresponds to a received index address;</claim-text>\n<claim-text>a first comparator, coupled to said tag array, for comparing the first tag and a tag compare data;</claim-text>\n<claim-text>during a later second time unit, said first data array coupled to said first tag array, being powered-up if the output from said first comparator indicates that there is a tag hit;</claim-text>\n<claim-text>wherein said first data array contains data which corresponds to the first tag index address.</claim-text>\n</claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00002\" num=\"00002\"><claim-text>2. The scalable pipelined cache architecture of <claim-ref idref=\"CLM-00001\">claim 1</claim-ref>, wherein the power consumed by the first tag array is a fraction of the power consumed by the first data array.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00003\" num=\"00003\"><claim-text>3. The scalable pipelined cache architecture of <claim-ref idref=\"CLM-00001\">claim 1</claim-ref>, further comprising a data enable circuit having a first input coupled to the comparator, and having a second input coupled to the data array, the data enable circuit enabling data onto a data bus if there is a first tag hit.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00004\" num=\"00004\"><claim-text>4. The scalable pipelined cache architecture of <claim-ref idref=\"CLM-00001\">claim 1</claim-ref>, further comprising a second one-way of associativity, coupled to the first one-way of associativity, comprising:\n<claim-text>during a first time unit, a second tag array being powered-up for finding a second tag that corresponds to the received index address; and</claim-text>\n<claim-text>a second comparator, coupled to the second tag array, for comparing the second tag and the tag compare data;</claim-text>\n<claim-text>during a later second time unit, a second data array coupled to the second tag array, being powered-up, if the output from said second comparator indicates that there is a second tag hit; wherein said second data array is capable of finding data which corresponds to the second tag index address.</claim-text>\n</claim-text></claim>"}, {"num": 5, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00005\" num=\"00005\"><claim-text>5. The scalable pipelined cache architecture of <claim-ref idref=\"CLM-00004\">claim 4</claim-ref>, wherein the power consumed by the second tag array is a fraction of the power consumed by the second data array.</claim-text></claim>"}, {"num": 6, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00006\" num=\"00006\"><claim-text>6. The scalable pipelined cache architecture of <claim-ref idref=\"CLM-00004\">claim 4</claim-ref>, further comprising a second data enable circuit having a first input coupled to the second comparator, and having a second input coupled to the second data array, the second data enable circuit enabling data onto a data bus if there is a second tag hit.</claim-text></claim>"}, {"num": 7, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00007\" num=\"00007\"><claim-text>7. The scalable pipelined cache architecture of <claim-ref idref=\"CLM-00004\">claim 4</claim-ref>, further comprising a second address decoder shared by both the second tag array and the second data array, the second address decoder having an input for receiving the index address, and having an output coupled directly to the second tag array.</claim-text></claim>"}, {"num": 8, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"CLM-00008\" num=\"00008\"><claim-text>8. A method for powering a scalable pipelined cache architecture,\n<claim-text>with at least a first one-way of associativity, comprising the steps of:</claim-text>\n<claim-text>decoding an indexing address shared by a first tag array and a first data array;</claim-text>\n<claim-text>powering-up the first tag array for finding a first tag that corresponds to the received index address during a first time unit; and</claim-text>\n<claim-text>comparing the first tag to a tag compare data; and</claim-text>\n<claim-text>if the output from said first tag comparision indicates that there is a tag hit,</claim-text>\n<claim-text>in a later second time unit powering-up the first data array coupled to the first tag arrays;</claim-text>\n<claim-text>and finding</claim-text>\n</claim-text><claim-text>data in the first data array which corresponds to the first tag using a decoded index address that is coupled directly to the first tag array.</claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00009\" num=\"00009\"><claim-text>9. The method of <claim-ref idref=\"CLM-00008\">claim 8</claim-ref>, wherein the power consumed by the first tag array is a fraction of the power consumed by the first data array.</claim-text></claim>"}, {"num": 10, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00010\" num=\"00010\"><claim-text>10. The method of <claim-ref idref=\"CLM-00008\">claim 8</claim-ref>, further comprising the steps of receiving a tag compare kill signal for halting any further activity.</claim-text></claim>"}, {"num": 11, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00011\" num=\"00011\"><claim-text>11. The method of <claim-ref idref=\"CLM-00008\">claim 8</claim-ref>, further comprising the step of enabling data onto a data bus if there is a first tag hit.</claim-text></claim>"}, {"num": 12, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00012\" num=\"00012\"><claim-text>12. The method of <claim-ref idref=\"CLM-00008\">claim 8</claim-ref>, further comprising the step of coupling a second one-way of associativity to the first one-way of associativity, the second one-way associativity comprising,\n<claim-text>decoding the indexing address shared by a second tag array and a second data array;</claim-text>\n<claim-text>powering-up the second tag array for finding a second tag that corresponds to the receiving index address during a first time unit; and</claim-text>\n<claim-text>powering-up the second data array coupled to the second tag array, for finding data which corresponds to the second tag during a later second time unit;</claim-text>\n<claim-text>wherein the indexed address is coupled directly to the second tag array.</claim-text>\n</claim-text></claim>"}, {"num": 13, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"CLM-00013\" num=\"00013\"><claim-text>13. A scalable pipelined cache architecture comprising:\n<claim-text>a first tag array, said first tag array being powered-up during a first pipeline cache clock edge, said first tag array being capable of finding a first tag that corresponds to a received index address;</claim-text>\n<claim-text>a first comparator, coupled to the said first tag array, for comparing the first tag and a tag compare data;</claim-text>\n<claim-text>a first data array coupled to the first tag array, said first data array being powered-up, during a second pipeline cache clock edge that is after the first pipeline cache clock edge, if the output from said first comparator indicates that there is a tag hit;</claim-text>\n<claim-text>said first data array being capable of finding data which corresponds to the first tag.</claim-text>\n</claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00014\" num=\"00014\"><claim-text>14. The pipelined cache architecture of <claim-ref idref=\"CLM-00013\">claim 13</claim-ref>, used for a pipelined processor in a computing system.</claim-text></claim>"}, {"num": 15, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00015\" num=\"00015\"><claim-text>15. The pipelined architecture of <claim-ref idref=\"CLM-00013\">claim 13</claim-ref>, wherein said first data array is not powered up if a tag compare kill signal is received.</claim-text></claim>"}, {"num": 16, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00016\" num=\"00016\"><claim-text>16. The scalable pipelined cache architecture of <claim-ref idref=\"CLM-00013\">claim 13</claim-ref>, further comprising a data enable circuit having a first input coupled to the first comparator, and having a second input coupled to the data array, the data enable circuit enabling data onto a data bus if there is a tag hit.</claim-text></claim>"}, {"num": 17, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"CLM-00017\" num=\"00017\"><claim-text>17. A method for powering a scalable pipelined cache architecture, comprising the steps of:\n<claim-text>decoding an indexing address suspected of being shared by a first tag array and a first data array;</claim-text>\n<claim-text>during at least a first pipeline cache clock edge, powering-up the first tag array for finding a first tag that corresponds to the index address; and</claim-text>\n<claim-text>comparing the first tag to a tag compare data; and</claim-text>\n<claim-text>if the output from said first tag comparison indicates that there is a tag hit, powering-up the first data array in a later second pipeline cache clock edge.</claim-text>\n</claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00018\" num=\"00018\"><claim-text>18. The method of <claim-ref idref=\"CLM-00017\">claim 17</claim-ref>, used to run a pipelined processor in a computing system.</claim-text></claim>"}, {"num": 19, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00019\" num=\"00019\"><claim-text>19. The method of <claim-ref idref=\"CLM-00017\">claim 17</claim-ref>, wherein said first data array is not powered up if a tag compare kill signal is received.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES16543301\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND INFORMATION</h4><p num=\"p-0002\">1. Field of the Invention</p><p num=\"p-0003\">The invention relates to integrated circuits, and particularly to memory system designs of a cache architecture.</p><p num=\"p-0004\">2. Description of Related Art</p><p num=\"p-0005\">In a mobile society at the start of a new millennium, a challenge in designing a compact or handheld device is to extend the battery power duration after a charge-up. A cache is an integral part of a computing system but draws a significant amount of system power. A design trend in the past has a dominant focus on finding new ways to increase the speed of a computing system. However, prolonging a battery power has become a primary focus in the design of wireless and mobile devices.</p><p num=\"p-0006\">A cache refers to a storage architecture in integrated circuits and software where the most commonly used data is tagged and stored for quick retrieval. A principle usage of a cache is to speed-up processing of information of an application program. A cache tags a piece of data or information using a tagging algorithm. The tag itself and the related data are stored. When a processor seeks to retrieve a piece of data, the same tagging algorithm is applied to generate a tag in which the tag is used to identify whether the data exists in the cache.</p><p num=\"p-0007\"><figref idrefs=\"DRAWINGS\">FIG. 1</figref> is a prior art diagram illustrating a conventional two-way associativity cache architecture <b>10</b>. Cache architecture <b>10</b> includes two one-way of associativities <b>11</b> and <b>12</b>. An address decoder <b>13</b> decodes an index address <b>25</b> for use in a tag array <b>14</b>, and a separate address decoder <b>16</b> is used for a data array <b>17</b> in associativity <b>11</b>. Similarly, an address decoder <b>19</b> is used for a tag array <b>20</b>, and a separate address decoder <b>22</b> is used for a data array <b>23</b> in associativity <b>12</b>. The same index line <b>25</b> is fed feed to all four address decoders <b>13</b>, <b>16</b>, <b>19</b>, and <b>22</b>.</p><p num=\"p-0008\">When index line-<b>25</b> is received by cache architecture <b>10</b>, all four address decoders <b>13</b>, <b>16</b>, <b>19</b>, and <b>22</b> are powered-up. A tag look-up and a data look-up are performed simultaneously in tag array <b>14</b>, data array <b>17</b>, tag array <b>20</b>, and data array <b>23</b>. A comparator <b>15</b> compares the tag from tag array <b>14</b> with a tag compare data <b>26</b> in associativity <b>11</b>, while a comparator <b>21</b> compares the tag from tag array <b>20</b> with a tag compare data <b>26</b> in associativity <b>12</b>. One of the two data enables <b>18</b> and <b>24</b> is enabled to generate the output on a data bus <b>27</b>. A shortcoming of this conventional cache architecture <b>10</b> is that a large amount of power is consumed by simultaneous activation of tag array <b>14</b>, tag array <b>20</b>, data array <b>17</b>, and data array <b>23</b>. When additional associativities are stacked over existing associativities, cache architecture draws an even greater amount of power as well as causing potential timing problems.</p><p num=\"p-0009\">Accordingly, it is desirable to have a cache architecture that is modular and scalable that consumes low-power.</p><h4>SUMMARY OF THE INVENTION</h4><p num=\"p-0010\">The invention provides a cache architecture that selectively powered-up a portion of data array in a pipelined cache architecture. A tag array is first powered-up, but the data array is not powered-up during this time, to determine whether there is a tag hit from the decoded index address comparing to the tag compare data. If there is a tag hit, during a later time, a data array is then powered-up at that time to enable a cache line which corresponds with the tag hit for placing onto a data bus. The power consumed by the tag represents a fraction of the power consumed by the data array. A significant power is conserved during the time in which the tag array is assessing whether a tag hit occurs while the data array is not powered-on at this point.</p><p num=\"p-0011\">Advantageously, the cache architecture in the present invention reduces power dissipation, increases modularity, and provides scalable associativity. The complexity of the cache architecture is also simplified by sharing circuits among tag and data arrays. Moreover, additional associativity can be added to the cache architecture without incurring additional costs.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><description-of-drawings><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p num=\"p-0012\"><figref idrefs=\"DRAWINGS\">FIG. 1</figref> (Prior Art) is a block diagram illustrating a conventional cache architecture.</p><p num=\"p-0013\"><figref idrefs=\"DRAWINGS\">FIG. 2</figref> is a block diagram illustrating a cache pipelined architecture in accordance with the present invention.</p><p num=\"p-0014\"><figref idrefs=\"DRAWINGS\">FIG. 3</figref> is a flow diagram illustrating a pipelined cache access in a cache architecture in accordance with the present invention.</p><p num=\"p-0015\"><figref idrefs=\"DRAWINGS\">FIG. 4</figref> is a timing diagram illustrating a pipelined cache access in a cache architecture in accordance with the present invention.</p></description-of-drawings><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF PREFERRED EMBODIMENT</h4><p num=\"p-0016\"><figref idrefs=\"DRAWINGS\">FIG. 2</figref> is a block diagram illustrating a cache pipelined architecture <b>30</b>, with an associativity <b>31</b>, and an associativity <b>32</b>. An address decoder <b>33</b> is shared by a tag array <b>34</b> and data array <b>35</b> in associativity <b>31</b>. Similarly, a common address decoder <b>38</b> is used in associativity <b>32</b> for decoding with a tag array <b>39</b>, and a data array <b>40</b>.</p><p num=\"p-0017\">Address decoder <b>33</b> serves to decode the incoming index address <b>43</b> in associativity <b>31</b>. Initially, tag array <b>34</b> is powered-up without supplying power to data array <b>35</b>. A comparator <b>36</b> compares the tag from tag array <b>34</b> with a tag compare data <b>44</b>. Comparator <b>36</b> generates an output signal that enables or disables a data enable <b>37</b> for powering up or not powering up data array <b>35</b>. When data array <b>35</b> is powered-up, and there is a tag hit, data enable <b>37</b> enables output to place data on a data bus <b>45</b>.</p><p num=\"p-0018\">Similar type of flow is generated through associativity <b>32</b>. Address decoder <b>38</b> decodes the incoming index address <b>43</b>. Initially, tag array <b>39</b> is powered-up without supplying power to data array <b>40</b>. A comparator <b>41</b> compares the tag from tag array <b>39</b> with a tag compare data <b>44</b>. Comparator <b>41</b> generates an output signal that enables or disables a data enable <b>42</b> on whether to powered-up data array <b>40</b>. When data array <b>40</b> is powered-up, and there is a tag hit, data enable <b>42</b> enables output to place data on a data bus <b>45</b>.</p><p num=\"p-0019\">Cache architecture <b>30</b> is designed in a serial or pipelined process with only one address decoder, i.e. address decoder <b>33</b>, rather than two decoders. The serial process allows cache architecture <b>30</b> to powered-up tag array <b>34</b>, while saving power supplied to data array <b>35</b> since data array <b>35</b> is not powered-up until a next phase if there is a tag hit.</p><p num=\"p-0020\">Tag array <b>34</b> has an array size which is only a percentage of the array size of data array <b>35</b>. For example, in one design implementation, the array size in tag array <b>34</b> is only 10% relative to the size of data array <b>35</b>, which would be 90%. In terms of power consumption, tag array <b>34</b> draws 10% of power while data array is not powered-on. Therefore, there is a saving of 90% power that is typically required had data array <b>35</b> be powered-on simultaneously with tag array <b>34</b>. The ratio of an array size between tag array <b>34</b> and data array <b>35</b> is merely an illustration. Other desirable ratio or representation of ratio such as fraction, a portion to, relative to, or similar types of fractional relationship can be designated in the design of associativity <b>31</b>.</p><p num=\"p-0021\"><figref idrefs=\"DRAWINGS\">FIG. 3</figref> is a flow diagram illustrating a pipelined cache access method <b>50</b>. Pipelined cache access method <b>50</b> starts <b>51</b> while waiting <b>52</b> for a new clock edge to arrive. If there is no clock edge, pipelined cache method <b>50</b> continues to wait for a new clock edge. After a new clock edge has arrived, pipelined cache method <b>50</b> detects whether a cache power is enabled <b>53</b>. When a cache power is enabled, pipelined cache method <b>50</b> decodes <b>54</b> the index address <b>43</b>. Pipelined cache method <b>50</b> then look-up <b>55</b> tag array <b>34</b>. After a tag in a tag array is found or not found, pipelined cache method <b>50</b> waits until the next clock edge to arrive. Optionally, a tag compare kill signal can be added <b>57</b> for halting the process flow. If the tag compare kill is not enabled, comparator <b>36</b> compares the tag from tag array <b>34</b> with tag compare data <b>44</b>. Pipelined cache method <b>50</b> determines <b>59</b> whether a tag hit occurs. On the one hand, if comparator <b>36</b> generates a tag miss, then the process returns to <b>51</b>. On the other hand, if comparator <b>36</b> determines that there is a tag hit, then pipelined cache method <b>50</b> waits <b>60</b> until the next block edge arrives. When the next clock edge arrives, pipelined cache method <b>50</b> performs <b>61</b> a data look-up in data array <b>35</b> and enables an output on data bus <b>45</b>.</p><p num=\"p-0022\">Pipelined cache access method <b>50</b> is generally divided into three phases or cycles. During the first phase, pipelined cache access method <b>50</b> detects <b>52</b> a new clock edge, enables or disables <b>53</b> cache power, decodes address <b>54</b>, and look-up <b>55</b> of tag array <b>34</b>. During the second phase, pipelined cache access method <b>50</b> receives or did not receive <b>57</b> a tag compare kill, compare <b>58</b> of a tag from tag array <b>34</b> with tag compare data <b>44</b>, and detects <b>59</b> a tag hit or a tag miss. During the third phase, pipelined cache access method <b>50</b> performs <b>61</b> a data look-up and enables data onto output data bus <b>45</b>.</p><p num=\"p-0023\">Preferably, a wide data is generated from data array <b>35</b> to data enable <b>37</b> in one-way of associativity <b>31</b>. Similarly, a wide data is generated from data array <b>40</b> to data enable <b>42</b> in one- way of associativity <b>32</b>.</p><p num=\"p-0024\"><figref idrefs=\"DRAWINGS\">FIG. 4</figref> is a timing diagram illustrating a pipelined cache access in a cache pipelined architecture. During a T1 phase, cache pipelined architecture <b>30</b> waits to receive a new clock edge from a clock signal <b>70</b> in T1 for decoding an index address or tag address <b>71</b>, and for powering and accessing tag array <b>34</b>. During a T2 phase, cache pipelined architecture <b>30</b> waits to receive a new clock edge in T2 for comparing index address <b>43</b> with a tag in tag array <b>34</b> to determine whether there is tag hit <b>72</b> or tag miss. During a T3 phase, if there is a tag hit, data array <b>35</b> is powered-on in T3 where a cache wordline <b>73</b> is asserted for locating the corresponding cache line in data array <b>40</b>. A cache data out signal <b>74</b> is enabled in data array <b>35</b> for enabling data onto data bus <b>45</b>.</p><p num=\"p-0025\">Optionally, the signals depicted in cache architecture <b>30</b> and pipelined cache architecture method <b>50</b> can be implemented in various forms. For example, the triggering of a clock signal and a tag compare kill signal can be designed for assertion singularly or in various combinations in address decoder <b>33</b>, tag array <b>34</b>, data array <b>35</b>, comparator <b>36</b>, or data enable <b>37</b>. Additionally, one of ordinary skilled in the art should recognize that the block diagram arrangement in cache architecture <b>30</b> can be modified in various sequence and combinations to achieve the power-saving in data array <b>35</b> while tag array <b>34</b> is initially powered-up.</p><p num=\"p-0026\">The above embodiments are only illustrative of the principles of this invention and are not intended to limit the invention to the particular embodiments described. For example, it is apparent to one of ordinary skilled in the art that a cache architecture can be implemented as a two-way of associativities, a four-way of associativities, or any binary or odd combination of associativities. Furthermore, although the term \u201cphase\u201d, which equals to one-half clock cycle, is used, other types of time units can be implemented, such as self-time, one or more clock cycles, or units less than a phase. The clock <b>70</b> can be triggered on a rising edge, a fallen edge, or in response to another signal. Accordingly, various modifications, adaptations, and combinations of various features of the described embodiments can be practiced without departing from the scope of the invention as set forth in the appended claims.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Mayank", "last_name": "Gupta", "name": ""}, {"first_name": "Edward T.", "last_name": "Pak", "name": ""}, {"first_name": "Javier", "last_name": "Villagomez", "name": ""}, {"first_name": "Peter H.", "last_name": "Voss", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "RMI CORPORATION"}, {"first_name": "", "last_name": "BROADCOM CORPORATION", "name": ""}, {"first_name": "", "last_name": "AVAGO TECHNOLOGIES GENERAL IP (SINGAPORE) PTE. LTD.", "name": ""}, {"first_name": "", "last_name": "BANK OF AMERICA, N.A., AS COLLATERAL AGENT", "name": ""}, {"first_name": "", "last_name": "BROADCOM CORPORATION", "name": ""}, {"first_name": "", "last_name": "NETLOGIC I LLC", "name": ""}, {"first_name": "", "last_name": "NETLOGIC MICROSYSTEMS, INC.", "name": ""}, {"first_name": "", "last_name": "NETLOGIC MICROSYSTEMS, INC.", "name": ""}, {"first_name": "", "last_name": "RMI CORPORATION", "name": ""}, {"first_name": "", "last_name": "RAZA MICROELECTRONICS, INC.", "name": ""}, {"first_name": "", "last_name": "VENTURE LENDING & LEASING IV, INC.", "name": ""}, {"first_name": "", "last_name": "RAZA MICROELECTRONICS, INC.", "name": ""}, {"first_name": "", "last_name": "SANDCRAFT INCORPORATED", "name": ""}], "ipc_classes": [], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   1/32        20060101AFI20090203BHUS"}, {"label": "G06F  12/00        20060101ALI20090203BHUS"}], "national_classes": [{"primary": true, "label": "713300"}, {"primary": false, "label": "711118"}], "ecla_classes": [{"label": "S06F212:1028"}, {"label": "G06F   1/32P5P8"}, {"label": "Y02B60:12F"}, {"label": "G06F   1/32P"}, {"label": "G06F  12/08B"}], "cpc_classes": [{"label": "G06F2212/1028"}, {"label": "G06F   1/3203"}, {"label": "G06F   1/3275"}, {"label": "Y02D  10/00"}, {"label": "G06F   1/3203"}, {"label": "G06F   1/3275"}, {"label": "G06F  12/0802"}, {"label": "Y02D  10/00"}, {"label": "G06F2212/1028"}, {"label": "G06F  12/0802"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "2000-05-01", "application_date": "2000-05-01", "family_members": [{"ucid": "US-7487369-B1", "titles": [{"lang": "EN", "text": "Low-power cache system and method"}]}]}