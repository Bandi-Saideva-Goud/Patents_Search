{"patent_number": "US-6504785-B1", "publication_id": 73249981, "family_id": 21834602, "publication_date": "2003-01-07", "titles": [{"lang": "EN", "text": "Multiprocessor system with integrated memory"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50433130\"><p>A multiprocessor processing <b>200 </b>includes a memory system having a memory controller <b>202 </b>for linking a plurality of processors <b>201 </b>with an integrated memory <b>203</b>. Integrated memory <b>203 </b>comprises a plurality of static random access arrays <b>603 </b>and a dynamic random access <b>407. </b></p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11442846\" source=\"national office\"><p>A multiprocessor processing 200 includes a memory system having a memory controller 202 for linking a plurality of processors 201 with an integrated memory 203. Integrated memory 203 comprises a plurality of static random access arrays 603 and a dynamic random access 407.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00001\" num=\"1\"><claim-text>1. In a multiprocessor processing system, a memory system comprising:</claim-text><claim-text>a controller for linking at least two processing units to a memory; and </claim-text><claim-text>an integrated memory coupled to said controller, said integrated memory having a plurality of banks, each having a plurality of static random access cell arrays and a dynamic random access cell array coupled to said SRAM, and a data port for coupling said plurality of banks to said controller each of said banks further comprising: </claim-text><claim-text>a plurality of sets of latches each for storing address bits associated with data stored in a corresponding one of said static random access cell arrays; and </claim-text><claim-text>bit comparison circuitry for comparing an address bit received from said controller with an address bit stored in each of said plurality of sets of latches, and enabling access to a selected one of said static random access cell arrays corresponding to a said set of latches storing an address bit matching said received address bits. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00002\" num=\"2\"><claim-text>2. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00001\">claim 1</claim-ref>, wherein each said bank further comprises circuitry for selectively exchanging data between said dynamic random access array to a selected one of said static random access memory arrays.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00003\" num=\"3\"><claim-text>3. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00001\">claim 1</claim-ref>, wherein each said bank further comprises column decoder circuitry including a plurality of column decoders, each said column decoder for accessing a corresponding one of said static random access cell arrays.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00004\" num=\"4\"><claim-text>4. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00001\">claim 1</claim-ref>, wherein each of said plurality of banks comprise:</claim-text><claim-text>a row address latch for storing received address bits; and </claim-text><claim-text>circuitry for modifying address bits stored in said address latch to produce second address bits. </claim-text></claim>"}, {"num": 5, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00005\" num=\"5\"><claim-text>5. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00002\">claim 2</claim-ref>, wherein said plurality of latches further comprises a plurality of data latches a predetermined number of which are associated with each of said plurality of banks, and coupled to said controller via a data port in said associated bank.</claim-text></claim>"}, {"num": 6, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00006\" num=\"6\"><claim-text>6. In a multiprocessor data processing system, a memory system comprising:</claim-text><claim-text>a memory controller for linking at least two processing devices to a memory; and </claim-text><claim-text>an integrated memory coupled to said memory controller, said integrated memory comprising a plurality of memory banks, each of said plurality of memory banks having a plurality of static cell arrays, a dynamic cell array coupled to said static cell arrays and wherein address bits for accessing said dynamic and static cell arrays of each of said banks associated with each memory bank span an address space of said integrated memory wherein each said bank further comprises: </claim-text><claim-text>a plurality of sets of latches each for storing address bits associated with data stored in a corresponding one of said static cell arrays; and </claim-text><claim-text>bit comparison circuitry for comparing an address bit received from said memory controller with an address bit stored in each of said plurality of sets of latches, and enabling access to a selected one of said static cell arrays corresponding to a said set of latches storing an address bit matching said received address bits. </claim-text></claim>"}, {"num": 7, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00007\" num=\"7\"><claim-text>7. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00007\">claim 7</claim-ref>, wherein each said bank further comprises circuitry for selectively exchanging data between said dynamic array to a selected one of said static memory arrays.</claim-text></claim>"}, {"num": 8, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00008\" num=\"8\"><claim-text>8. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00006\">claim 6</claim-ref>, wherein each said bank further comprises column decoder circuitry including a plurality of column decoders, each said column decoder for accessing a corresponding one of said static random access cell arrays.</claim-text></claim>"}, {"num": 9, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00009\" num=\"9\"><claim-text>9. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00006\">claim 6</claim-ref>, wherein each said bank further comprises:</claim-text><claim-text>a row address latch for storing received address bits associated with data stored in a corresponding one of said static cell arrays; and </claim-text><claim-text>circuitry for modifying address bits stored in said address latch to produce second address bits. </claim-text></claim>"}, {"num": 10, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00010\" num=\"10\"><claim-text>10. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00006\">claim 6</claim-ref> further comprises a plurality of data latches coupled to said dynamic cell array and said static cell arrays said plurality of data latches being coupled to said memory controller via a data port.</claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00011\" num=\"11\"><claim-text>11. In a multiprocessor data processing system, a memory system comprising:</claim-text><claim-text>a memory controller for linking at least two central processing units (CPU) to a memory; </claim-text><claim-text>a plurality of integrated memories each having a dynamic memory array and static random access memory cell array cache, each of the plurality of integrated memories comprising: </claim-text><claim-text>a plurality of sets of latches each for storing address bits associated with data stored in a corresponding one of said static random access cell arrays; and </claim-text><claim-text>bit comparison circuitry for comparing an address bit received from said memory controller with an address bit stored in each of said plurality of sets of latches, and enabling access to a selected one of said static random access cell arrays corresponding to a said set of latches storing an address bit matching said received address bits. </claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00012\" num=\"12\"><claim-text>12. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00011\">claim 11</claim-ref>, wherein each integrated memory occupies an independent memory space.</claim-text></claim>"}, {"num": 13, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00013\" num=\"13\"><claim-text>13. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00011\">claim 11</claim-ref>, wherein said memory controller links said at least two CPUs and said plurality of integrated memories in crossbar switch fashion.</claim-text></claim>"}, {"num": 14, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00014\" num=\"14\"><claim-text>14. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00011\">claim 11</claim-ref>, wherein each integrated memory comprises:</claim-text><claim-text>a plurality of static random access (SRAM) cell arrays, and a plurality of dynamic random access (DRAM) cell arrays coupled to said SRAM; and </claim-text><claim-text>a plurality of data latches coupled to said DRAM and said SRAM, said plurality of data latches being coupled to said memory controller via a data port. </claim-text></claim>"}, {"num": 15, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00015\" num=\"15\"><claim-text>15. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00011\">claim 11</claim-ref>, wherein each integrated memory further comprises:</claim-text><claim-text>circuitry for selectively exchanging data between said dynamic random access array to a selected one of said static random access memory arrays. </claim-text></claim>"}, {"num": 16, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00016\" num=\"16\"><claim-text>16. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00011\">claim 11</claim-ref>, wherein each integrated memory further comprises column decoder circuitry including a plurality of column decoders, each said column decoder for accessing a corresponding one of said static random access cell arrays.</claim-text></claim>"}, {"num": 17, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00017\" num=\"17\"><claim-text>17. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00011\">claim 11</claim-ref>, wherein each integrated memory further comprises:</claim-text><claim-text>a row address latch for storing received address bits associated with data stored in a corresponding one of said SRAM cell arrays; and </claim-text><claim-text>circuitry for modifying address bits stored in said address latch to produce second address bits. </claim-text></claim>"}, {"num": 18, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00018\" num=\"18\"><claim-text>18. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00011\">claim 11</claim-ref>, wherein each integrated memory further a plurality of memory banks.</claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00019\" num=\"19\"><claim-text>19. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00018\">claim 18</claim-ref>, wherein each memory bank further comprises:</claim-text><claim-text>a plurality of static random access (SRAM) cell arrays, and a plurality of dynamic random access (DRAM) cell arrays coupled to said SRAM; and </claim-text><claim-text>a plurality of data latches coupled to said DRAM and said SRAM, said plurality of data latches being coupled to said memory controller via a data port. </claim-text></claim>"}, {"num": 20, "parent": 19, "type": "dependent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00020\" num=\"20\"><claim-text>20. The memory system of <claim-ref idref=\"US-6504785-B1-CLM-00019\">claim 19</claim-ref>, wherein said plurality of data latches further comprises a plurality of data latches a predetermined number of which are associated with each of said plurality of memory banks, and coupled to said memory controller via a data port in said associated memory bank.</claim-text></claim>"}, {"num": 21, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6504785-B1-CLM-00021\" num=\"21\"><claim-text>21. An integrated circuit comprising:</claim-text><claim-text>at least one central processing unit (CPU); </claim-text><claim-text>a controller for linking said at least one central processing unit to a memory; and </claim-text><claim-text>an integrated memory coupled to said controller, said integrated memory having a plurality of banks, each having a plurality of static random access cell arrays and a dynamic random access cell array coupled to said SRAM, and a data port for coupling said plurality of banks to said controller, each of said plurality of banks comprising: </claim-text><claim-text>a plurality of sets of latches each for storing address bits associated with data stored in a corresponding one of said static random access cell arrays; and </claim-text><claim-text>bit comparison circuitry for comparing an address bit received from said controller with an address bit stored in each of said plurality of sets of latches, and enabling access to a selected one of said static cell random access arrays corresponding to a said set of latches storing an address bit matching said received address bits.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53858176\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>CROSS-REFERENCE TO RELATED APPLICATIONS</h4><p>The present invention is related to DRAM WITH INTEGRAL SRAM, U.S. Pat. No. 5,835,932, issued Nov. 10, 1998; filed on Mar. 13, 1997, and U.S. Pat. No. 5,890,195 issued Mar. 30, 1999; and This application is a Divisional Application of application Ser. No. 09/026,927, entitled \u201cMULTI-PORT DRAM WITH INTEGRATED SRAM AND SYSTEMS AND METHODS USING THE SAME\u201d, filed Feb. 20, 1998, now U.S. Pat. No. 6,173,356 issued Jan. 1, 2001, filed on May 14, 1997.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>TECHNICAL FIELD OF THE INVENTION</h4><p>The present invention relates in general to electronic memories and in particular to a dynamic random access memory (DRAM) with integral static random access memory (SRAM), and systems and methods using the same.</p><h4>BACKGROUND OF THE INVENTION</h4><p>Currently available dynamic random access memories (DRAMs) are generally based upon architectures which share the following characteristics. First, the typical general purpose DRAM has a single data port for writing and reading data to and from addressed storage locations (\u201cdual ported\u201d DRAMs are available which provide two data ports, typically one random and one serial port, however, these devices are normally limited to special memory applications). Second, data writes and reads are only made on a location by location basis, with each location typically being one bit, one byte or one word wide. Specifically, in a \u201crandom access mode\u201d, an access (read or write) is made to a single location per row address strobe (/RAS) active cycle and in a \u201cpage mode\u201d an access is made to a single location per column address strobe (/CAS) or master clock cycle of the row addressed during the given /RAS cycle. Alternatively, in synchronous DRAM, a memory access cycle is initiated by asserting an active command in the DRAM, during which row addresses are latched on the rising edge of a master clock. A read/write command causes column addresses to be latched on the rising edge of the master clock following which, after a latency period expires, data is clocked out with each rising edge on the master clock. Third, no method has generally been established to handle contention problems which arise when simultaneous requests for access are made to the same DRAM unit. Current techniques for handling contention problems depend on the DRAM and/or system architecture selected by the designer and range, for example, from \u201cuniform memory-noncontention\u201d methods to \u201cnon-uniform memory access\u201d (NUMA) methods.</p><p>Similarly, the system architectures of personal computers (PCs) generally share a number of common features. For example, the vast majority of today's PCs are built around a single central processing unit (CPU), which is the system \u201cmaster.\u201d All other subsystems, such as the display controller, disk drive controller, and audio controller then operate as slaves to the CPU. This master/slave organization is normally used no matter whether the CPU is a complex instruction set computer (CISC), reduced instruction set computer (RISC), Silicon Graphics MIPS device or Digital Equipment ALPHA device.</p><p>Present memory and PC architectures, such as those discussed above, are rapidly becoming inadequate for constructing the fast machines with substantial storage capacity required to run increasingly sophisticated application software. The problem has already been addressed, at least in part, in the mainframe and server environments by the use of multiprocessor (multiprocessing) architectures. Multiprocessing architectures however are not yet cost effective for application in the PC environment. Furthermore, memory contention and bus contention are still significant concerns in any multiprocessing system, and in particular in a multiprocessing PC environment.</p><p>A CPU typically exchanges data with memory in terms of \u201ccache lines.\u201d Cache lines are a unit of data by which operandi and results can be stored or retrieved from memory and operated on by the CPU in a coherent fashion. Cache lines accesses are made both to cache and to system memory.</p><p>In systems operating with CPUs having a 32-bit data I/O port, a cache line is normally eight (8) 32-bit words or 256 bits. In the foreseeable future, data I/O ports will be 64 bits wide, and cache lines may be comprised of 16 64-bit data words or 1024 bits in length. Typically, the CPU may read a cache line from a corresponding location in memory, perform an arithmetic or logic operation on that data and then write the result back to the same location in system or cache memory. A given location for a cache line can be in one or more physical rows in memory and therefore an access to cache line location may require multiple /RAS cycles. In any event, the CPU, depending on the operating system running, can generally access any location in memory for storing and retrieving operandi and results.</p><p>Often situations arise when the results from a given operation exceed the length of the cache line and therefore data can no longer be processed as coherent cache line units. For example, if the CPU performs a n by n bit integer multiplication, the result could be a maximum of 2n bits. In other words, while each operand can be retrieved from memory as a cache line, the result exceeds the length of a single cache line and coherency is lost. Similarly, when operandi containing decimal points or fractions are involved, exceeding the length of a cache line can also take place. In the case of fractions, long strings of bits, which exceed cache line length, may be required to minimize rounding errors and therefore increase the precision of the calculations.</p><p>In any computing system, and in particular multiprocessing systems, the ability to operate on data as cache lines substantially improves operating efficiency. Thus, when a cache line is exceeded during an operation, system performance is reduced. Specifically, when a cache line is exceeded, the CPU must either access that data as two cache lines or as a cache line and additional discrete words or doublewords of data. As a result, extra memory cycles are required to execute an operation and the transfer of data within the system is more difficult because the necessary data is no longer in proper cache line data structures. Moreover, performance in multiprocessor systems is impaired when one processor is waiting for a second processor to complete its read or write to memory before being able to read or write its data.</p><p>Thus, the need has arisen for new memory and system architectures in which operations can be performed on coherent units of data, even if cache lengths are exceeded. In particular in multiprocessor systems, there is a need for system and memory architectures in which multiple processors can operate on data simultaneously.</p><h4>SUMMARY OF THE INVENTION</h4><p>Among the many advantages, the principles of the present invention allow for the efficient accessing of blocks of data as required by the multiple CPU data processing system. For example, in a four bank embodiment, with two registers per bank, a contiguous block of eight rows of data and associated addresses can be stored in register for fast access. Typically, the CPU accesses data within such spatially or temporally contiguous blocks. Thus, when the CPU requires data from memory, and that data is already stored in register, data with a given spatial or temporal locality thereof is also most likely already in a register. In this fashion, the number of \u201chits\u201d to pre-stored data is substantially increased. The principles of the present invention also allow for high speed accesses directly from the registers, in addition to traditional accesses to the DRAM cell array. The advantages are particularly evident, in a single chip implementation according to the principles of the present invention.</p><p>A data port associated with each bank provides for independent access to each bank by the multiple processors. In an embodiment having an address port in each bank, the multiple processors may independently access incongruent memory locations in each bank. That is, memory cells having different relative locations within each bank are accessible, in this embodiment.</p><p>The foregoing has outlined rather broadly the features and technical advantages of the present invention in order that the detailed description of the invention that follows may be better understood. Additional features and advantages of the invention will be described hereinafter which form the subject of the claims of the invention. It should be appreciated by those skilled in the art that the conception and the specific embodiment disclosed may be readily utilized as a basis for modifying or designing other structures for carrying out the same purposes of the present invention. It should also be realized by those skilled in the art that such equivalent constructions do not depart from the spirit and scope of the invention as set forth in the appended claims.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>For a more complete understanding of the present invention, and the advantages thereof, reference is now made to the following descriptions taken in conjunction with the accompanying drawings, in which:</p><p>FIG. 1A is a drawing, in block form, of a data processing system including a memory system in accordance with the principles of the present invention.</p><p>FIG. 1B is a drawing, in block form, of an alternative embodiment of a data processing system including a memory system in accordance with the principles of the present invention.</p><p>FIG. 2A is a drawing, in block form, of an embodiment of multiprocessor system according to the principles of the present invention.</p><p>FIG. 2B is a drawing, in block form, of an alternative embodiment of a multiprocessor system according to the principles of the present invention.</p><p>FIG. 2C is a drawing, in block form, of another alternative embodiment of a multiprocessor system according to the principles of the present invention.</p><p>FIG. 3A is a diagram illustrating a \u201cloosely coupled\u201d multiprocessing architecture in which each processing node maintains a dedicated copy of the operating system and a dedicated copy of the applications program;</p><p>FIG. 3B depicts a \u201ctightly coupled\u201d multiprocessing system in which all processing nodes share a single copy of the operating system and a single copy of the applications program;</p><p>FIG. 3C depicts a diagram of a \u201csnugly coupled\u201d multiprocessing system in which each processing node maintains a dedicated copy of the operating system and all nodes share a single copy of the applications program;</p><p>FIG. 4 illustrates a more detailed functional block diagram of a multiprocessor system according to one embodiment of the principles of the present invention;</p><p>FIG. 5 depicts a more detailed diagram emphasizing the data and address routing circuitry of the system shown in FIG. 4;</p><p>FIG. 6 depicts a second dynamic random access memory device according to the principles of the present invention;</p><p>FIG. 7 depicts a more detailed drawing in block form of a memory system illustrated in FIG. <b>2</b>C.</p><p>FIG. 8 is a diagram of the bitfields of the mode register shown in FIG. 4; and</p><p>FIG. 9 illustrates a timing diagram for exemplary read/write operations to the memory of FIG. 4</p><p>FIG. 10 illustrates an alternative timing diagram for exemplary read/write operations to the memory of FIG. <b>4</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF THE INVENTION</h4><p>The principles of the present invention and their advantages are best understood by referring to the illustrated embodiment depicted in FIGS. 1-9 of the drawings, in which like numbers designate like parts. In the following description of the implementation of the present invention, the terms \u201cassert\u201d and \u201cnegate\u201d and various grammatical forms thereof, are used to avoid confusion when dealing with a mixture of \u201cactive high\u201d and \u201cactive low\u201d logic signals. \u201cAssert\u201d is used to refer to the rendering of a logic signal or register bit into its active, or logically true, state. \u201cNegate\u201d is used to refer to the rendering of a logic signal or register bit into its inactive, or logically false, state.</p><p>FIGS. 1A and 1B are block diagrams respectively depicting two basic system architectures <b>100</b>A and <b>100</b>B typical of those found in personal computers (PCs). While numerous variations on these basic architectures exist, FIGS. 1A and 1B are suitable for describing the basic structure and operation of most PCs.</p><p>Both systems <b>100</b>A and <b>100</b>B include a single central processing unit <b>101</b>, CPU local data bus <b>102</b>, CPU local address bus <b>103</b>, external (L2) cache <b>104</b>, core logic/memory controller <b>105</b>, and system memory <b>106</b>. In system <b>100</b>A, the peripherals <b>108</b> are coupled directly to core logic/memory controller <b>105</b> via a bus <b>107</b>. Bus <b>107</b> in this case is preferably a peripheral controller interface (PCI) bus, although alternatively it could be an ISA, general, or special purpose bus, as known in the art. In system <b>100</b>B, core logic/memory controller <b>105</b> is again coupled to bus <b>107</b>. A PCI bus bridge then interfaces bus <b>107</b> with a PCI bus <b>110</b>, to which the peripherals <b>111</b> couple. An additional bus <b>112</b>, which may be a ISA, PCI, VESA, IDE, general, or special purpose bus, is provided for access to peripherals <b>111</b> from an external device or system (not shown).</p><p>In single CPU systems <b>100</b>A and <b>100</b>B, CPU <b>101</b> is the \u201cmaster\u201d which, in combination with the operating system and applications software, controls the overall operation of system <b>100</b>. Among other things, CPU <b>101</b> performs various data processing functions including numerical and word processing, generates graphics data, and performs overall system management. CPU <b>101</b> may be, for example, a complex instruction set computer (CISC), such as an Intel Pentium\u2122 class microprocessor, a reduced instruction set computer (RISC), such as a Apple PowerPC microprocessor, or a very long instruction word (VLIW) machine.</p><p>CPU <b>101</b> communicates with the remainder of system <b>100</b> via CPU local address and data buses <b>102</b> and <b>103</b>, each of which may be, for example, a special bus, or a general bus, as known in the art.</p><p>Core logic/memory controller <b>105</b>, under the direction of CPU <b>101</b>, controls the exchange of data, addresses, control signals and instructions between CPU <b>101</b>, system memory <b>106</b>, and peripherals <b>108</b>/<b>111</b> via bus <b>107</b> and/or PCI bus bridge <b>109</b>. Although the core logic/memory controller allows tasks to be shifted from the CPU, thereby allowing the CPU to attend to other CPU-intensive tasks, the CPU can always override core logic/memory controller <b>105</b> to initiate execution of a higher priority task.</p><p>Core logic and memory controllers are widely available in the PC industry and their selection and application well known by those skilled in the art. The memory controller can be either a separate device or incorporated into the same chip set as the core logic. The memory controller is generally responsible for generating the memory clocks and control signals such as /RAS, /CAS, /WE (write enable), /OE (output enable) and bank select, and monitors and controls cell refresh. Alternatively, the memory controller generates the command and clock signals in an embodiment employing the principles of the present invention in which memory <b>106</b> may be synchronous DRAM (SDRAM). The memory controller may also have some address generation capability for accessing sequences of pages.</p><p>The core logic is typically comprised of a chip-set, with one or more chips typically being \u201caddress and system controller intensive\u201d and one or more chips typically being \u201cdata intensive.\u201d Among other things, the address intensive chip(s): interfaces CPU <b>101</b> with address bus <b>103</b>; maintains cache memory, including the cache tags, sets associative cache tags and other data necessary to insure cache coherency; performs cache \u201cbus snooping\u201d; generates the control signals required for DRAMs in the system memory or cache; and controls general management transactions. The data intensive chip(s) generally: interfaces CPU <b>101</b> with the data bus <b>102</b>; issues cycle completion responses; may abort operations if their cycles are incomplete; and arbitrates for the data path of bus <b>102</b>.</p><p>In an alternative embodiment, core logic may be implemented in a single chip architecture in which one or more processors, memory and the core logic memory controllers are fabricated on one chip.</p><p>CPU <b>101</b> communicates with system memory <b>106</b> via core logics/memory controller <b>105</b> through an SRAM interface, SRAM cache <b>104</b>. The fast SRAM interface is integrated into system memory <b>106</b>. In an embodiment of CPU system <b>100</b><i>a </i>and <b>100</b><i>b </i>according to the principles of the present invention, integrated SRAM cache <b>104</b> plays the role of external (L2) cache in the prior art. The CPU may also maintain a predetermined amount of on-chip (L1) cache. Integrated fast SRAM, such as SRAM cache <b>104</b>, may be particularly adapted to embodiments of the present invention implemented in a single chip architecture, wherein all of the circuitry disclosed herein is implemented in a single integrated circuit. Such single chip embodiments may be advantageous in \u201cpalm top\u201d computers and \u201cpersonal assistants.\u201d</p><p>PCI bus bridges, such as PCI bus bridge <b>109</b>, are also well known to those skilled in the art. In the typical PC, the CPU is the bus master for the entire system and hence devices such as PCI bus bridge are slave devices which operate under command of the CPU.</p><p>Peripherals <b>108</b>/<b>111</b> may include a display controller and associated frame buffer, floppy drive controller, disk driver controller, and/or modem, to name only a few options.</p><p>The principles of the present invention may advantageously be embodied in multiprocessing devices and systems. An embodiment of such a multiprocessing system is illustrated in FIG. 2 in which multiprocessor architecture <b>200</b>A is depicted. In the multiprocessor system <b>200</b>A, multiple CPUs, CPU <b>201</b>A and CPU <b>201</b>B, communicate with an integrated system memory <b>202</b> via core logic memory controller <b>203</b>. CPU <b>201</b>A and CPU <b>201</b>B are coupled to core logic memory/memory controller <b>203</b> via the respective buses, <b>204</b>A and <b>204</b>B. Bus <b>204</b>A and Bus <b>204</b>B carries both data and address information. Addresses are presented by core logic/memory controller <b>203</b> to integrated memory <b>202</b> on address bus <b>205</b>. Preferably, address bus <b>205</b> is non-multiplexed, whereby the full address required to access a word in memory is presented simultaneously and synchronized with a system clock. Data are routed between integrated memory <b>202</b> and core logic/memory controller <b>203</b> on data bus <b>206</b>. In accordance with the principles of the present invention, data bus <b>206</b> is non-multiplexed whereby each memory array within integrated memory <b>202</b> has dedicated data lines within data bus <b>206</b>. In this way, one CPU, for example, CPU <b>1</b>, <b>201</b>A, may read or write to one memory array with an integrated memory <b>202</b> while CPU <b>2</b>, <b>201</b>B, may read or write from a second memory array with an integrated memory <b>202</b>. Control information is communicated between core logic/memory controller <b>203</b> and integrated memory <b>202</b> on control line <b>207</b>. In an embodiment of multiprocessor system <b>200</b>A, in accordance with the principles of the present invention, multiprocessor system <b>200</b>A may preferentially be implemented on a single chip. It would be understood by one of ordinary skill in the art that multiprocessor system <b>200</b>A may include a predetermined number, n, number of processors <b>201</b>.</p><p>In FIG. 2B, another embodiment of a multiprocessor system <b>200</b>B employing the principles of the present invention is depicted. In multiprocessor system <b>200</b>B, a first CPU, CPU <b>1</b>, <b>201</b>A, and a second CPU, CPU <b>2</b>, <b>201</b>B, are coupled to core logic memory controller <b>202</b> via system bus <b>203</b>A and system bus <b>203</b>B, respectively. Core logic/memory controller <b>202</b> mediates communication between CPU <b>1</b>, <b>201</b>A, and CPU <b>2</b>, <b>201</b>B, with system memory <b>204</b>. System memory <b>204</b> includes a plurality of memory arrays <b>205</b>. Each of memory arrays <b>205</b> includes an address/control port <b>206</b> and a data interface <b>207</b>. Address/control ports <b>206</b> are coupled to core logic/memory controller <b>202</b> via address/control bus <b>210</b>. Address/control bus <b>210</b> may include address lines for addressing the full address space of system memory <b>204</b>. Thus, address lines with an address/control bus <b>210</b> preferably are nonmultiplexed.</p><p>Data are transferred between system memory <b>204</b> and core logic/memory controller <b>202</b> on data bus <b>211</b>. Each of memory arrays <b>205</b> is coupled to data bus <b>211</b> through its own data port <b>207</b>. Thus, CPU <b>1</b>, <b>201</b>A, and CPU <b>2</b>, <b>201</b>B, preferably may access system memory <b>204</b> simultaneously. Where an address conflict arises because more than one CPU is attempting to address the same memory array <b>205</b>, a processor serving as a master may resolve the conflict. In one such embodiment, core logic/memory controller <b>202</b> may itself be a general-purpose processor whereby it serves to establish a priority among contenders for access to system memory <b>204</b>. Alternatively, one of the multiprocessors in multiprocessor system <b>200</b>B may serve as the master. Although multiprocessor system <b>200</b>B has been illustrated as having two multiprocessors, CPU <b>1</b>, <b>201</b>A, and CPU <b>2</b>, <b>201</b>B, it would be understood by one of ordinary skill in the art that multiprocessor system <b>200</b>B may include a predetermined plurality of multiprocessors, N, in number, where N has a predetermined constant value. In an embodiment of the present invention having N processors, processor access may be sequenced, so that the memory is continuously operational. In this way, periods during which memory is idle may be reduced. Multiprocessor system <b>200</b>B preferably is implemented in a single chip architecture. System memory <b>204</b> will be discussed further below.</p><p>Yet another embodiment of a multiprocessor system according to the principles of the present invention, multiprocessor <b>200</b>C, is depicted in FIG. <b>2</b>C. Multiprocessor system <b>200</b>C includes CPU <b>1</b>, <b>201</b>A, and CPU<b>2</b>, <b>201</b>B, coupled to a memory controller <b>202</b>, via system bus <b>1</b>, <b>203</b>A, and system bus <b>2</b>, <b>203</b>B, respectively. Multiprocessor system <b>200</b>C includes multiple memory units, memory <b>1</b>, <b>204</b>A, and memory <b>2</b>, <b>204</b>B, coupled to memory controller <b>202</b>, via memory bus <b>1</b>, <b>205</b>A, and memory bus <b>2</b>, <b>205</b>B, respectively. Preferably, memory controller <b>202</b> serves as a \u201ccross bar\u201d switch whereby either of CPU <b>1</b>, <b>201</b>A, or CPU <b>2</b>, <b>202</b>B, may be dynamically coupled to either of memory <b>1</b>, <b>204</b>A, or memory <b>2</b>, <b>204</b>B. System bus <b>203</b>A and system bus <b>203</b>B include address, data, and control lines. Similarly, memory bus <b>205</b>A and memory bus <b>205</b>B include address, data, and control lines. Preferably, system bus <b>203</b>A and system bus <b>203</b>B have nonmultiplexed address lines whereby each of CPU <b>201</b>A and CPU <b>201</b>B may address the full memory space of memory <b>204</b>A and memory <b>204</b>B combined. In such an embodiment of the present invention, memory controller <b>202</b> resolves the addresses on the address lines in system bus <b>203</b>A and system bus <b>203</b>B and couples CPU <b>201</b>A and CPU <b>201</b>B to memory <b>204</b>A or memory <b>204</b>B. as appropriate, depending on the resolution of the addresses appearing on system bus <b>203</b>B. Memory <b>204</b>A and memory <b>204</b>B may each be multibanked memory. Memory bus <b>205</b>A and memory bus <b>205</b>B both include bidirectional data lines for transmitting data between memory <b>204</b>A, memory <b>204</b>B, CPU <b>201</b>A, and CPU <b>201</b>B via memory controller <b>202</b>. Data transmitted to CPU <b>201</b>A or CPU <b>201</b>B via memory controller <b>202</b> are carried on data lines within system bus <b>203</b>A and system bus <b>203</b>B, coupled to CPU <b>201</b>A and CPU <b>201</b>B, respectively. Depending on the address resolution, data lines within either memory bus <b>205</b>A and memory bus <b>205</b>B may be coupled to either of data lines within system bus <b>203</b>A or system bus <b>203</b>B through memory controller <b>202</b>. In an embodiment in which memory <b>204</b>A and memory <b>204</b>B are multibank memory, data lines within memory bus <b>205</b>A and memory bus <b>205</b>B preferentially are nonmultiplexed, as are data lines within system bus <b>203</b>A and system bus <b>203</b>B. In such an embodiment, CPU <b>201</b>A and CPU <b>201</b>B may simultaneously access either memory <b>204</b>A or memory <b>204</b>B.</p><p>Memory controller <b>202</b> may include a general-purpose processor serving as a bus master whereby address conflicts may be controlled. Simultaneous access to system memory, such as memory <b>204</b>A or memory <b>204</b>B will subsequently be discussed in more detail when memory in accordance with the principles of the present invention, such as memory <b>204</b>A or memory <b>204</b>B are further described.</p><p>Although multiprocessor system <b>200</b>C has been illustrated having two CPUs, CPU <b>201</b>A and CPU <b>201</b>B, it would be understood by one of ordinary skill in the art that the principles of the present invention may be embodied in a multiprocessor system having a plurality of multiprocessors, N, where N is a predetermined number of CPUs. Similarly, multiprocessor system <b>200</b>C may have a predetermined plurality of memory units, M, where M is a predetermined number of system memory units <b>204</b>. Moreover, it would be further understood by one of ordinary skill in the art that memory <b>204</b>A and memory <b>204</b>B need not be identical in structure. For example, it may be desired that one or more of a plurality of system memory units <b>204</b> be associated with a graphics or video controller whose memory access characteristics are \u201cpage\u201d intensive. In such an embodiment, it may be preferred that memory, such as memory <b>201</b>A and memory <b>201</b>B be configurable \u201con the fly.\u201d Configuring memory <b>201</b>A and memory <b>201</b>B in dynamical fashion will be discussed when memory in accordance with the principles of the present invention are further described in detail below. Multiprocessor system <b>200</b>C may preferably be implemented in a single chip architecture.</p><p>At the highest system level, there are a number of ways to implement the hardware architectures shown in FIGS. 2A, <b>2</b>B and <b>2</b>C in a complete hardware/software system. Three such systems are shown in FIGS. 3A-3C, respectively.</p><p>FIG. 3A is a diagram illustrating a \u201cloosely coupled\u201d multiprocessing architecture. In the loosely coupled architecture, each processing node <b>300</b> maintains a dedicated copy of both the operating system and the application programs. Loosely coupled architectures, such as that shown in FIG. 3A, are used often in embedded systems and in real-time systems in which tasks must be partitioned to different processing nodes for synchronization purposes. Embedded systems include those in which the CPU is fabricated on the same chip as logic, memory, a signal processor, or the like. In a multiprocessor system according to the principles of the present invention, single chip implementations are preferred. High speed interconnects are used to share data and pass messages between processing nodes <b>300</b>. While loosely coupled systems are more fault and error tolerant, their software programming is most often highly complex.</p><p>FIG. 3B depicts a \u201ctightly coupled\u201d system. In this case, a single copy of the operating system and a single copy of the application program are shared and executed by a single set of processors. Advantageously, writing software programs for a tightly coupled system is normally simpler than for writing programs to a loosely coupled system. However, tightly coupled systems, based only on single copies of the application programs and operating system, are less tolerant to errors and failures than the loosely coupled systems.</p><p>FIG. 3C is a diagram of a \u201csnugly coupled\u201d system in which each processing node <b>300</b> maintains a dedicated copy of the operating system and all nodes share a single copy of the applications program. The snugly coupled variation is a hybrid which provides the tolerance to failure/errors found in loosely coupled systems while still providing the simpler program found in tightly coupled systems.</p><p>Generally, a multiprocessor system will act differently depending upon the type of processor employed. For example, a CISC CPU may be \u201clatency\u201d dominated while a digital signal processor (DSP) based system may be \u201cdataflow\u201d dominated. Further, pipelined processing algorithms typically are dataflow intensive, since the processors perform operations on streams of data received from other processors in the system and then pass the results on to other processors.</p><p>There are major challenges which must be addressed in the design of almost any multiprocessing system. In each of the embodiments of systems <b>200</b>A-<b>200</b>C, system memory is shared by a plurality of processors. A technique must be developed to handle the situation in which several processors attempt to simultaneously access the shared memory, so-called memory contention. This problem is compounded by the fact that the contention issues must be dealt with from design to design, since different processors interface with memory differently. For example, a RISC processor requires substantial memory space while a CISC processor requires substantial register space.</p><p>In a memory device or subsystem with a single data input/output port and a single address port, contention problems can be solved by \u201cmemory locking.\u201d In this case, while one CPU (or controller) is accessing a given memory device or subsystem, the other CPU (controller) is \u201clocked out\u201d and cannot access that same device/subsystem. Memory locking is a memory management task which may be performed by the memory management unit (MMU) on-board the CPUs themselves or by a stand-alone device or subsystem. In any event, memory locking reduces the efficiency which multiprocessing was intended to increase, since during a contention situation, at least one processor must wait to access data.</p><p>Another major challenge is the software design. Symmetric multiprocessing operating systems are preferred, since this type of operating system is capable of seamlessly passing application programs to the CPUs as they become available. As discussed above, the selection of between tightly, loosely and snugly coupled software architecture requires substantial trade-offs, and in particular trade offs between ease of programming and fault/error tolerance.</p><p>Further, when multiple processors (or controllers) are coupled to the same bus, bus contention problems may also arise. Specifically, when a shared bus is employed, only one processor is normally granted access to the bus to perform a given bus task, while the remainder of the processors coupled to that bus must wait until their priority has been reached. One technique for minimizing bus contention problems is to provide a dedicated cache for each CPU so that a given CPU need only access the bus at times when required data are not found in the dedicated cache. As a result, cache coherency is a major concern in the design of a multiprocessing system. In other words, when a given processor modifies a location in memory, some technique must be provided for insuring that the data is modified in the cache memory of each of the other processors using the same data.</p><p>Refer now to FIG. 4 in which an embodiment of a multiprocessing system <b>400</b> in accordance with the principles of the present invention is illustrated. Multiprocessing system <b>400</b> includes a plurality, N number of memory banks <b>401</b>, with four such banks, <b>401</b><i>a</i>, <b>401</b><i>b</i>, <b>401</b><i>c</i>, and <b>401</b><i>d </i>being shown in FIG. <b>4</b>. Although four banks are shown in FIG. 4 for illustration, an actual number of banks will vary from application to application.</p><p>Memory banks <b>401</b> send and receive data, addresses and control signals via memory controller/core logic <b>402</b>.</p><p>A system clock (SYSCLK) <b>403</b> and command signals <b>404</b> are received by a clock generator <b>405</b> within each bank and a command register <b>406</b>, likewise in each bank, respectively. Preferably, memory banks <b>401</b> operate synchronously whereby all memory operations are synchronized with the system clock <b>403</b>. Internal memory clock signals synchronized to system clock <b>403</b> are generated within clock generator <b>405</b>. Each of memory banks <b>401</b><i>a</i>-<b>401</b><i>d </i>also receives a corresponding command signal <b>404</b><i>a</i>-<b>404</b><i>d</i>. Command signals <b>404</b> are stored within command register <b>406</b>, and inform each of the respective memory banks <b>401</b> whether the pending operation in the respective bank is a read operation or a write operation. Thus, simultaneous reads and writes may be made to each of memory banks <b>401</b> in accordance with the principles of the present invention. Command signals <b>404</b> may also selectively activate or deactivate one or more of memory banks <b>401</b>.</p><p>Each bank <b>401</b> includes an array <b>407</b> of dynamic random access memory (DRAM) cells arranged in N number rows and M number columns. As is known to those skilled in the art, each array <b>407</b> may be partitioned into multiple subarrays, with the columns organized using either an open-bitline or folded-bitline approach. Each bank <b>401</b> further includes a traditional DRAM decoder <b>408</b> coupled to the array word lines, and traditional DRAM sense amplifiers/column decoder circuitry <b>409</b> coupled to the array bitlines. The row and column decoders are preferably organized in hierarchical fashion in which a main decoder and one or more levels of subdecoders/drivers are used. Generally, each row decoder <b>408</b>, in response to a row address, selects one of N rows for access in response to system clock <b>403</b>. Because memory banks <b>401</b> preferably operate in synchronous fashion, an active memory cycle is timed off of SYSCLK <b>403</b> rather than a row address strobe signal as in nonsynchronous DRAM. A column decoder <b>409</b> selects P number of pages (locations of C number of columns (bits) from the M total number of columns in response to P number of column addresses for access during an active cycle.</p><p>Memory banks <b>401</b> receive addresses from core logic/memory controller <b>402</b> on address bus <b>410</b>. The addresses are received in address latch and last row read (LRR) <b>411</b> in each of memory banks <b>401</b>, and latched on an edge of SYSCLK <b>403</b> during an active command signal <b>404</b>. Address bus <b>410</b> may be a multiplexed bus. However, it is preferred that address bus <b>410</b> be nonmultiplexed whereby row addresses and column addresses are presented simultaneously, and thereby latched simultaneously in response to SYSCLK <b>403</b>. Thus, address bus <b>410</b> spans the entire address space of memory <b>400</b>, in a preferred embodiment in accordance with the principles of the present invention. Within an address space of W bits in width, a number of bits, R, are row address bits, and a number of bits, W-R, are column address bits. Within the portion of the address space corresponding to the row addresses, a number of bits, B, serve as bank select bits.</p><p>According to the principles of the present invention, each bank <b>401</b> further includes static random access memory (SRAM) registers/SRAM column decoder circuitry <b>412</b>. SRAM circuitry <b>412</b> will be discussed in further detail in conjunction with FIG. 6 but can generally be described at this point as follows. First, a linear array of M number of SRAM cells is included for storing the row of data transferred from the corresponding DRAM array <b>407</b>. Alternatively, another embodiment of the present invention may have a plurality, n, of linear arrays of M cells. Second, SRAM decoder circuitry is included for page accesses (reads and writes) of C-bit wide pages of data to the addressed row of data in the SRAM array in response to one or more column addresses.</p><p>Data latches/port <b>413</b> interface the DRAM sense amplifiers/column decoders <b>409</b> and the SRAM registers/column decoders <b>412</b> with a data bus <b>414</b>. Data bus <b>414</b> is preferably a nonmultiplexed data bus. In such an embodiment, in accordance with the principles of the present invention, memory <b>400</b> represents a multi-port memory for data. Data latches/port <b>413</b> incorporates circuitry for bidirectional access to the latches for interfacing with data bus <b>414</b>, which is a bidirectional bus. Each of data latches/port <b>413</b> may correspond to a data port of K (where K is a predetermined integer) data bits in width. Then, data bus <b>414</b> may preferably be 4K data bits wide.</p><p>Mode registers <b>415</b> are used to configure memory banks <b>401</b>. This may entail setting optional access modes such as page reads and writes or burst access with a selected burst type. Mode registers <b>415</b> may also contain bits activating or deactivating one or more of banks <b>401</b>. The specification of mode register data subsequently will be discussed in detail. In multiprocessor system <b>200</b>A the mode registers <b>208</b> have been illustrated as being incorporated in core logic/memory controller <b>203</b>. However, it will be recognized, particularly in the preferred single chip implementation, that such an incorporation is not essential to the practice of the present invention, and that the mode registers may be implemented within other structures on the chip. Whether the mode registers are incorporated within the memory controller circuitry, or elsewhere in the chip, each memory bank <b>401</b>, is associated with a corresponding mode register, and coupled thereto by the corresponding mode register bus <b>418</b>. Core logic/memory controller <b>402</b> configures memory bank <b>401</b> under the control of CPUs <b>416</b><i>a </i>and <b>416</b><i>b</i>, coupled to core logic/memory controller <b>402</b> by system busses <b>417</b><i>a </i>and <b>417</b><i>b</i>, respectively.</p><p>Multiprocessing system <b>400</b> has been illustrated as having two processors CPU <b>416</b><i>a </i>and CPU <b>416</b><i>b</i>. It would be understood that this is illustrative only and that other embodiments of multiprocessing systems according to the principles of the present invention may have a predetermined number, n, of CPUs.</p><p>FIG. 5 is a more detailed diagram emphasizing the data and address routing circuitry of a selected bank <b>401</b>. Preferentially, addresses are received in nonmultiplexed fashion on address bus <b>410</b>. The row address bits are latched into row address latch <b>501</b> on the falling edge of SYSCLK <b>403</b> while an active command is asserted on command bus <b>404</b>. Simultaneously, a first column address, represented by column address bits in address bus <b>410</b>, is latched into column address latch <b>502</b>. The row address bits from the previous memory access cycle are transferred to the last row read (LRR) latch <b>503</b> when the current row address is latched into row address latch <b>501</b>. Comparison circuitry <b>504</b> compares the present address latched into row address latch <b>501</b> with the last read row address held in latch <b>503</b>.</p><p>SRAM registers <b>505</b> store data associated with the row address bits stored in last read address latch <b>503</b>. A second column decoder <b>506</b> is provided to access locations in SRAM registers <b>505</b>. Row address increment/decrement and refresh control circuitry <b>507</b>, when enabled, steps through the rows of DRAM cell array <b>407</b> to perform cell refresh through the DRAM sense amps, and implements the address transformations detailed below. Refresh is timed off of SYSCLK <b>403</b> through clock generator <b>405</b>.</p><p>During an access, an address is received on address bus <b>410</b>. As previously described, the address is latched on a falling edge of SYSCLK <b>403</b>, provided an active command has been transmitted to command register <b>406</b>. If the active command corresponds to a read operation, then one of two read methods may be selected. Selection may be made by asserting or negating, depending on the read method to be used, a configuration bit in command register <b>406</b>.</p><p>In the first method of reading, comparator <b>504</b> in bank <b>401</b> corresponding to the address space of the row address received is enabled. Following the sending of the read command to command register <b>406</b>, and the latching of the address on the falling edge of SYSCLK <b>403</b>, a predetermined \u201crow\u201d latency elapses before data appears at data latches <b>413</b>. During this latency period, comparator <b>503</b> compares the current row address in row address latch <b>501</b> and the address stored in LRR address latch <b>503</b>. If a match occurs for the corresponding bank, the SRAM column address decoders <b>506</b> are selected and set-up to access the SRAM register <b>505</b> of that bank. In an alternative embodiment in which SRAM register <b>505</b> includes a plurality of rows, comparator <b>504</b> outputs row select bits, to SRAM register <b>505</b>. SRAM column decoder <b>506</b> allows access to a C-bit word at the column address latched into column address latch <b>502</b>. This data is set up on the C-bit wide portion of data bus <b>414</b> corresponding to the active bank <b>401</b> via data latch/port <b>413</b>. Alternatively, the SRAM may be sufficiently strong to drive data bus <b>414</b> directly. In such an embodiment latching circuitry need not be included in data latch/port <b>413</b>. Data from subsequent column address locations are clocked into data latch/ports <b>413</b> on succeeding cycles of SYSCLK <b>403</b>, depending on the selected read mode. Preferably, the enabled bank is the only bank <b>401</b> accessed by a data bus <b>414</b>, the word of data in data latches <b>413</b> of the other banks is simply not used. This will be discussed further below.</p><p>If the address bits in latches <b>501</b> and <b>503</b> do not match for the selected bank <b>401</b>, access must be made to the corresponding DRAM array. Specifically, for a read to the address bank <b>401</b>, the row is selected by DRAM row decoder <b>408</b> from the corresponding DRAM array <b>407</b> and an entire row of data transferred to the associated SRAM register <b>505</b> for output following the selected latency period. For the remaining banks <b>401</b>, the row addresses, with the bank select bit portions ignored, are incremented or decremented using the corresponding row address increment circuitry <b>507</b>. A row of data from these banks is similarly transferred to the corresponding SRAM register <b>505</b> and the new address is latched into the CRR latch <b>503</b> for those banks on the next cycle.</p><p>Assume for example, in the illustrated four bank system, that the received row address indicates the address space of bank <b>401</b><i>b </i>(Bank<b>1</b>) is to be accessed. Bank<b>1</b> is then enabled to make the comparison of the row current address and the address stored in Bank<b>1</b> LRR latch <b>503</b>. As above, the bank select portion of row addresses are ignored in the comparison. The row address MSBs as received are not modified for transferring data from the bank <b>401</b><i>b </i>DRAM array <b>407</b> to the Bank<b>1</b> SRAM <b>505</b>. However, for bank <b>401</b><i>a </i>(Bank<b>0</b>) the row address, ignoring bank select bits, is decremented by 01 by the corresponding row increment circuitry <b>507</b> and the row address for banks <b>401</b><i>c </i>(Bank<b>2</b>) and <b>401</b><i>d </i>(Bank<b>3</b>) are incremented by 01 and 10, respectively, again bank select bits are ignored. In other words, if the address to Bank<b>1</b> is designated address A+1, then address A+1 is decremented by one such that Bank<b>0</b> receives address A<b>0</b> and incremented such that Bank<b>2</b> receives address A+2 and Bank<b>3</b> receives address A+3. These addresses are used to access the associated bank's DRAM array <b>407</b> and the accessed data in DRAM transferred to the SRAM arrays. The new addresses are stored in address latches <b>501</b>.</p><p>During accessing of the addressed bank, assuming again for discussion purposes BANK<b>1</b>, the DRAM of any bank, including in this case the DRAM array <b>407</b> of Bank<b>1</b> can be refreshed. The DRAM column decoders <b>409</b> isolate the corresponding DRAM arrays <b>407</b> from the SRAM registers <b>412</b>. Thus, while data is being accessed from the SRAM array of the selected bank <b>401</b>, any or all of the DRAM arrays <b>407</b> can be refreshed without disturbing the data in the SRAM arrays. Refresh is preferably performed by incrementing the row addresses in each bank using increment and refresh circuitry <b>507</b> and latching each row of data using the DRAM sense amplifiers <b>409</b> in a conventional fashion.</p><p>In the preferred embodiment, once the data in the SRAM array <b>505</b> of the addressed bank has been accessed, the memory access cycle is complete. The data in the SRAM arrays <b>505</b> of the remaining banks <b>401</b> is available through the associated data latches <b>413</b>, and could be used, but typically is reserved for future memory access cycles. The current row address for the accessed banks and the new row address for the non-accessed banks are transferred to the LRR registers. The LRR bits are pipelined from row address latches <b>501</b> synchronous with SYSCLK <b>403</b> such that they are available to bit comparators <b>504</b> at the start of the next memory cycle. The corresponding data remain in the SRAM arrays. Advantageously, since the CPU and/or operating system typically accesses data within temporally or spatially adjacent areas in memory, the probability is substantial that a match will occur.</p><p>For a write operation the following is the preferred method. An address is received on address bus <b>410</b>. As above, a bank select bit portion of the address determines the bank <b>401</b> assigned to the corresponding row address space. Assume again for example, bank <b>401</b><i>b </i>(Bank<b>1</b>) is addressed. The received row address, with bank select bits ignored, is are taken as the address to the Bank<b>1</b> DRAM array <b>407</b>. As was done above, the row address increment circuitry <b>507</b> for Bank<b>0</b> decrements the received row to obtain a row address to the Bank<b>0</b> DRAM array and increments the received address by 01 and 10 to obtain row addresses to the DRAM arrays of Bank<b>2</b> and Bank<b>3</b>, respectively. In each instance, bank select bits are ignored. The row address bits for each bank <b>401</b> is written into the respective bank's LRR register <b>503</b>.</p><p>In the second method of accessing, which may be selected by asserting a configuration bit in command register <b>406</b>, the received address, less bank select bits, is compared by all the comparators <b>503</b>. If a match occurs in any one or more banks <b>401</b>, the data from all the banks is taken, although the data from the non-matching banks may be discarded or left in the data latches.</p><p>Refer now to FIG. 6 in which is illustrated a memory bank <b>600</b> including an embodiment of an SRAM cache <b>601</b> according to the principles of the present invention.</p><p>As shown in FIG. 6, each SRAM cache <b>601</b> includes a plurality of SRAM column decoders <b>602</b> coupled to a corresponding SRAM register <b>603</b>. In the illustrated embodiments, two SRAM registers <b>603</b><i>a </i>and <b>603</b><i>b</i>, associated with a pair of SRAM column decoders <b>602</b><i>a </i>and <b>602</b><i>b </i>are shown for brevity and clarity. However, it should be recognized that an SRAM cache <b>601</b> may be constructed with additional SRAM registers <b>603</b> along with a corresponding number of SRAM decoders <b>602</b>.</p><p>Additionally, a corresponding number of LRR latches <b>604</b> are provided to support the multiple SRAM registers <b>603</b>. Hence, if in a given embodiment includes n number of registers <b>603</b>, there will preferably also be n number of LRR registers <b>604</b>, although this is not an absolute requirement. In the illustrated embodiment where two SRAM registers <b>603</b><i>a </i>and <b>603</b><i>b </i>are depicted, a corresponding pair of LRR latches <b>604</b><i>a </i>and <b>604</b><i>b </i>are also shown.</p><p>DRAM cell array <b>407</b>, row address decoder <b>408</b>, address latches/LRR comparison circuitry <b>504</b> and row address increment/decrement and refresh circuitry <b>507</b> all substantially operate as described above.</p><p>Assume that each DRAM cell array <b>407</b> is arranged in m number of rows and n number of columns. Row address decoder <b>408</b> will be coupled with the wordline controlling access to each row of cells. In the most straightforward embodiment, n number of sense amplifiers are provided with one sense amplifier coupled to a bitline associated with each column of cells. DRAM column decoder/sense amplifiers <b>409</b> includes a data interface with SRAM column decoders <b>602</b> allowing data to be exchanged, between DRAM array <b>407</b> and SRAM registers <b>603</b>, either individually or in combination. SRAM and DRAM column decoders <b>602</b> and <b>409</b> are all coupled to column address latch <b>504</b>.</p><p>In the illustrated embodiment, DRAM array <b>407</b> may be n columns wide and each SRAM register <b>701</b> correspondingly may be a linear array of n number of cells disposed in a single row. In such an embodiment, the cache width is n and the cache depth is two. Each row in either cache or DRAM memory stores p number of cache lines, wherein p equals n divided by b, the number of bits per cache line.</p><p>The multiple SRAM register/column decoder structure of each SRAM cache <b>601</b> has further advantages. For example, if SRAM column address decoders <b>602</b> are static devices, then while DRAM cell array <b>407</b> of any bank <b>601</b> is in precharge, one or more of the corresponding SRAM registers <b>603</b> can be accessed either in a random or page fashion. Of course, column address decoders <b>409</b> can be dynamic devices which are inactive during precharge thereby providing for substantial power savings across the banks <b>600</b>.</p><p>SRAM registers <b>603</b> of given bank <b>600</b> can be used in various combinations with the associated DRAM cell array <b>407</b> and its column decoder <b>407</b> to optimize data exchanges. For example, one SRAM register <b>603</b> of the selected bank <b>600</b> can access data through data latches <b>413</b>, while simultaneously data can be exchanged between the associated DRAM cell array <b>407</b> and any of the remaining SRAM registers <b>603</b> in the same cache unit <b>601</b>. At the same time, data exchanges can occur between the SRAM registers <b>603</b> and the associated DRAM cell array <b>407</b> of each of the other banks <b>601</b>.</p><p>For any SRAM registers <b>603</b> in memory <b>400</b> which are not being accessed through data latches, a number of different data exchanges between the data cell array <b>407</b> and the SRAM registers <b>603</b> can be set up. Among other things, the contents of a SRAM register <b>603</b> can be copied to a row in the corresponding DRAM cell array <b>407</b> or vice versa; data can be copied from DRAM to a SRAM register. During accesses through the data latches, each port can be individually configured such that reads and writes are made to the cache unit <b>601</b> only, to the DRAM cell array <b>407</b> only, or to both an SRAM register <b>603</b> in the cache <b>601</b> and to the DRAM array <b>407</b>.</p><p>The multiple SRAM register embodiment of bank <b>600</b> illustrated in FIG. 6 allows for the selective implementation of one of a number of addressing schemes. For example, assume that the bank select portion of the row address received selects BANK <b>1</b> for access. Assuming a two SRAM register embodiment, the remaining row address bits are then compared with two addresses stored in the LRR address latches <b>604</b> (one corresponding to each row of data stored in a corresponding SRAM register <b>603</b>). If a match occurs, then the SRAM register <b>603</b> corresponding to the matching addresses is accessed. In this case the probability of a match (cache hit) are increased since the number of SRAM registers in which the desired data could possibly be stored in has increased.</p><p>Alternatively, the multiple registers <b>603</b> of each bank <b>600</b> could be considered as residing in a single address space. In this case, the most significant bits of an incoming row address are compared against a single stored LRR address. If a match occurs, all of the SRAM registers of the given bank are then accessed in a predetermined sequence. Among other things, this scheme would allow paging of data in multiple page lengths, depending on the length of each row in SRAM and the number of SRAM registers accessed. Provisions can be made during the setup of Mode Registers <b>603</b> to accomplish varying embodiments.</p><p>In an additional alternative, data can be loaded from the DRAM arrays such that the data in a selected SRAM register in a given bank is associated with an address non-contiguous with the addresses associated with the contents of other registers in that bank. For example, row address increments/decrements <b>507</b> could be configured such that if Bank <b>0</b>, SRAM register <b>603</b><i>a </i>is loaded with data corresponding to Addr<b>0</b>, the associated register <b>603</b><i>b </i>is loaded with data corresponding to Addr<b>4</b>. For bank <b>1</b> registers <b>603</b><i>a </i>and <b>603</b><i>b </i>are respectively loaded with data corresponding to Addr<b>1</b> and Addr<b>5</b>. Similarly, the SRAM registers of Bank <b>2</b> hold data at addresses Addr<b>2</b> and Addr<b>6</b> and Bank <b>3</b> data at addresses Addr<b>3</b> and Addr<b>7</b>. Numerous other combinations/permutations are possible.</p><p>Refer now to FIG. 7 in which an embodiment of a multiprocessing system <b>200</b>B is illustrated in further detail. Integrated memory <b>204</b>, including four memory banks <b>205</b>A-<b>205</b>D, is coupled to memory controller/core logic <b>202</b>. Although integrated memory <b>204</b> is shown having four memory banks, it would be understood that other embodiments of integrated memory according to the principles of the present invention may include a predetermined number, N, of memory banks. Memory controller/core logic <b>202</b> interfaces integrated memory <b>204</b> with CPUs <b>201</b>A and <b>201</b>B coupled thereto via system busses <b>203</b>A and <b>203</b>B respectively.</p><p>In multiprocessing system <b>200</b>B, each of memory banks <b>205</b> occupies its own region of the system memory address space. Each of banks <b>205</b>A-<b>205</b>D has a corresponding address port <b>703</b><i>a</i>-<b>703</b><i>d </i>for receiving addresses. Address port <b>703</b>A may correspond to addresses Add<b>0</b>-AddX\u22121. Similarly, address port <b>703</b>B is associated with addresses AddX-AddX+Y\u22121 when address port <b>703</b>B is Y bits wide, address port <b>703</b>C with addresses AddX+Y-AddX+Y+Z\u22121, address port <b>703</b>C being Z bits wide, and address port <b>703</b>D with addresses AddX+Y+Z-AddX+Y+Z+W\u22121 where address port <b>703</b>D is W bits wide. If the system memory address apace is K bits wide, then X, Y, Z, and W must sum to K. In an embodiment of integrated memory <b>204</b> according to the principles of the present invention, X, Y, Z, and W may all be equal in which case each would be K/4 bits wide. However, it would be understood that this is not essential and other embodiments may have unequal values for X, Y, Z, and W.</p><p>Address ports <b>703</b> are coupled to memory controller/core logic <b>202</b> via address bus <b>705</b> which connects to memory controller/core logic <b>202</b> through address registers <b>706</b>. Each of address ports <b>703</b>A-<b>703</b>D communicates with memory controller/core logic <b>202</b> via its corresponding address register <b>706</b>A-<b>706</b>D. In this embodiment, address registers <b>706</b> may independently interface CPU<b>1</b><b>201</b>A and CPU<b>2</b><b>201</b>B with memory <b>204</b>. Thus, CPU<b>1</b><b>201</b>A and CPU<b>2</b><b>201</b>B may simultaneously access memory through memory controller <b>202</b>.</p><p>Similarly, control signals are communicated through bidirectional control ports <b>704</b>A-<b>704</b>D. Control ports <b>704</b> are coupled to memory controller/core logic <b>202</b> through control bus <b>707</b>, which connects to memory controller/core logic <b>202</b> via control registers <b>708</b>. Control bits sent back to memory controller <b>202</b> from banks <b>205</b> via control ports <b>704</b> may inform memory controller <b>202</b> that one of CPUs <b>201</b> is accessing a corresponding bank. Control registers <b>708</b> might also embody mode registers <b>415</b> in the multiprocessor system of FIG. <b>4</b>. The bitfields associated with a mode register portion of control registers <b>708</b>, or mode registers <b>415</b> in an embodiment according to multiprocessing system <b>400</b>, will be described below.</p><p>Banks <b>205</b> send and receive data through data port <b>709</b>. Just as in the embodiment of a multiprocessing system described in association with FIG. 4, each of banks <b>205</b>A-<b>205</b>D has an associated data latch (not shown) coupled to the associated data port <b>709</b>A-<b>709</b>D. Each of data ports <b>709</b> may be M bits wide in which case data bus <b>211</b> would be 4M bits in width, in an embodiment having four banks <b>205</b>. Alternatively, data ports <b>707</b> need not be of equal width, although the widths of the respective busses would sum to a preselected bus width, L. Data bus <b>211</b> couples data ports <b>709</b> to data registers <b>710</b> in memory controller/core logic <b>202</b>. Each of data ports <b>709</b>A-<b>709</b>D is connected to a respective data register <b>710</b>A-<b>710</b>D. In this way each of CPUs <b>201</b>A and <b>201</b>B may independently communicate with banks <b>205</b>A-<b>205</b>D.</p><p>In an embodiment of integrated memory <b>204</b> according to the principles of the present invention, each memory bank <b>205</b> may include the circuitry of memory bank <b>600</b>. In such an embodiment, command register <b>406</b> and clock generator are coupled to control port <b>704</b>. Address bus <b>410</b> in FIG. 6 would then constitute address bus <b>705</b>, and control signals <b>404</b> would constitute control bus <b>707</b>. The operation of SRAM cache <b>601</b> (SRAM registers <b>603</b> and associated column decoders <b>602</b>), DRAM cell <b>407</b>, LRR address latches <b>604</b>, row address latches <b>501</b>, bit compare <b>504</b> and column address latches <b>502</b> is as described with respect to FIG. <b>6</b> and will not be repeated here.</p><p>However, in multiprocessor system <b>200</b>B, the multiport address scheme means that the DRAM arrays <b>407</b> within each of banks <b>205</b>A-<b>205</b>D can be simultaneously accessed. Additionally, the SRAM registers in each of the banks <b>205</b>A-<b>205</b>D can be simultaneously accessed in the same manner as described in conjunction with FIG. <b>6</b>. Moreover, control signals <b>404</b> may include processor select signals, whereby one CPU <b>201</b> may access a memory bank <b>205</b> and another CPU <b>201</b> simultaneously access a second memory bank <b>205</b>. Memory controller/core logic <b>202</b> may be \u201cintelligent,\u201d with capability as a general purpose processor serving as a \u201cmaster\u201d resolving memory conflicts among CPUs <b>201</b>, and providing access prioritization. CPU accesses may be sequenced as well, thereby providing \u201ccontinuously operational\u201d memory. Such continuously operational memory might be implemented by sequencing SRAM accesses, or by accessing the DRAM continuously, within appropriate refresh constraints.</p><p>Multiprocessor system <b>200</b>B is preferably implemented in a single chip approach. As such, it would be recognized that the structural blocks identified in FIG. 7 are somewhat arbitrarily drawn in that they better reflect functionality rather than discrete structural elements interconnected by other discrete structural elements. For example, address registers <b>706</b> and control registers <b>708</b> may be implemented, in alternative embodiments, with the integrated memory itself, rather than incorporated within the memory controller/core logic <b>202</b>. That is, it would be understood that the incorporation of the address <b>706</b> and control registers <b>708</b> in memory controller <b>202</b> is not essential to the practice of the present invention, and that other embodiments may employ alternative topologies. Similarly, other structures forming multiprocessing system <b>200</b>B might be arranged in alternative topologies. In such, CPUs <b>201</b> as well as memory controller <b>202</b> might themselves be processing engines within a multiprocessor. It would be recognized that these alternative topologies would be within the spirit and scope of the principles of the present invention.</p><p>FIG. 8 is a diagram of the bitfields associated with a mode register portion of control register <b>708</b>, or mode register <b>415</b>. Each of these bitfields can be described as follows.</p><p>Bits <b>0</b>-<b>2</b> set the burst length for integrated memory such as integrated memory <b>701</b>. The burst length is the number of words clocked into or out of data latches/port <b>413</b> of the bank <b>702</b> in integrated memory <b>701</b> being accessed.</p><p>Bit <b>3</b> defines the burst type. In the illustrated embodiment, if zero is written into the bit <b>3</b> position, the burst output will be serial and if a Logic One is written thereto, an interleaved burst will take place.</p><p>The bitfield comprised of bits <b>4</b>-<b>6</b> define the read latency. Typically, it takes (slightly) longer to perform a write than it does to perform a read. A read after write, or write after read takes even longer, in today's commercial SDRAM's, especially when one switches from Bank X to Bank Y. In this invention, since all banks are normally \u2018on\u2019, there is no such penalty. In other words, the minimum write latency is slightly longer than the minimum read latency. These bits therefore allow the read latency to be adjusted to optimize read/write timing. Specifically, the burst latency is the delay in the output of the first bit of a burst of data from the high to low transition of SYSCLK during the assertion of a read command. The desired delay is generated using an internal clock optimized for DRAM operations with SRAM register operations.</p><p>In the illustrated embodiment, the bitfield consisting of bits <b>7</b>-<b>8</b>, the bitfield consisting of bits <b>13</b>-<b>16</b>, and bit <b>23</b> are reserved for future use.</p><p>Bit <b>9</b> is used to select between single word bursting and bursting in bursts of the length specified in the burst length register. For example, if a zero is written into the bit <b>9</b> position, then the write burst length will be as defined by the burst length bits written into the bitfield <b>0</b>-<b>2</b>. If a logic one is loaded into bit position <b>9</b>, the write burst length will be one word. In other words, writes will be made on a word by word basis.</p><p>Bit position <b>12</b> holds a bit which defines the adjustment resolution of the read data latency. If a zero is written into bit position <b>12</b>, then the data latency is programmed in integers of the system clock CLK (e.g., latencies of 1, 2, 3, . . . n CLK periods). If a logic one is written into bit position <b>12</b>, data latency is set in 0.5 clock increments (e.g., latencies of 0.5, 2.5, 3.5 . . . CLK periods).</p><p>The bitfield consisting of bits <b>17</b>-<b>20</b> holds the bank status bits. Using these bits, the CPU (and core logic) and operating system can selectively activate and deactivate banks such as banks <b>702</b> in integrated memory <b>701</b>. Hence, the CPU is given the ability to repeatedly access a specified amount of memory. Further, by appropriately setting this register, predictive/speculative execution of instructions by the CPU can be implemented. The bit encoding for banks status bitfield is provided in Table 1.</p><p><tables id=\"TABLE-US-00001\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"2\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"77pt\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"140pt\"></colspec><thead><row><entry nameend=\"2\" namest=\"1\" rowsep=\"1\">       TABLE I</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" nameend=\"2\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry>Bank Status</entry><entry></entry></row><row><entry>Register Bits</entry><entry>Bank Status</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"8\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"14pt\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"21pt\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"21pt\"></colspec><colspec align=\"center\" colname=\"4\" colwidth=\"21pt\"></colspec><colspec align=\"center\" colname=\"5\" colwidth=\"35pt\"></colspec><colspec align=\"center\" colname=\"6\" colwidth=\"35pt\"></colspec><colspec align=\"center\" colname=\"7\" colwidth=\"35pt\"></colspec><colspec align=\"center\" colname=\"8\" colwidth=\"35pt\"></colspec><tbody valign=\"top\"><row><entry>    20</entry><entry>19</entry><entry>18</entry><entry>17</entry><entry>Bank<sub>n</sub></entry><entry>Bank<sub>n+1</sub></entry><entry>Bank<sub>n+2</sub></entry><entry>Bank<sub>n+3</sub></entry></row><row><entry align=\"center\" nameend=\"8\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry>0</entry><entry>0</entry><entry>0</entry><entry>0</entry><entry>A</entry><entry>D</entry><entry>D</entry><entry>D</entry></row><row><entry>0</entry><entry>0</entry><entry>0</entry><entry>1</entry><entry>D</entry><entry>A</entry><entry>D</entry><entry>D</entry></row><row><entry>0</entry><entry>0</entry><entry>1</entry><entry>0</entry><entry>D</entry><entry>D</entry><entry>A</entry><entry>D</entry></row><row><entry>0</entry><entry>0</entry><entry>1</entry><entry>1</entry><entry>D</entry><entry>D</entry><entry>D</entry><entry>A</entry></row><row><entry>0</entry><entry>1</entry><entry>0</entry><entry>0</entry><entry>A</entry><entry>A</entry><entry>D</entry><entry>D</entry></row><row><entry>0</entry><entry>1</entry><entry>0</entry><entry>1</entry><entry>D</entry><entry>A</entry><entry>A</entry><entry>D</entry></row><row><entry>0</entry><entry>1</entry><entry>1</entry><entry>0</entry><entry>D</entry><entry>D</entry><entry>A</entry><entry>A</entry></row><row><entry>0</entry><entry>1</entry><entry>1</entry><entry>1</entry><entry>A</entry><entry>D</entry><entry>D</entry><entry>A</entry></row><row><entry>1</entry><entry>0</entry><entry>0</entry><entry>0</entry><entry>A</entry><entry>D</entry><entry>A</entry><entry>D</entry></row><row><entry>1</entry><entry>0</entry><entry>0</entry><entry>1</entry><entry>D</entry><entry>A</entry><entry>A</entry><entry>D</entry></row><row><entry>1</entry><entry>0</entry><entry>1</entry><entry>0</entry><entry>D</entry><entry>A</entry><entry>D</entry><entry>A</entry></row><row><entry>1</entry><entry>0</entry><entry>1</entry><entry>1</entry><entry>A</entry><entry>A</entry><entry>A</entry><entry>D</entry></row><row><entry>1</entry><entry>1</entry><entry>0</entry><entry>0</entry><entry>D</entry><entry>A</entry><entry>A</entry><entry>A</entry></row><row><entry>1</entry><entry>1</entry><entry>0</entry><entry>1</entry><entry>A</entry><entry>D</entry><entry>A</entry><entry>A</entry></row><row><entry>1</entry><entry>1</entry><entry>1</entry><entry>0</entry><entry>A</entry><entry>A</entry><entry>D</entry><entry>A</entry></row><row><entry>1</entry><entry>1</entry><entry>1</entry><entry>1</entry><entry>A</entry><entry>A</entry><entry>A</entry><entry>A</entry></row><row><entry align=\"center\" nameend=\"8\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry align=\"left\" nameend=\"8\" namest=\"1\">A = Active </entry></row><row><entry align=\"left\" nameend=\"8\" namest=\"1\">D = Deactivated </entry></row></tbody></tgroup></table></tables></p><p>The bitfield consisting of bit positions <b>21</b> and <b>22</b> is the SRAM output field. This field allows the CPU and operating system to selectively sequence through banks <b>202</b>. The encoding for this field is provided in Table. II. In Table II, Bank<sub>n </sub>represents the bank which contains the target data, as identified by bit comparison circuitry using the procedure described above. From then on, depending on the bank status register bits, additional banks can be accessed in specified sequence.</p><p><tables id=\"TABLE-US-00002\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"1\" colwidth=\"35pt\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"35pt\"></colspec><colspec align=\"left\" colname=\"3\" colwidth=\"147pt\"></colspec><thead><row><entry nameend=\"3\" namest=\"1\" rowsep=\"1\">TABLE II</entry></row><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry>Register</entry><entry>Register</entry><entry></entry></row><row><entry>Bit</entry><entry>Bit</entry></row><row><entry>22</entry><entry>21</entry><entry>Bank Access Sequence</entry></row><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row></thead><tbody valign=\"top\"><row><entry>0</entry><entry>0</entry><entry>Bank<sub>n</sub>\u2192 Bank<sub>n + 1</sub>\u2192 Bank<sub>n + 2</sub>\u2192 Bank<sub>n + 3</sub></entry></row><row><entry>0</entry><entry>1</entry><entry>Bank<sub>n</sub>\u2192 Bank<sub>n + 1</sub>\u2192 Bank<sub>n + 2</sub></entry></row><row><entry>1</entry><entry>0</entry><entry>Bank<sub>n</sub>\u2192 Bank<sub>n + 1</sub></entry></row><row><entry>1</entry><entry>1</entry><entry>Bank<sub>n </sub>only</entry></row><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row></tbody></tgroup></table></tables></p><p>Assume for discussion purposes that an 01 is written into SRAM output bitfield. Also assume that, from the row address, comparator <b>504</b> has identified Bank <b>2</b> (<b>702</b><i>b</i>) as the bank initially containing desired data. For the case where each bank includes two SRAM registers <b>603</b>, the first SRAM register <b>603</b><i>a </i>of bank <b>2</b> is accessed followed by the second SRAM register <b>603</b><i>b </i>for that bank. In response to the 01 programmed into the SRAM output field, the next bank accessed is bank Bn+1 (i.e., Bank<b>3</b>) is accessed, with SRAM register <b>1</b> and SRAM register <b>2</b> sequentially accessed from Bank<b>1</b>. The process repeats itself for Bankn+2 (i.e., Bank<b>4</b>)</p><p>In this example, the loop of accesses from Banks <b>2</b>,<b>3</b> and <b>4</b> can continue as long as the CPU needs to repeatedly access those locations. It should be recognized that the access could also be made from the bank DRAM cell arrays <b>407</b> for the given bank in addition to the SRAM registers. If the looping option is chosen, the CPU and the operating system must be configured to recognize how data are stored and retrieved from the sequence of banks such that the proper data is accessed at the proper time. FIG. 9 is a timing diagram for a memory operation (read or write) operation in an embodiment of a multiprocessing system according to the principles of the present invention.</p><p>As is shown in FIG. 9, a read from or write to memory, such as integrated memory <b>600</b>, is timed by SYSCLK. All timings of memory operations are synchronized to SYSCLK, with internal timings generated therefrom by clock generator <b>405</b>.</p><p>On the falling edge of SYSCLK with an active command loaded in command register <b>406</b>, a row address input on the address bus, such as address bus <b>205</b> in multiprocessing system <b>200</b>A, or address/control bus <b>210</b> in multiprocessing system <b>200</b>B is latched into row address latch <b>501</b>. The row address bits are then processed as discussed above using LRR address latch <b>502</b> and bit compare circuitry <b>503</b> of the selected bank.</p><p>Column addresses are latched into column address latch <b>502</b> on a falling edge of SYSCLK during a read command or write command as appropriate to the operation to be performed. Data is then output through the data latches <b>413</b> and data bus <b>414</b> connected thereto, starting at the initial column address, after the lapse of a latency period. For a write operation, the latency period is predetermined. For a read operation, the latency period may be a predetermined value, or may be extended by a preselected amount, as discussed in conjunction with FIG. <b>8</b>. If a plurality of banks have been selected for the access, a bank switch is made once the data is SRAM registers <b>603</b><i>a </i>and <b>603</b><i>b </i>of the initial bank have been paged out. The paging from the next bank in the loop starts from the initial address latched in the column address latches <b>502</b>.</p><p>The timing of memory operation illustrated in FIG. 9 does not take advantage of nonmultiplexed address bus <b>410</b>. Therefore, the timing of memory operations illustrated in FIG. 10 is to be preferred. Row and column addresses are simultaneously presented by the memory controller (not shown in FIG. 6) on address bus <b>410</b> during the assertion of a read command or write command, as appropriate, in command register <b>406</b>, and are latched on the falling edge of SYSCLK into row address latch <b>501</b>, and column address latch <b>502</b>, respectively. Following a latency period as discussed hereinabove, data are read or written as appropriate to the operation signalled in command register <b>406</b>. Again, data are accessed by words starting at the location corresponding to the received column address from SRAM register <b>1</b> of the initial bank and continuing through the locations of SRAM register <b>2</b>. At the conclusion of the write to SRAM register <b>2</b>, the write switches to register <b>1</b> of the next bank in the loop, starting with the location corresponding to the column address latched in that banks column address latch <b>502</b>.</p><p>As discussed above, data loaded into the SRAM registers can be accessed in a number of different ways, including non-contiguous accessing within a bank, between banks, or both. For example, consider the case where Bank <b>0</b> registers <b>603</b><i>a </i>and <b>603</b><i>b </i>are respectively accessed with Addr<b>0</b> and Addr<b>4</b>, the SRAM registers of Bank <b>1</b> with addresses Addr<b>1</b> and Addr<b>5</b>, the SRAM registers of Bank <b>2</b> with addresses Addr<b>2</b> and Addr<b>6</b>, and the Bank <b>3</b> data at addresses Addr<b>3</b> and Addr<b>7</b>. This type of access can be implemented as follows.</p><p>During a first memory cycle, a row address is received on the address bus, and latched as previously described. On the falling edge of SYSCLK the row addresses are latched in the row address latches <b>501</b> of all the banks. Bank selection is determined by the bank select bits in the row address, in an embodiment of a multiprocessing system such as multiprocessing system <b>200</b>A. Alternatively, bank selection is automatic in an embodiment in which the full address space spanned by each bank is unique, such as in multiprocessing system <b>200</b>B. For the selected bank, comparison circuitry <b>504</b> compares the incoming row address, less any bank select bits, with the LRR row address stored in each of the plurality of LRR latches <b>604</b> for the selected bank.</p><p>Assume that Bank <b>0</b> is the bank being accessed and that a match occurs between the incoming row address, less any bank select bits, (Addr) and the bits stored in LRR latch <b>1</b>. After the latency period elapses, SRAM register <b>1</b> of Bank <b>0</b> is accessed. For Bank <b>0</b>, the address row address register is incremented to produce address Addr+4. This new address is used to access Bank <b>0</b> DRAM array <b>407</b>. At the same time, the incoming address in row latch <b>502</b> of Bank <b>1</b> is incremented to Addr+5, in the Bank <b>2</b> row address latch to Addr+6 and in the Bank <b>3</b> row address latch to Addr+7.</p><p>It should be noted that if the incoming row addresses, less any bank select bits, instead match with the bits in an LRR Latch <b>2</b> (<b>603</b><i>b</i>) of Bank <b>0</b>, then the Bank <b>0</b> SRAM register <b>2</b> (<b>603</b><i>b</i>) is accessed and the contents of the row address registers are incremented in all banks to load SRAM registers <b>1</b> for all banks. In this example, the incoming row address latched in Bank <b>0</b> would be decremented to Addr\u22124, in Bank <b>1</b> to Addr\u22123, in Bank to Addr\u22122 and Bank <b>3</b> to Addr\u22121. SRAM registers <b>1</b> would the be loaded from DRAM in accordance to the modified addresses.</p><p>At the start of the second memory cycle, the contents of row address registers <b>502</b> for all banks are copied to the LRR latches corresponding to the set of SRAM registers which were loaded during the first memory cycle. For example, if SRAM registers <b>2</b> (<b>603</b><i>b</i>) were loaded during the first cycle, the modified contents of row address latches are transferred to the associated LRR latches (<b>604</b><i>b</i>).</p><p>Then in the second memory cycle, the addresses could be modified as follows for purposes of loading SRAM registers <b>1</b>. If Bank <b>0</b> is the accessed bank, the contents of its row address latches are decremented to the original input address Addr. Similarly the addresses in the Banks <b>1</b>, <b>2</b> and <b>3</b> are respectively modified to become Addr+1, Addr+2 and Addr+3.</p><p>Continuing with the example above where the first memory cycle loaded SRAM registers <b>1</b>, with Bank <b>0</b> being the accessed bank, accessing SRAM, registers <b>2</b> in the second cycle may be performed by the following address operations: the modified row address in the Bank <b>0</b> row address latch is decremented back to Addr, in Bank <b>1</b> to Addr+1, in Bank <b>2</b> to Addr+2, and Bank <b>3</b> to Addr+3. These twice-modified addresses are used to load SRAM registers <b>2</b> from DRAM and at the start of the following memory cycle are loaded into the corresponding LRR latches <b>2</b>.</p><p>The procedure is the same no matter which bank is accessed through its data latch (thereby determining the address modification sequence) and no matter how the row addresses are modified. In sum, numerous sequences and address modifications are possible, as required to access a predetermined block of data, with a given spatial and/or temporal coherency, in the SRAM registers.</p><p>When no match occurs between the received row address, less any bank select bits, and any of the row addresses stored in SRAM registers <b>603</b> and the selected bank, the accessing procedure changes. In this case, no SRAM registers <b>603</b> are accessed via the data latches and all SRAM registers <b>603</b> (if desired) are loaded from DRAM.</p><p>During the first memory cycle, the received row address is latched in and then used to transfer data from the DRAM array <b>407</b> of the bank being accessed to a selected one of the plurality of SRAM registers <b>603</b>. The choice of which SRAM register <b>603</b> to be loaded can be a function of anyone of a number of factors. For example, it may be the encoding of the address bits themselves (e.g. odd parity sends the data to one register and even parity to another register) or to obtain specific temporal locality (e.g. the register which has not be reloaded in the immediately previous cycle or cycles). At the same time, the received row address, less any bank select bits, are modified in the address latches <b>501</b> of the remaining banks, as desired to define the first half of a block of data to be stored in SRAM. Preferably these registers correspond to the SRAM register selected in the accessed bank (i.e. if register <b>1</b> is loaded in the accessed bank, register <b>1</b> is loaded in the remaining banks). The SRAM registers <b>603</b> of these banks are subsequently loaded using the modified addresses.</p><p>At the start of the second memory cycle, the contents of the row address latches <b>501</b> are copied to the LRR latches <b>604</b> corresponding to the SRAM registers <b>603</b> accessed during the first memory cycle. Row address increment/decrement circuitry <b>507</b> then modifies the addresses in row address latches <b>501</b> as required to access a block of data within DRAM and transfer that block of data into the remaining registers of SRAM cache. The SRAM registers <b>603</b> not loaded during the first memory cycle are loaded during this cycle, in accordance with the new addresses stored in row address latches <b>501</b>. At the start of the subsequent memory cycle, these addresses will be copied to the appropriate LRR latches <b>604</b>, depending on the SRAM registers loaded.</p><p>Although the invention has been described with reference to specific embodiments, these descriptions are not meant to be construed in a limiting sense. Various modifications of the disclosed embodiments as well as alternative embodiments of the invention will become apparent to persons skilled in the art upon reference to the description of the invention. It is therefore, contemplated that the claims will cover any such modifications or embodiments that fall within the true scope of the invention.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "G. R. Mohan", "last_name": "Rao", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "SILICON AQUARIUS, INC."}, {"first_name": "", "last_name": "S. AQUA SEMICONDUCTOR, LLC", "name": ""}, {"first_name": "", "last_name": "S. AQUA SEMICONDUCTOR", "name": ""}], "ipc_classes": [{"primary": true, "label": "G11C   8/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMEP"}, {"label": "G06F   9/38        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "36523005"}, {"primary": false, "label": "711E12023"}, {"primary": false, "label": "712E09047"}, {"primary": false, "label": "36518907"}, {"primary": false, "label": "36518905"}, {"primary": false, "label": "36523008"}, {"primary": false, "label": "711E12041"}], "ecla_classes": [{"label": "G06F  12/08B4"}, {"label": "G06F  12/08B22"}, {"label": "G06F   9/38D2"}], "cpc_classes": [{"label": "G06F  12/0806"}, {"label": "G06F   9/383"}, {"label": "G06F  12/0893"}, {"label": "G06F   9/383"}, {"label": "G06F  12/0893"}, {"label": "G06F  12/0806"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1998-02-20", "application_date": "2000-07-27", "family_members": [{"ucid": "US-6504785-B1", "titles": [{"lang": "EN", "text": "Multiprocessor system with integrated memory"}]}, {"ucid": "US-6173356-B1", "titles": [{"lang": "EN", "text": "Multi-port DRAM with integrated SRAM and systems and methods using the same"}]}]}