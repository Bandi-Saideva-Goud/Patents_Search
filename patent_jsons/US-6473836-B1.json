{"patent_number": "US-6473836-B1", "publication_id": 73210110, "family_id": 15431787, "publication_date": "2002-10-29", "titles": [{"lang": "EN", "text": "Computing system and cache memory control apparatus controlling prefetch in hierarchical cache memories"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50402504\"><p>A cache memory control apparatus provided to prevent necessary data from being ejected from hierarchical cache memories and to avoid conflicts of process in the main pipeline of a processing unit (arithmetic unit), even when prefetch commands are issued at high frequency. The apparatus includes a command control section to issue a prefetch command instruction that speculative data is to be fetched (prefetched) from a main storage unit into a plurality of hierarchically arranged cache memories, and a prefetch control section to changeably select at least one of the hierarchically arranged cache memories as a destination to receive prefetch data when the prefetch command issued from the command control section is executed, according to at least one of status information of one of the cache memories and a type or kind of prefetch. The apparatus is particularly useful when applied to a computer system of the type having a prefetch operation.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00001\" num=\"1\"><claim-text>1. A cache memory control apparatus for controlling a plurality of hierarchically arranged cache memories into which data of high-frequency of access is copied from a main storage unit by a processing unit of a computer system, which processing unit executes various processes using data stored in the main storage unit of the computer system, said apparatus comprising:</claim-text><claim-text>a command control section issuing a prefetch command to fetch speculative data, which is inclined to become necessary for near future use in the processing unit, from the main storage unit into the hierarchically arranged cache memories prior to execution of a corresponding process by the processing unit; and </claim-text><claim-text>a prefetch control section controlling the hierarchically arranged cache memories, when the prefetch command issued by said command control section is executed, such that at least one of the hierarchically arranged cache memories to which said speculative data is to be fetched is changeably selected as one or more destination cache memories, according to at least either of status information about one of the hierarchically arranged cache memories and a kind of prefetch for the prefetch command. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00002\" num=\"2\"><claim-text>2. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00001\">claim 1</claim-ref>, wherein the command control section further comprises:</claim-text><claim-text>a prefetch kind setting section setting kind-of-prefetch information about a kind of prefetch as a prefetch-destination change-over control condition for the prefetch command to be issued by said command control section, and </claim-text><claim-text>a prefetch kind identifying section identifying the kind of prefetch set for the prefetch command to be issued and outputting a result of an identification of the kind of prefetch to said prefetch control section so that said prefetch control section controls a change-over of one or more destination cache memories based on the kind of prefetch received from said prefetch kind identifying section. </claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00003\" num=\"3\"><claim-text>3. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00001\">claim 1</claim-ref>, wherein selection of a hierarchically arranged cache memory is based on state-of-use information about a storage area of the hierarchically arranged cache memory or state-of-contention information about the hierarchically arranged cache memory.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00004\" num=\"4\"><claim-text>4. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00001\">claim 1</claim-ref>, wherein conflicts in the processing unit are substantially avoided.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00005\" num=\"5\"><claim-text>5. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00001\">claim 1</claim-ref>, further comprising:</claim-text><claim-text>a status information detecting unit detecting status information about one of the cache memories disposed near the processing unit and outputting the detected status information to said prefetch control section so that said prefetch control section controls the cache memories so as to change-over said one or more destination cache memories in accordance with said detected status information. </claim-text></claim>"}, {"num": 6, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00006\" num=\"6\"><claim-text>6. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00005\">claim 5</claim-ref>, wherein selection of a hierarchically arranged cache memory is based on state-of-use information about a storage area of the hierarchically arranged cache memory or state-of-contention information about the hierarchically arranged cache memory.</claim-text></claim>"}, {"num": 7, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00007\" num=\"7\"><claim-text>7. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00005\">claim 5</claim-ref>, wherein conflicts in the processing unit are substantially avoided.</claim-text></claim>"}, {"num": 8, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00008\" num=\"8\"><claim-text>8. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00005\">claim 5</claim-ref>, wherein the command control section further comprises:</claim-text><claim-text>a prefetch kind setting section setting kind-of-prefetch information about a kind of prefetch as a prefetch-destination change-over control condition for the prefetch command to be issued by said command control section, and </claim-text><claim-text>a prefetch kind identifying section identifying the kind of prefetch set for the prefetch command to be issued and outputting a result of an identification of the kind of prefetch to said prefetch control section so that said prefetch control section controls a change-over of one or more destination cache memories based on the kind of prefetch received from said prefetch kind identifying section. </claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00009\" num=\"9\"><claim-text>9. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00008\">claim 8</claim-ref>, wherein selection of a hierarchically arranged cache memory is based on state-of-use information about a storage area of the hierarchically arranged cache memory or state-of-contention information about the hierarchically arranged cache memory.</claim-text></claim>"}, {"num": 10, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00010\" num=\"10\"><claim-text>10. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00008\">claim 8</claim-ref>, wherein conflicts in the processing unit are substantially avoided.</claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00011\" num=\"11\"><claim-text>11. A computer system comprising:</claim-text><claim-text>a main storage unit; </claim-text><claim-text>a processing unit executing various processes using data stored in said main storage unit; </claim-text><claim-text>a plurality of hierarchically arranged cache memories to which data of high-frequency of access by said processing unit is to be fetched from said main storage unit; and </claim-text><claim-text>a cache memory control apparatus controlling the plurality of cache memories and including </claim-text><claim-text>a command control section issuing a prefetch command to fetch speculative data, which is inclined to become necessary for near future use in said processing unit, from said main storage unit into said hierarchically arranged cache memories prior to execution of a corresponding process by said processing unit, and </claim-text><claim-text>a prefetch control section controlling said hierarchically arranged cache memories, when the prefetch command issued by said command control section is executed, such that at least one of said hierarchically arranged cache memories to which said speculative data is to be fetched is changeably selected as one or more destination cache memories, according to at least either of status information about one of the hierarchically arranged cache memories and a kind of prefetch for the prefetch command. </claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00012\" num=\"12\"><claim-text>12. The computer system according to <claim-ref idref=\"US-6473836-B1-CLM-00011\">claim 11</claim-ref>, further comprising:</claim-text><claim-text>a prefetch kind setting section setting kind-of-prefetch information about a kind of prefetch as a prefetch-destination change-over control condition for the prefetch command to be issued by said command control section; and </claim-text><claim-text>a prefetch kind identifying section identifying the kind of prefetch set for the prefetch command to be issued and outputting a result of an identification of the kind of prefetch to said prefetch control section so that said prefetch control section controls a change-over of one or more destination cache memories based on the kind of prefetch received from said prefetch kind identifying section. </claim-text></claim>"}, {"num": 13, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00013\" num=\"13\"><claim-text>13. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00011\">claim 11</claim-ref>, wherein selection of a hierarchically arranged cache memory is based on state-of-use information about a storage area of the hierarchically arranged cache memory or state-of-contention information about the hierarchically arranged cache memory.</claim-text></claim>"}, {"num": 14, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00014\" num=\"14\"><claim-text>14. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00011\">claim 11</claim-ref>, wherein conflicts in the processing unit are substantially avoided.</claim-text></claim>"}, {"num": 15, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00015\" num=\"15\"><claim-text>15. The computer system according to <claim-ref idref=\"US-6473836-B1-CLM-00011\">claim 11</claim-ref>, further comprising:</claim-text><claim-text>a status information detecting unit detecting status information about one of the cache memories disposed near the processing unit and outputting the detected status information to said prefetch control section so that said prefetch control section controls the cache memories so as to change-over said one or more destination cache memories in accordance with said detected status information. </claim-text></claim>"}, {"num": 16, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00016\" num=\"16\"><claim-text>16. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00015\">claim 15</claim-ref>, wherein selection of a hierarchically arranged cache memory is based on state-of-use information about a storage area of the hierarchically arranged cache memory or state-of-contention information about the hierarchically arranged cache memory.</claim-text></claim>"}, {"num": 17, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00017\" num=\"17\"><claim-text>17. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00015\">claim 15</claim-ref>, wherein conflicts in the processing unit are substantially avoided.</claim-text></claim>"}, {"num": 18, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00018\" num=\"18\"><claim-text>18. The computer system according to <claim-ref idref=\"US-6473836-B1-CLM-00015\">claim 15</claim-ref>, further comprising:</claim-text><claim-text>a prefetch kind setting section setting kind-of-prefetch information about a kind of prefetch as a prefetch-destination change-over control condition for the prefetch command to be issued by said command control section; and </claim-text><claim-text>a prefetch kind identifying section identifying the kind of prefetch set for the prefetch command to be issued and outputting a result of an identification of the kind of prefetch to said prefetch control section so that said prefetch control section controls a change-over of one or more destination cache memories based on the kind of prefetch received from said prefetch kind identifying section. </claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00019\" num=\"19\"><claim-text>19. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00018\">claim 18</claim-ref>, wherein selection of a hierarchically arranged cache memory is based on state-of-use information about a storage area of the hierarchically arranged cache memory or state-of-contention information about the hierarchically arranged cache memory.</claim-text></claim>"}, {"num": 20, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00020\" num=\"20\"><claim-text>20. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00018\">claim 18</claim-ref>, wherein conflicts in the processing unit are substantially avoided.</claim-text></claim>"}, {"num": 21, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00021\" num=\"21\"><claim-text>21. A cache memory control apparatus for controlling a plurality of hierarchically arranged cache memories into which data of high-frequency of access is copied from a main storage unit by a processing unit of a computer system, which processing unit executes various processes using data stored in a main storage unit of the computer system, said apparatus comprising:</claim-text><claim-text>a command control section issuing a prefetch command to fetch speculative data, which is inclined to become necessary for near future use in the processing unit, from the main storage unit into the hierarchically arranged cache memories prior to execution of a corresponding process by the processing unit; </claim-text><claim-text>a prefetch control section controlling the hierarchically arranged cache memories, when the prefetch command issued by said command control section is executed, such that at least one of the hierarchically arranged cache memories to which said speculative data is to be fetched is changeably selected as one or more destination cache memories; and </claim-text><claim-text>a status information detecting unit detecting status information about one of the cache memories disposed near the processing unit and outputting the detected status information to said prefetch control section so that said prefetch control section controls the cache memories so as to change-over said one or more destination cache memories in accordance with said detected status information. </claim-text></claim>"}, {"num": 22, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00022\" num=\"22\"><claim-text>22. The cache memory control apparatus according to <claim-ref idref=\"US-6473836-B1-CLM-00021\">claim 21</claim-ref>, wherein the command control section further comprises:</claim-text><claim-text>a prefetch kind setting section setting kind-of-prefetch information about a kind of prefetch as a prefetch-destination change-over control condition for the prefetch command to be issued by said command control section, and </claim-text><claim-text>a prefetch kind identifying section identifying the kind of prefetch set for the prefetch. command to be issued and outputting a result of an identification of the kind of prefetch to said prefetch control section so that said prefetch control section controls a change-over of one or more destination cache memories based on the kind of prefetch received from said prefetch kind identifying section. </claim-text></claim>"}, {"num": 23, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00023\" num=\"23\"><claim-text>23. A cache memory control apparatus for controlling a plurality of hierarchically arranged cache memories into which data of high-frequency-of access is copied from a main storage unit by a processing unit of a computer system, which processing unit executes various processes using data stored in a main storage unit of the computer system, said apparatus comprising:</claim-text><claim-text>a command control section issuing a prefetch command to fetch speculative data, which is inclined to become necessary for near future use in the processing unit, from the main storage unit into the hierarchically arranged cache memories prior to execution of a corresponding process by the processing unit, the command control section includes </claim-text><claim-text>a prefetch kind setting section setting kind-of-prefetch information about a kind of prefetch as a prefetch-destination change-over control condition for the prefetch command to be issued by said command control section, and </claim-text><claim-text>a prefetch kind identifying section identifying the kind of prefetch set for the prefetch command to be issued and outputting a result of an identification of the kind of prefetch to said prefetch control section so that said prefetch control section controls a change-over of one or more destination cache memories based on the kind of prefetch received from said prefetch kind identifying section; and </claim-text><claim-text>a prefetch control section controlling the hierarchically arranged cache memories, when the prefetch command issued by said command control section is executed, such that at least one of the hierarchically arranged cache memories to which said speculative data is to be fetched is changeably selected as one or more destination cache memories. </claim-text></claim>"}, {"num": 24, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00024\" num=\"24\"><claim-text>24. A computer system comprising:</claim-text><claim-text>a main storage unit; </claim-text><claim-text>a processing unit executing various processes using data stored in said main storage unit; </claim-text><claim-text>a plurality of hierarchically arranged cache memories to which data of high-frequency of access by said processing unit is to be fetched from said main storage unit; </claim-text><claim-text>a cache memory control apparatus controlling the plurality of cache memories and including </claim-text><claim-text>a command control section issuing a prefetch command to fetch speculative data, which is inclined to become necessary for near future use in said processing unit, from said main storage unit into the hierarchically arranged cache memories prior to execution of a corresponding process by said processing unit, and </claim-text><claim-text>a prefetch control section controlling the hierarchically arranged cache memories, when the prefetch command issued by said command control section is executed, such that at least one of the hierarchically arranged cache memories to which said speculative data is to be fetched is changeably selected as one or more destination cache memories; and </claim-text><claim-text>a status information detecting unit detecting status information about one of the cache memories disposed near the processing unit and outputting the detected status information to said prefetch control section so that said prefetch control section controls the cache memories so as to change-over said one or more destination cache memories in accordance with said detected status information. </claim-text></claim>"}, {"num": 25, "parent": 24, "type": "dependent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00025\" num=\"25\"><claim-text>25. A computer system according to <claim-ref idref=\"US-6473836-B1-CLM-00024\">claim 24</claim-ref>, wherein the command control section further comprises:</claim-text><claim-text>a prefetch kind setting section setting kind-of-prefetch information about a kind of prefetch as a prefetch-destination change-over control condition for the prefetch command to be issued by said command control section, and </claim-text><claim-text>a prefetch kind identifying section identifying the kind of prefetch set for the prefetch command to be issued and outputting a result of an identification of the kind of. prefetch to said prefetch control section so that said prefetch control section controls a change-over of one or more destination cache memories based on the kind of prefetch received from said prefetch kind identifying section. </claim-text></claim>"}, {"num": 26, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6473836-B1-CLM-00026\" num=\"26\"><claim-text>26. A computer system comprising:</claim-text><claim-text>a main storage unit; </claim-text><claim-text>a processing unit executing various processes using data stored in said main storage unit; </claim-text><claim-text>a plurality of hierarchically arranged cache memories to which data of high-frequency of access by said processing unit is to be fetched from said main storage unit; and </claim-text><claim-text>a cache memory control apparatus controlling the plurality of cache memories and including </claim-text><claim-text>a command control section issuing a prefetch command to fetch speculative data, which is inclined to become necessary for near future use in said processing unit, from said main storage unit into the hierarchically arranged cache memories prior to execution of a corresponding process by said processing unit, and </claim-text><claim-text>a prefetch control section controlling the hierarchically arranged cache memories, when the prefetch command issued by said command control section is executed, such that at least one of the hierarchically arranged cache memories to which said speculative data is to be fetched is changeably selected as one or more destination cache memories; </claim-text><claim-text>a prefetch kind setting section setting kind-of-prefetch information about a kind of prefetch as a prefetch-destination change-over control condition for the prefetch command to be issued by said command control section; and </claim-text><claim-text>a prefetch kind identifying section identifying the kind of prefetch set for the prefetch command to be issued and outputting a result of an identification of the kind of prefetch to said prefetch control section so that said prefetch control section controls a change-over of one or more destination cache memories based on the kind of prefetch received from said prefetch kind identifying section. </claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53659712\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>The present invention relates generally to a cache memory control apparatus for controlling hierarchical cache memories disposed between a main storage unit and a processing unit, which executes various processes using data stored in the main storage unit, and a computer system equipped with the cache memory control apparatus. More particularly, the invention is directed to a cache memory control apparatus for use in a computer system having a prefetch function of fetching speculative data, which is inclined to become necessary in the processing unit, from the main storage unit into the hierarchical cache memories in advance, and also directed to a computer system equipped with the cache memory control apparatus.</p><p>2. Description of the Related Art</p><p>A conventional computer system (data processing machine), as shown in FIG. 5 of the accompanying drawings, generally comprises a main storage unit (hereinafter called MSU) <b>12</b> storing programs and various data to be processed by the programs, and a central processing unit (hereinafter called CPU) <b>11</b> for executing various processes using the data stored in the MSU <b>12</b>.</p><p>Recently, with increasing improvement of throughput of the CPU <b>11</b> and increasing enlargement of capacity of the MSU <b>12</b> as well, the data processing speed in the CPU <b>11</b> has been much faster as compared to the speed of access to the MSU <b>12</b>. Assuming that the CPU <b>11</b> and the MSU <b>12</b> are combined and are respectively regarded as the data consumption side and the data supply side, shortage of supply of data tends to occur so that the CPU <b>11</b> would spend most of the processing time waiting for data from the MSU <b>12</b>, lowering the effective throughput of the CPU <b>11</b> even though its processing speed is increased.</p><p>As a solution, it has been customary to minimize the apparent access time of the MSU <b>12</b>, as viewed from the CPU <b>11</b>, by placing a cache memory, which is smaller in capacity and higher in processing speed than the MSU <b>12</b>, either inside or outside an operationally near the CPU <b>11</b> and by using the cache memory to adjust the access delay of the MSU <b>12</b> with respect to the cycle time of the CPU <b>11</b>.</p><p>This cache memory usually assumes only a single level or class (of the hierarchy), in the form of a block of plural words, between the MSU <b>12</b> and a register <b>11</b><i>b </i>in the CPU <b>12</b>. Alternatively, however, if the difference between the access time of the MSU <b>12</b> and the cycle time of the CPU <b>11</b> is considerably large, one or more additional levels or classes (blocks of plural words) are placed between the MSU <b>12</b> and the register <b>11</b><i>b </i>in the CPU <b>11</b>. In the example shown in FIG. 5, a primary cache memory <b>13</b> and a secondary cache memory <b>14</b> are placed between the MSU <b>12</b> and the register <b>11</b><i>b</i>, which is coupled to an arithmetic unit <b>11</b><i>a</i>, in the CPU <b>11</b> to form a two-level or a two-class cache memory hierarchy. Both the primary and secondary cache memories <b>13</b>, <b>14</b> are disposed inside the CPU <b>11</b>.</p><p>Specifically the primary cache memory <b>13</b> is disposed hierarchically near the arithmetic unit <b>11</b><i>a</i>while the secondary memory <b>14</b> is disposed hierarchically near the MSU <b>12</b>. Generally the secondary cache memory <b>14</b> is set to be larger in storage capacity than the primary cache memory <b>13</b>; that is, in a multi-cache-memory hierarchy, the nearer a cache memory is disposed with respect to the arithmetic unit <b>11</b><i>a</i>, the smaller its storage capacity should be set.</p><p>In the computer system equipped with the foregoing cache memories <b>13</b>, <b>14</b> with the two-level or two-class hierarchy, if the CPU <b>11</b> needs a certain kind of data D, first the CPU <b>11</b> discriminates whether the data D is stored in the primary cache memory <b>13</b>. If the same data D is stored in the primary cache memory <b>13</b> (if a \u201ccache hit\u201d results with respect to the primary cache memory <b>13</b>), the CPU <b>11</b> reads the data D from the primary cache memory <b>13</b> without having to access either the secondary cache memory <b>14</b> or the MSU <b>12</b>.</p><p>On the contrary, if the data D is not stored in the primary cache memory <b>13</b> (if a \u201ccache miss\u201d results with respect to the primary cache memory <b>13</b>), the CPU <b>11</b> discriminates whether the data D is stored in the secondary cache memory <b>14</b>. As a result, if a cache hit then results with respect to the secondary cache memory <b>14</b> (if information retrieval has taken place successfully with respect to the secondary cache memory <b>14</b>), the CPU <b>11</b> reads a data block containing the data D from the secondary cache memory <b>14</b> and then writes the data block into the primary cache memory <b>13</b>, whereupon the CPU <b>11</b> reads the data D from the primary cache memory <b>13</b>.</p><p>Further, if the data D is not stored even in the secondary cache memory <b>14</b> (if a cache miss results with respect to the secondary cache memory <b>14</b>), the CPU <b>11</b> reads a data block containing the data D from the MSU <b>12</b> and writes the data block into the primary and secondary cache memories <b>13</b>, <b>14</b>, whereupon the CPU <b>11</b> reads the data D from the primary cache memory <b>13</b>.</p><p>As mentioned above, if a cache miss has resulted with respect to the primary cache memory <b>13</b> or the secondary cache memory <b>14</b>, the data D must be read from the secondary cache memory <b>14</b> or the MSU <b>12</b>, respectively, which would take more time to read the data D. In the meantime, although recent computer systems have sharply increased the clock frequency of the CPU <b>11</b>, the performance of MSU <b>12</b>, such as in the form of DRAM (dynamic random access memory), has not kept up with the improvement in the increased throughput of the CPU <b>11</b>. As a result, the MSU <b>12</b> would be located far from CPU <b>11</b> since as previously mentioned that the difference between the access time of the MSU <b>12</b> and the cycle time of the CPU <b>11</b> is considerably large the throughput of the CPU <b>11</b> would increasingly be impaired due to the foregoing unsuccessful accessing of the cache memories <b>13</b> ending.</p><p>In order to avoid the penalty for unsuccessful access of the cache memories <b>13</b> and <b>14</b>, it has been a common practice to fetch necessary data from the MSU <b>12</b> into the cache memories <b>13</b>, <b>14</b> prior to the arithmetic processing.</p><p>For this purpose, the CPU <b>11</b> issues, in addition to a loading command to fetch data from the MSU <b>12</b> into the register <b>11</b><i>b</i>, a dedicated-to-loading command to fetch the data from the MSU <b>12</b> into the primary and secondary cache memories <b>13</b>, <b>14</b>, but not into the register <b>11</b><i>b</i>, whereupon the CPU <b>11</b> can execute a separate process (the arithmetic process in the illustrated example) without managing or monitoring the state of execution of the dedicated-to-loading command, thus leaving a process or processes associated with the dedicated-to-loading command to the primary and secondary cache memories <b>13</b>, <b>14</b>. This dedicated-to-loading command is also called a \u201cprefetch\u201d command because of its function.</p><p>Now assuming that the arithmetic unit <b>11</b><i>a </i>performs consecutive arithmetic processes as data of the first to N-th items are substituted for concerned items of a predetermined equation one item in each arithmetic process, the CPU <b>11</b> issues a prefetch command to fetch (i+k)th item data to the primary cache memory <b>13</b>, prior to execution of the arithmetic process with respect to i-th item data, thereby resulting in the arithmetic unit <b>11</b><i>a </i>executing the respective arithmetic process without a cache miss.</p><p>As a result, the (i+k)th item data, which is inclined to become necessary in a forthcoming arithmetic process succeeding the arithmetic process of the i-th item data (by the arithmetic unit <b>11</b><i>a</i>) by k steps, is fetched into the primary and secondary cache memories <b>13</b>, <b>14</b> in parallel to the arithmetic process of the i-th item data. Therefore, by the time the CPU <b>11</b> should fetch the (i+k)th item data from the primary cache memory <b>13</b> into the register <b>11</b><i>b </i>for the arithmetic process coming k steps later, the (i+k)th item data will have existed in the primary cache memory <b>13</b> so that a cache miss can be prevented, thus avoiding any penalty for the cache miss.</p><p>However, the following problems have been encountered with the conventional technology if such a prefetch command is issued repeatedly in order to surely avoid penalties for possible cache misses:</p><p>(1) If the throughput (frequency of occurrence) of prefetch commands rises, a particular prefetch command for the data of a certain item would be issued much earlier than necessary, so that the data of the certain item would be fetched into the primary cache memory <b>13</b> too early. Because the primary cache memory <b>13</b> has usually only a limited storage capacity, the existing data could be ejected from the primary cache memory <b>13</b> as additional data is stored into the primary cache memory <b>13</b> from the MSU <b>12</b> in response to the execution of another prefetch command issued later. In that event, the prefetched data does not exist in the primary cache memory <b>13</b> when it actually becomes necessary for the forthcoming arithmetic process to be performed by the arithmetic unit <b>11</b><i>a</i>, which would result in cache miss with respect to the primary cache memory <b>13</b>. Consequently that data must be prefetched again from the secondary cache memory <b>14</b> or the MSU <b>12</b> into the primary cache memory <b>13</b>, which would in turn reduce the throughput of the CPU <b>11</b>.</p><p>(2) The primary cache memory <b>13</b> must be the one which the arithmetic unit <b>11</b><i>a </i>can have high-speed access. For this purpose, not only the storage capacity of the primary cache memory <b>13</b> but also the number of ports, at which simultaneous accessing is allowable, in the primary cache memory <b>13</b> are restricted so that, if the throughput (frequency of issue) of prefetch commands could be increased, the storing of data from the primary cache memory <b>13</b> into the register <b>11</b><i>b </i>and from the secondary cache memory <b>14</b> into the primary cache memory <b>13</b> would be delayed due to the execution of the prefetch commands. In other words, in the main pipeline of the CPU <b>11</b>, the access to the primary cache memory <b>13</b> which access will be inclined to become necessary in the ordinary process would collide with the access to the primary cache memory <b>13</b> for execution of the prefetch command, lowering the throughput of the CPU <b>11</b>.</p><h4>SUMMARY OF THE INVENTION</h4><p>With the foregoing problems in view, it is an object of the present invention to provide a cache memory control apparatus and a computer system which prevent ejection of necessary data from a cache memory confliction in the main pipeline of a processing unit, guaranteeing high-speed processing of the computer system even if the prefetch commands are issued at high frequency.</p><p>In order to accomplish the above-mentioned object, according to a first aspect of the present invention, there is provided a cache memory control apparatus for controlling a plurality of hierarchically arranged cache memories into which data of high-frequency of access by a processing unit, which executes various processes using data stored in a main storage unit, the apparatus comprising: a command control section for issuing a prefetch command instruction to fetch speculative data, which is inclined to become necessary for near future use in the processing unit, from the main storage unit into the cache memories, prior to execution of the individual process by the processing unit and a prefetch control section for controlling the hierarchically arranged cache memories, when the prefetch command issued by the command control section is executed, in such a manner that at least one of the hierarchically arranged cache memories, to which the speculative data which is inclined to become necessary for near future use in the processing unit is to be fetched, which is inclined to be is changeably selected as one or more destination cache memories.</p><p>According to a second aspect of the present invention, a computer system comprises: a main storage unit, a processing unit for executing various processes using data stored in the main storage unit, a plurality of hierarchically arranged cache memories to which data of high-frequency of access by the processing unit is to be fetched from the main storage unit and a cache memory control apparatus for controlling the plurality of hierarchically arranged cache memories. The cache memory control apparatus includes a command control section for issuing a prefetch command instruction to fetch speculative data, which is inclined to become necessary for near future use in the processing unit, from the main storage unit into the cache memories prior to execution of the individual process by the processing unit, and a prefetch control section for controlling the cache memories, when the prefetch command issued by the command control section is executed, in such a manner that at least one of the cache memories to which the speculative data is to be fetched is changeably selected as one or more destination cache memories.</p><p>Preferably, the cache memory control apparatus also comprises a status information detecting means for detecting status information about one of the cache memories disposed operationally near the processing unit and outputting the detected status information to the prefetch control section so that the prefetch control section controls the cache memories so as to change over the one or more destination cache memories in accordance with the detected status information.</p><p>The command control section of the cache memory control apparatus preferably includes; a prefetch kind setting section for setting kind-of-prefetch information about the kind of prefetch as a prefetch-destination change over control condition (i.e., the change-over control condition for prefetching data to the primary and/or secondary caches) for the prefetch command to be issued by the command control section; and a prefetch kind identifying section for identifying the kind of prefetch set for the prefetch command and outputting the result of the identification of the kind of prefetch to the prefetch control section so that the prefetch control section controls the changeover of the destination cache memory based on the kind of prefetch received from the prefetch kind identifying section.</p><p>In the above-mentioned cache memory control apparatus and computer system of the present invention, the prefetch control section changeably selects one or more destination cache memories among the plural hierarchical cache memories in accordance with the status information about one hierarchical cache memory hierarchically near the processing unit (state-of-use information about the storage area or state-of-contention information about the ports), when a prefetch command is executed.</p><p>Namely, when a prefetch command is executed, the data prefetched from the main storage unit is not always stored in all of the hierarchical cache memories but is copied into only appropriate cache memories in accordance with the status information about one hierarchical cache memory hierarchically near the processing unit.</p><p>Accordingly, it is possible to restrict access (prefetch) to the cache memory hierarchically near the processing unit (the primary cache memory) in accordance with the status of the primary cache memory. And particularly if the prefetch commands are issued at high frequency, it is possible to avoid ejecting or replacing (sweeping) necessary data from the cache memory and incurring a conflict in the main pipeline of the processing unit, realizing high-speed processing in the computer system.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>FIG. 1 is a block diagram schematically showing a computer system having a cache memory control apparatus according to one embodiment of the present invention;</p><p>FIG. 2 is a diagram schematically showing the format of a prefetch command to be used in one embodiment of the present invention;</p><p>FIG. 3 is a table illustrating various prefetches in code to be used in one embodiment of the present invention;</p><p>FIG. 4 is a diagram illustrating the manner in which main storage requests are issued in the one embodiment of the present invention; and</p><p>FIG. 5 is a block diagram schematically showing a conventional computer system having two cache memories hierarchically arranged.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF THE PREFERRED EMBODIMENT</h4><p>Throughout this specification, the term \u201cprefetch\u201d means to store or copy a speculative part of data of a main storage unit or the like into or onto a cache memory prior to execution of an arithmetic process. The term \u201ccache miss\u201d means the absence of speculative data in a destination cache memory due to the unsuccessful execution of a prefetch command and the term \u201ccache hit\u201d means the presence of speculative data in a destination cache memory as the result of the successful execution of a prefetch command.</p><p>One preferred embodiment of the present invention will now be described in detail with reference to the accompanying drawings.</p><p>FIG. 1 is a block diagram schematically showing a computer system having a cache memory control apparatus according to one embodiment of the present invention. The computer system <b>10</b>, like the conventional computer system shown in FIG. 5, as shown in FIG. 1, basically comprises a main storage unit (hereinafter called MSU) <b>12</b> and a central processing unit (hereinafter called CPU) <b>20</b>.</p><p>The MSU <b>12</b> stores programs and various items of data to be processed by the programs and, in the meantime, the CPU <b>20</b> executes various processes using data stored in the MSU <b>12</b>. The CPU <b>20</b> comprises two cache memories hierarchically arranged (a primary cache memory <b>13</b> and a secondary cache memory <b>14</b>), a register <b>21</b>, an arithmetic unit (processing section) <b>22</b>, a command control section <b>23</b>, a main storage request issuing section <b>24</b> and a prefetch control section <b>25</b>. The primary cache memory <b>13</b> includes a status information detecting section <b>30</b>.</p><p>The cache memory control apparatus comprises the command control section <b>23</b>, the prefetch control section <b>25</b> and the status information detecting section <b>30</b> of the primary cache memory <b>13</b> (jointly), which together constitute the cache memory control apparatus, for the purpose of controlling the hierarchically arranged primary and secondary cache memories <b>13</b> and <b>14</b>.</p><p>In the CPU <b>20</b>, the primary and secondary cache memories <b>13</b> and <b>14</b> are arranged between the MSU <b>12</b> and the register <b>21</b>, and speculative data of high-frequency access by the CPU <b>20</b> (for example by the arithmetic unit <b>22</b>) is copied from the MSU <b>12</b>. The primary cache memory <b>13</b> and secondary cache memory <b>14</b> are disposed hierarchically near the arithmetic unit <b>22</b> and the MSU <b>12</b>, respectively. In general, the storage capacity of the secondary cache memory <b>14</b> is larger than that of the primary cache memory <b>13</b>.</p><p>The register <b>21</b> temporarily stores data (operand) to become necessary for arithmetic process (operation) in the arithmetic unit <b>22</b>, and also stores the result of the arithmetic process by the arithmetic unit <b>22</b> which result is to be rewritten into the cache memories <b>13</b>, <b>14</b> and the MSU <b>12</b>. The arithmetic unit (processing section) <b>22</b> executes the arithmetic process based on the data stored in the register <b>21</b> and writes the result of the process into the register <b>21</b>.</p><p>The command control section <b>23</b> controls the issuing of various prefetch commands in accordance with processes of the CPU <b>20</b> (the arithmetic processes of the arithmetic unit <b>22</b>). Assuming that the speculative data necessary for process in the arithmetic unit <b>22</b> does not exist in either one of the primary and secondary cache memories <b>13</b>, <b>14</b>, namely, in the presence of a cache miss, the command control section <b>23</b> issues to the MSU <b>12</b> a load command instruction to store the speculative data, which produced the cache miss into the cache memories <b>13</b>, <b>14</b> and the register <b>21</b>, and issues a prefetch command instruction to store additional speculative data, which is inclined to become necessary in the arithmetic unit <b>22</b>, into the cache memories <b>13</b>, <b>14</b> from the MSU <b>12</b> in advance.</p><p>The command control section <b>23</b> includes a prefetch kind setting section <b>26</b> and a prefetch kind identifying section <b>27</b>. The prefetch kind setting section <b>26</b> sets, for each prefetch command, kind-of-prefetch information used as a changeover condition of the destination cache memory in the prefetch control section <b>25</b>, and the prefetch kind identifying section <b>27</b> identifies the kind of prefetch set for the individual prefetch command and outputs such prefetch command to the prefetch control section <b>25</b>.</p><p>The main storage request issuing section <b>24</b> receives commands including one or more prefetch commands issued from the command control section <b>23</b> and issues requests to the MSU <b>12</b> in the command issuing order. The main storage request issuing section <b>24</b> includes an issuing history queue <b>24</b><i>a </i>which holds commands issued from the command control section <b>23</b> (in fact, the main storage space address of the to-be-accessed data in accordance with the commands is held in the history queue <b>24</b><i>a</i>) in the command issuing order.</p><p>The status information detecting section <b>30</b> automatically detects, as status information of the primary cache memory <b>13</b>, state-of-use information of a storage area in the primary cache memory <b>13</b> (in-use information) and status-of-contention information of access ports of the primary cache memory <b>13</b> (conflict information). The status information detecting section <b>30</b> then outputs the detected status information to the prefetch control section <b>25</b>. Further, the status information detecting section <b>30</b> also detects the in-use information of the primary cache memory <b>13</b>, specifically information as to whether the prospective prefetched data read from the MSU <b>12</b> can be stored into the primary cache memory <b>13</b> without ejecting the existing data in the primary cache memory <b>13</b>.</p><p>The prefetch control section <b>25</b> changeably selects one destination cache memory from the two hierarchical cache memories <b>13</b>, <b>14</b> in accordance with both the status information of the primary cache memory <b>13</b> which information has been detected by the status information detecting section <b>30</b> and the kind of prefetch identified by the prefetch kind identifying section <b>27</b>, when the prefetch command issued from the command control section <b>23</b> is executed.</p><p>The format of a prefetch command used in the illustrated embodiment will now be described using FIG. <b>2</b>. In general, a command issued from the command control section <b>23</b> has a field F<b>1</b> for storing an operation code (a command code), a field F<b>2</b> for storing an extended operation code or operand designation information, and a field F<b>3</b> for storing a desired main storage space address of the data to be accessed according to the operation code.</p><p>In the illustrated embodiment, as shown in FIG. 2, when a prefetch command is issued from the command control section <b>23</b>, a code \u201cprefetch\u201d designating the prefetch command is set in the field F<b>1</b>, and a main storage space address (an address in the MSU <b>12</b>) of the speculative data block to be accessed (prefetched) according to the prefetch command is set in the field F<b>3</b>. And, a code \u201ckind-of-prefetch\u201d designating the kind of prefetch is set in the field P<b>2</b> by the function of the prefetch kind setting section <b>26</b>.</p><p>The codes designating the kind of prefetch (kind-of-prefetch) a reassigned in 3-bit data, for example, shown in FIG. <b>3</b>. When the prefetch command whose kind has been set is executed, the kind of prefetch designated by the code is used as a changeover condition of the destination cache memory in the prefetch control section <b>25</b>. In the illustrated example in which the CPU <b>20</b> has the two cache memories <b>13</b>, <b>14</b> hierarchically arranged, there are assigned the following five kind-of-prefetch codes as shown in FIG. <b>3</b>.</p><p>{circle around (1)} When one kind-of-prefetch code \u201c000\u201d is executed, the prefetch control section <b>25</b> performs a prefetch control such that data is prefetched from the MSU <b>12</b> into both the primary cache memory <b>13</b> and the secondary cache memory <b>14</b>, irrespective of the status information of the primary cache memory <b>13</b> which information has been detected by the status information detecting section <b>30</b>.</p><p>{circle around (2)} When another kind-of-prefetch code \u201c001\u201d is executed, the prefetch control section <b>25</b> performs an alternative prefetch control such that data is prefetched from the MSU <b>12</b> into the secondary cache memory <b>14</b>. And if an adequate part of the access ports of the primary cache memory <b>13</b> is found void of data determined by consultation with the conflict information detected by the status information detecting section <b>30</b>, the prefetch control section <b>25</b> performs another alternative prefetch control such that data is prefetched from MSU <b>12</b> into the primary cache memory via the free access port.</p><p>{circle around (3)} When still another kind-of-prefetch code \u201c010\u201d is executed, the prefetch control section <b>25</b> performs still another alternative prefetch control such that data is prefetched from the MSU <b>12</b> into the secondary cache memory <b>14</b>. If data stored in the primary cache memory <b>13</b> will not be ejected (replaced) from the primary cache memory <b>13</b> determined by consultation with the in-use information by the status information detecting section <b>30</b>, the prefetch control section <b>25</b> performs a further alternative prefetch control such that data is prefetched from the MSU <b>12</b> into the primary cache memory <b>13</b>.</p><p>{circle around (4)} When a further kind-of-prefetch code \u201c011\u201d is executed, the prefetch control section <b>25</b> performs another alternative prefetch control such that data is prefetched from the MSU <b>12</b> into secondary cache memory <b>14</b>. If data stored in primary cache memory <b>13</b> will not be ejected (replaced) from the primary cache memory <b>13</b> determined by consultation with the in-use information by the status information detecting section <b>30</b> and also if adequate part of access ports of the primary cache memory <b>13</b> is found void of data determined by consultation with the conflict information detected by the status information detecting section <b>30</b>, the prefetch control section <b>25</b> performs still another alternative prefetch control such that data is prefetched from the MSU <b>12</b> into the primary cache memory via the free access port.</p><p>{circle around (5)} When a still further kind-of-prefetch code \u201c100\u201d is executed, the prefetch control section performs further prefetch control such that data is prefetched from the MSU <b>12</b> into the primary cache memory <b>13</b> and is prevented from being stored into the secondary cache memory <b>14</b>, irrespective of the status information of the primary cache memory <b>13</b> detected by the status information detecting section <b>30</b>.</p><p>In the meantime, the main storage request issuing section <b>24</b> in the CPU <b>20</b> includes the above-mentioned issuing history queue <b>24</b><i>a </i>for issuing a multiplicity of main storage requests in succession, including requests responsive to prefetch commands and requests responsive to cache misses. Assuming that the history queue is of a forty-stage form, the issuing history queue <b>24</b><i>a </i>can store forty commands (addresses).</p><p>Accordingly, as shown in FIG. 4, the CPU <b>20</b> can execute forty commands including one or more prefetch commands concurrently (in parallel with the corresponding arithmetic processes in the arithmetic unit <b>22</b>).</p><p>Further, the cache memories <b>13</b>, <b>14</b> in the computer system <b>10</b> are provided with a sufficient number of read/write ports (I/O ports and access ports) so as not to cause a conflict even if loads/prefetches from the MSU <b>12</b> into the primary and secondary cache memories <b>13</b>, <b>14</b> and those from the primary cache memory <b>13</b> into register <b>21</b> are concurrently executed. For example, each of the cache memories <b>13</b>, <b>14</b> is in the form of a multi-port RAM.</p><p>The operation of the computer system <b>10</b> having the cache memory control apparatus of this embodiment will be now described.</p><p>First of all, the CPU <b>20</b> reads a program stored in the MSU <b>12</b> and executes an arithmetic process using the data read out from the MSU <b>12</b> and then writes the result of the arithmetic process into the MSU <b>12</b>.</p><p>In general, object data to be processed in the arithmetic unit <b>22</b> is read from the MSU <b>12</b> by issuing a load command or a prefetch command, which are issued by the command control section <b>23</b>, to the MSU <b>12</b> via the main storage command issuing section <b>24</b> and is thereby stored in the two-level or two-class cache memories <b>13</b>, <b>14</b> and the register <b>21</b>. Subsequently, the data read from the register <b>21</b> by execution of the arithmetic command is supplied to the arithmetic unit <b>22</b> and is processed therein. The result of the process by the arithmetic unit <b>22</b> is stored in the register <b>21</b> whereupon the same result is stored from the register <b>21</b> into the two cache memories <b>13</b>, <b>14</b> and the MSU <b>12</b> by execution of a store command issued at a suitable timing.</p><p>In the computer system <b>10</b> of the illustrated embodiment, when a prefetch command issued by the command control section <b>23</b> is executed, the prefetch control section <b>25</b> changes over the destination cache memory between the primary and secondary cache memories <b>13</b>, <b>14</b>, as described hereinbelow, based on the in-use information and the conflict information detected by the status information detecting section <b>30</b> and the kind of prefetch identified by the prefetch kind identifying section <b>27</b>.</p><p>When the second-named kind-of-prefetch code \u201c001\u201d is set, data from the MSU <b>12</b> is prefetched into the secondary cache memory <b>14</b>; and if adequate part of access ports of primary cache memory <b>13</b> is found void of data, data from MSU <b>12</b> is also prefetched into primary cache memory <b>13</b> via the free access port. Accordingly, if adequate part of access ports of the primary cache memory <b>13</b> is found not void of data, data from MSU <b>12</b> is prefetched into only the secondary cache memory <b>14</b>.</p><p>When the third-named kind-of-prefetch code \u201c010\u201d is set, data from the MSU <b>12</b> is prefetched into the secondary cache memory <b>14</b>; and if data stored in the primary cache memory <b>13</b> will not be ejected from the primary cache memory <b>13</b> in execution of prefetch, data from the MSU <b>12</b> is also prefetched into the primary cache memory <b>13</b>. Otherwise if data stored in the primary cache memory <b>13</b> will be ejected from the primary cache memory in executing prefetch, the data from the MSU <b>12</b> is prefetched into only the secondary cache memory <b>14</b>.</p><p>When the forth-named kind-of-prefetch code \u201c011\u201d is set, the data from the MSU <b>12</b> is prefetched into the secondary cache memory <b>14</b>; and if data stored in primary cache memory <b>13</b> will not be ejected from primary cache memory <b>13</b> in execution of prefetch and also if adequate part of access ports of the primary cache memory <b>13</b> is found void of data, data from the MSU <b>12</b> is also prefetched into the primary cache memory <b>13</b> via the free access port. Accordingly, if adequate part of access ports of the primary cache memory <b>13</b> is found not void of data, or if the data stored in the primary cache memory <b>13</b> will be ejected from the primary cache memory <b>13</b> in executing of prefetch, or if adequate part of access ports of the primary cache memory <b>13</b> is found not void of data and the data stored in primary cache memory <b>13</b> will be ejected from primary cache memory <b>13</b> in executing prefetch, the data from the MSU <b>12</b> is prefetched into only the secondary cache memory <b>14</b>.</p><p>When the first-named kind-of-prefetch code \u201c000\u201d is set, the data from the MSU <b>12</b> is always prefetched into both the primary cache memory <b>13</b> and secondary cache memory <b>14</b>, irrespective of the status information of primary cache memory <b>13</b>. When the fifth-named kind-of-prefetch code \u201c100\u201d is set, the data from the MSU <b>12</b> is always prefetched into only the primary cache memory <b>13</b>, irrespective of the status information of primary cache memory <b>13</b>.</p><p>According to this preferred embodiment, when a prefetch command is executed, the data stored from MSU <b>12</b> is not always prefetched into both the primary and secondary cache memories <b>13</b>, <b>14</b>; this is, the destination of prefetch is selected between both the primary and secondary cache memories <b>13</b>, <b>14</b>, or only the primary cache memory <b>13</b>, or only the secondary cache memory <b>14</b> based on the in-use information and conflict information of the primary cache memory <b>13</b>.</p><p>Therefore prefetching data from the MSU <b>12</b> into the primary cache memory <b>13</b> is restricted based on the status of the primary cache memory <b>13</b>; if prefetch commands are issued at high frequency, high-speed processing in the computer system <b>10</b> is possible because ejection of necessary data from the primary cache memory <b>13</b> and occurrence of a conflict in the main pipeline of the CPU <b>20</b> are prevented.</p><p>If capacity of the primary cache memory <b>13</b> is particularly small, it is possible to prevent over-ejection of data from the primary cache memory <b>13</b> because ejection of data from the primary cache memory <b>13</b> is restricted.</p><p>Further, since selection of kind of access of prefetch (kind-of-prefetch) is controlled by a program and/or hardware (prefetch kind setting section <b>26</b> in the illustrated embodiment), it is possible to abate the load of the primary cache memory <b>13</b>, thus improving the effective performance of the CPU <b>20</b> (the computer system <b>10</b>) drastically.</p><p>In the computer system <b>10</b> of the present invention, when cache misses are detected successively in CPU <b>20</b>, not only the main storage request corresponding to the cache misses but also those corresponding to a multiplicity of prefetch commands can be issued in succession.</p><p>Assuming that a cache-miss penalty (time required for reading the desired data into the register from the MSU <b>12</b> after the CPU <b>20</b> detects a cache miss) in the computer system <b>10</b> of this illustrated embodiment is forty cycles (40\u03c4), forty load commands including one or more prefetch commands are executed apart from the CPU <b>20</b> through a forty-stage issuing history queue <b>24</b><i>a </i>in the main storage request issuing section <b>24</b>.</p><p>Specifically, as shown in FIG. 4, even if cache misses have occurred in succession, the main storage request corresponding to the leading cache miss will have been completed when the forty-first cache miss has occurred so that hardware of computer system <b>10</b> can be recycled. As a result, the CPU <b>20</b> can continue to process without halting due to the: cache-miss penalty.</p><p>Further, since the cache memories <b>13</b>, <b>14</b> have an adequate number of read/write ports as mentioned above, the main pipeline of the CPU <b>20</b> can operate without encountering a conflict during prefetch operation; i.e. data is stored into the cache memories <b>13</b>, <b>14</b> from the MSU <b>12</b>.</p><p>Furthermore, in this embodiment, the illustrated issuing history queue <b>24</b><i>a </i>is a forty-stage queue provided so as to cope with a forty-cycle (40\u03c4) cache-miss penalty. Practically, however, it is extremely rare that cache misses occurred in every cycle in the actual program. Therefore the issuing history queue <b>24</b><i>a </i>may be composed of less than forty stages, depending on the compromise between the behaviors of many programs and the costs of the hardware.</p><p>Thus in the computer system <b>10</b> of the illustrated embodiment, partly because the processing in the CPU <b>20</b> can be prevented from being halted, and partly because the hardware resource for storing into cache memories <b>13</b>, <b>14</b> from the MSU <b>12</b> is not exhausted even when a multiplicity of prefetch commands are issued, it is possible to avoid any cache-miss penalty, reducing possible impairment over the main pipeline of the CPU <b>20</b> to a minimum.</p><p>The present invention should by no means be limited to the illustrated embodiment, and various other changes or modification may be suggested without departing from the gist of the inventive concept.</p><p>For example, in this embodiment, two cache memories are hierarchically arranged; alternatively more than two cache memories may be hierarchically arranged-with the same result.</p><p>Further, in this illustrated embodiment, the secondary cache memory <b>14</b> is disposed in the CPU <b>20</b>; alternatively the secondary cache memory and subsequent cache memories may be disposed outside of the CPU <b>20</b>.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Masayuki", "last_name": "Ikeda", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "FUJITSU LIMITED"}, {"first_name": "", "last_name": "FUJITSU LIMITED", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  13/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMJP"}, {"label": "G06F   9/38        20060101ALI20051220RMJP"}, {"label": "G06F   9/32        20060101ALI20051220RMJP"}, {"label": "G06F   9/30        20060101ALI20051220RMJP"}], "national_classes": [{"primary": true, "label": "711137"}, {"primary": false, "label": "711122"}, {"primary": false, "label": "711E12057"}], "ecla_classes": [{"label": "G06F  12/08B8"}], "cpc_classes": [{"label": "G06F  12/0862"}, {"label": "G06F  12/0862"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1999-05-27", "application_date": "2000-02-25", "family_members": [{"ucid": "JP-3512678-B2", "titles": [{"lang": "JA", "text": "\u30ad\u30e3\u30c3\u30b7\u30e5\u30e1\u30e2\u30ea\u5236\u5fa1\u88c5\u7f6e\u304a\u3088\u3073\u8a08\u7b97\u6a5f\u30b7\u30b9\u30c6\u30e0"}, {"lang": "EN", "text": "Cache memory control device and computer system"}]}, {"ucid": "JP-2000339157-A", "titles": [{"lang": "JA", "text": "\u30ad\u30e3\u30c3\u30b7\u30e5\u30e1\u30e2\u30ea\u5236\u5fa1\u88c5\u7f6e\u304a\u3088\u3073\u8a08\u7b97\u6a5f\u30b7\u30b9\u30c6\u30e0"}, {"lang": "EN", "text": "CACHE MEMORY CONTROLLER AND COMPUTER SYSTEM"}]}, {"ucid": "US-6473836-B1", "titles": [{"lang": "EN", "text": "Computing system and cache memory control apparatus controlling prefetch in hierarchical cache memories"}]}]}