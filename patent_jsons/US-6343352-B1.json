{"patent_number": "US-6343352-B1", "publication_id": 72918685, "family_id": 22036203, "publication_date": "2002-01-29", "titles": [{"lang": "EN", "text": "Method and apparatus for two step memory write operations"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50271374\"><p>A method and apparatus for storing data in a memory device is described. The apparatus is configured to perform the following steps. The method employs a two-step technique which allows the out-of-order completion of read and write operations. When a write operation requires a resource needed for the completion of a read operation, the data being written is stored in a write data buffer in the memory device. The write data is stored in the buffer until a datapath is available to communicate the data to the memory device's memory core. Once the resource is free (or the memory device, or its controller force the write to complete) the data is written to the memory core of the memory device using the now-free datapath.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00001\" num=\"1\"><claim-text>1. A memory device, comprising:</claim-text><claim-text>a memory core for storing data, said memory core having a plurality of control inputs and a datapath; </claim-text><claim-text>control circuitry, operable to be coupled to an external bus, to receive control information including a first write command followed by a read command followed by a second write command and to receive data information including write data information, said control circuitry configured to re-order said read and write commands so as to issue said read command followed by said first write command; </claim-text><claim-text>a write delay buffer to hold said first and second write commands and to issue a first delayed write command relative to said first write command under control of said control circuitry; and </claim-text><claim-text>a write data buffer coupled to said control circuitry to receive and store said write data information for each corresponding write command, said write data buffer coupled to said datapath of said memory core; </claim-text><claim-text>wherein </claim-text><claim-text>said write data buffer is configured to receive said write data information at a time that is offset by at least one command cycle from a time at which the corresponding write command is received by said write delay buffer; whereby a minimum dead time between said read command and said second write command on said external bus is reduced by an amount corresponding to said offset, thereby enabling higher utilization of said external bus and said memory device. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00002\" num=\"2\"><claim-text>2. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00001\">claim 1</claim-ref> wherein said write data buffer is configured to retire said write data information in response to any operation code signal other than one associated with processing of a read command to the memory device.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00003\" num=\"3\"><claim-text>3. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00001\">claim 1</claim-ref> wherein said write data buffer is configured to retire said write data information in response to a dedicated retire control signal.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00004\" num=\"4\"><claim-text>4. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00001\">claim 1</claim-ref> wherein said write data buffer is configured to retire said write data information in the absence of any operation code signal.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00005\" num=\"5\"><claim-text>5. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00001\">claim 1</claim-ref> wherein said memory device is configured to identify correspondence between said first write command within said write delay buffer and said read command.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00006\" num=\"6\"><claim-text>6. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00001\">claim 1</claim-ref> wherein said memory device is configured to substantially equalize at said control circuitry the time to store said write data information and retrieve said read data information.</claim-text></claim>"}, {"num": 7, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00007\" num=\"7\"><claim-text>7. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00001\">claim 1</claim-ref> wherein said control information and said data information are generated by a master memory device coupled to said external bus.</claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00008\" num=\"8\"><claim-text>8. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00007\">claim 7</claim-ref> wherein said master memory device is configured to generate a plurality of write commands followed by any operation code other than a read or write to said memory device and said read command to said memory device, said memory device being configured to respond thereto by processing said read command prior to completing the processing of said plurality of write commands without causing a column resource conflict at said memory core.</claim-text></claim>"}, {"num": 9, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00009\" num=\"9\"><claim-text>9. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00007\">claim 7</claim-ref> wherein said master memory device is configured to generate separated write commands in order to prevent data overrun in buffers of said memory device.</claim-text></claim>"}, {"num": 10, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00010\" num=\"10\"><claim-text>10. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00007\">claim 7</claim-ref> wherein said master memory device is configured to generate write commands separated by other operation commands in order to prevent data overrun in buffers of said memory device.</claim-text></claim>"}, {"num": 11, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00011\" num=\"11\"><claim-text>11. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00007\">claim 7</claim-ref> wherein said master memory device is configured to generate a plurality of write commands, wherein said plurality of write commands are for different slave memory devices in order to prevent data overrun in buffers of said memory device.</claim-text></claim>"}, {"num": 12, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00012\" num=\"12\"><claim-text>12. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00007\">claim 7</claim-ref> wherein said master memory device is configured to generate a plurality of write commands separated by read commands in order to prevent data overrun in buffers of said memory device.</claim-text></claim>"}, {"num": 13, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00013\" num=\"13\"><claim-text>13. A memory device, comprising:</claim-text><claim-text>a memory core for storing data, said memory core having a plurality of control inputs and a datapath; </claim-text><claim-text>control circuitry, operable to be coupled to an external bus, to receive a sequence of read and write commands, the sequence including a first write command followed by a first read command and a second read command followed by a second write command; said control circuitry configured to re-order said first read and write commands so as to issue said first read command followed by said first write command; </claim-text><claim-text>a write delay buffer to hold said first and second write commands and to issue a first delayed write command relative to said first write command under control of said control circuitry; and </claim-text><claim-text>a write data buffer coupled to said control circuitry to receive and store write data information for each corresponding write command, said write data buffer coupled to said datapath of said memory core; </claim-text><claim-text>wherein </claim-text><claim-text>said write data buffer is configured to receive said write data information at a time that is offset by at least one command cycle from a time at which the corresponding write command is received by said write delay buffer; whereby a minimum dead time between each read command followed by a write command in said sequence on said external bus is reduced by an amount corresponding to said offset, thereby enabling higher utilization of said external bus and said memory device. </claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00014\" num=\"14\"><claim-text>14. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00013\">claim 13</claim-ref> wherein the second read command is the same command as the first read command.</claim-text></claim>"}, {"num": 15, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00015\" num=\"15\"><claim-text>15. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00013\">claim 13</claim-ref> wherein said write data buffer is configured to retire said write data information in response to any operation code signal other than one associated with processing of a read command to the memory device.</claim-text></claim>"}, {"num": 16, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00016\" num=\"16\"><claim-text>16. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00013\">claim 13</claim-ref> wherein said write data buffer is configured to retire said write data information in response to a dedicated retire control signal.</claim-text></claim>"}, {"num": 17, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00017\" num=\"17\"><claim-text>17. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00013\">claim 13</claim-ref> wherein said write data buffer is configured to retire said write data information in the absence of any operation code signal.</claim-text></claim>"}, {"num": 18, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00018\" num=\"18\"><claim-text>18. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00013\">claim 13</claim-ref> wherein said memory device is configured to identify correspondence between said first write command within said write delay buffer and said read command.</claim-text></claim>"}, {"num": 19, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00019\" num=\"19\"><claim-text>19. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00013\">claim 13</claim-ref> wherein said memory device is configured to substantially equalize at said control circuitry the time to store said write data information and retrieve said read data information.</claim-text></claim>"}, {"num": 20, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00020\" num=\"20\"><claim-text>20. A memory device, comprising:</claim-text><claim-text>a memory core for storing data, said memory core having a plurality of control inputs and a datapath; </claim-text><claim-text>control circuitry, operable to be coupled to an external bus, to receive a sequence of read and write commands, the sequence including a first read command followed by a first write command; </claim-text><claim-text>a write delay buffer to hold said first write command and to issue a first delayed write command relative to said first write command under control of said control circuitry; and </claim-text><claim-text>a write data buffer coupled to said control circuitry to receive and store write data information, said write data buffer coupled to said datapath of said memory core; </claim-text><claim-text>wherein </claim-text><claim-text>said write data buffer is configured to receive said write data information at a time that is offset by at least one command cycle from a time at which the corresponding write command is received by said control circuitry; whereby a minimum dead time between each read command followed by a write command in said sequence on said external bus is reduced by an amount corresponding to said offset, thereby enabling higher utilization of said external bus and said memory device. </claim-text></claim>"}, {"num": 21, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00021\" num=\"21\"><claim-text>21. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00020\">claim 20</claim-ref> wherein said write data buffer is configured to retire said write data information in response to any operation code signal other than one associated with processing of a read command to the memory device.</claim-text></claim>"}, {"num": 22, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00022\" num=\"22\"><claim-text>22. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00020\">claim 20</claim-ref> wherein said write data buffer is configured to retire said write data information in response to a dedicated retire control signal.</claim-text></claim>"}, {"num": 23, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00023\" num=\"23\"><claim-text>23. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00020\">claim 20</claim-ref> wherein said write data buffer is configured to retire said write data information in the absence of any operation code signal.</claim-text></claim>"}, {"num": 24, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00024\" num=\"24\"><claim-text>24. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00020\">claim 20</claim-ref> wherein said memory device is configured to identify correspondence between said first write command within said write delay buffer and said read command.</claim-text></claim>"}, {"num": 25, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00025\" num=\"25\"><claim-text>25. The memory device of <claim-ref idref=\"US-6343352-B1-CLM-00020\">claim 20</claim-ref> wherein said memory device is configured to substantially equalize at said control circuitry the time to store said write data information and retrieve said read data information.</claim-text></claim>"}, {"num": 26, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00026\" num=\"26\"><claim-text>26. A method of operating a memory device coupled to an external bus, the method comprising:</claim-text><claim-text>receiving in control circuitry control information including a first write command followed by a read command followed by a second write command; </claim-text><claim-text>receiving in said control circuitry data information including write data information; </claim-text><claim-text>re-ordering said read and write commands so as to issue said read command followed by said first write command; </claim-text><claim-text>receiving said first and second write commands in a write delay buffer and issuing a first delayed write command relative to said first write command; and </claim-text><claim-text>receiving in a write data buffer said write data information for each corresponding write command at a time that is offset by at least one command cycle from a time at which the corresponding write command is received by said write delay buffer; whereby a minimum dead time between said read command and said second write command on said external bus is reduced by an amount corresponding to said offset, thereby enabling higher utilization of said external bus and said memory device. </claim-text></claim>"}, {"num": 27, "parent": 26, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00027\" num=\"27\"><claim-text>27. The method of <claim-ref idref=\"US-6343352-B1-CLM-00026\">claim 26</claim-ref> further comprising:</claim-text><claim-text>retiring said write data information in response to any operation code signal other than one associated with processing of a read command to the memory device. </claim-text></claim>"}, {"num": 28, "parent": 26, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00028\" num=\"28\"><claim-text>28. The method of <claim-ref idref=\"US-6343352-B1-CLM-00026\">claim 26</claim-ref> further comprising:</claim-text><claim-text>retiring said write data information in response to a dedicated retire control signal. </claim-text></claim>"}, {"num": 29, "parent": 26, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00029\" num=\"29\"><claim-text>29. The method of <claim-ref idref=\"US-6343352-B1-CLM-00026\">claim 26</claim-ref> further comprising:</claim-text><claim-text>retiring said write data information in the absence of any operation code signal. </claim-text></claim>"}, {"num": 30, "parent": 26, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00030\" num=\"30\"><claim-text>30. The method of <claim-ref idref=\"US-6343352-B1-CLM-00026\">claim 26</claim-ref> further comprising:</claim-text><claim-text>identifying correspondence between said first write command within said write delay buffer and said read command. </claim-text></claim>"}, {"num": 31, "parent": 26, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00031\" num=\"31\"><claim-text>31. The method of <claim-ref idref=\"US-6343352-B1-CLM-00026\">claim 26</claim-ref> further comprising:</claim-text><claim-text>substantially equalizing the time to store said write data information and retrieve said read data information. </claim-text></claim>"}, {"num": 32, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00032\" num=\"32\"><claim-text>32. A method of operating a memory device coupled to an external bus, the method comprising:</claim-text><claim-text>receiving in control circuitry a sequence of read and write commands including a first write command followed by a first read command and a second read command followed by a second write command; </claim-text><claim-text>receiving in said control circuitry data information including write data information; </claim-text><claim-text>re-ordering said read and write commands so as to issue said first read command followed by said first write command; </claim-text><claim-text>receiving said first and second write commands in a write delay buffer and issuing a first delayed write command relative to said first write command; and </claim-text><claim-text>receiving in a write data buffer said write data information for each corresponding write command at a time that is offset by at least one command cycle from a time at which the corresponding write command is received by said write delay buffer; whereby a minimum dead time between each read command followed by a write command in said sequence on said external bus is reduced by an amount corresponding to said offset, thereby enabling higher utilization of said external bus and said memory device. </claim-text></claim>"}, {"num": 33, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00033\" num=\"33\"><claim-text>33. The method of <claim-ref idref=\"US-6343352-B1-CLM-00032\">claim 32</claim-ref> further comprising:</claim-text><claim-text>retiring said write data information in response to any operation code signal other than one associated with processing of a read command to the memory device. </claim-text></claim>"}, {"num": 34, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00034\" num=\"34\"><claim-text>34. The method of <claim-ref idref=\"US-6343352-B1-CLM-00032\">claim 32</claim-ref> further comprising:</claim-text><claim-text>retiring said write data information in response to a dedicated retire control signal. </claim-text></claim>"}, {"num": 35, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00035\" num=\"35\"><claim-text>35. The method of <claim-ref idref=\"US-6343352-B1-CLM-00032\">claim 32</claim-ref> further comprising:</claim-text><claim-text>retiring said write data information in the absence of any operation code signal. </claim-text></claim>"}, {"num": 36, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00036\" num=\"36\"><claim-text>36. The method of <claim-ref idref=\"US-6343352-B1-CLM-00032\">claim 32</claim-ref> further comprising:</claim-text><claim-text>identifying correspondence between said first write command within said write delay buffer and said first read command. </claim-text></claim>"}, {"num": 37, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00037\" num=\"37\"><claim-text>37. The method of <claim-ref idref=\"US-6343352-B1-CLM-00032\">claim 32</claim-ref> further comprising:</claim-text><claim-text>substantially equalizing the time to store said write data information and retrieve said read data information. </claim-text></claim>"}, {"num": 38, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00038\" num=\"38\"><claim-text>38. A method of operating a memory device coupled to an external bus, the method comprising:</claim-text><claim-text>receiving in control circuitry control information including a sequence of read and write commands, wherein said sequence including a first read command followed by a first write command; </claim-text><claim-text>receiving in said control circuitry data information including write data information; </claim-text><claim-text>receiving said first write command in a write delay buffer and issuing a first delayed write command relative to said first write command; and </claim-text><claim-text>receiving in a write data buffer said write data information for each corresponding write command at a time that is offset by at least one command cycle from a time at which the corresponding write command is received by said write delay buffer; whereby a minimum dead time between each read command followed by a write command in said sequence on said external bus is reduced by an amount corresponding to said offset, thereby enabling higher utilization of said external bus and said memory device. </claim-text></claim>"}, {"num": 39, "parent": 38, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00039\" num=\"39\"><claim-text>39. The method of <claim-ref idref=\"US-6343352-B1-CLM-00038\">claim 38</claim-ref> further comprising:</claim-text><claim-text>retiring said write data information in response to any operation code signal other than one associated with processing of a read command to the memory device. </claim-text></claim>"}, {"num": 40, "parent": 38, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00040\" num=\"40\"><claim-text>40. The method of <claim-ref idref=\"US-6343352-B1-CLM-00038\">claim 38</claim-ref> further comprising:</claim-text><claim-text>retiring said write data information in response to a dedicated retire control signal. </claim-text></claim>"}, {"num": 41, "parent": 38, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00041\" num=\"41\"><claim-text>41. The method of <claim-ref idref=\"US-6343352-B1-CLM-00038\">claim 38</claim-ref> further comprising:</claim-text><claim-text>retiring said write data information in the absence of any operation code signal. </claim-text></claim>"}, {"num": 42, "parent": 38, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00042\" num=\"42\"><claim-text>42. The method of <claim-ref idref=\"US-6343352-B1-CLM-00038\">claim 38</claim-ref> further comprising:</claim-text><claim-text>identifying correspondence between said first write command within said write delay buffer and said read command. </claim-text></claim>"}, {"num": 43, "parent": 38, "type": "dependent", "paragraph_markup": "<claim id=\"US-6343352-B1-CLM-00043\" num=\"43\"><claim-text>43. The method of <claim-ref idref=\"US-6343352-B1-CLM-00038\">claim 38</claim-ref> further comprising:</claim-text><claim-text>substantially equalizing the time to store said write data information and retrieve said read data information.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53514082\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><p>This application claims priority to the provisional patent application entitled \u201cTwo Step Writes\u201d, Ser. No. 60/061,503, filed Oct. 10, 1997.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>The present invention relates to the transfer of data in digital systems. More specifically, the present invention relates to a protocol and apparatus that provide improved interconnect utilization. In particular, a two-step write operation according to the present invention avoids resource conflicts, thus permitting read and write operations to be issued in any order while maintaining continuous data traffic.</p><p>2. Description of the Related Art</p><p>A computer, such as a computer system <b>10</b> shown in FIG. 1A, typically includes a bus <b>12</b> which interconnects the system's major subsystems such as a central processing unit (CPU) <b>14</b>, a main memory <b>16</b> (e.g., DRAM), an input/output (I/O) adapter <b>18</b>, an external device such as a display screen <b>24</b> via a display adapter <b>26</b>, a keyboard <b>32</b> and a mouse <b>34</b> via an I/O adapter <b>18</b>, a SCSI host adapter <b>36</b>, and a floppy disk drive <b>38</b> operative to receive a floppy disk <b>40</b>. SCSI host adapter <b>36</b> may act as a storage interface to a fixed disk drive <b>42</b> or a CD-ROM player <b>44</b> operative to receive a CD-ROM <b>46</b>. Fixed disk <b>42</b> may be a part of computer system <b>10</b> or may be separate and accessed through other interface systems. A network interface <b>48</b> may provide a connection to a LAN (e.g., a TCP/IP-based local area network (LAN)) or to the Internet itself. Many other devices or subsystems (not shown) may be connected in a similar manner. Also, it is not necessary for all of the devices shown in FIG. 1A to be present to practice the present invention, as discussed below. The configuration of the devices and subsystems shown in FIG. 1A may vary substantially from one computer to the next.</p><p>In today's high-performance computers, the link between the CPU and its associated main memory (e.g., CPU <b>14</b> and main memory <b>16</b>, respectively) is critical. Computer programs currently available place imposing demands on a computer's throughput capabilities. This need for increasingly higher bandwidth will continue.</p><p>One method for improving the throughput of this interface is to provide a dedicated bus between CPU <b>14</b> and main memory <b>16</b>. Such a bus is shown in FIG. 1A as a memory bus <b>50</b>. Memory bus <b>50</b> allows CPU <b>14</b> to communicate data and control signals directly to and from main memory <b>16</b>. This improves computational performance by providing a pathway directly to the system's main memory that is not subject to traffic generated by the other subsystems in computer system <b>10</b>. In such systems, the pathway between main memory <b>16</b> and bus <b>12</b> may be by way of a direct memory access (DMA) hardware construct for example.</p><p>FIG. 1B illustrates a block diagram in which components (e.g., CPU <b>14</b> and main memory <b>16</b>) communicate over an interconnect <b>60</b> in order to process data. Interconnect <b>60</b> is a generalization of memory bus <b>50</b>, and allows one or more master units such as master units <b>70</b>(<b>1</b>)-(N) and one or more slave units, such as slave units <b>80</b>(<b>1</b>)-(N). (The term \u201cN\u201d is used as a general variable, its use should not imply that the number of master units is identical to the number of slave units.) Components attached to interconnect <b>60</b> may contain master and slave memory elements. In the case where interconnect <b>60</b> serves as memory bus <b>50</b>, CPU <b>14</b> communicates with main memory <b>16</b> over interconnect <b>60</b> using pipelined memory operations. These pipelined memory operations allow maximum utilization of interconnect <b>60</b>, which is accomplished by sending data over interconnect <b>60</b> as continuously as is reasonably possible given the throughput capabilities of main memory <b>16</b>.</p><p>The block diagram of FIG. 1B is applicable to intrachip, as well as interchip, communications. It will be understood that one or more of slave units <b>80</b>(<b>1</b>)-(N) may consist of other components in addition to memory (e.g., a processor of some sort). The block diagram of FIG. 1B can, of course, be simplified to the case of a system having only a single master.</p><p>FIG. 1C shows a memory device <b>100</b>. Memory device <b>100</b> might be used in a computer system, for example, as main memory <b>16</b> of computer system <b>10</b>, or in combination with similar devices to form main memory <b>16</b>. Memory device <b>100</b> is capable of being read from and written to by a memory controller (not shown). An interconnect <b>110</b> is used to communicate control information over control lines <b>112</b> and data over data lines <b>114</b> from the memory controller to memory device <b>100</b>. Interconnect <b>110</b> is thus analogous to memory bus <b>50</b>. To support such communications and the storage of data, memory device <b>100</b> typically includes three major functional blocks.</p><p>The first of these, a transport block <b>120</b>, is coupled to interconnect <b>110</b>. Interconnect <b>110</b>, which includes control signal lines <b>112</b> and data signal lines <b>114</b>, is used to read from and write to memory device <b>100</b>. Interconnect <b>110</b> provides the proper control signals and data when data is to be written to memory device <b>100</b>. Transport block <b>120</b> receives these signals and takes the actions necessary to transfer this information to the remaining portions of memory device <b>100</b>. When memory device <b>100</b> is read, transport block <b>120</b> transmits data as data signal lines <b>114</b> in response to control signal lines <b>112</b>. Transport block <b>120</b> includes a control transport unit <b>122</b> which receives control signal lines <b>112</b>, and controls a read data transport unit <b>124</b> and a write data transport unit <b>126</b> to support the communication protocol used in transferring information over interconnect <b>110</b> (e.g., transferring information between CPU <b>14</b> and main memory <b>16</b> over memory bus <b>50</b>).</p><p>In its simplest form, transport block <b>120</b> is merely wiring, without any active components whatsoever. In that case, control transport unit <b>122</b> would simply be wires, as read data transport unit <b>124</b> and write data transport unit <b>126</b> would require no control. In effect, transport block <b>120</b> is not implemented in such a case. Another possible configuration employs amplifiers to provide the functionality required of transport block <b>120</b>. In yet another possible configuration, transport block <b>120</b> includes serial-to-parallel converters. In this case, control transport unit <b>122</b> controls the conversion performed by read data transport unit <b>124</b> and write data transport unit <b>126</b> (which would be the serial-to-parallel converters). Other equivalent circuits may also be used with equal success.</p><p>The second of the major functional blocks is an operations block <b>130</b>. Operations block <b>130</b> receives control information from transport block <b>120</b>, more specifically from control transport unit <b>122</b>, which provides the requisite signals to a control operation unit <b>150</b>.</p><p>In FIG. 1C, control operation unit <b>150</b> is implemented as an architecture designed to control generic DRAM memory cells. A specific DRAM memory cell architecture (or other architecture), however, may require different control signals, some or all of which may not be provided in the architecture shown in FIG. <b>1</b>C. Control operation unit <b>150</b> includes a sense operation unit <b>132</b>, a precharge operation unit <b>134</b>, and a core transfer operation unit <b>136</b>.</p><p>Data being read is transferred from the third functional block, a memory core <b>180</b>, via data I/O bus <b>185</b> to a read data operation unit <b>160</b>. From read data operation unit <b>160</b>, the data being read is transferred to read data transport unit <b>124</b> (and subsequently, onto data signal lines <b>114</b>) in response to control signals from control operation unit <b>150</b>. Read data operation unit <b>160</b> may consist of, for example, data buffers (not shown) that buffer the outgoing data signals to drive read data transport unit <b>124</b>.</p><p>Data to be written is transferred from write data transport unit <b>126</b> to a write operation unit <b>170</b> in response to control signals from control transport unit <b>122</b> (if used) and control operation unit <b>150</b>. Write data operation unit <b>170</b> receives write data from write transport unit <b>126</b>, which is passed on to memory core <b>180</b> via data I/O bus <b>185</b>. As shown, write data operation unit <b>170</b> may be controlled by core transfer operation unit <b>136</b>. Write data operation unit <b>170</b> may consist of, for example, data buffers (not shown) that buffer the incoming data signals.</p><p>Write data operation unit <b>170</b> may also contain mask buffers that buffer mask information received from write data transport unit <b>126</b>. As with data buffering, these actions may be taken under the control of core transfer operation unit <b>136</b>. The mask information is then passed to memory core <b>180</b> via data I/O bus <b>185</b>, as well. The mask information is used by the memory core to selectively write parts of the data within the memory core. Alternatively, no mask is employed, with the result that all the data is written unconditionally.</p><p>The circuitry of control operation unit <b>150</b> may take any number of appropriate configurations, depending in part on the architecture of the memory core employed. For example, the memory cells of memory core <b>180</b> may be static random access memory (SRAM) cells, read-only memory (ROM) cells (which can, of course, only be read), dynamic RAM (DRAM) cells, or another type of memory cell. The type of memory cell employed in memory core <b>180</b> affects the architecture of control operation unit <b>150</b>, as different memory cells often require different control signals for their operation.</p><p>Operational block <b>130</b> thus contains core transfer operation unit <b>150</b>, read data operation unit <b>160</b>, and write data operation unit <b>170</b>. Again, in the simplest configuration of transport block <b>120</b>, the subsystems of transport block <b>120</b> are merely wires. Moreover, the functionality provided by the subsystems of transport block <b>120</b> is merely one of transferring data and control information.</p><p>Assuming that the memory core employs DRAM-type memory cells, operations which may be performed on memory core <b>180</b> (referred to herein as core operations) may be generalized into four primary categories:</p><p>1) Precharge;</p><p>2) Sense;</p><p>3) Read; and</p><p>4) Write.</p><p>While these generalized operations are dealt with in detail later in this section, they are introduced here to illustrate the following effects on the block diagram of FIG. <b>1</b>C. Given the generalized operations to be performed, the circuitry of control operation unit <b>150</b> may be logically divided into the three subsystems mentioned previously: sense operation unit <b>132</b>, precharge operation unit <b>134</b>, and core transfer operation unit <b>136</b>. Core transfer operation unit <b>136</b> controls read data operation unit <b>160</b> and write data operation unit <b>170</b> when transferring data from and to memory core <b>180</b>, respectively (i.e., read and write operations). Core transfer operation unit <b>136</b> also controls memory core <b>180</b>, causing memory core <b>180</b> to store write data and output read data. Precharge operation unit <b>134</b> controls memory core precharge operations, which precharge the selected banks in memory core <b>180</b>. Sense operation unit <b>132</b> is provided for the control of memory core sense operations.</p><p>The subsystems of operations block <b>130</b> uses the control information received to coordinate movement of control and data information to and from memory core <b>180</b>. Read data operation unit <b>160</b> and a write data operation unit <b>170</b> contain circuitry specific to the functions which read and write data from and to memory core <b>180</b>, respectively. Core transfer operation unit <b>150</b> contains circuitry used to control memory core <b>180</b>, including circuitry for the control of read and write operations. Core interface signals <b>190</b> are provided to control memory core <b>180</b>.</p><p>FIG. 2 illustrates a memory core <b>200</b>, which can serve as memory core <b>180</b> in FIG. <b>1</b>C. Memory core <b>200</b> typically includes several basic functional blocks. Memory core <b>200</b> is illustrated as including multiple memory banks, memory banks <b>205</b>(<b>1</b>)-(N). Alternatively, memory core <b>200</b> can be implemented using only a single memory bank (e.g., memory bank (<b>1</b>)). Included in each of memory banks <b>205</b>(<b>1</b>)-(N) are a storage array, exemplified by storage arrays <b>210</b>(<b>1</b>)-(N), and a set of sense amplifiers, exemplified by sense amplifiers <b>215</b>(<b>1</b>)-(N). Storage arrays <b>210</b>(<b>1</b>)-(N) are central to the function of memory core <b>200</b>, actually holding the data to be stored. Storage arrays <b>210</b>(<b>1</b>)-(N) are connected to sense amplifiers <b>215</b>(<b>1</b>)-(N) by bit lines <b>220</b>(<b>1</b>)-(N), respectively. Such storage arrays are normally organized into rows and columns of storage cells, each of which typically stores one bit of information, although configurations for storing multiple bits are known in the art.</p><p>Also included in memory core <b>200</b> are a row decoder <b>225</b> and a column decoder <b>230</b>. A row address <b>235</b> is provided to row decoder <b>225</b>, along with row control signals <b>240</b>, which cause row decoder <b>225</b> to latch a row address thus presented. In turn, row decoder <b>225</b> presents this address information to memory banks <b>205</b>(<b>1</b>)-(N) via row select lines <b>245</b>. Similarly, a column address <b>250</b> is provided to column decoder <b>230</b>, along with column control signals <b>255</b>, which cause column decoder <b>230</b> to latch a column address thus presented. In turn, column decoder <b>230</b> presents this address information to memory banks <b>205</b>(<b>1</b>)-(N) via column select lines <b>260</b> to select which sense amplifiers are connected to the column amplifiers. The column control signals <b>255</b> may include mask bit signals to selectively mask individual sense amplifiers in accordance with a predetermined masking scheme.</p><p>Column control signals <b>255</b> are also provided to column amplifiers <b>265</b>. Column amplifiers <b>265</b> are coupled to sense amplifiers <b>215</b>(<b>1</b>)-(N) by column I/O lines <b>266</b>, and amplify the data signals input to and output from sense amplifiers <b>215</b>(<b>1</b>)-(N). <b>30</b> Column amplifiers <b>265</b> are also coupled to data I/O bus <b>185</b> (from FIG. <b>1</b>C), permitting the communication of control signals from operations block <b>130</b> to the various control structures within memory core <b>200</b>. The signals aggregated as core interface signals <b>190</b> (as illustrated in FIG. 1C) thus include row address <b>235</b>, row control signals <b>240</b>, column address <b>250</b>, and column control signals <b>255</b>. Thus, the interface to a memory core generally consists of a row address, a column address, a datapath, and various control signals, including mask signals.</p><p>As shown in FIG. 2, memory cores can have multiple banks, which allows simultaneous row operations within a given core. The use of multiple banks improves memory performance through increased concurrency and a reduction of conflicts. Each bank has its own storage array and can have its own set of sense amplifiers to allow for independent row operation. The column decoder and datapath are typically shared between banks in order to reduce cost and area requirements, as previously described.</p><p>FIG. 3 illustrates a generic storage array <b>300</b>, in which data is stored in storage cells <b>305</b>(<b>1</b>,<b>1</b>)-(N,N). Thus, storage array <b>300</b> is capable of storing N<sup>2 </sup>bits, using a common storage cell implementation. As shown, each one of word lines <b>310</b>(<b>1</b>)-(N) accesses a row of storage cells <b>305</b>(<b>1</b>,<b>1</b>)-(N,N) (e.g., storage cells <b>305</b>(<b>1</b>,<b>1</b>)-(<b>1</b>,N)), which in turn transfers the stored data onto internal bit lines <b>320</b>(<b>1</b>)-(N). Internal bit lines <b>320</b>(<b>1</b>)-(N) emerge from storage array <b>300</b> as bit lines <b>220</b> (i.e., an aggregate of bit lines <b>220</b>(<b>1</b>)-(N), which are connected to sense amplifiers <b>215</b>(<b>1</b>)-(N)).</p><p>Accessing the information in a storage array (i.e., reading data stored in storage arrays <b>210</b>(<b>1</b>)-(N)) is typically a two step process. First, data is transferred between storage array <b>300</b> and a corresponding set of sense amplifiers <b>215</b>(<b>1</b>)-(N). Next, the data is transferred between the sense amplifiers involved and the column amplifiers <b>265</b>. Certain memory core architectures do away with the column amplifiers, transferring the data from the sense amplifiers directly to the data I/O bus (i.e., data I/O bus <b>190</b>).</p><p>The first major step, transferring information between storage arrays <b>210</b>(<b>1</b>)-(N) and sense amplifiers <b>215</b>(<b>1</b>)-(N), is known as a \u201crow access\u201d and is broken down into the minor steps of precharge and sense. The precharge step prepares the sense amplifiers and bit lines for sensing, typically by equilibrating them to a midpoint reference voltage. During the sense operation, the row address is decoded, a single word line is asserted, the contents of the storage cell is placed on the bit lines, and the sense amplifiers amplify the value to full rail (i.e., a full digital high value), completing the movement of the information from the storage array to the sense amplifiers. Of note is the fact that the sense amplifiers can also serve as a local cache which stores a \u201cpage\u201d of data which can be more quickly accessed with column read or write accesses. The second major step, transferring information between the sense amplifiers and the interface, is called a \u201ccolumn access\u201d and is typically performed in one step. However, variations are possible in which this major step is broken up into two minor steps, e.g. putting a pipeline stage at the output of the column decoder. In this case the pipeline timing should be adjusted to account for the extra time involved.</p><p>These two steps give rise to the four basic memory operations mentioned previously: precharge, sense, read, and write. A typical memory core can be expected to support these four operations (or some subset thereof). However, certain memory types may require additional operations to support architecture-specific features. The general memory core described provides the basic framework for memory core structure and operations. However, a variety of memory core types, each with slight differences in their structure and function, exist. The three major memory core types are:</p><p>Dynamic Random-Access Memory (DRAM)</p><p>Static Random-Access Memory (SRAM)</p><p>Read-Only Memory (ROM)</p><p>The structure of a conventional DRAM core is similar to the generic memory core in FIG. <b>2</b>. Like memory core <b>200</b>, the conventional DRAM structure has a row and column storage array organization and uses sense amplifiers to perform row access. As a result, the four primary memory operations (sense, precharge, read and write) are supported. Memory core <b>200</b> includes an additional column amplifier block and column amplifiers <b>265</b>, which are commonly used to speed column access in DRAM (and other memory core types, as well). Also illustrated by FIG. 2 is the use of multiple banks, a common configuration for conventional DRAM cores. As before, the row decoder, column decoder, and column amplifiers are shared among the banks. An alternative configuration replicates these elements for each bank. However, replication typically requires larger die area and thus incurs greater cost.</p><p>Inexpensive core designs with multiple banks typically share row decoders, column decoders, and column datapaths between banks to minimize die area, and therefore cost.</p><p>Conventional DRAM cores use a single transistor cell, known as a 1T cell. The single transistor accesses a data value stored on a capacitor. The 1T cell is one of the storage cell architectures that employs a single bit line, as referred to previously. This simple storage cell achieves high storage density, and hence a low cost per bit. However, designs employing such storage cells are subject to two limitations. First, such storage cell architectures exhibit slower access times than certain other storage cells, such as SRAM storage cells. Since the passive storage capacitor can only store a limited amount of charge, row sensing for conventional DRAM storage cells (i.e., 1T cells) takes longer than for other memory types with actively-driven cells (e.g., SRAM storage cells). Hence, the use of a 1T storage cell architecture generally results in relatively slow row access and cycle times.</p><p>Second, such storage cell architectures require that the data held in each cell be refreshed periodically. Because the bit value is stored on a passive capacitor, the leakage current in the capacitor and access transistor result in degradation of the stored value. As a result, the cell value must be \u201crefreshed\u201d periodically. The refresh operation consists of reading the cell value and re-writing the value back to the cell. These two additional memory operations are named refresh sense and refresh precharge, respectively. In traditional cores, refresh sense and refresh precharge were the same as regular sense and precharge operations. However, with multiple bank cores, special refresh operations may be advantageous to enable dedicated refresh circuits and logic to support multibank refresh.</p><p>To perform a row access in a conventional DRAM having a single bank, bit lines <b>220</b>(<b>1</b>)-(N) and sense amplifiers <b>215</b>(<b>1</b>)-(N) must first be precharged, typically to one-half of the supply voltage (Vdd/2). The row precharge time, t<sub>RP</sub>, is the time required to precharge the row to be sensed. To perform a sense operation, row decoder <b>225</b> drives a single word line (e.g., one of word lines <b>310</b>(<b>1</b>)-(N)) to turn on each of the memory cells' access transistors (not shown) in the row being sensed. The charge on each of the memory cells' storage capacitors (also not shown) transfers to its respective bit line, slightly changing the corresponding bit line's voltage. The sense amplifier detects this small voltage change and drives the bit lines to either Vdd or ground, depending on the voltage change produced by the capacitor's charge. The wordline must be held high a minimum time period of t<sub>RAS,MIN </sub>to complete the sensing operation. At some time before the bit lines reach their final value, a column read or write access can begin. The time between the start of the sense operation and the earliest allowable column access time is t<sub>RCD </sub>(the row-to-column access delay). The total time to perform both precharge and sense is t<sub>RC</sub>, the row cycle time, and is a primary metric for core performance.</p><p>Row access timing for DRAMs with multiple banks, such as that illustrated in FIG. 2, differs slightly from the preceding example. The delay t<sub>PP </sub>specifies the minimum delay between precharge operations to different banks. This indicates that the precharge circuitry is able to precharge the next row (which may be the same row originally precharged) after a period of t<sub>PP</sub>. Typically, t<sub>PP </sub>is approximately equal (or even less than) t<sub>RP</sub>, assuming the same memory core and device architecture are employed. Similarly, t<sub>SS </sub>specifies the minimum delay between performing sense operations on different banks. As before, the sensing on different banks can be carried out more quickly than repeated sensing on the same bank. These parameters indicate that, while the precharge circuitry can precharge a row every t<sub>PP </sub>seconds and sense circuitry can sense every t<sub>SS </sub>seconds (both of which are usually measured in ns), a single bank's storage array can only be precharged (or sensed) every t<sub>RC </sub>seconds (measured in ns). Thus, a memory core employing multiple banks can be read from and written to more quickly in situations where different banks are being accessed.</p><p>Typical column cycle times and access times greatly depend on the type of sense amplifier circuit employed. This is because the sense amplifiers drive the selected data onto the column data I/O wires, and must be able to drive the capacitance that those wires represent (i.e., the amplifier must be able to charge that capacitance in the requisite time). Increased speeds can be achieved by improving the sense amplifier's drive capability, thus charging the column data I/O wires capacitance more quickly. This could be done by using more or larger transistors in the sense amplifier circuit. However, such modifications greatly increase die area, and so cost, especially because the sense amplifier circuit is so heavily replicated. Thus, the desire to minimize the die area of commodity DRAMs limits the further reduction of column access speeds by this technique.</p><p>In a conventional DRAM, the column decoder's output drives a single column select line, which selects some or all of the outputs from the sense amplifiers. The column decoder's output may be placed in a register for pipelined designs. The selected sense amplifiers then drive their respective data onto the column I/O wires. To speed column access time, the column I/O lines are typically differential and sensed using differential column amplifiers (e.g., column amplifiers <b>265</b> in FIG. <b>2</b>), which amplify small voltage differences on the column I/O wires and drive data I/O bus <b>185</b>. The width of the column I/O bus determines the data granularity of each column access (also known as CAS block granularity).</p><p>Unfortunately, the preceding DRAM timing parameters (and others) can vary widely due to variations in manufacturing processes, supply voltage, operating temperature, and process generations, among other factors. In order for a memory architecture to operate properly given such variations, it is important for a DRAM protocol to be able to support these varied row and column timings.</p><p>In a conventional DRAM, column control signals <b>255</b> of FIG. 2 typically include a column latch signal, a column cycle signal, and write mask signals. The column latch signal precedes the column cycle signal, and causes column decoder <b>230</b> to latch the column address (column address <b>250</b>). In this type of architecture, the column cycle signal indicates the actual beginning of the column access process, and therefore is required to wait for the column address to be latched. Some DRAM memory cores also include the ability to mask write data. With masking, a write operation is performed such that some bits or bytes of the datapath are not actually written to the storage array depending on the mask pattern. Typically, the mask pattern is delivered to the column amplifier write circuit, which inhibits the write data in an appropriate manner. Moreover, data I/O bus <b>185</b> and/or column I/O lines <b>266</b> can be either bidirectional, in which case write and read data are multiplexed on the same bus, or unidirectional, in which case separate write and read datapaths are provided. While FIG. 2 illustrates data I/O bus <b>185</b> as a bidirectional bus, the use of a unidirectional bus can easily be envisioned.</p><p>FIG. 2 may also be used to illustrate a memory core employing an SRAM storage cell architecture. The typical SRAM memory core architecture shares the core structure and functionality of the conventional DRAM memory architecture discussed previously. Moreover, accesses are performed in a two-step process similar to that used in accessing data held in a DRAM memory core. First, during the sense operation, the information is transferred between the storage array and the sense amplifiers. Second, in the column access operation, the information is transferred between the sense amplifiers and the interface. Another similarity to DRAM is the need to precharge the bitlines prior to sensing operations, although typical precharge value is the supply voltage, not half of the supply voltage normally used in conventional DRAM architectures.</p><p>SRAM memory cores differ markedly from DRAM memory cores in the architecture of the storage cells used in each. In an SRAM memory architecture, data is stored statically, typically using a circuit of several transistors. A typical SRAM storage cell uses cross-coupled CMOS inverters to store a single data bit, and employs the bit line pairs as illustrated in FIG. 3 (internal bit lines <b>220</b>(<b>1</b>)-(N), e.g., differential bit lines). A word line (one of word lines <b>310</b>(<b>1</b>)-(N)) turns on access transistors within the selected SRAM storage cells (e.g., storage cells <b>305</b>(<b>1</b>,<b>1</b>)-(<b>1</b>,N)), which connect each cell in the row to the differential bit lines (internal bit lines <b>320</b>(<b>1</b>)-(N)). Unlike a DRAM cell, however, each SRAM storage cell actively drives the stored value onto its respective bit line pair. This results in faster access times. The static nature of the SRAM cell also eliminates the need for refresh operations. However, the static cell uses more transistors and therefore requires more area than a DRAM cell. As with the DRAM, the four primitive operations of an SRAM are sense, precharge, read, and write. However, because an SRAM storage cell operates so quickly, precharge and sense may be performed for each read (even within page). This is in contrast to DRAM devices (known as page-mode DRAM), which save time by storing a page of data in the device's sense amplifiers, as noted previously.</p><p>Read-only memory (ROM) cores store information according to an electrical connection at each cell site which join rows to columns. Typically, a single transistor forms the electrical connection at each cell site. There are a variety of ROM cell types, including erasable programmable ROM storage (EPROM), electrically erasable programmable ROM (EEPROM), flash ROM, and mask-programmable ROM. Their differences lie in the type of transistor used in each architecture's storage cell. However, ROMs share the storage array architecture illustrated in FIG. 2, which requires a row and column decode of the address for each data access.</p><p>Unlike SRAM and DRAM devices, not all ROM devices include sense amplifier circuits (e.g., sense amplifiers <b>215</b>(<b>1</b>)-(N)). Sense amplifiers are only used in certain ROM architectures which require fast access times. For such ROM devices, the primitive operations are sense, precharge, and read. For slower ROM devices that do not use sense amplifiers, the selected data values are driven directly from the storage cell circuitry to output amplifiers, which in turn drive the data I/O bus. For these ROMs, the single primitive operation is read.</p><p>A significant limitation on the effective bandwidth of memory bus <b>50</b> (i.e., interconnect <b>110</b>) can arise as the result of the issuance of certain combinations of read and write operations. For example, the issuance of certain read/write combinations may intrinsically introduce inefficiencies in the utilization of interconnect <b>110</b>. For example, a delay (also known as a data bubble) may occur when a write operation is followed by a read operation. Because the write data is immediately present on interconnect <b>110</b> and the read data is not present until a later time (determined by the access time of the device being read), a data bubble between the write data and read data naturally occurs. This data bubble obviously impairs the efficient utilization of interconnect <b>110</b> and the column I/O datapath.</p><p>Moreover, because it is preferable to share certain interconnect resources <b>110</b>, certain combinations of read and write operations are not allowable. These combinations result in data bubbles between the data transferred by certain of the read and write operations within these combinations. These delays, also known as data bubbles, are of particular importance in systems which are configured to maintain full or almost full utilization of interconnect <b>110</b> by constantly (or nearly constantly) transferring data to and from components attached thereto (e.g., CPU <b>14</b> and main memory <b>16</b>), and within the memory devices which make up main memory <b>16</b>.</p><p>In a conventional memory of the design shown in FIGS. 2 and 3, the resource ordering for read and write operations differs slightly. A read operation uses resources in the order:</p><p>control signal lines <b>112</b></p><p>column I/O datapath (including data I/O bus <b>185</b> and column I/O lines <b>266</b>)</p><p>data signal lines <b>114</b></p><p>while a write operation uses them in the order:</p><p>control signal lines <b>112</b></p><p>data signal lines <b>114</b></p><p>column I/O datapath (including data I/O bus <b>185</b> and column I/O lines <b>266</b>) These differences in the ordering of resource usage give rise to resource conflicts when read and write operations are issued because control signals issued over control signal lines <b>114</b> cause data to be transferred immediately, in relative terms. Thus, if data signal lines <b>114</b> and the column I/O datapath are bidirectional (as is desirable), conflicts can occur between read data and write data because each transfer requires the use of these resources.</p><p>What is therefore desirable is a protocol and apparatus that provide improved interconnect utilization. In particular, the protocol should permit read and write operations to be issued in any order without the need to delay one or more of the operations because of resource conflicts. Moreover, the apparatus should be configured to perform this function in the case of bidirectional interconnect and column I/O datapaths.</p><h4>SUMMARY OF THE INVENTION</h4><p>The present invention relates to the transfer of data in computer systems. More specifically, the present invention relates to a protocol and apparatus that provide improved interconnect utilization. In particular, a two-step write operation according to the present invention avoids resource conflicts, thus permitting read and write operations to be issued in any order while maintaining continuous data traffic.</p><p>In one embodiment of the present invention, a method for storing data in a memory device is described. The method includes the following steps. The method employs a two-step technique which allows the out-of-order completion of read and write operations. When a write operation requires a resource needed for the completion of a read operation, the data being written is stored in a write data buffer in the memory device. The write data is stored in the buffer until a datapath is available to communicate the data to the memory device's memory core. Once the resource is free (or the memory device, or its controller force the write to complete) the data is written to the memory core of the memory device using the now-free datapath.</p><p>In another embodiment of the present invention, a memory device is described. The memory device includes a memory core in which data may be stored. The memory core includes a storage array, in which the data is actually stored, and a bidirectional datapath coupled to the storage array, which allows data to be read from and written to the storage array. The memory device also includes a datapath that is coupled to the memory core's bidirectional datapath, and allows data to be communicated into and out of the memory device. The memory device also includes a write data buffer coupled to the datapath. This data buffer is configured to store the data to be written to the memory core. In this manner, the data buffer allows one or more quanta of data to be stored for a period of time, again allowing their related write operations to complete in an out-of-order sequence by waiting until the memory core's bidirectional datapath is free.</p><p>These and other embodiments of the present invention, as well as its advantages and features are described in more detail in conjunction with the text below and attached figures.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>For a better understanding of the invention, reference should be made to the following detailed description taken in conjunction with the accompanying drawings, in which:</p><p>FIG. 1A is a block diagram of a computer system of the prior art;</p><p>FIG. 1B is a block diagram of an interconnect of the prior art;</p><p>FIG. 1C is a block diagram of a generic memory device of the prior art;</p><p>FIG. 2 is a block diagram of a generic memory core of the prior art;</p><p>FIG. 3 is a block diagram of a generic storage array of the prior art;</p><p>FIG. 4 is a timing diagram showing the data bubble that can occur in a memory device;</p><p>FIG. 5 is a timing diagram showing the reduction of the data bubble of FIG. 4;</p><p>FIG. 6 is a block diagram of one embodiment of a memory device containing circuitry that reduces the data bubble of FIG. 4;</p><p>FIG. 7 is a block diagram of one embodiment of a memory device containing circuitry that may be utilized in accordance with the present invention;</p><p>FIG. 8 is a timing diagram showing a data bubble which may be remedied using the circuit of FIG. 7;</p><p>FIG. 9 is a block diagram of one embodiment of a memory device containing circuitry according to the present invention;</p><p>FIG. 10 is a timing diagram showing the reduction of the data bubble using the circuitry of FIG. 9;</p><p>FIG. 11 is a block diagram of one embodiment of a memory device containing circuitry according to the present invention;</p><p>FIG. 12 is a timing diagram showing the reduction of the data bubble using the circuitry of FIG. 11 in the case of a write operation followed by a read operation;</p><p>FIG. 13 is a timing diagram showing the reduction of the data bubble as in FIG. 9, but with a no-op operation between the write and read operations;</p><p>FIG. 14 is a timing diagram showing the reduction of the data bubble using the circuitry of FIG. 11 in a second case of a write operation followed by a read operation;</p><p>FIG. 15 is a block diagram of one embodiment of a memory device containing circuitry according to the present invention which provides for bypassing;</p><p>FIG. 16 is a block diagram of one embodiment of a blender, as illustrated in FIG. 15;</p><p>FIG. 17 is a timing diagram illustrating the operation of the circuitry of FIG. 15; and</p><p>FIG. 18 is a timing diagram illustrating the operation of the circuitry of FIG. <b>15</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><p>Like reference numerals refer to corresponding parts throughout the drawings.</p><h4>DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>I. Introduction</p><p>The present invention provides a protocol, which may be implemented in a memory device, that supports improved utilization of an interconnect between a bus master (e.g., CPU <b>14</b> of FIG. 1A) and a bus slave (e.g., main memory <b>16</b> of FIG. <b>1</b>A). In particular, a two-step write operation is used to avoid resource conflicts. In this manner, a memory device according to the present invention permits the issuance of read and write operations in any order while maintaining continuous data traffic.</p><p>A memory device according to the present invention maintains continuous data traffic by employing a two-step technique which allows the out-of-order completion of read and write operations. When a write operation requires a resource needed for the completion of a read operation, the data being written is stored in a write data buffer in the memory device. The write data is stored in the buffer until a datapath is available to communicate the data to the memory device's memory core. Once the resource is free (or the memory device, or its controller force the write to complete) the data is written to the memory core of the memory device using the now-free datapath.</p><p>II. The Use of Delayed Write Operations</p><p>FIG. 4 illustrates a situation in which a data bubble is formed by a write operation followed by a read operation. Write operations <b>400</b> and <b>405</b>, followed by a read operation <b>410</b> and write operations <b>415</b> and <b>420</b>, are communicated over control signal lines <b>112</b> to memory device <b>100</b>, which forwards this control information to memory core <b>200</b>. Write operations <b>400</b> and <b>405</b> input write data <b>425</b> and <b>430</b> to memory device <b>100</b> via data signal lines <b>114</b>. Write data <b>425</b> and <b>430</b> are communicated to memory core <b>200</b>, and then to one or more of memory banks <b>205</b>(<b>1</b>)-(N) via column I/O lines <b>266</b>.</p><p>Read operation <b>410</b> reads data from memory device <b>100</b> by causing memory core <b>200</b> to output read data <b>435</b> on column I/O lines <b>266</b>, as shown in FIG. 2, and then to data I/O bus <b>185</b>. Read data <b>435</b> is then communicated to data signal lines <b>114</b> via operations block <b>130</b> and transport block <b>120</b>. In a fashion similar to the preceding write operations, write operations <b>415</b> and <b>420</b> input write data <b>440</b> and <b>445</b> to memory device <b>100</b> via data signal lines <b>114</b>, and then to one or more of memory banks <b>205</b> (<b>1</b>)-(N) via column I/O lines <b>266</b>.</p><p>As can be seen in FIG. 4, no resource conflicts are observed in the case where a write operation follows another write operation (e.g., write operations <b>400</b> and <b>405</b>). Moreover, data can also be efficiently transferred in the case where a write operation follows a read operation (e.g., read operation <b>410</b> and write operation <b>415</b>). This is because the read data can be followed immediately with write data. Although not illustrated in FIG. 4, the case where a read operation is followed by another read operation also experiences no resource conflicts. These combinations fail to experience such conflicts because the data transfer requested by the given operations are not in contention for the same resources. For example, write data <b>425</b> is transferred from data signal lines <b>114</b> to column I/O lines <b>266</b> before write data <b>430</b> needs data signal lines <b>114</b>. Thus, no resource conflict occurs.</p><p>However, a data bubble <b>450</b> occurs in the transfer of data on interconnect <b>110</b> in the case where a read operation follows a write operation (e.g., write operation <b>405</b> and read operation <b>410</b>). In that case, because the write data is presented immediately and the read data is not present until a later time, a data bubble between the write data and read data naturally occurs. The data bubble appears regardless of whether write operation <b>405</b> and read operation <b>410</b> are directed to the same or to different memory devices (e.g., memory devices within main memory <b>16</b>) attached to interconnect <b>110</b>. It is noted that the delay from control signals <b>112</b> to column I/O lines <b>266</b> is identical for read and write operations.</p><p>The solution to the problem created by data bubble <b>450</b> is to match the timing of the write operation's use of datapath resources to the read operation's use of those resources. Typically, the optimal delay for a write operation does not quite match the delay for a read operation because interconnect <b>110</b> has an intrinsic turnaround time. This turnaround time is the time required to switch the direction of the circuitry which drives interconnect <b>110</b> (e.g., the time it takes to switch the direction of bidirectional buffers or amplifiers). Instead, the delay for a write operation should be equal to the minimum read delay minus the minimum turnaround time for interconnect <b>110</b>. There is no need to change the control-to-data delay for the write operation as a function of memory device position on interconnect <b>110</b> because the turnaround delay grows as the read delay grows.</p><p>FIG. 5 shows the result of delaying the write to match the read. The delay from the issuance of the write control to the beginning of the data write is set to match the delay from the issuance of the read control to the beginning of the data read. As long as different column data paths are used to perform the read column cycle and the write column cycle (i.e., the read and write operations are to different memory devices), the data bubble is shrunk to the minimum required by channel turnaround requirements and is no longer a function of control or data resource conflicts. This is illustrated in FIG. 5 by the use of column I/O lines <b>266</b>(A) and <b>266</b>(B), each of which designates the column I/O lines of separate memory devices ((A) and (B)). As long as different column data paths are used to perform the read column cycle and the write column cycle, the data bubble is shrunk to the minimum required by channel turnaround requirements and is no longer a function of control or data resource conflicts. The need for this restriction is illustrated by the fact that read data <b>435</b> is accessed at the same time as write data <b>425</b> and write data <b>430</b>. Moreover, since write latency is not vitally important to application performance, this modification does not cause any loss in application performance, so long as the writes and reads are directed to separate column data paths and the write occurs before the expiration of t<sub>RAS,MIN </sub>(the minimum time between sensing a row and precharging another row in the same bank).</p><p>Delaying write operations thus helps optimize data bandwidth efficiency over a bidirectional datapath. The technique adds a delay between control signals indicating a write operation and data being written so that the delay between the two is similar to that of read operations. Maintaining this \u201cpattern\u201d for read and write operations improves pipeline efficiency over a bidirectional datapath. As noted, this is true only for operations to separate column resources. It is to be understood that, due to the timing relationship between column control signals and column I/O data, the limitations experienced by one column resource are substantially the same constraints experienced by the other column resource. In other words, because the time between a column control operation and the data resulting from that operation is so short, a resource conflict on one column resource will imply a resource conflict on the other column resource.</p><p>FIG. 6 illustrates the modifications to memory device <b>100</b> necessary to provide delayed write functionality. Column access control information is delayed for writes relative to when the column control information is presented to the core for reads by a write delay block <b>600</b>. The outputs of write delay block <b>600</b> and control operation block <b>150</b> are coupled to a multiplexer <b>610</b> which selects between these outputs, under the control of control operation block <b>150</b>. The output selected depends upon the type of operation to be performed (i.e., whether the current operation is a read or a write). If the current operation is a read operation, control operation block <b>150</b> to select the output of control operation block <b>150</b>, while a write operation would cause control operation block <b>150</b> selects the output of write delay block <b>600</b>. While a multiplexer is shown in FIG. 6, other embodiments of this mechanism may be employed, as would be obvious to those skilled in the art. For example, a state machine could be used to introduce new delaying state transitions when the operation is a write.</p><p>However, even if a delayed write technique is employed, a data bubble <b>450</b> may still be observed in the transfer of data over column I/O lines <b>266</b> (and data I/O bus <b>185</b>). For example, given the operations illustrated in FIG. 5, if the operations are all to be performed within a single device, there will obviously be a resource conflict on column I/O lines <b>266</b>, as well as on column control signals <b>255</b> (assuming that device has bidirectional datapaths).</p><p>The resource conflict which gives rise to data bubble <b>450</b> occurs within memory device <b>100</b>. More specifically, the resource conflict occurs on the datapath within memory core <b>200</b>. This is because column I/O lines <b>266</b> are bidirectional, as is data I/O bus <b>185</b>. Column I/O lines <b>266</b> are normally designed to be bidirectional to reduce the cost and area of the given memory design. As noted, the offending write and read operations must be directed to the same device for this phenomenon to occur. However, this resource conflict could still exist notwithstanding the use of delayed write techniques. The fundamental problem is the resource conflict which occurs when a read and a write operation require the use of a device's column resources. Thus, a solution to the problem of a resource conflict with regard to a device's column resources is made necessary by such a situation.</p><p>III. The Use of Two-Step Write Operations</p><p>If a write operation is patterned so that the data interconnect utilization is not limited by read/write conflicts when employing independent column paths, the case of using a single column path to achieve the same utilization must be addressed, in order to avoid data bubbles within memory device <b>100</b>. The root of the problem exposed in this section is the interaction of the bidirectional data interconnect resource with the bidirectional column I/O resource. We could resolve this problem by making one or both of these resources unidirectional. (The two-step write technique disclosed herein would, of course, only be applicable to resolving a conflict on a column resource). In the preferred embodiment we make them both bidirectional for cost reasons. It is possible that changes in manufacturing technology would make it cost effective for one or the other of the data resources to be unidirectional.</p><p>If nothing more than delaying write operations is done, then a write followed by a read results in the timing shown in FIG. <b>5</b>. As noted, a delayed write causes a delay for a read to the same device because the write operation is committed once the write control information is presented on the control interconnect and so the column circuitry must wait for the write data so that it can complete the write into memory core <b>180</b>, using the column I/O resource before the core access step of the read operation can begin. This not only wastes bandwidth on the data resource, but also delays the read, raising the average service time for reads.</p><p>The basic problem is to achieve the timing of the write control, addressing, mask, and data at the memory core implied by FIG. 5 even though the data resource timing has been delayed. This timing of the write information needs to be achieved without introducing the delay shown in FIG. <b>4</b>. Moreover, if a write delay is employed, the write must be performed without removing the delay of the write data introduced to avoid the resource contention for interconnect <b>110</b> solved by the circuitry of FIG. <b>6</b>.</p><p>One solution is to breakup writes into a two-step sequence. In one step, the data is transferred from the master to a buffer in the memory device. This step will be referred to herein as the transport step. In the second step, the data is transferred from the buffer into the memory core via the column I/O datapath. This step will be referred to herein as the retire step.</p><p>FIG. 7 shows the structure of the core transfer operation unit <b>136</b>, read data operation unit <b>160</b>, and write data operation unit <b>170</b> for a memory that performs operations that are signaled on the control lines. The operation block <b>130</b> of FIG. 1C is shown in greater detail in FIG. <b>7</b>. Control signals <b>700</b> are received from control transport unit <b>122</b>. Transfer, control, distribution, and sequence (TCDS) block <b>705</b> produces signals to control the memory core <b>180</b>, the read data operation unit <b>160</b>, and write data operation unit <b>170</b>. TCDS block <b>705</b> handles transfer, control, signal distribution, and sequencing responsibilities, in this configuration, as analogous blocks do in the block diagrams described below. Signals <b>710</b> are the edge based control signals for the memory core. Signals <b>715</b> are signals that are presented to the core for a duration of time, and usually have setup and hold requirements with respect to the transition times of signals <b>710</b>, and are produced by control buffer <b>720</b>. For a read operation, control buffer <b>720</b> receives control signals directly from TCDS block <b>705</b> via signals <b>725</b> through multiplexer <b>730</b> which is controlled by signal <b>735</b>. For a write operation, control buffer <b>720</b> receives control signals from TCDS block <b>705</b> via write control buffer <b>740</b>, signals <b>745</b>, write control buffer <b>750</b>, signals <b>755</b>, and multiplexer <b>730</b> (under the control of signal <b>735</b>). Write control buffers <b>740</b> and <b>750</b> are controlled by signals <b>760</b>. For write control buffer write operations, signals <b>710</b> are timed to correspond to the arrival of the operation to signals <b>715</b>. Write control buffers <b>740</b> and <b>750</b> delay the application of the operation control to the memory core. This delay allows the data corresponding to the buffered write operation to be issued later, better matching the timing of the write operation to that of the read operation. Other embodiments may use fewer or additional blocks to change the amount of the delay.</p><p>Read data buffer <b>765</b> receives read data on signals <b>770</b> from memory core <b>180</b>, at times controlled by signal <b>771</b>. This data is passed on to the transport block <b>120</b> via signals <b>775</b>. In another embodiment, read data buffer <b>765</b> is an amplifier driving signals <b>775</b> without timing signal <b>771</b>. In yet another embodiment, read data operation unit <b>160</b> is comprised only of interconnect. Other variations for read data operation unit <b>160</b> are possible, depending on specific drive and timing characteristics of memory core <b>180</b>.</p><p>Write data buffer <b>780</b> receives write data from transport block <b>120</b> via signals <b>781</b> at times controlled by signal <b>782</b>. This data is passed on to the memory core <b>180</b> via signals <b>783</b>. Write mask buffer <b>785</b> receives mask data from the transport unit on signals <b>786</b> at times controlled by signal <b>787</b>. The mask information is passed on to memory core <b>180</b> via signals <b>788</b>. Mask data is used by memory core <b>180</b> to selectively write, or not write, parts of the data within the memory core. In another embodiment, no mask is used, with the result that all the data is written unconditionally.</p><p>FIG. 8 is a timing diagram illustrating the segregated control and data signals associated with FIG. <b>1</b>C and FIG. <b>7</b>. The control signals <b>700</b> are applied to TCDS block <b>705</b>. The write data sent to the memory device is delivered on signals <b>781</b>, while the read data from the memory device is sent by signals <b>775</b>. In one embodiment, the data signal lines are not segregated so that read data and write data are transmitted on the same wires at different times. In another embodiment, the data signal lines are further segregated so that some wires transmit only write data and other wires transmit only read data. The write mask is sent on either the control signal lines <b>112</b>, or the data signal lines. In one embodiment, the write mask is sent only on the control signal lines. Alternatively, the write mask may be sent only on data signal lines <b>114</b>. In another embodiment, the write mask is sent on both of control signal lines <b>112</b> and data signal lines <b>114</b>.</p><p>The highlighted write operation in FIG. 8 shows the write control and the write data being transmitted at separate times on control signal lines <b>112</b> and data signal lines <b>114</b>, and used to operate the core with signals <b>710</b>, <b>715</b>, <b>783</b> and <b>788</b>. The timing relationship, in contrast to FIG. 4, shows the delay between control and data on control signal lines <b>112</b> and data signal lines <b>114</b>. After the arrival of the data, the application of control and data and mask signals to memory core <b>180</b> is done to complete the operation.</p><p>The highlighted read operation in FIG. 8 shows the read control being transmitted on control signal lines <b>112</b>, which causes memory core <b>180</b> to be controlled by signals <b>710</b> and <b>715</b>. The characteristics of memory core <b>180</b> affect the time at which the read data is available and delivered via signals <b>775</b>, which are transmitted from memory device <b>180</b> on data signal lines <b>114</b>.</p><p>The similar timing relationships for a read operation and a write operation, on control signal lines <b>112</b> and data signal lines <b>114</b>, allow back-to-back operations for read and write, in either order. In order to do so for a write followed by a read, however, the operations must be directed to a different device, which may be done only in a memory system comprised of multiple memory devices which are all connected by control signal lines <b>112</b> and data signal lines <b>114</b>. FIG. 8 illustrates that, when a write is followed by a read to the same device, the read operation on wires <b>710</b> and <b>715</b> must be timed to follow the write operation on the same wires. This necessitates the separation of the two operations on control signal lines <b>112</b> and data signal lines <b>114</b>, so that a data bubble exists on data signal lines <b>114</b>. In one embodiment, the time of both the read control, the read data, the write control and the write data are 4 cycles of a synchronizing clock. In this embodiment, the memory core has timing characteristics supporting the relationships shown in FIG. <b>8</b>. For such an embodiment, the loss of utilization of the data signal lines <b>114</b> is shown in FIG. 8 as a data bubble of 10 cycles in duration. In other embodiments, the data bubble may be of a different duration and timed by different means.</p><p>The loss of the utilization for data signal lines <b>114</b> causes a decrease in effectiveness for the memory system which contains the memory device. This loss of utilization is significant because the occurrence of writes followed by reads to the same device may be frequent, depending on the usage of the memory system, especially when there are one or a small number of memory devices comprising the memory subsystem connected by control signal lines <b>112</b> and data signal lines <b>114</b>.</p><p>FIG. 9 shows the structure of the core transfer operation, read data operation and write data operation units for a memory that performs operations that are signaled on the control lines as soon as is practical. Control signals <b>700</b> are received from the transport block <b>120</b>. TCDS block <b>705</b>, read data operation unit <b>160</b>, and write operation unit <b>170</b> produce signals to control the memory core <b>180</b>. Signals <b>710</b> are the control signals for the memory core and are preferably edge based. Signals <b>715</b> are signals that are presented to memory core <b>180</b> for a duration of time, and usually have setup and hold requirements with respect to the transition times of signals <b>710</b>, and are produced by block <b>720</b>. For a read operation, control buffer <b>720</b> receives control signals directly from block TCDS <b>705</b> via signals <b>725</b> through multiplexer <b>730</b>, which is controlled by signal <b>735</b>. For a write operation, control buffer <b>720</b> receives control signals from TCDS block <b>705</b> via write control buffer <b>740</b>, signals <b>745</b>, write control buffer <b>750</b>, signals <b>755</b> and multiplexer <b>730</b>. Write buffers <b>740</b> and <b>750</b> are controlled by signals <b>760</b>. For a write operation, signals <b>760</b> are timed to correspond to the arrival of the operation via signals <b>715</b>. The effect of the write control buffers <b>740</b> and <b>750</b> is to delay the application of the operation control to the memory core. Another effect of write control-buffers <b>740</b> and <b>750</b> is to allow storage of the write control information so that they may be passed on to the memory core for operation based on some later control indication, rather than just passing through on some fixed schedule. Other embodiments may use fewer or additional blocks to change the amount of the delay and storage. The operation of write control buffers <b>740</b> and <b>750</b> of FIG. 9 can thus parallel that of write control buffers <b>740</b> and <b>750</b> of FIG. 7, if desired, but need not do so.</p><p>Read data buffer <b>765</b> receives read data on signals <b>770</b> from the memory core <b>180</b>, at times controlled by signal <b>771</b>. The data is passed on to transport block <b>120</b> via signals <b>775</b>. In another embodiment, read data buffer <b>765</b> is an amplifier capable of driving signals <b>775</b>, without the need for timing signal <b>771</b>. In yet another embodiment, read data operation unit <b>160</b> includes only interconnect. Other variations for read data operation unit <b>160</b> are possible, depending on specific drive and timing characteristics of the memory core.</p><p>Write data buffer <b>13202</b> receives write data from transport block <b>120</b> on signals <b>781</b> and is controlled by signal <b>13201</b>. Write data buffer <b>13200</b> is an additional write data buffer, that is also controlled by signal <b>13201</b> so that it passes data through to write data buffer <b>13200</b> directly in some cases, but stores the data for later passing to write data buffer <b>13200</b> in other cases. The write data buffer <b>13200</b> receives write data from write data buffer <b>1320</b> via signals <b>13203</b>, under the control of signal <b>13201</b>, and presents the data to memory core <b>180</b> on signals <b>783</b>. In an analogous fashion, mask data is passed using signals <b>786</b>, <b>13208</b>, and <b>788</b> with mask data being stored in write mask buffers <b>13207</b> and <b>13205</b>. Mask data is used by memory core <b>180</b> to selectively write, or not write, parts of the data within the memory core. In another embodiment, no mask is used so that all the data is written unconditionally.</p><p>By providing write data buffer <b>13200</b> (and write mask buffer <b>13205</b>), memory device <b>100</b> allows write operations to be split into two operations, transport and retire. First, the write data (and mask) is transported to write data buffer <b>13200</b> (and write mask buffer <b>13205</b>) using, for example, interconnect <b>110</b>. Upon receiving a retire command (in whatever form), the write data is communicated to memory core <b>180</b>. This allows write operations, which might otherwise be in contention for the column resources of memory device <b>100</b>, to complete at a time when no conflicts exist with regard to the now-available column resources.</p><p>FIG. 10 is a timing diagram relating the segregated control and data signals from FIG. <b>1</b>C and FIG. <b>9</b>. The control signals are sent via signals <b>700</b>. The write data sent to the memory device is received via signals <b>781</b>, while the read data from memory device <b>100</b> is sent via signals <b>775</b>. Write mask data is received via signals <b>786</b>. In one embodiment, the data wires are not segregated so that read data and write data are transmitted on the same wires at different times. In another embodiment, the data wires are further segregated so that some wires transmit only write data and other wires transmit only read data. The write mask is sent over either the control wires or the data wires. In one embodiment, the write mask is sent using only the control signal lines. In another embodiment, the write mask is sent using only the data signal lines. In another embodiment, the write mask is sent on both control signal lines and the data signal lines.</p><p>The write operation labeled \u201ca\u201d in FIG. 10 shows the write control and the write data being transmitted at different times on control signal lines <b>112</b> and data signal lines <b>114</b>, and used to operate memory <b>180</b> core with signals <b>710</b>, <b>715</b>, <b>783</b>, and <b>788</b>. The timing relationship is the same as for all the write operations of FIG. <b>8</b>. After the arrival of the data, the application of control and data and mask to the memory core is done to fulfill the operation.</p><p>The highlighted write operation labeled \u201cd\u201d and its predecessor illustrate a different timing relationship. The operation of these writes at memory core <b>100</b> via signals <b>710</b> and <b>715</b> are reordered to follow the read that the writes precede on control signal lines <b>112</b>. This timing relationship is made possible by the separation of the control that signals the transport of the write data from the control that causes the write operation at the memory core, referred to as a retire operation. In one embodiment the retire control is a specific operation code as part of a control sequence. This is an example of an explicit retire command. In another embodiment, the retire control is implicitly indicated by the reception of any control that arrives when write data is arrived at <b>783</b> and any control is indicated that does not require a read operation to be performed. In another embodiment, the retire control is indicated when write data is arrived at wires <b>783</b> and either no further operation is signaled on control signal lines <b>112</b>, or any control is indicated that does not require a read operation to be performed.</p><p>The highlighted read operation in FIG. 10 shows the read control being transmitted on control signal lines <b>112</b>, which causes the memory core to be controlled by signals <b>710</b> and <b>715</b>. The characteristics of memory core <b>180</b> affect the time at which the read data is available and delivered via signals <b>775</b>, which are transmitted from the memory device on data signal lines <b>114</b>.</p><p>The similar timing relationships for a read operation and a write operation, on control signal lines <b>112</b> and data signal lines <b>114</b>, allow back-to-back operations for read and write. This may be performed when the operations are to different devices (as in the case illustrated in FIGS. 3, <b>7</b> and <b>8</b>), but also when the operations are to the same device, due to the reordering that the retire control allows.</p><p>In general, one control indicator is used to send the write data on data signal lines <b>114</b>. A retire control indicator is used to perform the operation at the memory core. Additional control indicators may be used to signal any other control information for the write, such as the addresses or masks, as long as all the control information arrives in time for the memory operation indicated by the retire control indicator.</p><p>The ability to generally perform back-to-back write and read operations allows high utilization of data signal lines <b>114</b> providing a high performance memory system.</p><p>The reordered writes of FIGS. 9 and 10 allow a loss of coherency if the data read is from the same location as one of the writes that has been delayed. The structure indicated relies on the originator of the memory operations to maintain coherency. This may be done in many ways know to one skilled in the art. In one instance, if the read location corresponds to one of the write locations, the read operation is delayed until the corresponding write operation is retired. In another instance, a copy of the write data is maintained by originator and merged with the read information, or replaces the read operation.</p><p>FIG. 11 shows a structure similar to that of FIG. 9, except that one bank of the write data and mask buffers is removed, as a cost consideration. In this case, the master unit (e.g., a memory controller) holds the write data (that would normally be held in a write buffer in memory device <b>100</b>) until that data is needed or is scheduled to arrive as the write buffer is freed.</p><p>FIG. 11 shows the structure of the memory core transfer operation, read data operation, and write data operation units for a memory that performs operations that are signaled on the control lines as soon as is practical. Control signals <b>700</b> are received from transport block <b>120</b>. TCDS block <b>705</b>, read data operation unit <b>160</b>, and write operation unit <b>170</b> produce signals to control memory core <b>180</b>. Signals <b>710</b> are the control signals for memory core <b>180</b> and are preferably edge based. Signals <b>715</b> are signals that are presented to memory core <b>180</b> for a duration of time, and usually have setup and hold requirements with respect to the transition times of signals <b>710</b>, and are produced by block <b>720</b>. For a read operation, control buffer <b>720</b> receives control signals directly from block TCDS <b>705</b> via signals <b>725</b> through multiplexer <b>730</b>, which is controlled by signal <b>735</b>. For a write operation, control buffer <b>720</b> receives control signals from TCDS block <b>705</b> via write control buffer <b>740</b>, signals <b>745</b>, write control buffer <b>750</b>, signals <b>755</b> and multiplexer <b>730</b>. Write buffers <b>740</b> and <b>750</b> are controlled by signals <b>760</b>. For a write operation, signals <b>710</b> are timed to correspond to the arrival of the-operation via signals <b>715</b>. The effect of the blocks <b>740</b> and <b>750</b> is to delay the application of the operation control to the memory core. Another effect of write control buffers <b>740</b> and <b>750</b> is to allow storage of the write control information so that they may be passed on to the memory core for operation based on some later control indication, rather than just passing through on some fixed schedule. Other embodiments may use fewer or additional blocks to change the amount of the delay and storage.</p><p>Read data buffer <b>765</b> receives read data on signals <b>770</b> from the memory core <b>180</b>, at times controlled by signal <b>771</b>. The data is passed on to transport block <b>120</b> via signals <b>775</b>. In another embodiment, read data buffer <b>765</b> is an amplifier capable of driving signals <b>775</b>, without the need for timing signal <b>771</b>. In yet another embodiment, read data operation unit <b>160</b> includes only interconnect. Other variations for read data operation unit <b>160</b> are possible, depending on specific drive and timing characteristics of the memory core.</p><p>Write data buffer <b>15200</b> receives write data from transport block <b>120</b> on signals <b>781</b> and is controlled by signal <b>782</b> and presents the data to memory core <b>180</b> via signals <b>783</b>. In an analogous fashion, mask data is passed using signals <b>786</b> and <b>787</b> with mask data being stored in write mask buffer <b>15205</b>. Mask data is used by memory core <b>180</b> to selectively write, or not write, parts of the data within the memory core. In another embodiment, no mask is used so that all the data is written unconditionally.</p><p>By providing write data buffer <b>15200</b> (and write mask buffer <b>15205</b>), memory device <b>100</b> allows write operations to be split into two operations, transport and retire. First, the write data (and mask) is transported to write data buffer <b>15200</b> (and write mask buffer <b>15205</b>) using, for example, interconnect <b>110</b>. Upon receiving a retire command (in whatever form), the write data is communicated to memory core <b>180</b>. This allows read operations, which might otherwise be in contention for the column resources of memory device <b>100</b>, to complete at a time when no conflicts exist with regard to the now-available column resources.</p><p>However, unlike the circuit in FIG. 9, the circuit of FIG. 11 has only one write data buffer, write data buffer <b>15200</b> (and so, only one write mask buffer, write mask buffer <b>15205</b>). Thus, to avoid overwriting the data (and mask) held in memory device <b>100</b>, the memory controller must hold the last write \u201ctransported,\u201d (or schedule its transport to coincide with the freed write buffer) as it cannot necessarily be written (along with the related mask data) to write data buffer <b>15200</b> (and write mask buffer <b>15205</b>). Moreover, the memory controller, in such a configuration, must maintain information on the write it is holding, and must be made aware of the retiring of the write held in the memory controller. Thus, the complexity of the memory controller is increased in this embodiment, to provide the necessary capabilities for maintaining and reacting to such information. The benefit of this embodiment, however, is the reduction in complexity enjoyed by memory device <b>100</b>. The reduction in complexity of memory device <b>100</b> is important for two reasons, among others. First, the cost reduction such a configuration provides to memory device <b>100</b> affects the commercial viability of such a system, reducing the cost per chip. Second, because there are far more memory devices than controllers in the average system, the cost of the system also can be expected to drop. Thus, pushing the complexity from the memory devices to the memory controller is an important step in reducing the overall system cost and complexity.</p><p>FIG. 12, FIG. <b>13</b> and FIG. 14 illustrate that the use of a reduced structure such as that shown in FIG. 11 is still capable of providing the benefits of the two-step write process. FIG. 12 is a timing diagram illustrating the segregated control and data signals from FIG. <b>11</b>. FIG. 12 illustrates the use of a two-step write technique in the circuit of FIG. 11 (i.e., with one data buffer), in a situation where a write operation is abandoned in favor of a following read operation, to allow the read operation to complete prior to the write operation requiring the column resources of memory device <b>100</b>.</p><p>FIG. 12 shows that the master unit issuing these read and write operations can abandon one of the write operations to perform the read. It should be noted that write \u201cc\u201d data is overwritten without a write operation being performed for it. In this embodiment, the master unit is assumed to have kept all the necessary information associated with the write operation stored at the master unit so that the write operation can be reissued. The control signals are sent via signals <b>700</b>. The write data is sent to the memory device via signals <b>781</b>, while the read data from memory device <b>100</b> is sent via signals <b>775</b>. Write mask data is received via signals <b>786</b>. In one embodiment, the data wires are not segregated so that read data and write data are transmitted on the same wires at different times (a bidirectional bus). In another embodiment, the data wires are further segregated so that some wires transmit only write data and other wires transmit only read data (a unidirectional bus). The write mask is sent over either the control wires or the data wires. In one embodiment, the write mask is sent using only the control signal lines. In another embodiment, the write mask is sent using only the data signal lines. In another embodiment, the write mask is sent on both control signal lines and the data signal lines.</p><p>The write operation labeled \u201ca\u201d in FIG. 12 shows the write control and the write data being transmitted at different times on control signal lines <b>112</b> and data signal lines <b>114</b>, and used to operate memory <b>180</b> core with signals <b>710</b>, <b>715</b>, <b>783</b> and <b>788</b>. After the arrival of the data, the application of control and data and mask signals to memory core <b>180</b> is done to complete the operation.</p><p>The highlighted write operation labeled \u201cd\u201d and its predecessor (write operation \u201cc\u201d, which is the write operation that is abandoned) illustrate a different timing relationship. The operation of write operation \u201cd\u201d at memory core <b>100</b> via signals <b>710</b> and <b>715</b> is reordered to follow the read that the write precedes on control signal lines <b>112</b>. This timing relationship is made possible by the separation of the control that signals the transport of the write data from the control that causes the write operation at the memory core, referred to as a retire operation. In one embodiment the retire control is a specific operation code as part of a control sequence. This is an example of an explicit retire command. In another embodiment, the retire control is implicitly indicated by the reception of any control that arrives when write data is arrived at <b>783</b> and any control is indicated that does not require a read operation to be performed. In another embodiment, the retire control is indicated when write data is arrived at wires <b>783</b> and either no further operation is signaled on control signal lines <b>112</b>, or any control is indicated that does not require a read operation to be performed.</p><p>The highlighted read operation in FIG. 12 shows the read control being transmitted on control signal lines <b>112</b>, which causes the memory core to be controlled by signals <b>710</b> and <b>715</b>. The characteristics of memory core <b>180</b> affect the time at which the read data is available and delivered via signals <b>775</b>, which are transmitted from the memory device on data signal lines <b>114</b>.</p><p>The similar timing relationships for a read operation and a write operation, on control signal lines <b>112</b> and data signal lines <b>114</b>, allow back-to-back operations for read and write. This may be performed when the operations are to different devices (as in the case illustrated in FIGS. 3, <b>7</b> and <b>8</b>), but also when the operations are to the same device, due to the reordering that the retire control allows.</p><p>In general, one control indicator is used to send the write data on data signal lines <b>114</b>. A retire control indicator is used to perform the operation at the memory core. Additional control indicators may be used to signal any other control information for the write, such as the addresses or masks, as long as all the control information arrives in time for the memory operation indicated by the retire control indicator. The ability to generally perform back-to-back write and read operations allows high utilization of data signal lines <b>114</b> providing a high performance memory system.</p><p>The reordered writes of FIGS. 11, <b>12</b>, <b>13</b>, and <b>14</b> indicate that a loss of coherency may occur if the data read is from the same location as one of the writes that has been delayed. The structure indicated relies on the originator of the memory operations to maintain coherency. This may be done in many ways known to one skilled in the art. In one instance, if the read location corresponds to one of the write locations, the read operation is delayed until the corresponding write operation is retired. In another instance, a copy of the write data is maintained by the originator and is merged with the read information, or replaces the read operation.</p><p>FIG. 13 illustrates the use of a two-step write technique in the circuit of FIG. 11 (i.e., with one data buffer), in a situation where a read operation is delayed after a write stream, to allow the read operation to complete in the proper sequence with regard to the write operations requiring the column resources of memory device <b>100</b>. FIG. 13 shows that a small bubble can be inserted to allow the write \u201cc\u201d data to be retired. This is done by inserting a \u201cno-operation\u201d (no-op) operation in the command stream on control signal lines <b>112</b>. Write operation \u201cd\u201d is still reordered, and the bubble is smaller than it would be if not for the two step write. However, write \u201cd\u201d now has enough time to be stored in the retire buffer, again avoiding a conflict in the column resources of memory device <b>100</b>.</p><p>Here again, write \u201cd\u201d is delayed to avoid the creation of a data bubble on interconnect <b>110</b>. However, a no-op is inserted to delay the read so as to avoid a conflict on the column resources of memory device <b>100</b>. By delaying the memory core's provision of the read data on the column resources, write \u201cd\u201d may be stored in the write data buffer, thus avoiding a conflict with the read operation. This allows a read operation to interrupt a stream of write operations without causing a conflict and without causing the data held in the write buffer to be overwritten. Those skilled in the art will appreciate that the \u201cno-op\u201d may be substituted with any operation that is not a read or write to memory device <b>100</b>, including read or write operations to other memory devices.</p><p>FIG. 14 illustrates the use of a two-step write technique in the circuit of FIG. 11 (i.e., with one data buffer), in a situation where a read operation is issued with unstreamed write operations, to allow the read operation to complete in the proper sequence with regard to the write operations requiring the column resources of memory device <b>100</b>. FIG. 14 shows that the dilemma of having a second reordered write overwriting another write operation will be avoided if the writes are not streamed. If the originator schedules the writes with enough separation for one operation, as either \u201cno operation\u201d (or \u201cno-op\u201d) or a read, or a write to another device, then a read to this device can occur without any added delay, and without causing data to be overwritten in the one set of write data/mask buffers. Those skilled in the art will appreciate that a \u201cno-op\u201d can be substituted with any operation that does not involve a read or a write, such as a precharge operation.</p><p>FIGS. 15, <b>16</b>, <b>17</b>, and <b>18</b> illustrate an embodiment of a memory device according to the present invention in which the memory device also provides for coherency internally. This relieves the originator of the data and control signals (typically, a memory controller at the master) of having to keep track and maintain coherency in the operations the master unit had issued.</p><p>The concept here is that portions of the data needed to satisfy a read operation may exist in one of several places (e.g., one or both of the write buffers, and/or in memory core <b>180</b>). Thus, a mechanism could be provided to allow data to exist in any one of those places and still be accessible to a read operation, assuming such operations are allowed in the architecture of memory device <b>100</b>. This relieves the master unit (e.g., a memory controller) from having to keep track of where data is at any one time.</p><p>FIG. 15 shows the structure of FIG. 9 with the addition of comparators to compare an incoming read address with the two buffered write addresses. If a memory read address matches the address of one or both buffered writes, the additional circuitry in the data path below allows the merging of the read data with either or both of the buffered write data. If there is no mask, the merge is a simple multiplexer operation and the read need not be performed at memory core <b>180</b>. In general, with a mask, the read does need to be performed and the data/mask combinations from the two buffered writes are used to update the read from memory core <b>180</b> to provide the latest information coherently.</p><p>Control signals <b>700</b> are received from the transport block <b>120</b>. TCDS block <b>705</b>, read data operation unit <b>160</b>, and write operation unit <b>170</b> produce signals to control the memory core <b>180</b>. Signals <b>710</b> are the control signals for the memory core and are preferably edge based. Signals <b>715</b> are signals that are presented to memory core <b>180</b> for a duration of time, and usually have setup and hold requirements with respect to the transition times of signals <b>710</b>, and are produced by block <b>720</b>. For a read operation, control buffer <b>720</b> receives control signals directly from block TCDS <b>705</b> via signals <b>725</b> through multiplexer <b>730</b>, which is controlled by signal <b>735</b>. For a write operation, control buffer <b>720</b> receives <b>10</b> control signals from TCDS block <b>705</b> via write control buffer <b>740</b>, signals <b>745</b>, write control buffer <b>750</b>, signals <b>755</b> and multiplexer <b>730</b>. Write buffers <b>740</b> and <b>750</b> are controlled by signals <b>760</b>. For a write operation, signals <b>760</b> are timed to correspond to the arrival of the operation via signals <b>715</b>.</p><p>The effect of write control buffer <b>740</b> and <b>750</b> is to delay the application of the operation control to the memory core. Another effect of write control buffers <b>740</b> and <b>750</b> is to allow storage of the write control information so that they may be passed on to the memory core for operation based on some later control indication, rather than just passing through on some fixed schedule. Other embodiments may use fewer or additional blocks to change the amount of the delay and storage.</p><p>Read data buffer <b>765</b> receives read data on signals <b>770</b> from the memory core <b>180</b>, at times controlled by signal <b>771</b>. The data is passed on to a blender <b>19195</b>. Blender <b>19195</b> blends bits (or other quanta of data) to satisfy a read operation which may require data held in one of the write data buffers and/or memory core <b>180</b>. The requisite data is then passed on to transport block <b>120</b> via signals <b>775</b>. In another embodiment, read data buffer <b>765</b> is an amplifier capable of driving signals <b>19142</b>, without the need for timing signal <b>771</b>. In yet another embodiment, read data buffer <b>765</b> includes only interconnect. Other variations for read data operation unit <b>160</b> are possible, depending on specific drive and timing characteristics of the memory core.</p><p>Write data buffer <b>19202</b> receives write data from transport block <b>120</b> on signals <b>781</b> and is controlled by signal <b>19201</b>. Write data buffer <b>19202</b> is an additional write data buffer, that is also controlled by signal <b>19201</b> so that it passes data through to write data buffer <b>19200</b> directly in some cases, but stores the data for later passing to write data buffer <b>19200</b> in other cases. The write data buffer <b>19200</b> receives write data from write data buffer <b>19202</b> via signals <b>19203</b>, under the control of signal <b>19201</b>, and presents the data to memory core <b>180</b> via signals <b>783</b>. In an analogous fashion, mask data is passed using signals <b>786</b>, <b>19208</b>, and <b>788</b> with mask data being stored in write mask buffers <b>19207</b> and <b>19205</b>. Mask data is used by memory core <b>180</b> to selectively write, or not write, parts of the data within the memory core.</p><p>In another embodiment, no mask is used so that all the data is written unconditionally.</p><p>By providing write data buffer <b>19200</b> (and write mask buffer <b>19205</b>), memory device <b>100</b> allows write operations to be split into two operations, transport and retire. First, the write data (and mask) is transported to write data buffer <b>19200</b> (and write mask buffer <b>19205</b>) using, for example, interconnect <b>110</b>. Upon receiving a retire command (in whatever form), the write data is communicated to memory core <b>180</b>. This allows write operations, which might otherwise be in contention for the column resources of memory device <b>100</b>, to complete at a time when no conflicts exist with regard to the now-available column resources.</p><p>Additionally, the circuit of FIG. 15 permits data to be bypassed around memory core <b>180</b> in the case of a read requiring data held in write data buffers <b>19200</b> and <b>19202</b> (as indicated in part by write mask buffer <b>19205</b> and <b>19207</b>). This is done by blender <b>19195</b> selecting signals <b>19203</b> and/or <b>19142</b>, either in whole or in part using signals <b>19208</b> to account for masking of data <b>19203</b> (enabled by the bit-slice architecture of blender <b>19195</b>). Data held in write data buffer <b>19200</b> may also be blended by using signals <b>783</b> (and signals <b>788</b> to account for masking of that data). Those skilled in the art will appreciate how to adapt the coherency mechanisms from FIG. 15 into the circuitry of FIG. 11 where there is only one data buffer.</p><p>FIG. 16 shows an embodiment for a blender circuit. FIG. 16 illustrates the circuitry for a single bit in detail. The multiplexer combines the compare hit control information and the mask bit to select either the upstream data bit or substitute the bit from the write data buffer. The upstream multiplexer selects between the read operation data bit and the oldest write buffer data. The downstream multiplexer selects between the upstream multiplexer and the youngest write buffer data.</p><p>FIG. 16 illustrates a blender such as that shown in FIG. 15 as blender <b>19195</b>. The function of this circuit to provide the necessary data to satisfy a read operation that requires data that is held in one or both of the write buffers and also possibly in memory core <b>180</b>. The function performed by a blender of this type is to take data, portions of which may be masked, and portions of which may exist in various locations due to the architecture of a memory device implementing a 2-step write technique.</p><p>FIG. 16 shows a blender <b>2000</b> which comprises a multiplexer <b>2020</b> and a multiplexer <b>2040</b> which select data from various sources to combine the data in satisfying the data requirements of the read operation. Multiplexer <b>2020</b> selects between data from read data buffer <b>765</b> and data from write data buffer <b>19200</b>. Information held in write mask buffer <b>19205</b> is combined with control signals from TCDS <b>705</b> by a circuit <b>2010</b>. Alternatively, this can be seen as the bit of write data being conditioned by the write mask bit held in the write mask buffer when the addresses compare. The results of this combination selects the input of multiplexer <b>2020</b> by indicating the selection on a signal line <b>2015</b>. The result of this selection is output on signal line <b>2025</b>, which is input to multiplexer <b>2040</b>.</p><p>Multiplexer <b>2040</b> selects between the output of multiplexer <b>2020</b> and the output of write data buffer <b>19202</b>, again conditioned by the write mask information held in write mask buffer <b>19207</b> and address comparison. As shown in FIG. 16, the write mask information held in write mask buffer <b>19207</b> is actually combined with control signals from TCDS <b>705</b> by a circuit <b>2021</b>, in the circuit shown in FIG. 16 (although numerous alternatives might easily be envisioned). The result of this operation causes multiplexer <b>2040</b> to select one of its inputs by indicating the desired selection on a signal line <b>2035</b>. The output on signal line <b>2045</b> forms a part of output <b>755</b>. As noted, blender <b>2000</b> represents only one bit-slice of a larger circuit. Thus, various portions of read data may come from various locations within memory device <b>100</b>, and may also be made to depend on the value of a write mask and the results of address comparison.</p><p>FIG. 17 is a timing diagram when neither write control buffer matches the incoming read. This is indicated as a low level on read comparison signal <b>19216</b>. It can be seen that this timing diagram is substantially similar to the preceding timing diagrams (e.g., FIGS. <b>10</b> and <b>12</b>), with the exception that signals relating to the bypass operations are shown. These include read comparison signal <b>19216</b>, which indicates a match of some or all of the data held in the write control buffers. Additionally, a read operation in such a system can be seen to require a small amount of extra time, allowing for the sequencing of comparison operations, and the potential blend of the read data and write buffers.</p><p>FIG. 18 is a timing diagram showing the new signals and the blending performed to produce a coherent read data transmission. In this timing diagram, the read address matches the addresses of write data held in both write data buffer <b>19200</b> and write data buffer <b>19202</b>. This is indicated as a high level on read comparison signals <b>19216</b>. In this example, data from memory core <b>180</b> (exemplified by the results of read operation as read \u201ce\u201d), write \u201cd\u201d (held in write data buffer <b>19202</b>), and write \u201cc\u201d (held in write data buffer <b>19200</b>).</p><p>IV. Variations on the Basic Two-Step Write Control Paradigm</p><p>In general, one control indicator is used to send the write data on data signal lines <b>114</b>. A distinct retire control indicator is used to perform the operation at the memory core. Additional control indicators may be used to signal any other control information for the write, such as the addresses or masks, as long as all the control information arrives in time for the memory operation indicated by the retire control indicator.</p><p>As previously described, a two-step write comprises a transport and a retire step. The transport step communicates the data and a portion of the address and mask information. Some or all of the transport information is buffered in the memory device. The retire step communicates the balance of the address and mask information and causes the data to be written to the memory core, using whatever information may have been buffered from the transport step.</p><p>Thus, the mask information can be sent with the transport operation (or even before that point in time), with the retire operation, or as a separate operation, depending upon the system requirements. Indeed, these choices are applicable not only to write mask information, but to any of the control information that might need to be sent to memory device <b>100</b>. Thus, these operations may occur at any time before the write retires.</p><p>In one embodiment, all of the address and mask information is transmitted with the transport step while the retire step indicates to the memory device that it is time for that buffered information to be written to the core. For example, all of the device, bank, and column addressing information plus the masking information can be sent in the transfer step, with the timing of the data transport associated with this step. In this embodiment, the retire step just provides memory core write timing.</p><p>In another embodiment, only the device address is provided with the transport step that sends data to the memory device. In this embodiment the remaining information, such as the bank and column address, as well as the mask information, are sent when the data is to be written into the memory core. Other alternative embodiments are possible. In these embodiments, different elements of information are associated with either the transport or retire steps. In one embodiment, device, bank, and column addressing information are associated with the transport step, while masking information is associated with the retire step. This association allows maximum commonality of operation arguments, such as addressing information, to occur between read and write operations while, as a second order constraint, minimizing the buffering, since reads do not use masking information.</p><p>In addition to the variations discussed above, the retire step can be either explicit or implicit. An explicit retire requires that an operation code or some means of coding that is discrete from the other operations in the packet, such as an independent bit, be provided for, and supplied to the memory device when it is time for the retire to occur. In addition to the means of indicating that the operation is to be performed there must also be a means to indicate which buffered information is to be retired. For example, this may be by means of a device address. However, other methods are possible, for example, each device could keep track of how many transports have occurred but have not been retired prior to a transport directed to it. A first-in-first-out (FIFO) policy might be implemented, in which case the device can do a retire at an appropriate time of its own choosing, without an explicit device address being needed.</p><p>An implicit retire presumes that the memory device can determine when it can perform the write of the buffered information to the memory core without an explicit instruction to do so. There are many methods to do this. For example:</p><p>If no transfer operation is directed to the memory device, it autonomously does a column write operation.</p><p>When the memory device detects that an alternative operation is taking place that cannot require the column I/O resource then it performs the column write operation.</p><p>If the retire is done autonomously, this eliminates the high level of control over resource consumption by the master unit (i.e., a memory controller). In some embodiments, it is desirable for the master unit to have a high level of control over resource consumption. This is because once the write information has been placed into the memory device, the memory device may proceed to use the column I/O resource at its discretion. If the master unit does not keep the column I/O resource busy, then the resource's usage will be triggered by the memory device, even if the master unit would prefer to use the column I/O resource before the resource goes idle again.</p><p>If the retire is triggered by an alternative operation, this allows the controller to continue to exert control over the timing of the memory core write operation, without having to explicitly allocate control bandwidth to do so. This method may be implemented in several ways. In one embodiment, the memory device performs a retire operation whenever:</p><p>control information is received, and</p><p>the retire buffer is not empty (both control and data), and</p><p>the control is read or write control and control information is either</p><p>directed to a different column I/O path, or</p><p>directed to the same column I/O path but is not a read operation</p><p>the control is not read or write control</p><p>Presuming that the transfer control information can arrive no faster than any column I/O path can perform a single transfer cycle it is impossible for a resource conflict to occur given the rules above.</p><p>Another modification is varying the number of retire buffers employed. As noted, to avoid resource conflicts with the bidirectional column I/O bus in the core, the write operation may be divided into two (or more) steps. The write control information may include, for example, device, bank and column address and write mask. In one embodiment, the retire buffer stores write data and transport write control information until the retire command is issued and holds the write data valid long enough to meet the hold time requirements of the core interface. The actual write command signals to start the write operation are issued after the control logic receives the retire command. The depth of the retire buffers can be one or greater. A deeper retire buffer can eliminate loss due to certain read-write combinations that otherwise introduce performance bubbles in the pipeline, but do so at the cost of increased hardware and complexity.</p><p>The method of the present invention is not intended to be limited by the preceding statements. A person of ordinary skill in the art will realize that different circuitry can be used and alterations can be made to the protocol of the present invention without departing from the spirit of the invention. Other equivalent or alternative protocols and apparatus according to the present invention will be apparent to those skilled in the art. For example, any number of retire buffers may be employed, allowing any amount of write data to be delayed, to account for resource conflicts at any point in the datapath of memory device <b>100</b>. These equivalents and alternatives are intended to be included within the scope of the present invention.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Paul G.", "last_name": "Davis", "name": ""}, {"first_name": "Frederick A.", "last_name": "Ware", "name": ""}, {"first_name": "Craig E.", "last_name": "Hampel", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "RAMBUS INC."}, {"first_name": "", "last_name": "JPMORGAN CHASE BANK, AS COLLATERAL AGENT", "name": ""}, {"first_name": "", "last_name": "RAMBUS INCORPORATED", "name": ""}, {"first_name": "", "last_name": "RAMBUS INCORPORATED", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/12"}], "locarno_classes": [], "ipcr_classes": [{"label": "G11C   5/00        20060101A I20060506RMEP"}, {"label": "G11C   7/10        20060101A I20070721RMEP"}, {"label": "G11C   7/22        20060101A I20070721RMEP"}, {"label": "G06F  13/16        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711158"}, {"primary": false, "label": "365194"}, {"primary": false, "label": "711167"}, {"primary": false, "label": "710005"}], "ecla_classes": [{"label": "G11C   7/10L"}, {"label": "G11C   7/10M5"}, {"label": "G11C   7/22"}, {"label": "G11C  11/4076"}, {"label": "S11C207:22B"}, {"label": "S11C207:22W"}, {"label": "G06F  13/16A2R"}], "cpc_classes": [{"label": "G11C  11/4076"}, {"label": "G06F  13/1626"}, {"label": "G11C   7/22"}, {"label": "G11C   7/1039"}, {"label": "G11C2207/229"}, {"label": "G11C   7/1006"}, {"label": "G11C2207/2218"}, {"label": "G11C   7/22"}, {"label": "G11C2207/229"}, {"label": "G11C   7/1039"}, {"label": "G06F  13/1626"}, {"label": "G11C   7/1006"}, {"label": "G11C  11/4076"}, {"label": "G11C2207/2218"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1997-10-10", "application_date": "1998-10-09", "family_members": [{"ucid": "US-20050248995-A1", "titles": [{"lang": "EN", "text": "Memory system and method for two step memory write operations"}]}, {"ucid": "US-7437527-B2", "titles": [{"lang": "EN", "text": "Memory device with delayed issuance of internal write command"}]}, {"ucid": "US-7047375-B2", "titles": [{"lang": "EN", "text": "Memory system and method for two step memory write operations"}]}, {"ucid": "US-7421548-B2", "titles": [{"lang": "EN", "text": "Memory system and method for two step memory write operations"}]}, {"ucid": "US-20050169065-A1", "titles": [{"lang": "EN", "text": "Memory system and method for two step memory write operations"}]}, {"ucid": "US-20120179866-A1", "titles": [{"lang": "EN", "text": "Memory Component Having Write Operation with Multiple Time Periods"}]}, {"ucid": "US-20020046331-A1", "titles": [{"lang": "EN", "text": "Memory system and method for two step write operations"}]}, {"ucid": "US-20090031093-A1", "titles": [{"lang": "EN", "text": "Memory System and Method for Two Step Memory Write Operations"}]}, {"ucid": "US-8140805-B2", "titles": [{"lang": "EN", "text": "Memory component having write operation with multiple time periods"}]}, {"ucid": "US-20070177436-A1", "titles": [{"lang": "EN", "text": "Memory System and Method for Two Step Memory Write Operations"}]}, {"ucid": "WO-1999019805-A1", "titles": [{"lang": "EN", "text": "METHOD AND APPARATUS FOR TWO STEP MEMORY WRITE OPERATIONS"}, {"lang": "FR", "text": "PROCEDE ET APPAREIL POUR OPERATIONS D'ECRITURE DANS UNE MEMOIRE EN DEUX TEMPS"}]}, {"ucid": "US-20110093669-A1", "titles": [{"lang": "EN", "text": "Memory System and Method for Two Step Memory Write Operations"}]}, {"ucid": "US-6889300-B2", "titles": [{"lang": "EN", "text": "Memory system and method for two step write operations"}]}, {"ucid": "US-7870357-B2", "titles": [{"lang": "EN", "text": "Memory system and method for two step memory write operations"}]}, {"ucid": "AU-9604698-A", "titles": [{"lang": "EN", "text": "Method and apparatus for two step memory write operations"}]}, {"ucid": "US-8504790-B2", "titles": [{"lang": "EN", "text": "Memory component having write operation with multiple time periods"}]}, {"ucid": "US-6343352-B1", "titles": [{"lang": "EN", "text": "Method and apparatus for two step memory write operations"}]}]}