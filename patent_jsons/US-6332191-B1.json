{"patent_number": "US-6332191-B1", "publication_id": 72894755, "family_id": 22876548, "publication_date": "2001-12-18", "titles": [{"lang": "EN", "text": "System for canceling speculatively fetched instructions following a branch mis-prediction in a microprocessor"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72657298\"><p>A line predictor is configured to speculatively fetch instructions following a branch instruction. The line predictor stores a plurality of lines that each contain instruction line information. Each line stored by the line predictor includes a fetch address, information regarding one or more instructions, and one or more next fetch addresses. In response to receiving a fetch address, the line predictor is configured to provide instruction line information corresponding to the one or more instructions located at the fetch address to an alignment unit. The line predictor is also configured to provide a next fetch address associated with the fetch address to an instruction cache for speculative fetching and to a branch prediction unit for a branch prediction. The next fetch address is further fed back into the line predictor to generate the instruction line information associated with it and a subsequent next fetch address. A next fetch address may be the sequential address following the last instruction associated with the instruction line information of the fetch address. If an instruction within the instruction line information of the fetch address is a branch instruction, however, the next fetch address may be the target address of the branch instruction. The branch prediction unit is configured to generate a branch prediction in response to receiving a next fetch address if a branch instruction is detected in the instruction line information of the next fetch address. The branch prediction is then compared to a subsequent next fetch address. If the branch prediction differs from a subsequent next fetch address, operations that were initiated using the subsequent next fetch address are canceled, the subsequent next fetch address is updated in the line predictor, and the updated subsequent next fetch address is refetched.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00001\" num=\"1\"><claim-text>1. A microprocessor comprising:</claim-text><claim-text>an instruction cache coupled to receive a first fetch address corresponding to two or more instructions terminated by a branch instruction, wherein the first fetch address locates an initial one of the two or more instructions in memory, and wherein the instruction cache is configured to store a plurality of instructions; and </claim-text><claim-text>a branch prediction unit coupled to receive the first fetch address, the branch prediction unit comprising a storage storing branch prediction information corresponding only to branch instructions, wherein the storage is configured to output first branch prediction information in response to the first fetch address, and wherein the branch prediction unit is configured to generate a first branch prediction for the branch instruction terminating the two or more instructions located by the first fetch address responsive to the first branch prediction information, wherein the brancih prediction unit further comprises logic coupled to receive the first branch prediction and a second branch prediction corresponding to the first fetch address, and wherein the logic is configured to signal that the two or more instructions are to be cancelled if the first branch prediction disagrees with the second branch prediction. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00002\" num=\"2\"><claim-text>2. The microprocessor as recited in claim <b>1</b> wherein the storage comprises a branch history table configured to store a plurality of branch predictions, and wherein the branch history table is configured to output the first branch prediction of the plurality of branch predictions in response to the first fetch address.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00003\" num=\"3\"><claim-text>3. The microprocessor as recited in claim <b>1</b> wherein the branch instruction is located in memory by a second address different from the first fetch address received by the branch prediction unit.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00004\" num=\"4\"><claim-text>4. The microprocessor as recited in claim <b>1</b> wherein the instruction cache and the branch prediction unit receive the first fetch address concurrently.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00005\" num=\"5\"><claim-text>5. The microprocessor as recited in claim <b>1</b> wherein the logic is configured to cause a fetch of a second fetch address corresponding to the first branch prediction if the first branch prediction disagrees with the second branch prediction.</claim-text></claim>"}, {"num": 6, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00006\" num=\"6\"><claim-text>6. A method comprising:</claim-text><claim-text>generating a first fetch address corresponding to two or more instructions terminated by a branch instruction, the first fetch address locating an initial one of the two or more instructions in memory; </claim-text><claim-text>providing the first fetch address to a branch prediction unit, the branch prediction unit comprising a storage storing branch prediction information corresponding only to branch instructions; </claim-text><claim-text>outputting first branch prediction information in response to the providing; </claim-text><claim-text>generating a first branch prediction for the branch instruction in response to the first branch prediction information; </claim-text><claim-text>generating a second branch prediction concurrent with generating the first fetch address; </claim-text><claim-text>determining that the first branch prediction and the second branch prediction disagree; and </claim-text><claim-text>cancelling the two or more instructions responsive to the determining. </claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00007\" num=\"7\"><claim-text>7. The method as recited in claim <b>6</b> wherein the branch instruction is located in memory by a second address different from the first fetch address used in the generating the first branch prediction.</claim-text></claim>"}, {"num": 8, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00008\" num=\"8\"><claim-text>8. The method as recited in claim <b>6</b> further comprising fetching instructions from a second fetch address corresponding to the first branch prediction responsive to the determining.</claim-text></claim>"}, {"num": 9, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00009\" num=\"9\"><claim-text>9. A computer system comprising:</claim-text><claim-text>a microprocessor including: </claim-text><claim-text>an instruction cache coupled to receive a first fetch address corresponding to two or more instructions terminated by a branch instruction, wherein the first fetch address locates an initial one of the two or more instructions in memory, and wherein the instruction cache is configured to store a plurality of instructions; and </claim-text><claim-text>a branch prediction unit coupled to receive the first fetch address, the branch prediction unit comprising a storage storing branch prediction information corresponding only to branch instructions, wherein the storage is configured to output first branch prediction information in response to the first fetch address, and wherein the branch prediction unit is configured to generate a first branch prediction for the branch instruction terminating the two or more instructions located by the first fetch address responsive to the first branch prediction information, wherein the branch prediction unit further comprises logic coupled to receive the first branch prediction and a second branch prediction corresponding to the first fetch address, and wherein the logic is configured to signal that the two or more instructions are to be cancelled if the first branch prediction disagrees with the second branch prediction; and </claim-text><claim-text>a peripheral device for communicating between the computer system and another computer system. </claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00010\" num=\"10\"><claim-text>10. The computer system as recited in claim <b>9</b> wherein the peripheral device comprises a modem.</claim-text></claim>"}, {"num": 11, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00011\" num=\"11\"><claim-text>11. The computer system as recited in claim <b>9</b> further comprising an audio peripheral device.</claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00012\" num=\"12\"><claim-text>12. The computer system as recited in claim <b>11</b> wherein the audio peripheral device includes a sound card.</claim-text></claim>"}, {"num": 13, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00013\" num=\"13\"><claim-text>13. The computer system as recited in claim <b>9</b> further comprising a second processor including:</claim-text><claim-text>a second instruction cache coupled to receive a second fetch address corresponding to two or more instructions terminated by a second branch instruction, wherein the second fetch address locates an initial one of the two or more instructions in memory, and wherein the second instruction cache is configured to store a plurality of instructions; </claim-text><claim-text>a second branch prediction unit coupled to receive the second fetch address, the second branch prediction unit comprising a second storage storing second branch prediction information corresponding only to branch instructions, wherein the second storage is configured to output second branch prediction information in response to the second fetch address, and wherein the second branch prediction unit is configured to predict the second branch instruction terminating the two or more instructions located by the second fetch address responsive to the second branch prediction information. </claim-text></claim>"}, {"num": 14, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00014\" num=\"14\"><claim-text>14. The computer system as recited in claim <b>9</b> wherein the storage comprises a branch history table configured to store a plurality of branch predictions, and wherein the branch history table is configured to output the first branch prediction of the plurality of branch predictions in response to the first fetch address.</claim-text></claim>"}, {"num": 15, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00015\" num=\"15\"><claim-text>15. The computer system as recited in claim <b>9</b> wherein the branch instruction is located in memory by a second address different from the first fetch address received by the branch prediction unit.</claim-text></claim>"}, {"num": 16, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00016\" num=\"16\"><claim-text>16. The computer system as recited in claim <b>9</b> wherein the instruction cache and the branch prediction unit receive the first fetch address concurrently.</claim-text></claim>"}, {"num": 17, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6332191-B1-CLM-00017\" num=\"17\"><claim-text>17. The computer system as recited in claim <b>9</b> wherein the logic is configured to cause a fetch of a second fetch address corresponding to the first branch prediction if the first branch prediction disagrees with the second branch prediction.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54773174\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>This invention is related to the field of processors and, more particularly, to speculatively fetching instructions following a branch instruction in a microprocessor.</p><p>2. Description of the Related Art</p><p>Superscalar processors attempt to achieve high performance by issuing and executing multiple instructions per clock cycle and by employing the highest possible clock frequency consistent with the design. Over time, the number of instructions concurrently issuable and/or executable by superscalar processors has been increasing in order to increase the performance of superscalar processors.</p><p>Unfortunately, many of the concurrently issuable and/or executable instructions are branch instructions where the address of the instruction subsequent to the branch instruction may not be known prior to execution of the branch instruction. Consequently, branch instructions may incorporate significant delays into a superscalar microprocessor.</p><p>One mechanism to counter the delays caused by branch instructions is a branch prediction unit. A branch prediction unit is typically configured to provide a branch prediction address in response to receiving the address of a branch instruction. In order to generate a branch prediction address, however, a branch instruction typically must be fetched and decoded. After a branch instruction is fetched and decoded, a branch prediction unit must then spend one or more clock cycles generating a branch prediction address. Although a branch prediction unit reduces the delay associated with branch instructions, significant delays still occur between fetching a branch instruction and generating a corresponding branch prediction address. It would be desirable to minimize the time between fetching a branch instruction and generating a branch prediction address.</p><h4>SUMMARY OF THE INVENTION</h4><p>The problems outlined above are in large part solved by a line predictor configured to speculatively fetch instructions following a branch instruction. The line predictor stores a plurality of lines that each contain instruction line information. Each line stored by the line predictor includes a fetch address, information regarding one or more instructions, and one or more next fetch addresses. In response to receiving a fetch address, the line predictor is configured to provide instruction line information corresponding to the one or more instructions located at the fetch address to an alignment unit. The line predictor is also configured to provide a next fetch address associated with the fetch address to an instruction cache for speculative fetching and to a branch prediction unit for a branch prediction. The next fetch address is further fed back into the line predictor to generate the instruction line information associated with it and a subsequent next fetch address.</p><p>Generally speaking, a next fetch address may be a next sequential fetch address or a branch target address. A next sequential fetch address is the sequential address following the last instruction associated with the instruction line information of the fetch address unless an instruction within the line is a branch instruction. If an instruction within the instruction line information of the fetch address is a branch instruction, the next fetch address may be the target address of the branch instruction.</p><p>The branch prediction unit is configured to generate a branch prediction in response to receiving a next fetch address if a branch instruction is detected in the instruction line information of the next fetch address. The branch prediction is then compared to a subsequent next fetch address. If the branch prediction differs from a subsequent next fetch address, operations that were initiated using the next fetch address are canceled, the subsequent next fetch address is updated in the line predictor, and the updated subsequent next fetch address is refetched. In one particular embodiment, the line predictor contains multiple banks that each contain a plurality of lines for storing instruction line information. In this embodiment, the line predictor can be dual ported to update and refetch the subsequent next fetch address in the same clock cycle.</p><p>Broadly speaking, a microprocessor comprising a line predictor, an instruction cache, and a branch prediction unit is contemplated. The line predictor is configured to store instruction line information regarding two or more instructions terminated by a branch instruction. The line predictor is further configured to store a first fetch address corresponding to the two or more instructions and a second fetch address corresponding to the branch instruction. The instruction cache is coupled to the line predictor and configured to store a plurality of instructions. The branch prediction unit is coupled to the line predictor and is configured to store a plurality of branch predictions. The line predictor is also configured to provide the first fetch address to the instruction cache and the branch prediction unit during a first clock cycle.</p><p>A method for speculatively fetching one or more instructions following a branch instruction is also contemplated. A first fetch address is generated in a line predictor corresponding to a line of instruction line information regarding two or more instructions terminated by a branch instruction during a first clock cycle. A second fetch address is also generated in a line predictor corresponding to a predicted address of the branch instruction during a second clock cycle. The first fetch address is provided to a branch prediction unit during the first clock cycle where a branch prediction is generated for the branch instruction based on the first fetch address.</p><p>A microprocessor is also contemplated comprising a line predictor, an instruction cache, and a branch prediction unit. The line predictor includes a first and a second line of instruction line information. Each line of instruction line information includes a fetch address field configured to store a fetch address, a plurality of instruction/ROP fields, and a next fetch address field configured to store a next fetch address. The instruction cache is coupled to the line predictor and is configured to store a plurality of instructions. The branch prediction unit is coupled to the line predictor and is configured to store a plurality of branch predictions. A last of the plurality of instruction fields in the second line corresponds to a branch instruction and the second next fetch address corresponds to an instruction subsequent to the branch instruction. Also, the line predictor is configured to provide the first next fetch address to the instruction cache and the branch prediction unit during a first clock cycle.</p><p>A computer system comprising a microprocessor and an input/output device coupled to the microprocessor is also contemplated. The microprocessor comprises a line predictor, an instruction cache, and a branch prediction unit. The line predictor is configured to store instruction line information regarding two or more instructions terminated by a branch instruction. The line predictor is further configured to store a first fetch address corresponding to the two or more instructions and a second fetch address corresponding to the branch instruction. The instruction cache is coupled to the line predictor and configured to store a plurality of instructions. The branch prediction unit is coupled to the line predictor and is configured to store a plurality of branch predictions. The line predictor is configured to provide the first fetch address to the instruction cache and the branch prediction unit during a first clock cycle. The input/output device is configured to communicate between the computer system and another computer system coupled to the input/output device.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Other objects and advantages of the invention will become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p><p>FIG. 1 is a block diagram of one embodiment of a processor.</p><p>FIG. 2 is a block diagram of portions of one embodiment of the processor shown in FIG. 1, including a line predictor, an instruction cache, an alignment unit, a branch prediction unit, and a PC silo and redirect unit.</p><p>FIG. 3 is a block diagram of portions of one embodiment of the processor shown in FIG. 1, including a line predictor and a branch prediction unit.</p><p>FIG. 4 is a block diagram illustrating one embodiment of a line predictor.</p><p>FIG. 5 is a block diagram illustrating an alternative embodiment of a line predictor.</p><p>FIG. 6 is a diagram illustrating one embodiment of an instruction information line shown in FIG. <b>4</b>.</p><p>FIG. 7 is a diagram illustrating an alternative embodiment of an instruction information line shown in FIG. <b>4</b>.</p><p>FIG. 8 is a block diagram illustrating one embodiment of a branch prediction unit.</p><p>FIG. 9 is a flow chart illustrating the operation of one embodiment of a line predictor.</p><p>FIG. 10 is a timing diagram illustrating the timing of one embodiment of the microprocessor of FIG. <b>1</b>.</p><p>FIG. 11 is a block diagram of a computer system including the processor shown in FIG. <b>1</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><p>While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><h4>DETAILED DESCRIPTION OF THE INVENTION</h4><p>Turning now to FIG. 1, a block diagram of one embodiment of a processor <b>10</b> is shown. Other embodiments are possible and contemplated. In the embodiment of FIG. 1, processor <b>10</b> includes a line predictor <b>12</b>, an instruction cache (I-cache) <b>14</b>, an alignment unit <b>16</b>, a branch history table <b>18</b>, an indirect address cache <b>20</b>, a return stack <b>22</b>, a decode unit <b>24</b>, a predictor miss decode unit <b>26</b>, a microcode unit <b>28</b>, a map unit <b>30</b>, a map silo <b>32</b>, an architectural renames block <b>34</b>, a pair of instruction queues <b>36</b>A-<b>36</b>B, a pair of register files <b>38</b>A-<b>38</b>B, a pair of execution cores <b>40</b>A-<b>40</b>B, a load/store unit <b>42</b>, a data cache (D-cache) <b>44</b>, an external interface unit <b>46</b>, a PC silo and redirect unit <b>48</b>, and an instruction TLB (ITB) <b>50</b>. Line predictor <b>12</b> is coupled to ITB <b>50</b>, predictor miss decode unit <b>26</b>, branch history table <b>18</b>, indirect address cache <b>20</b>, return stack <b>22</b>, PC silo and redirect block <b>48</b>, alignment unit <b>16</b>, and I-cache <b>14</b>. I-cache <b>14</b> is coupled to alignment unit <b>16</b>. Alignment unit <b>16</b> is further coupled to predictor miss decode unit <b>26</b> and decode unit <b>24</b>. Decode unit <b>24</b> is further coupled to microcode unit <b>28</b> and map unit <b>30</b>. Map unit <b>30</b> is coupled to map silo <b>32</b>, architectural renames block <b>34</b>, instruction queues <b>36</b>A-<b>36</b>B, load/store unit <b>42</b>, execution cores <b>40</b>A-<b>40</b>B, and PC silo and redirect block <b>48</b>. Instruction queues <b>36</b>A-<b>36</b>B are coupled to each other and to respective execution cores <b>40</b>A-<b>40</b>B and register files <b>38</b>A-<b>318</b>B. Register files <b>38</b>A-<b>38</b>B are coupled to each other and respective execution cores <b>40</b>A-<b>40</b>B. Execution cores <b>40</b>A-<b>40</b>B are further coupled to load/store unit <b>42</b>, data cache <b>44</b>, and PC silo and redirect unit <b>48</b>. Load/store unit <b>42</b> is coupled to PC silo and redirect unit <b>48</b>, D-cache <b>44</b>, and external interface unit <b>46</b>. D-cache <b>44</b> is coupled to register files <b>38</b>, and external interface unit <b>46</b> is coupled to an external interface <b>52</b>. Elements referred to herein by a reference numeral followed by a letter will be collectively referred to by the reference numeral alone. For example, instruction queues <b>36</b>A-<b>36</b>B will be collectively referred to as instruction queues <b>36</b>.</p><p>Generally speaking, line predictor <b>12</b> is configured to generate a next fetch address corresponding to the first instruction in a subsequent line of instruction operations. Line predictor <b>12</b> then provides the next fetch address to I-cache <b>14</b>, to branch prediction unit <b>60</b> (see FIG. <b>2</b>), and back into itself. Line predictor <b>12</b> uses the next fetch address to generate a subsequent next fetch address. In one embodiment, line predictor <b>12</b> is configured to select a subsequent next fetch address from either a next sequential address or a target address using a next address selector field. In another embodiment, line predictor <b>12</b> is configured to select a stored subsequent next fetch address. If the line of instruction operations corresponding to the next fetch address terminates in a branch instruction, branch prediction unit <b>60</b> uses the next fetch address to generate a branch prediction. Branch prediction unit <b>60</b> then uses the branch prediction to determine if line predictor <b>12</b> selected the most updated subsequent next fetch address. If branch prediction unit <b>60</b> determines that the most updated subsequent next fetch address was not selected, line predictor <b>12</b> is updated using the branch prediction, operations corresponding to the outdated subsequent next fetch address are canceled, and the updated subsequent next fetch address is fetched.</p><p>In the embodiment of FIG. 1, processor <b>10</b> employs a variable byte length, complex instruction set computing (CISC) instruction set architecture. For example, processor <b>10</b> may employ the x86 instruction set architecture (also referred to as IA-<b>32</b>). Other embodiments may employ other instruction set architectures including fixed length instruction set architectures and reduced instruction set computing (RISC) instruction set architectures. Certain features shown in FIG. 1 may be omitted in such architectures.</p><p>Line predictor <b>12</b> is configured to generate fetch addresses for I-cache <b>14</b> and is additionally configured to provide information regarding a line of instruction operations to alignment unit <b>16</b>. Generally, line predictor <b>12</b> stores information regarding lines of instruction operations previously speculatively fetched by processor <b>10</b> and one or more next fetch addresses corresponding to each line to be selected upon fetch of the line. In one embodiment, line predictor <b>12</b> is configured to store 1K entries, each defining one line of instruction operations. Line predictor <b>12</b> may be banked into, e.g., four banks of <b>256</b> entries each to allow concurrent read and update without dual porting, if desired.</p><p>Line predictor <b>12</b> provides the next fetch address to I-cache <b>14</b> to fetch the corresponding instruction bytes. I-cache <b>14</b> is a high speed cache memory for storing instruction bytes. According to one embodiment I-cache <b>14</b> may comprise, for example, a 256 Kbyte, four way set associative organization employing 64 byte cache lines. However, any I-cache structure may be suitable. Additionally, the next fetch address is provided back to line predictor <b>12</b> as an input to fetch information regarding the corresponding line of instruction operations. The next fetch address may be overridden by an address provided by ITB <b>50</b> in response to exception conditions reported to PC silo and redirect unit <b>48</b>.</p><p>The next fetch address provided by the line predictor may be the address sequential to the last instruction within the line (if the line terminates in a non-branch instruction). Alternatively, the next fetch address may be a target address of a branch instruction terminating the line. In yet another alternative, the line may be terminated by return instruction, in which case the next fetch address is drawn from return stack <b>22</b>.</p><p>Responsive to a fetch address, line predictor <b>12</b> provides information regarding a line of instruction operations beginning at the fetch address to alignment unit <b>16</b>. Alignment unit <b>16</b> receives instruction bytes corresponding to the fetch address from I-cache <b>14</b> and selects instruction bytes into a set of issue positions according to the provided instruction operation information. More particularly, line predictor <b>12</b> provides a shift amount for each instruction within the line instruction operations, and a mapping of the instructions to the set of instruction operations which comprise the line. An instruction may correspond to multiple instruction operations, and hence the shift amount corresponding to that instruction may be used to select instruction bytes into multiple issue positions. An issue position is provided for each possible instruction operation within the line. In one embodiment, a line of instruction operations may include up to 8 instruction operations corresponding to up to 6 instructions. Generally, as used herein, a line of instruction operations refers to a group of instruction operations concurrently issued to decode unit <b>24</b>. The line of instruction operations progresses through the pipeline of microprocessor <b>10</b> to instruction queues <b>36</b> as a unit. Upon being stored in instruction queues <b>36</b>, the individual instruction operations may be executed in any order.</p><p>The issue positions within decode unit <b>24</b> (and the subsequent pipeline stages up to instruction queues <b>36</b>) defining the program order of the instruction operations within the line for the hardware within those pipeline stages. An instruction operation aligned to an issue position by alignment unit <b>16</b> remains in that issue position until it is stored within an instruction queue <b>36</b>A-<b>36</b>B. Accordingly, a first issue position may be referred to as being prior to a second issue position if an instruction operation within the first issue position is prior to an instruction operation concurrently within the second issue position in program order. Similarly, a first issue position may be referred to as being subsequent to a second issue position if an instruction operation within the first issue position is subsequent to instruction operation concurrently within the second issue position in program order. Instruction operations within the issue positions may also be referred to as being prior to or subsequent to other instruction operations within the line.</p><p>As used herein, an instruction operation (or ROP) is an operation which an execution unit within execution cores <b>40</b>A-<b>40</b>B is configured to execute as a single entity. Simple instructions may correspond to a single instruction operation, while more complex instructions may correspond to multiple instruction operations. Certain of the more complex instructions may be implemented within microcode unit <b>28</b> as microcode routines. Furthermore, embodiments employing non-CISC instruction sets may employ a single instruction operation for each instruction (i.e. instruction and instruction operation may be synonymous in such embodiments). In one particular embodiment, a line may comprise up to eight instruction operations corresponding to up to 6 instructions. Additionally, the particular embodiment may terminate a line at less than 6 instructions and/or 8 instruction operations if a branch instruction is detected. Additional restrictions regarding the instruction operations to the line may be employed as desired.</p><p>The next fetch address generated by line predictor <b>12</b> is routed to branch history table <b>18</b>, indirect address cache <b>20</b>, and return stack <b>22</b>. Branch history table <b>18</b> provides a branch history for a conditional branch instruction which may terminate the line identified by the next fetch address. Line predictor <b>12</b> may use the prediction provided by branch history table <b>18</b> to determine if a conditional branch instruction terminating the line should be predicted taken or not taken. In one embodiment, line predictor <b>12</b> may store a branch prediction to be used to select taken or not taken, and branch history table <b>18</b> is used to provide a more accurate prediction which may cancel the line predictor prediction and cause a different next fetch address to be selected. Indirect address cache <b>20</b> is used to predict indirect branch target addresses which change frequently. Line predictor <b>12</b> may store, as a next fetch address, a previously generated indirect target address. Indirect address cache <b>20</b> may override the next fetch address provided by line predictor <b>12</b> if the corresponding line is terminated by an indirect branch instruction. Furthermore, the address subsequent to the last instruction within a line of instruction operations may be pushed on the return stack <b>22</b> if the line is terminated by a subroutine call instruction. Return stack <b>22</b> provides the address stored at its top to line predictor <b>12</b> as a potential next fetch address for lines terminated by a return instruction.</p><p>In addition to providing next fetch address and instruction operation information to the above mentioned blocks, line predictor <b>12</b> is configured to provide next fetch address and instruction operation information to PC silo and redirect unit <b>48</b>. PC silo and redirect unit <b>48</b> stores the fetch address and line information and is responsible for redirecting instruction fetching upon exceptions as well as the orderly retirement of instructions. PC silo and redirect unit <b>48</b> may include a circular buffer for storing fetch address and instruction operation information corresponding to multiple lines of instruction operations which may be outstanding within processor <b>10</b>. Upon retirement of a line of instructions, PC silo and redirect unit <b>48</b> may update branch history table <b>18</b> and indirect address cache <b>20</b> according to the execution of a conditional branch and an indirect branch, respectively. Upon processing an exception, PC silo and redirect unit <b>48</b> may purge entries from return stack <b>22</b> which are subsequent to the exception-causing instruction. Additionally, PC silo and redirect unit <b>48</b> routes an indication of the exception-causing instruction to map unit <b>30</b>, instruction queues <b>36</b>, and load/store unit <b>42</b> so that these units may cancel instructions which are subsequent to the exception-causing instruction and recover speculative state accordingly.</p><p>In one embodiment, PC silo and redirect unit <b>48</b> assigns a sequence number (R#) to each instruction operation to identify the order of instruction operations outstanding within processor <b>10</b>. PC silo and redirect unit <b>48</b> may assign R#s to each possible instruction operation with a line. If a line includes fewer than the maximum number of instruction operations, some of the assigned R#s will not be used for that line. However, PC silo and redirect unit <b>48</b> may be configured to assign the next set of R#s to the next line of instruction operations, and hence the assigned but not used R#s remain unused until the corresponding line of instruction operations is retired. In this fashion, a portion of the R#s assigned to a given line may be used to identify the line within processor <b>10</b>. In one embodiment, a maximum of 8 ROPs may be allocated to a line. Accordingly, the first ROP within each line may be assigned an R# which is a multiple of 8. Unused R#s are accordingly automatically skipped.</p><p>The preceding discussion has described line predictor <b>12</b> predicting next addresses and providing instruction operation information for lines of instruction operations. This operation occurs as long as each fetch address hits in line predictor <b>12</b>. Upon detecting a miss in line predictor <b>12</b>, alignment unit <b>16</b> routes the corresponding instruction bytes from I-cache <b>14</b> to predictor miss decode unit <b>26</b>. Predictor miss decode unit <b>26</b> decodes the instructions beginning at the offset specified by the missing fetch address and generates a line of instruction operation information and a next fetch address. Predictor miss decode unit <b>26</b> enforces any limits on a line of instruction operations as processor <b>10</b> is designed for (e.g. maximum number of instruction operations, maximum number of instructions, terminate on branch instructions, etc.). Upon completing decode of a line, predictor miss decode unit <b>26</b> provides the information to line predictor <b>12</b> for storage. It is noted that predictor miss decode unit <b>26</b> may be configured to dispatch instructions as they are decoded. In FIG. 1, this option is illustrated with a dotted line. Alternatively, predictor miss decode unit <b>26</b> may decode the line of instruction information and provide it to line predictor <b>12</b> for storage. Subsequently, the missing fetch address may be reattempted in line predictor <b>12</b> and a hit may be detected. Furthermore, a hit in line predictor <b>12</b> may be detected and a miss in I-cache <b>14</b> may occur. The corresponding instruction bytes may be fetched through external interface unit <b>46</b> and stored in I-cache <b>14</b>.</p><p>In one embodiment, line predictor <b>12</b> and I-cache <b>14</b> employ physical addressing. However, upon detecting an exception, PC silo and redirect unit <b>48</b> will be supplied a logical (or virtual) address. Accordingly, the redirect addresses are translated by ITB 50 for presentation to line predictor <b>12</b> (and in parallel to I-Cache <b>14</b> for reading the corresponding instruction bytes). Additionally, PC silo and redirect unit <b>48</b> maintains a virtual lookahead PC value for use in PC relative calculations such as relative branch target addresses. The virtual lookahead PC corresponding to each line is translated by ITB 50 to verify that the corresponding physical address matches the physical fetch address produced by line predictor <b>12</b>. If a mismatch occurs, line predictor <b>12</b> is updated with the correct physical address and the correct instructions are fetched. PC silo and redirect unit <b>48</b> further handles exceptions related to fetching beyond protection boundaries, etc. PC silo and redirect unit <b>48</b> also maintains a retire PC value indicating the address of the most recently retired instructions. In the present embodiment, PC silo and redirect unit <b>48</b> may retire a line of instruction operations concurrently. Accordingly, PC silo and redirect unit <b>48</b> may transmit an R# indicative of the line to map unit <b>30</b>, instruction queues <b>36</b>A-<b>36</b>B, and load/store unit <b>42</b>.</p><p>Decode unit <b>24</b> is configured to receive instruction operations from alignment unit <b>16</b> in a plurality of issue positions, as described above. Decode unit <b>24</b> decodes the instruction bytes aligned to each issue position in parallel (along with an indication of which instruction operation corresponding to the instruction bytes is to be generated in a particular issue position). Decode unit <b>24</b> identifies source and destination operands for each instruction operation and generates the instruction operation encoding used by execution cores <b>40</b>A-<b>40</b>B. Decode unit <b>24</b> is also configured to fetch microcode routines from microcode unit <b>28</b> for instructions which are implemented in microcode.</p><p>According to one particular embodiment, the following instruction operations are supported by processor <b>10</b>: integer, floating point add (including multimedia), floating point multiply (including multimedia), branch, load, store address generation, and store data. Each instruction operation may employ up to 2 source register operands and one destination register operand. According to one particular embodiment, a single destination register operand may be assigned to integer ROPs to store both the integer result and a condition code (or flags) update. The corresponding logical registers will both receive the corresponding PR# upon retirement of the integer operation. Certain instructions may generate two instruction operations of the same type to update two destination registers (e.g. POP, which updates the ESP and the specified destination register).</p><p>The decoded instruction operations and source and destination register numbers are provided to map unit <b>30</b>. Map unit <b>30</b> is configured to perform register renaming by assigning physical register numbers (PR#s) to each destination register operand and source register operand of each instruction operation. The physical register numbers identify registers within register files <b>38</b>A-<b>38</b>B. Additionally, map unit <b>30</b> assigns a queue number (IQ#) to each instruction operation, identifying the location within instruction queues <b>36</b>A-<b>36</b>B assigned to store the instruction operation. Map unit <b>30</b> additionally provides an indication of the dependencies for each instruction operation by providing queue numbers of the instructions which update each physical register number assigned to a source operand of the instruction operation. Map unit <b>30</b> updates map silo <b>32</b> with the physical register numbers and instruction to numbers assigned to each instruction operation (as well as the corresponding logical register numbers). Furthermore, map silo <b>32</b> may be configured to store a lookahead state corresponding to the logical registers prior to the line of instructions and an R# identifying the line of instructions with respect to the PC silo. Similar to the PC silo described above, map silo <b>32</b> may comprise a circular buffer of entries. Each entry may be configured to store the information corresponding one line of instruction operations.</p><p>Map unit <b>30</b> and map silo <b>32</b> are further configured to receive a retire indication from PC silo <b>48</b>. Upon retiring a line of instruction operations, map silo <b>32</b> conveys the destination physical register numbers assigned to the line and corresponding logical register numbers to architectural renames block <b>34</b> for storage. Architectural renames block <b>34</b> stores a physical register number corresponding to each logical register, representing the committed register state for each logical register. The physical register numbers displaced from architectural renames block <b>34</b> upon update of the corresponding logical register with a new physical register number are returned to the free list of physical register numbers for allocation to subsequent instructions. In one embodiment, prior to returning a physical register number to the free list, the physical register numbers are compared to the remaining physical register numbers within architectural renames block <b>34</b>. If a physical register number is still represented within architectural renames block <b>34</b> after being displaced, the physical register number is not added to the free list. Such an embodiment may be employed in cases in which the same physical register number is used to store more than one result of an instruction. For example, an embodiment employing the x86 instruction set architecture may provide physical registers large enough to store floating point operands. In this manner, any physical register may be used to store any type of operand. However, integer operands and condition code operands do not fully utilize the space within a given physical register. In such an embodiment, processor <b>10</b> may assign a single physical register to store both integer result and a condition code result of an instruction. A subsequent retirement of an instruction which overwrites the condition code result corresponding to the physical register may not update the same integer register, and hence the physical register may not be free upon committing a new condition code result. Similarly, a subsequent retirement of an instruction which updates the integer register corresponding to the physical register may not update the condition code register, and hence the physical register may not be free upon committing the new integer result.</p><p>Still further, map unit <b>30</b> and map silo <b>32</b> are configured to receive exception indications from PC silo <b>48</b>. Lines of instruction operations subsequent to the line including the exception-causing instruction operation are marked invalid within map silo <b>32</b>. The physical register numbers corresponding to the subsequent lines of instruction operations are freed upon selection of the corresponding lines for retirement (and architectural renames block <b>34</b> is not updated with the invalidated destination registers). Additionally, the lookahead register state maintained by map unit <b>30</b> is restored to the lookahead register state corresponding to the exception-causing instruction.</p><p>The line of instruction operations, source physical register numbers, source queue numbers, and destination physical register numbers are stored into instruction queues <b>36</b>A-<b>36</b>B according to the queue numbers assigned by map unit <b>30</b>. According to one embodiment, instruction queues <b>36</b>A-<b>36</b>B are symmetrical and can store any instructions. Furthermore, dependencies for a particular instruction operation may occur with respect to other instruction operations which are stored in either instruction queue. Map unit <b>30</b> may, for example, store a line of instruction operations into one of instruction queues <b>36</b>A-<b>36</b>B and store a following line of instruction operations into the other one of instruction queues <b>36</b>A-<b>36</b>B. An instruction operation remains in instruction queue <b>36</b>A-<b>36</b>B at least until the instruction operation is scheduled for execution. In one embodiment, instruction operations remain in instruction queues <b>36</b>A-<b>36</b>B until retired.</p><p>Instruction queues <b>36</b>A-<b>36</b>B, upon scheduling a particular instruction operation for execution, determine at which clock cycle that particular instruction operation will update register files <b>38</b>A-<b>38</b>B. Different execution units within execution cores <b>40</b>A-<b>40</b>B may employ different numbers of pipeline stages (and hence different latencies). Furthermore, certain instructions may experience more latency within a pipeline than others. Accordingly, a countdown is generated which measures the latency for the particular instruction operation (in numbers of clock cycles). Instruction queues <b>36</b>A-<b>36</b>B await the specified number of clock cycles (until the update will occur prior to or coincident with the dependent instruction operations reading the register file), and then indicate that instruction operations dependent upon that particular instruction operation may be scheduled. For example, in one particular embodiment dependent instruction operations may be scheduled two clock cycles prior to the instruction operation upon which they depend updating register files <b>38</b>A-<b>38</b>B. Other embodiments may schedule dependent instruction operations at different numbers of clock cycles prior to or subsequent to the instruction operation upon which they depend completing and updating register files <b>38</b>A-<b>38</b>B. Each instruction queue <b>36</b>A-<b>36</b>B maintains the countdowns for instruction operations within that instruction queue, and internally allow dependent instruction operations to be scheduled upon expiration of the countdown. Additionally, the instruction queue provides indications to the other instruction queue upon expiration of the countdown. Subsequently, the other instruction queue may schedule dependent instruction operations. This delayed transmission of instruction operation completions to the other instruction queue allows register files <b>38</b>A-<b>38</b>B to propagate results provided by one of execution cores <b>40</b>A-<b>40</b>B to the other register file. Each of register files <b>38</b>A-<b>38</b>B implements the set of physical registers employed by processor <b>10</b>, and is updated by one of execution cores <b>40</b>A-<b>40</b>B. The updates are then propagated to the other register file. It is noted that instruction queues <b>36</b>A-<b>36</b>B may schedule an instruction once its dependencies have been satisfied (i.e. out of order with respect to its order within the queue).</p><p>Instruction operations scheduled from instruction queue <b>36</b>A read source operands according to the source physical register numbers from register file <b>38</b>A and are conveyed to execution core <b>40</b>A for execution. Execution core <b>40</b>A executes the instruction operation and updates the physical register assigned to the destination within register file <b>38</b>A. Some instruction operations do not have destination registers, and execution core <b>40</b>A does not update a destination physical register in this case. Additionally, execution core <b>40</b>A reports the R# of the instruction operation and exception information regarding the instruction operation (if any) to PC silo and redirect unit <b>48</b>. Instruction queue <b>36</b>B, register file <b>38</b>B, and execution core <b>40</b>B may operate in a similar fashion.</p><p>In one embodiment, execution core <b>40</b>A and execution core <b>40</b>B are symmetrical. Each execution core <b>40</b> may include, for example, a floating point add unit, a floating point multiply unit, two integer units, a branch unit, a load address generation unit, a store address generation unit, and a store data unit. Other configurations of execution units are possible.</p><p>Among the instruction operations which do not have destination registers are store address generations, store data operations, and branch operations. The store address/store data operations provide results to load/store unit <b>42</b>. Load/store unit <b>42</b> provides an interface to D-cache <b>44</b> for performing memory data operations. Execution cores <b>40</b>A-<b>40</b>B execute load ROPs and store address ROPs to generate load and store addresses, respectively, based upon the address operands of the instructions. More particularly, load addresses and store addresses may be presented to D-cache <b>44</b> upon generation thereof by execution cores <b>40</b>A-<b>40</b>B (directly via coupleions between execution cores <b>40</b>A-<b>40</b>B and D-Cache <b>44</b>). Load addresses which hit D-cache <b>44</b> result in data being routed from D-cache <b>44</b> to register files <b>38</b>. On the other hand, store addresses which hit are allocated a store queue entry. Subsequently, the store data is provided by a store data instruction operation (which is used to route the store data from register files <b>38</b>A-<b>38</b>B to load/store unit <b>42</b>). Accordingly, a store may comprise a store address instruction operation and a store data instruction operation in this embodiment. In other words, the store address instruction operation and corresponding store data instruction operation are derived from the same instruction. The store may be an instruction, or may be an implicit portion of another instruction having a memory destination operand. Upon retirement of the store instruction, the data is stored into D-cache <b>44</b>. Additionally, load/store unit <b>42</b> may include a load/store buffer for storing load/store addresses which miss D-cache <b>44</b> for subsequent cache fills (via external interface <b>46</b>) and re-attempting the missing load/store operations. Load/store unit <b>42</b> is further configured to handle load/store memory dependencies.</p><p>Turning now to FIG. 2, a block diagram of one embodiment of portions of processor <b>10</b> is shown. Other embodiments are possible and contemplated. In the embodiment of FIG. 2, a line predictor <b>12</b>, an I-cache <b>14</b>, an alignment unit <b>16</b>, a PC silo and redirect unit <b>48</b>, and a branch prediction unit <b>60</b> are shown. Line predictor <b>12</b> is coupled to I-cache <b>14</b>, PC silo and redirect unit <b>48</b>, and branch prediction unit <b>60</b> via bus <b>310</b>. Line predictor is further coupled to alignment unit <b>16</b> and PC silo and redirect via bus <b>314</b>. I-cache <b>14</b> is coupled to alignment unit <b>16</b> via bus <b>320</b>. PC silo and redirect unit <b>48</b> is coupled to branch prediction unit <b>60</b> via bus <b>316</b>. Branch prediction unit <b>60</b> is also coupled to line predictor <b>12</b> via bus <b>312</b>. Lastly, bus <b>318</b> is coupled to PC silo and redirect to indicate a branch misprediction. In one embodiment not shown in FIG. 2, branch mispredictions are generated in either execution core <b>0</b> or execution core <b>1</b> and transmitted to PC silo and redirect via bus <b>318</b>.</p><p>Line predictor <b>12</b> is configured to generate fetch addresses for I-cache <b>14</b> and is additionally configured to provide information regarding a line of instruction operations to alignment unit <b>16</b>. Generally, line predictor <b>12</b> stores information regarding lines of instruction operations previously speculatively fetched by processor <b>10</b> and one or more next fetch addresses corresponding to each line to be selected upon fetch of the line. In one embodiment, line predictor <b>12</b> is configured to store 1K entries, each defining one line of instruction operations. Line predictor <b>12</b> may be banked into, e.g., four banks of 256 entries each to allow concurrent read and update without dual porting, if desired. Other banking schemes are possible and contemplated.</p><p>Line predictor <b>12</b> is configured to provide a next fetch address to I-cache <b>14</b> to fetch the corresponding instruction bytes and to branch prediction unit <b>60</b> to generate a branch prediction. Additionally, the next fetch address is provided back to line predictor <b>12</b> as an input to fetch information regarding the corresponding line of instruction operations. The next fetch address provided by the line predictor may be the address sequential to the last instruction within the line (if the line terminates in a non-branch instruction). Alternatively, the next fetch address may be a target address of a branch instruction terminating the line.</p><p>Responsive to a fetch address, line predictor <b>12</b> provides information regarding a line of instruction operations beginning at the fetch address to alignment unit <b>16</b>. Alignment unit <b>16</b> receives instruction bytes corresponding to the fetch address from I-cache <b>14</b> and selects instruction bytes into a set of issue positions according to the provided instruction operation information. More particularly, line predictor <b>12</b> provides a shift amount for each instruction within the line instruction operations, and a mapping of the instructions to the set of instruction operations which comprise the line. An instruction may correspond to multiple instruction operations, and hence the shift amount corresponding to that instruction may be used to select instruction bytes into multiple issue positions. An issue position is provided for each possible instruction operation within the line. In one embodiment, a line of instruction operations may include up to 8 instruction operations corresponding to up to 6 instructions. The embodiment may terminate a line at less than 6 instructions and/or 8 instruction operations if a branch instruction is detected. Additional restrictions regarding the instruction operations to the line may be employed as desired.</p><p>In one embodiment of line predictor <b>12</b>, a stored line of instruction line information in line predictor <b>12</b> may comprise information for up to eight instruction operations corresponding to up to 6 instructions in addition to a shift amount for the 6 instructions. The stored line of instruction line information may also include an indication of whether the line includes a branch instruction. Accordingly, the stored line r instruction line information corresponds to a line of instruction operations as may be routed to decode unit <b>24</b>.</p><p>The next fetch address generated by line predictor <b>12</b> is routed to branch prediction unit <b>60</b> in addition to being routed back into line predictor <b>12</b>. In one embodiment, branch prediction unit may comprise branch history table <b>18</b>, indirect address cache <b>20</b>, and/or return stack <b>22</b> (see FIG. <b>1</b> and FIG. <b>8</b>). Other embodiments of branch prediction unit are possible and contemplated. If the line of instruction operations corresponding to the next fetch address terminates in a branch instruction, branch prediction unit <b>60</b> is configured to generate a branch prediction. Branch prediction unit <b>60</b> is configured to generate a branch prediction based upon the next fetch address as distinguished from the actual address corresponding to the branch instruction. Generally speaking, a next fetch address corresponds to the address of the first instruction in a particular line. If a branch instruction is the first instruction in a particular line, then the address corresponding to the branch instruction will be the same as the next fetch address corresponding to that line. If a branch instruction is not the first instruction in a particular line, however, then branch prediction unit <b>60</b> will use the next fetch address and not the address of the branch instruction to determine the branch prediction.</p><p>After generating a branch prediction, branch prediction unit <b>60</b> compares the branch prediction to a subsequent next fetch address generated by line predictor <b>12</b> using the next fetch address. If the branch prediction does not correspond to the subsequent next fetch address, then operations based upon the subsequent next fetch address are canceled and instructions based upon the branch prediction are fetched. In addition, line predictor <b>12</b> is updated using the branch prediction. Specifically, the branch prediction is used to update the subsequent next fetch address corresponding to the next fetch address. In one embodiment, a branch prediction comprises one or more bits that indicate whether a branch instruction is predicted taken or not taken. In an alternative embodiment, a branch prediction may be the address most recently used by the branch instruction. In another alternative embodiment, a branch prediction may be a pre-calculated or a historybased address generated by branch prediction unit <b>60</b>.</p><p>In addition to providing next fetch address and instruction operation information as described above, line predictor <b>12</b> is configured to provide next fetch address and instruction operation information to PC silo and redirect unit <b>48</b>. PC silo and redirect unit <b>48</b> stores the next fetch address and line information and is responsible for redirecting instruction fetching upon exceptions as well as the orderly retirement of instructions. PC silo and redirect unit <b>48</b> may include a circular buffer for storing next fetch address and instruction operation information corresponding to multiple lines of instruction operations which may be outstanding within processor <b>10</b>. Upon retirement of a line of instructions, PC silo and redirect unit <b>48</b> may update branch prediction unit <b>60</b> according to the execution of a branch instruction.</p><p>Turning now to FIG. 3, a block diagram depicting one embodiment of a processor <b>10</b> is shown. Other embodiments are possible and contemplated. In the embodiment of FIG. 3, a line predictor <b>12</b>, a multiplexer <b>70</b>, and a branch prediction unit <b>60</b> are shown. Branch prediction unit <b>60</b> is shown comprising branch history table <b>18</b> and prediction verification logic <b>74</b>. Branch prediction unit <b>60</b> may also include indirect address cache <b>20</b> and return stack <b>22</b> as shown in FIG. <b>8</b>. Other embodiments of branch prediction unit <b>60</b> are possible and contemplated. Line predictor <b>12</b> is coupled to multiplexer <b>70</b> via buses <b>410</b>, <b>412</b> and conductor <b>414</b> and to branch prediction unit <b>60</b> via conductor <b>414</b>. Multiplexer <b>70</b> is coupled to branch prediction unit via bus <b>310</b> and back into line predictor <b>12</b> via bus <b>310</b>. Multiplexer <b>70</b> is shown in FIG. 3 to be separate from line predictor <b>12</b> and branch prediction unit <b>60</b> for illustrative purposes. In other embodiments, multiplexer <b>70</b> may be located within line predictor <b>12</b> or branch prediction unit <b>60</b> or in other functional blocks. In addition, in certain embodiments multiplexer <b>70</b> may be omitted entirely or replaced by an equivalent logical structure.</p><p>In the embodiment shown in FIG. 3, line predictor <b>12</b> provides a next sequential address, a target address, and a next address selector to multiplexer <b>70</b>. In response, multiplexer <b>70</b> generates a next fetch address by selecting either the next sequential address from bus <b>410</b> or the target address from bus <b>412</b> using the next address selector provided on conductor <b>414</b>. In one embodiment, the next address selector comprises one or more bits stored in each line of instruction information in the line predictor. The value of the next address selector is set to select either the next sequential address or the target address.</p><p>Once generated, the next fetch address is provided to branch prediction unit <b>60</b> and is fed back into line predictor <b>12</b>. If the line of instructions corresponding to the next fetch address, i.e. the line of instructions containing the instruction identified by the next fetch address, terminates in a branch instruction, then branch prediction unit <b>60</b> is configured to generate a branch prediction in response to receiving the next fetch address.</p><p>In the embodiment of FIG. 3, a branch prediction comprises one or more bits that indicate whether a branch instruction is taken or not taken. Branch predictions are stored in branch history table <b>18</b>. In one particular embodiment of FIG. 3, a branch prediction comprises a two bit bi-modal counter. A bi-modal counter is configured to indicate one of two possible states based upon one or more of the prior states. For example, a bi-modal counter employed in FIG. 3 could be configured to predict a branch as taken only after determining that the branch was taken the two previous times the instruction was executed. Conversely, the bi-modal counter could be configured to predict a branch as not taken only after determining that branch was not taken the two previous times the instruction was executed. Other configurations of a bi-modal counter are apparent. In an alternative embodiment, a branch prediction may be the actual address most recently selected upon executing a branch instruction. In another alternative embodiment, a branch prediction may be a pre-calculated or history-based address generated by branch prediction unit <b>60</b>.</p><p>As shown in FIG. 3, a branch prediction is provided to prediction verification logic <b>74</b> along with the next address selector. Prediction verification logic <b>74</b> is configured to compare the branch prediction and the next address selector. If the branch prediction and the next address selector match, then line predictor <b>12</b> has predicted the most recent next fetch address and instruction fetching and execution continue normally. If the branch prediction and the next address selector differ, then line predictor <b>12</b> has not predicted the most recent next fetch address and instruction fetching and execution corresponding to the mispredicted next fetch address are canceled. In addition, the most recent next fetch address is refetched and the mispredicted next fetch address is updated into line predictor <b>12</b>.</p><p>After discovering a mispredicted next fetch address, the next address selector for a particular line of instruction information in line predictor <b>12</b> is updated using the branch prediction. Also, the most recent next fetch address is input into line predictor <b>12</b> to fetch the proper line of instruction information. In one embodiment, line predictor is banked and dual ported as discussed above with regard to FIG. <b>1</b> and FIG. 2 (see also FIG. 5 discussed below). If line predictor <b>12</b> is banked and dual ported, then the updated branch prediction can be written as the next address selector during the same clock cycle. With banking and dual porting, the minimum delay associated with a mispredicted fetch address would be one clock cycle. In another embodiment where line predictor <b>12</b> is not banked and/or dual ported, the steps of updating line predictor <b>12</b> and refetching using the most recent next fetch address are accomplished in successive clock cycles. This embodiment would produce at least a two cycle delay in processor <b>10</b> for each mispredicted next fetch address.</p><p>Turning now to FIG. 4, a block diagram of one embodiment of line predictor <b>12</b> is shown. Other embodiments are possible and contemplated. Line predictor <b>12</b> includes a storage device <b>400</b> configured to store a plurality of instruction information lines <b>420</b>A-N. Storage device <b>400</b> could include any conventional storage structure including a content addressable memory array. In addition, storage device <b>400</b> could include any number of instruction information lines <b>420</b>A-N. In one embodiment, storage device <b>400</b> is configured to store 1K instruction information lines. Line predictor <b>12</b> is configured to receive a next fetch address via bus <b>310</b> and to access the instruction information line associated with the next fetch address in storage device <b>400</b>. Line predictor <b>12</b> is further configured to provide instruction information on bus <b>314</b> and a subsequent next fetch address on bus <b>310</b> associated with the next fetch address. As can be seen on FIG. 2, instruction information provided on bus <b>314</b> can be sent to the alignment unit <b>16</b> and PC silo and redirect unit <b>48</b>. In addition, a subsequent next fetch address provided on bus <b>310</b> can be sent to I-cache <b>14</b>, branch prediction unit <b>60</b>, and PC silo and redirect unit <b>48</b> and can be input back into the line predictor <b>12</b> as a next fetch address.</p><p>Turning now to FIG. 5, a block diagram of another embodiment of line predictor <b>12</b> is shown. Other embodiments are possible and contemplated. Line predictor <b>12</b> includes a plurality of storage devices <b>400</b>A-N (collectively referred to as storage devices <b>400</b>) that are each configured to store a plurality of instruction information lines <b>420</b>A-N. The storage devices <b>400</b> can also be referred to as instruction line information banks <b>0</b>-N as shown. The storage devices <b>400</b> could include any conventional storage structures including content addressable memory arrays. In addition, each one of storage devices <b>400</b> could include any number of instruction information lines <b>420</b>A-N. In one embodiment, line predictor <b>12</b> includes four storage devices <b>400</b> that are each configured to store 256 instruction information lines. Line predictor <b>12</b> is configured to receive a next fetch address via bus <b>310</b> and to access the instruction information line associated with the next fetch address in one of the storage devices <b>400</b>. Line predictor <b>12</b> is farther configured to provide instruction information on bus <b>314</b> and a subsequent next fetch address on bus <b>310</b> associated with the next fetch address. As can be seen on FIG. 2, instruction information provided on bus <b>314</b> can be sent to the alignment unit <b>16</b> and PC silo and redirect unit <b>48</b>. In addition, a subsequent next fetch address provided on bus <b>310</b> can be sent to I-cache <b>14</b>, branch prediction unit <b>60</b>, and PC silo and redirect unit <b>48</b> and can be input back into the line predictor <b>12</b> as a next fetch address.</p><p>Turning now to FIG. 6, a diagram of one embodiment of an instruction information line <b>420</b> shown in FIG. 4 is depicted. Other embodiments are possible and contemplated. Instruction line information <b>420</b> includes a fetch address field <b>601</b>, a plurality of instruction/ROP fields <b>602</b>, a next address selector field <b>603</b>, a next sequential address field <b>604</b>, a target address field <b>605</b>, and a control field <b>606</b>.</p><p>The plurality of instruction/ROP fields <b>602</b> is configured to store information for one or more instructions and ROPS. In one embodiment, the plurality of instruction/ROP fields include six instruction information fields and eight ROP information fields. These instruction information fields and ROP information fields are configured to store information for up to eight ROPs that corresponds to up to six instructions. The instruction information fields can be configured to store shift amounts for instructions and to provide a mapping of the instructions within a line of instructions. The ROP information fields may be used to select a particular ROP instruction. It is noted that the line predictor could be configured to store any combination of instruction and ROP information fields.</p><p>Fetch address field <b>601</b> is configured to store the fetch address associated with instruction information line <b>420</b>. In one embodiment, the fetch address stored in fetch address field <b>601</b> corresponds to the instruction whose information is stored in the first of the plurality of instruction information fields. The next address selector field <b>603</b> is configured to store one or more bits indicating the next fetch address to be selected by line predictor <b>12</b>. In one embodiment, the next fetch address selector is used to select the next fetch address from either a next sequential address stored the next sequential address field or a target address stored in the target address field and hence one bit is used. In the absence of a branch instruction associated with one of the instruction information fields, the next fetch address selector field is generally set to select the next sequential address. If one of the instruction information fields is associated with a branch instruction, then the next address selector field is generally set based upon whether the branch was taken or not taken in one or more of the recent executions of the instruction.</p><p>Next sequential address field <b>604</b> is configured to store the next sequential address corresponding to the instruction following the one or more instructions whose information is stored in instruction line information <b>420</b>. In one embodiment, the next sequential address field stores the address subsequent to the address of the instruction whose information is stored in the last of the instruction information fields. The target address field <b>605</b> is configured to store the target address associated with a branch instruction whose information is stored in one of the instruction information fields. Control field <b>606</b> is configured to store bits indicating whether the instruction line information includes a branch instruction. If control field <b>606</b> indicates the presence of a branch instruction, line predictor <b>12</b> may be configured to select the target address from target address field <b>605</b> using next address selector <b>603</b>.</p><p>Turning now to FIG. 7, a diagram illustrating an alternative embodiment of an instruction information line shown in FIG. 4 is depicted. Other embodiments are possible and contemplated. Instruction line information <b>420</b> includes a fetch address field <b>701</b>, a plurality of instruction/ROP fields <b>702</b>, a next fetch address field <b>703</b>, and a control field <b>704</b>.</p><p>The plurality of instruction/ROP fields <b>702</b> is configured to store information for one or more instructions and ROPs. In one embodiment, the plurality if instruction/ROP fields include six instruction information fields and eight ROP information fields. These instruction information fields and ROP information fields are configured to store information for up to eight ROPs that corresponds to up to six instructions. The instruction information fields can be configured to store shift amounts for instructions and to provide a mapping of the instructions within a line of instructions. The ROP information fields may be used to select a particular ROP instruction. It is noted that the line predictor could be configured to store any combination of instruction and ROP information fields.</p><p>Fetch address field <b>701</b> is configured to store the fetch address associated with instruction information line <b>420</b>. In one embodiment, the fetch address stored in fetch address field <b>701</b> corresponds to the instruction whose information is stored in the first of the plurality of instruction information fields. Next fetch address field <b>703</b> is configured to store a predicted next fetch address. The predicted next fetch address will generally correspond to the next sequential address for the last instruction whose information is contain in the last of the instruction information fields. If information corresponding to a branch instruction appears in one of the instruction information fields, however, the predicted next fetch address may correspond to a predicted target address of the branch instruction.</p><p>Control field <b>704</b> is configured to store bits indicating whether the instruction line information includes a branch instruction. If control field <b>704</b> indicates the presence of a branch instruction, line predictor <b>12</b> may be configured to store a predicted target address of the branch instruction in next fetch address field <b>703</b>.</p><p>Turning now to FIG. 8, a block diagram of one embodiment of branch prediction unit <b>60</b> is shown. Other embodiments are possible and contemplated. As shown, this embodiment of branch prediction unit <b>60</b> comprises a branch history table <b>18</b>, an indirect address cache <b>20</b>, and a return stack <b>22</b>. Branch prediction unit <b>60</b> is configured to receive a next fetch address on bus <b>310</b> and provide that address to branch history table <b>18</b>, indirect address cache <b>20</b>, and return stack <b>22</b>. Branch prediction unit is also configured to convey a branch prediction on bus <b>416</b> from either branch history table <b>18</b>, indirect address cache <b>20</b>, or return stack <b>22</b>.</p><p>Turning now to FIG. 9, a flow chart illustrating the operation of one embodiment of a line predictor. Other embodiments are possible and contemplated. Block <b>901</b> depicts generating a fetch address in line predictor. Block <b>902</b> depicts providing the fetch address to branch prediction unit and line predictor input. Block <b>903</b>A depicts generating a next fetch address in line predictor. Block <b>903</b>B depicts generating a branch prediction in branch prediction unit. Decision block <b>904</b> depicts determining if the branch prediction and next fetch address differ. Block <b>905</b> depicts updating the next fetch address in line predictor and refetching the next fetch address if operation shown in decision block <b>904</b> is yes. Block <b>906</b> depicts continuing execution if the operation shown in decision block <b>904</b> is no.</p><p>In one embodiment, FIG. 9 illustrates the method of operation of line predictor <b>12</b> within processor <b>10</b>. A fetch address is generated in line predictor <b>12</b> and provided to branch prediction unit <b>60</b> and line predictor <b>12</b> as shown in blocks <b>901</b> and <b>902</b>. A next fetch address is generated in line predictor <b>12</b> based on the fetch address as shown in block <b>903</b>A. In addition, a branch prediction is generated in branch prediction unit <b>60</b> as shown in block <b>903</b>B. The branch prediction and the next fetch address are then compared as determined in block <b>904</b>. If the branch prediction and next fetch address differ, then operations initiated by the next fetch address are canceled, the next fetch address is updated in line predictor <b>12</b>, and the next fetch address is refetched as shown in block <b>905</b>. If the branch prediction and next fetch address do not differ, execution continues normally.</p><p>Turning now to FIG. 10, a timing diagram illustrating the timing of one embodiment of the microprocessor of FIG. 1 is shown. Other embodiments are possible and contemplated. The diagram illustrates events that happen during particular clock cycles if a branch instruction is found in a fetched instruction line. Each column represents a clock cycle and is labeled with the clock cycle number at the top of each column.</p><p>In clock cycle 0, address A is generated in the line predictor and provided to I-cache, line predictor, and branch prediction unit. In clock cycle 1, address A's instructions are fetched in I-cache <b>14</b> and a branch prediction is generated in branch prediction unit <b>60</b> using address A. In addition, address B is generated in the line predictor using address A. In clock cycle 2, address B is compared with the branch prediction. If address B differs from the branch prediction, address B operations are canceled, line predictor is updated using the branch prediction, and an address corresponding to the branch prediction is fetched. In one embodiment, the events in clock cycles 0, 1, and 2 occur consecutively without any intervening clock cycles. In this embodiment, processor <b>10</b> is configured to encounter a one clock cycle delay if a branch instruction is mispredicted. In another embodiment, one or more clock cycles occur between clock cycles 0, 1, and 2. In a further embodiment, one or more of the events described occur in either the clock cycle precedent to or subsequent to the clock cycle in which it is depicted in FIG. <b>10</b>.</p><p>Turning now to FIG. 11, a block diagram of one embodiment of a computer system <b>200</b> including processor <b>10</b> coupled to a variety of system components through a bus bridge <b>202</b> is shown. Other embodiments are possible and contemplated. In the depicted system, a main memory <b>204</b> is coupled to bus bridge <b>202</b> through a memory bus <b>206</b>, and a graphics controller <b>208</b> is coupled to bus bridge <b>202</b> through an AGP bus <b>210</b>. Finally, a plurality of PCI devices <b>212</b>A-<b>212</b>B are coupled to bus bridge <b>202</b> through a PCI bus <b>214</b>. A secondary bus bridge <b>216</b> may further be provided to accommodate an electrical interface to one or more EISA or ISA devices <b>218</b> through an EISA/ISA bus <b>220</b>. Processor <b>10</b> is coupled to bus bridge <b>202</b> through external interface <b>52</b>.</p><p>Bus bridge <b>202</b> provides an interface between processor <b>10</b>, main memory <b>204</b>, graphics controller <b>208</b>, and devices attached to PCI bus <b>214</b>. When an operation is received from one of the devices coupled to bus bridge <b>202</b>, bus bridge <b>202</b> identifies the target of the operation (e.g. a particular device or, in the case of PCI bus <b>214</b>, that the target is on PCI bus <b>214</b>). Bus bridge <b>202</b> routes the operation to the targeted device. Bus bridge <b>202</b> generally translates an operation from the protocol used by the source device or bus to the protocol used by the target device or bus.</p><p>In addition to providing an interface to an ISA/EISA bus for PCI bus <b>214</b>, secondary bus bridge <b>216</b> may further incorporate additional functionality, as desired. An input/output controller (not shown), either external from or integrated with secondary bus bridge <b>216</b>, may also be included within computer system <b>200</b> to provide operational support for a keyboard and mouse <b>222</b> and for various serial and parallel ports, as desired. An external cache unit (not shown) may further be coupled to external interface <b>52</b> between processor <b>10</b> and bus bridge <b>202</b> in other embodiments. Alternatively, the external cache may be coupled to bus bridge <b>202</b> and cache control logic for the external cache may be integrated into bus bridge <b>202</b>.</p><p>Main memory <b>204</b> is a memory in which application programs are stored and from which processor <b>10</b> primarily executes. A suitable main memory <b>204</b> comprises DRAM (Dynamic Random Access Memory). For example, main memory <b>204</b> may comprise a plurality of banks of SDRAM (Synchronous DRAM). Alternatively, main memory <b>204</b> may comprise RAMBUS DRAM (RDRAM) or any other suitable DRAM.</p><p>PCI devices <b>212</b>A-<b>212</b>B are illustrative of a variety of peripheral devices such as, for example, network interface cards, video accelerators, audio cards, hard or floppy disk drives or drive controllers, SCSI (Small Computer Systems Interface) adapters and telephony cards. Similarly, ISA device <b>218</b> is illustrative of various types of peripheral devices, such as a modem, a sound card, and a variety of data acquisition cards such as GPIB or field bus interface cards.</p><p>Graphics controller <b>208</b> is provided to control the rendering of text and images on a display <b>226</b>. Graphics controller <b>208</b> may embody a typical graphics accelerator generally known in the art to render three-dimensional data structures which can be effectively shifted into and from main memory <b>204</b>. Graphics controller <b>208</b> may therefore be a master of AGP bus <b>210</b> in that it can request and receive access to a target interface within bus bridge <b>202</b> to thereby obtain access to main memory <b>204</b>. A dedicated graphics bus accommodates rapid retrieval of data from main memory <b>204</b>. For certain operations, graphics controller <b>208</b> may further be configured to generate PCI protocol transactions on AGP bus <b>210</b>. The AGP interface of bus bridge <b>202</b> may thus include functionality to support both AGP protocol transactions as well as PCI protocol target and initiator transactions. Display <b>226</b> is any electronic display upon which an image or text can be presented. A suitable display <b>226</b> includes a cathode ray tube (\u201cCRT\u201d), a liquid crystal display (\u201cLCD\u201d), etc.</p><p>It is noted that, while the AGP, PCI, and ISA or EISA buses have been used as examples in the above description, any bus architectures may be substituted as desired. It is further noted that computer system <b>200</b> may be a multiprocessing computer system including additional processors (e.g. processor <b>10</b><i>a </i>shown as an optional component of computer system <b>200</b>). Processor <b>10</b><i>a </i>may be similar to processor <b>10</b>. More particularly, processor <b>10</b><i>a </i>may be an identical copy of processor <b>10</b>. Processor <b>10</b><i>a </i>may share external interface <b>52</b> with processor <b>10</b> (as shown in FIG. 9) or may be coupled to bus bridge <b>202</b> via an independent bus.</p><p>It is noted that various signals are described as being asserted and deasserted herein. A particular signal may be defined to be asserted when carrying a logical one value and deasserted when carrying a logical zero value. Alternatively, a particular signal may be defined to be asserted when carrying a logical zero value and deasserted when carrying a logical one value. It is a matter of design choice which definition is applied to a particular signal. Additionally, components that are said to be coupled together include any direct or indirect method that allows communication between the described components.</p><p>Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "David B.", "last_name": "Witt", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "ADVANCED MICRO DEVICES, INC."}, {"first_name": "", "last_name": "GLOBALFOUNDRIES U.S. INC.", "name": ""}, {"first_name": "", "last_name": "GLOBALFOUNDRIES INC.", "name": ""}, {"first_name": "", "last_name": "WILMINGTON TRUST, NATIONAL ASSOCIATION", "name": ""}, {"first_name": "", "last_name": "GLOBALFOUNDRIES INC.", "name": ""}, {"first_name": "", "last_name": "ADVANCED MICRO DEVICES, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F   9/32"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/38        20060101A I20051008RMEP"}, {"label": "G06F   9/30        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "712240"}, {"primary": false, "label": "712237"}, {"primary": false, "label": "712E09057"}, {"primary": false, "label": "712E0906"}, {"primary": false, "label": "712E09029"}], "ecla_classes": [{"label": "G06F   9/38B2B"}, {"label": "G06F   9/38B9"}, {"label": "G06F   9/30T2"}, {"label": "G06F   9/38H"}], "cpc_classes": [{"label": "G06F   9/30149"}, {"label": "G06F   9/3806"}, {"label": "G06F   9/322"}, {"label": "G06F   9/3806"}, {"label": "G06F   9/30149"}, {"label": "G06F   9/3861"}, {"label": "G06F   9/3861"}, {"label": "G06F   9/3816"}, {"label": "G06F   9/322"}, {"label": "G06F   9/3816"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1999-01-19", "application_date": "1999-01-19", "family_members": [{"ucid": "US-6332191-B1", "titles": [{"lang": "EN", "text": "System for canceling speculatively fetched instructions following a branch mis-prediction in a microprocessor"}]}]}