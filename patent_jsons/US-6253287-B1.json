{"patent_number": "US-6253287-B1", "publication_id": 72695570, "family_id": 22533970, "publication_date": "2001-06-26", "titles": [{"lang": "EN", "text": "Using three-dimensional storage to make variable-length instructions appear uniform in two dimensions"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72575150\"><p>A microprocessor capable of predecoding variable-length instructions and storing them in a three-dimensional instruction cache is disclosed. The microprocessor may comprise a predecode unit, an instruction cache, and an address translation table. The predecode unit receives variable-length instructions from a main memory subsystem. These instructions are then predecoded by detecting instruction field boundaries within each variable-length instruction. Instructions fields that are not present in a particular instruction may be added by inserting padding constants so that the instruction matches a predetermined format having all instruction fields. The predecoded instruction is stored in the instruction cache, which may be logically and physically structured as a three-dimensional array. Each instruction is stored in the cache so that it has a fixed length in two dimensions. The address translation table maintains address translations for each instruction stored in the instruction cache. Fetch addresses are input to the address translation table and, if there is a cache hit, corresponding pointers that points to the desired instruction storage locations within the instruction cache are output. The address translation table may maintain more than one pointer for each fetch address and may also store branch prediction information. A corresponding method and computer system are also disclosed.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00001\" num=\"1\"><claim-text>1. A microprocessor comprising:</claim-text><claim-text>a predecode unit coupled to receive variable-length instructions from a main memory subsystem, wherein the predecode unit is configured to predecode the variable-length instructions by detecting instruction field boundaries within each variable-length instruction; and </claim-text><claim-text>an instruction cache coupled to the predecode unit, wherein the instruction cache comprises a plurality of two-dimensional arrays of instruction field storage locations, wherein each array of instruction field storage locations is configured to store a particular instruction field. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00002\" num=\"2\"><claim-text>2. The microprocessor as recited in claim <b>1</b>, further comprising an address translation table coupled to the instruction cache, wherein the address translation table comprises a plurality of address entries, wherein each address entry comprises at least a portion of an instruction address and a corresponding first pointer that points to a particular instruction field storage location within one or more of the arrays in the instruction cache.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00003\" num=\"3\"><claim-text>3. The microprocessor as recited in claim <b>2</b>, wherein the instruction fields include at least prefix fields, opcode fields, and immediate data fields.</claim-text></claim>"}, {"num": 4, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00004\" num=\"4\"><claim-text>4. A microprocessor comprising:</claim-text><claim-text>a predecode unit coupled to receive variable-length instructions from a main memory subsystem, wherein the predecode unit is configured to predecode the variable-length instructions by detecting instruction field boundaries within each variable-length instruction; and </claim-text><claim-text>an instruction cache coupled to the predecode unit, wherein the instruction cache comprises an array of instruction storage locations, wherein each instruction storage location comprises an array of individual, fixed length, instruction field storage locations, wherein each instruction field storage location is configured to store a particular type of instruction field and has a fixed length that is at least long enough to store the maximum number of instruction bytes possible for the corresponding type of instruction field, and wherein the instruction cache is configured as a three-dimensional array of memory cells, wherein one of the three dimensions comprises different die layers. </claim-text></claim>"}, {"num": 5, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00005\" num=\"5\"><claim-text>5. The microprocessor as recited in claim <b>4</b>, wherein each instruction field storage location comprises a plurality of byte storage locations, wherein each byte is stored on one of the different die layers.</claim-text></claim>"}, {"num": 6, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00006\" num=\"6\"><claim-text>6. The microprocessor as recited in claim <b>4</b>, wherein each instruction field storage location comprises a plurality of bit storage locations, wherein each bit storage location is on one of the different die layers.</claim-text></claim>"}, {"num": 7, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00007\" num=\"7\"><claim-text>7. The microprocessor as recited in claim <b>4</b>, further comprising an address translation table coupled to the instruction cache, wherein the address translation table comprises a plurality of address entries, wherein each address entry comprises at least a portion of an instruction address and a corresponding first pointer that points to a particular instruction storage location within the instruction cache, wherein each instruction field storage location further comprises storage for a valid bit.</claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00008\" num=\"8\"><claim-text>8. The microprocessor as recited in claim <b>7</b>, wherein each storage location in the address translation table further comprises storage for a second pointer, wherein the second pointer indicates the instruction storage location storing the instruction that follows, in program order, the instruction corresponding to the first pointer.</claim-text></claim>"}, {"num": 9, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00009\" num=\"9\"><claim-text>9. The microprocessor as recited in claim <b>7</b>, wherein each storage location in the address translation table further comprises storage for a run value, wherein the run value indicates the number of sequential instructions following, in program order, the first instruction, wherein the sequential instructions comprise non-branch instructions and branch instruction that are predicted not taken.</claim-text></claim>"}, {"num": 10, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00010\" num=\"10\"><claim-text>10. The microprocessor as recited in claim <b>7</b>, wherein each storage location in the address translation table further comprises storage for branch prediction information.</claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00011\" num=\"11\"><claim-text>11. A microprocessor comprising:</claim-text><claim-text>a means for predecoding coupled to receive variable-length instructions from a main memory subsystem, wherein the means for predecoding is configured to predecode the variable-length instructions by detecting field boundaries within each variable-length instruction; and </claim-text><claim-text>a cache coupled to the predecode unit, wherein the cache comprises a fixed number of fixed-length instruction storage locations configured into an array, wherein each instruction storage location is configured to receive and store one variable-length instruction, wherein each instruction storage location comprises a plurality of sets of memory cells, wherein each set is configured to store a particular instruction field or a constant in the event that the particular instruction field is not present in the stored instruction, wherein the cache is configured as a three-dimensional array, and wherein one dimension of said three-dimensional array comprises different layers of the microprocessor's die. </claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00012\" num=\"12\"><claim-text>12. The microprocessor as recited in claim <b>11</b>, wherein the variable-length instructions are x86 instructions.</claim-text></claim>"}, {"num": 13, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00013\" num=\"13\"><claim-text>13. The microprocessor as recited in claim <b>11</b>, further comprising an address translation table coupled to the instruction cache, wherein the address translation table comprises a plurality of address entries, wherein each address entry comprises at least a portion of an instruction address and a corresponding first pointer that points to the first instruction field of particular instructions stored within the instruction cache.</claim-text></claim>"}, {"num": 14, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00014\" num=\"14\"><claim-text>14. A microprocessor comprising:</claim-text><claim-text>a predecode unit coupled to receive variable-length instructions from a main memory subsystem, wherein the predecode unit is configured to predecode the variable-length instructions by detecting field boundaries within each variable-length instruction; and </claim-text><claim-text>an instruction cache coupled to the predecode unit, wherein the instruction cache comprises a fixed number of fixed-length instruction storage locations configured into an array, wherein each instruction storage location is configured to receive and store one variable-length instruction, wherein each instruction storage location comprises a plurality of sets of memory cells, wherein each set is configured to store a particular instruction field or a constant in the event that the particular instruction field is not present in the stored instruction, wherein each address entry further comprises additional pointers that point to the remaining instruction fields of the particular instructions. </claim-text></claim>"}, {"num": 15, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00015\" num=\"15\"><claim-text>15. The microprocessor as recited in claim <b>13</b>, wherein each storage location in each array, excluding the final array, contains a pointer to the following storage location in the next array.</claim-text></claim>"}, {"num": 16, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00016\" num=\"16\"><claim-text>16. A method for storing instructions in a microprocessor comprising:</claim-text><claim-text>receiving instruction bytes from a memory subsystem; </claim-text><claim-text>predecoding the instruction bytes to identify instructions and field boundaries within the instructions; </claim-text><claim-text>storing each field for each instruction in an instruction cache that is logically configured as three dimensional; </claim-text><claim-text>storing at least a portion of each instruction's address into an address translation table; </claim-text><claim-text>storing a run counter with the address portion in the address translation table, wherein the run counter is indicative of the number of instructions occurring in program order after the particular instruction before a branch instruction that is predicated taken is received. </claim-text></claim>"}, {"num": 17, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00017\" num=\"17\"><claim-text>17. The method as recited in claim <b>16</b>, further comprising:</claim-text><claim-text>storing a first pointer with the address portion in the address translation table, wherein the first pointer points to the storage location within the instruction cache that stores the instruction that is associated with the corresponding address portion. </claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00018\" num=\"18\"><claim-text>18. The method as recited in claim <b>17</b>, further comprising storing a second pointer with the address portion in the address translation table, wherein the second pointer points to the storage location within the instruction cache storing the instruction immediately following, in program order, the instruction that is associated with the corresponding address portion.</claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00019\" num=\"19\"><claim-text>19. The method as recited in claim <b>18</b>, wherein one of the three dimensions of the instruction cache comprises different layers of a die upon which the instruction cache is formed.</claim-text></claim>"}, {"num": 20, "parent": 19, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00020\" num=\"20\"><claim-text>20. The method as recited in claim <b>19</b>, wherein each field for each instruction is stored into a separate array configured to store each particular type of instruction field.</claim-text></claim>"}, {"num": 21, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00021\" num=\"21\"><claim-text>21. A computer system comprising:</claim-text><claim-text>a main memory subsystem configured to transmit variable length instructions, wherein the variable length instructions comprise a plurality of fields, wherein each field may hold a predetermined maximum number of bytes in data cells; </claim-text><claim-text>a first microprocessor comprising: </claim-text><claim-text>a predecode unit coupled to receive variable-length instructions from a main memory subsystem, wherein the predecode unit is configured to predecode the variable-length instructions by detecting instruction field boundaries within each variable-length instruction, wherein the variable-length instructions each comprise one or more instruction fields; and </claim-text><claim-text>an instruction cache coupled to the predecode unit, wherein the instruction cache comprises an array of instruction storage locations, wherein each instruction storage location comprises an array of fixed length instruction field storage locations, wherein each instruction storage location comprises a dedicated instruction field storage location for each type of possible instruction field, regardless of whether the particular instruction stored therein has each possible type of instruction field, wherein each instruction field storage location is configured to store a particular type of instruction field, wherein each instruction field storage location comprises at least enough memory cells to store the maximum number of instruction bytes possible for the corresponding type of instruction field, and wherein the instruction cache is logically configured as a three-dimensional array of memory cells; </claim-text><claim-text>a CPU bus coupled to the first microprocessor; and </claim-text><claim-text>a modem coupled to the CPU bus via a bus bridge. </claim-text></claim>"}, {"num": 22, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00022\" num=\"22\"><claim-text>22. The computer system as recited in claim <b>21</b>, further comprising a second microprocessor coupled to the first microprocessor via the CPU bus.</claim-text></claim>"}, {"num": 23, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00023\" num=\"23\"><claim-text>23. A microprocessor implemented on a multi-layer die, the microprocessor comprising:</claim-text><claim-text>a predecode unit configured to receive variable-length instructions and configured to predecode the variable-length instructions by detecting instruction field boundaries within each variable-length instruction; </claim-text><claim-text>an instruction cache coupled to the predecode unit, wherein the instruction cache comprises an array of instruction storage locations, wherein each instruction storage location comprises an array of instruction field storage locations, wherein each instruction field storage location is configured to store a particular type of instruction field and comprises at least enough memory cells to store the maximum number of instruction bytes possible for the corresponding type of instruction field, and wherein the instruction cache is logically configured as a three-dimensional array of memory cells, wherein one of the three dimensions comprises different layers of the microprocessor's die. </claim-text></claim>"}, {"num": 24, "parent": 23, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00024\" num=\"24\"><claim-text>24. The microprocessor as recited in claim <b>23</b>, further comprising an address translation table coupled to the instruction cache, wherein the address translation table comprises a plurality of address entries, wherein each address entry comprises at least a portion of an instruction address and a corresponding first pointer that points to a particular instruction storage location within the instruction cache.</claim-text></claim>"}, {"num": 25, "parent": 24, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00025\" num=\"25\"><claim-text>25. The microprocessor as recited in claim <b>24</b>, wherein each storage location in the address translation table comprises storage for a second pointer, wherein the second pointer indicates the instruction storage location storing the instruction that follows, in program order, the instruction corresponding to the first pointer.</claim-text></claim>"}, {"num": 26, "parent": 23, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00026\" num=\"26\"><claim-text>26. The microprocessor as recited in claim <b>23</b>, wherein each instruction field storage location comprises a fixed length plurality of byte storage locations, wherein each byte is stored on a different layer of the microprocessor's die.</claim-text></claim>"}, {"num": 27, "parent": 23, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00027\" num=\"27\"><claim-text>27. The microprocessor as recited in claim <b>23</b>, wherein each instruction field storage location comprises a plurality of bit storage locations, wherein each bit storage location is on a different layer of the microprocessor's die.</claim-text></claim>"}, {"num": 28, "parent": 23, "type": "dependent", "paragraph_markup": "<claim id=\"US-6253287-B1-CLM-00028\" num=\"28\"><claim-text>28. The microprocessor as recited in claim <b>23</b>, wherein each storage location in the address translation table further comprises storage for a run value, wherein the run value indicates the number of sequential instructions following, in program order, the first instruction, wherein the sequential instructions comprise non-branch instructions and branch instruction that are predicted not taken.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54566031\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>This invention relates to storing and scanning variable-length instructions in a microprocessor.</p><p>2. Description of the Relevant Art</p><p>The number of software applications written for the x86 instruction set is quite large. As a result, despite the introduction of newer and more advanced instruction sets, microprocessor designers have continued to design microprocessors capable of executing the x86 instruction set.</p><p>The x86 instruction set is relatively complex and is characterized by a plurality of variable-length instructions. A generic format illustrative of the x86 instruction set is shown in FIG. <b>1</b>. As illustrated in the figure, an x86 instruction consists of from one to five optional prefix bytes <b>102</b>, followed by an operation code (opcode) field <b>104</b>, an optional addressing mode (Mod R/M) byte <b>106</b>, an optional scale-index-base (SIB) byte <b>108</b>, an optional displacement field <b>110</b>, and an optional immediate data field <b>112</b>.</p><p>The opcode field <b>104</b> defines the basic operation for a particular instruction. The default operation of a particular opcode may be modified by one or more prefix bytes <b>102</b>. For example, one of prefix bytes <b>102</b> may be used to change the address or operand size for an instruction, to override the default segment used in memory addressing, or to instruct the processor to repeat a string operation a number of times. The opcode field <b>104</b> follows prefix bytes <b>102</b>, if present, and may be one or two bytes in length. The addressing mode (Mod R/M) byte <b>106</b> specifies the registers used as well as memory addressing modes. The scale-index-base (SIB) byte <b>108</b> is used only in 32-bit base-relative addressing using scale and index factors. A base field within SIB byte <b>108</b> specifies which register contains the base value for the address calculation, and an index field within SIB byte <b>108</b> specifies which register contains the index value. A scale field within SIB byte <b>108</b> specifies the power of two by which the index value will be multiplied before being added, along with any displacement, to the base value. The next instruction field is a displacement field <b>110</b>, which is optional and may be from one to four bytes in length. Displacement field <b>110</b> contains a constant used in address calculations. The optional immediate field <b>112</b>, which may also be from one to four bytes in length, contains a constant used as an instruction operand. The shortest x86 instructions are only one byte long, and comprise a single opcode byte. The 80286 sets a maximum length for an instruction at 10 bytes, while the 80386 and 80486 both allow instruction lengths of up to 15 bytes.</p><p>The complexity of the x86 instruction set poses many difficulties in implementing high performance x86-compatible microprocessors. In particular, the variable length of x86 instructions makes decoding instructions difficult. Decoding instructions typically involves determining the boundaries of an instruction and then identifying each field within the instruction, e.g., the opcode and operand fields. Decoding typically takes place once the instruction is fetched from the instruction cache before execution.</p><p>One method for determining the boundaries of instructions involves generating a number of predecode bits for each instruction byte read from main memory. The predecode bits provide information about the instruction byte they are associated with. For example, an asserted predecode start bit indicates that the associated instruction byte is the first byte of an instruction. Similarly, an asserted predecode end bit indicates that the associated instruction byte is the last byte of an instruction. Once the predecode bits for a particular instruction byte are calculated, they are stored together with the instruction byte in an instruction cache. When a \u201cfetch\u201d is performed, i.e., a number of instruction bytes are read from the instruction cache, the associated start and end bits are also read. The start and end bits may then be used to generate valid masks for the individual instructions with the fetch. A valid mask is a series of bits in which each bit corresponds to a particular instruction byte. Valid mask bits associated with the first byte of an instruction, the last byte of the instruction, and all bytes in between the first and last bytes of the instruction are asserted. All other valid mask bits are not asserted.</p><p>Turning now to FIG. 2, an exemplary valid mask is shown. The figure illustrates a portion of a fetch <b>120</b> and its associated start and end bits <b>122</b> and <b>124</b>. Assuming a valid mask <b>126</b> for instruction B <b>128</b> is to be generated, start and end bits <b>122</b> and <b>124</b> would be used to generate the mask. Valid mask <b>126</b> could then be used to mask off all bytes within fetch <b>120</b> that are not part of instruction B <b>128</b>.</p><p>Once the boundaries of an instruction have been determined, the fields within the instruction, e.g., the opcode and operand fields, may be identified. Once again, the variable length of x86 instructions complicates the identification process. In addition, the optional prefix bytes within an x86 instruction create further complications. For example, in some instructions the opcode will begin with the first byte of the instruction, while others may begin with the second, third, or fourth byte.</p><p>To perform the difficult task of decoding x86 instructions, a number of cascaded levels of logic are typically used. Thus, decoding may require a number of clock cycles and may create a significant delay before any instructions are available to the functional stages of the microprocessor's pipeline. As microprocessors increase the number of instructions they are able to execute per clock cycle, instruction decoding may become a performance limiting factor. Therefore, a mechanism for simplifying the complexity and time required for instruction decoding is needed.</p><h4>SUMMARY OF THE INVENTION</h4><p>The problems outlined above may in part be solved by a microprocessor capable of predecoding instructions to fixed field lengths and then storing them in a \u201cthreedimensional\u201d instruction cache. Broadly speaking, in one embodiment a microprocessor capable of efficient instruction decoding comprises a predecode unit and an instruction cache. The predecode unit is coupled to receive variable-length instructions from a main memory subsystem. The variable-length instructions are then predecoded by detecting instruction field boundaries within each instruction. Once the instruction fields have been determined, the instruction is conveyed to the instruction cache for storage. The instruction cache is coupled to the predecode unit and comprises an array of instruction storage locations. Each instruction storage location in turn comprises an array of instruction field storage locations. Each instruction field storage location is configured to store a particular type of instruction field and comprises at least enough memory cells to store the maximum number of instruction bytes possible for the corresponding type of instruction field. The instruction cache may be logically configured as a three-dimensional array of memory cells. The instruction cache may also be physically configured as a three-dimensional array by forming the constituent memory cells on different layers of the microprocessor's die.</p><p>Using a three-dimensional configuration may advantageously allow each instruction to have the same length in two dimensions. This may in turn greatly simplify the task of determining the boundaries of instructions read from the instruction cache. Using a three-dimensional configuration may also allow instructions to be stored in fixed field width format. This, in turn, may potentially reduce or even eliminate the delay and hardware associated with determining instruction field boundaries for instructions read from the instruction cache.</p><p>In another embodiment, the instruction cache may be configured as a plurality of two-dimensional arrays, each configured to store a particular type of instruction field. These arrays may be formed either on one layer or on multiple layers of the die.</p><p>In another embodiment, the microprocessor may further comprise an address translation table coupled to the instruction cache. The address translation table may comprise a plurality of address entries, each comprising a fetch address tag and a corresponding pointer that points to a particular instruction storage location within the instruction cache. The fetch address tag is compared with the fetch address to find the correct pointer. In one embodiment, the address translation table may store multiple pointers and branch prediction information with each address tag.</p><p>A method for storing instructions in a microprocessor is also contemplated. In one embodiment, the method comprises receiving instruction bytes from a memory subsystem, predecoding the instruction bytes to identify instructions and instruction field boundaries within the instructions, and storing each field for each instruction in an instruction cache that is logically configured as three dimensional. The method may further comprise storing at least a portion of each instruction's address into an address translation table as a fetch address tag. One or more pointers may be stored with the fetch address tag in the address translation table. These pointers point to the storage location within the instruction cache that is storing an instruction associated with the corresponding fetch address tag. In another embodiment, a run counter may be stored with the address tag and pointers. The run counter is indicative of the number of instructions occurring in program order before a branch instruction that is predicated taken.</p><p>A computer system capable of utilizing one or more of the microprocessors described above is also contemplated. In one embodiment, the computer system comprises a microprocessor as described above and a bus bridge. The bus bridge may be coupled to the microprocessor via a high-speed CPU bus. Peripherals, such as a modem, may be coupled to the microprocessor via the bus bridge. In another embodiment, the computer system may comprise a second microprocessor coupled to the first via the CPU bus.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Other objects and advantages of the invention will become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p><p>FIG. 1 is a block diagram of the generic x86 instruction format.</p><p>FIG. 2 is a block diagram illustrating the formation of one type of valid mask.</p><p>FIG. 3 is a block diagram of one embodiment of a microprocessor that is configured to predecode variable-length instructions.</p><p>FIG. 4 is a block diagram illustrating details of one embodiment of the central window from the microprocessor of FIG. <b>3</b>.</p><p>FIG. 5 is a block diagram illustrating further details of one embodiment of the central window from FIG. <b>4</b>.</p><p>FIG. 6A is a block diagram showing more detail of one embodiment of the instruction cache from FIG. <b>3</b>.</p><p>FIG. 6B is a block diagram showing another embodiment of the instruction cache from FIG. <b>3</b>.</p><p>FIG. 7 is a block diagram showing more detail of another embodiment of the instruction cache from FIG. <b>3</b>.</p><p>FIG. 8 is a block diagram showing more detail of yet another embodiment of the instruction cache from FIG. <b>3</b>.</p><p>FIG. 9 is a block diagram showing more detail of still another embodiment of the instruction cache from FIG. <b>3</b>.</p><p>FIG. 10 is a block diagram of one embodiment of a computer system configured to use the microprocessor from FIG. <b>3</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><p>While the present invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><h4>DETAILED DESCRIPTION OF SEVERAL EMBODIMENTS</h4><p>Turning now to FIG. 3, a block diagram of one embodiment of a microprocessor <b>10</b> that is configured to decode instructions out of order is shown. In this embodiment, microprocessor <b>10</b> includes a prefetch/predecode unit <b>12</b>, a branch prediction unit <b>14</b>, and an address translation table <b>60</b> coupled to an instruction cache <b>16</b>. Central dispatch window <b>18</b> is coupled between instruction cache <b>16</b> and reservation stations <b>22</b>A-C. A microcode read-only memory (MROM) unit <b>34</b> is also coupled to central dispatch window <b>18</b>. Reservations stations <b>22</b>A-C are coupled to a corresponding number of functional units <b>24</b>A-C, and load/store unit <b>26</b> is coupled to a data cache <b>28</b>. Finally, a result bus <b>38</b> couples functional units <b>24</b>A-C and data cache <b>28</b> to reorder buffer <b>32</b>, register/future file <b>30</b>, load/store unit <b>26</b>, and reservations stations <b>22</b>A-C.</p><p>Generally speaking, instruction cache <b>16</b> is a high speed cache memory provided to temporarily store instructions before they are fetched and conveyed to central dispatch window <b>18</b>. In one embodiment, instruction cache <b>16</b> is configured to store up to 32 kilobytes of instruction code. Instruction cache <b>16</b> may be organized as an array of 16 byte instruction storage locations. Each instruction storage location, in turn, may be configured as an array of instruction field storage locations. This configuration is described in greater detail below. During operation, instructions are provided to instruction cache <b>16</b> by prefetching instruction bytes from a main memory (not shown) through prefetch/predecode unit <b>12</b>. It is noted that instruction cache <b>16</b> may be implemented in set-associative, fully-associative, or direct-mapped configurations.</p><p>As noted above, prefetch/predecode unit <b>12</b> prefetches instruction code from the main memory for storage within instruction cache <b>16</b>. In one embodiment, prefetch/predecode unit <b>12</b> is configured receive and decode 64-bit wide bursts of code from the main memory. It is understood that a variety of specific code prefetching techniques and algorithms may be employed by prefetch/predecode unit <b>12</b>.</p><p>As prefetch/predecode unit <b>12</b> fetches instruction bytes from main memory, instruction boundaries within the stream of instruction bytes are detected. Prefetch/predecode unit <b>12</b> then uses these boundaries to predecode each instruction. As used herein, predecoding refers to determining which instruction fields are present within a particular instruction and where those instruction fields' boundaries are within each instruction. Once an instruction has been predecoded, it is conveyed to instruction cache <b>16</b> for storage. Prefetch/predecode unit <b>12</b> may be configured to convey the predecoded instructions in a predetermined format (e.g., across a high speed dedicated bus having separate lines for each field) so that each instruction is stored in instruction cache <b>16</b> in a fixed format. Since some instructions may not have all possible instruction fields, prefetch/predecode unit <b>12</b> may be configured to \u201cpad\u201d the empty instruction fields by filling them with constants (e.g., zero). In another embodiment, predecode unit <b>12</b> may be configured to also convey a valid bit for each instruction field indicative of whether the field is present in the current instruction.</p><p>Prefetch/predecode unit <b>12</b> may further be configured to calculate and convey a plurality of predecode bits with each predecoded instruction. The predecode bits may convey additional information about each instruction. For example, one predecode bit may indicate whether the given instruction may be decoded executed directly by functional units <b>24</b>A-B, or whether the instruction is to be executed by invoking a sequence of microcode instructions stored within MROM unit <b>18</b>, as described in further detail below. Another predecode bit may indicate the presence of a two-byte opcode. For example, in the x86 instruction set all instructions having two-byte opcodes have a value of 0F (hex) as their first opcode byte. Advantageously, using a predecode bit to indicate the presence of two-byte instruction opcode may allow all opcodes to be stored in a single-byte field. The predecode bits (collectively referred to herein as tags) may be stored along with the instructions in instruction cache <b>16</b>. To improve the flow of data from predecode unit <b>12</b> to instruction cache <b>16</b>, predecode unit <b>12</b> may have a FIFO (first-in first-out) memory buffer at its input to receive and store the instruction byte sequences until predecode unit <b>12</b> is ready to predecode them.</p><p>Address translation table <b>50</b> is coupled to predecode unit <b>12</b> and instruction cache <b>16</b> and is configured to store address translations for instructions stored within instruction cache <b>16</b>. As noted above, instructions having fewer than the maximum number of instruction fields may be padded with constants. This may shift a particular instruction's location relative to its original address. Address translation table <b>50</b> provides a look-up table that correlates an instruction's original address to a pointer or index that points to the storage location in instruction cache <b>16</b> in which the instruction is stored.</p><p>Before describing the interaction between predecode unit <b>12</b>, address translation table <b>50</b>, and instruction cache <b>16</b> in greater detail (see section below entitled \u201cDetails of Instruction Cache\u201d), other general aspects of microprocessor <b>10</b> will be discussed.</p><p>When an instruction is fetched, the fetch address is translated by address o translation table <b>50</b>, and the corresponding instruction storage location in instruction cache <b>16</b> is read. In one embodiment, instruction cache <b>16</b> may be configured to output three stored instructions to central dispatch window <b>18</b> per clock cycle. Central window <b>18</b> receives the instructions and begins the process of retrieving any necessary operands (e.g., memory and or register operands). In one embodiment, central window <b>18</b> may be configured to monitor result bus <b>38</b> for results that are referenced as operands by stored instructions. If such a result is detected, central window <b>18</b> may forward the result to the corresponding pending instruction. Similarly, data from load instructions executed by load/store unit <b>26</b> may also be monitored and forwarded.</p><p>Central window <b>18</b> also determines which instructions should be dispatched to reservation stations <b>22</b>A-C and load/store unit <b>26</b> each clock cycle. Central window <b>18</b> may base its selection of which instructions to dispatch based upon a number of factors, including: (1) the availability of reservation stations <b>22</b>A-C and the corresponding functions performed by their respective functional units <b>24</b>A-C, (2) whether the particular instruction in question is ready for dispatch (i.e., its operand values have been received), and (3) the relative position in program order of the instruction in question (i.e., the oldest instructions ready for dispatch are dispatched first). Advantageously, by waiting for an instruction's operand values to be received before dispatching the instruction, dependency checking may be performed. Note in other embodiments of microprocessor <b>10</b>, dependency checking may also be performed in reservation stations <b>22</b>A-C. As long as there are no dependencies, central window <b>18</b> may dispatch instructions out-of-order. Advantageously, out-of-order execution in combination with speculative execution tends to increase performance by preventing functional units <b>24</b>A-C from stalling. In the embodiment illustrated, instructions may be speculatively executed based upon branch prediction information stored in branch prediction unit <b>14</b>.</p><p>In one embodiment, central window <b>18</b> may normally receive three instructions per clock cycle and also dispatch three instructions per clock cycle. Once an instruction is dispatched from central window <b>18</b>, its storage location may be cleared or marked as empty, e.g., by setting or clearing a corresponding status bit. In one embodiment, central window may be configured as an array with each row in the array storing three instructions. After each clock cycle, the storage locations may be configured to shift a variable amount in order to fill in any gaps created by dispatched instructions.</p><p>Central window <b>18</b> may also receive instructions from MROM unit <b>34</b>. When instruction cache <b>16</b> detects that an instruction being output is too complex for functional units <b>24</b>A-<b>24</b>C to execute directly, it may route the instruction to MROM unit <b>34</b> in lieu of central window <b>18</b>. In response, MROM unit <b>34</b> conveys a sequence of simpler microcode instructions to central window <b>18</b>. The microcode instructions are stored in central window <b>18</b> and receive reorder buffer tags in a similar manner to non-MROM (\u201cfast-path\u201d) instructions.</p><p>While central window <b>18</b> is receiving instructions from instruction cache <b>16</b>, reorder buffer <b>32</b> may be configured to issue each instruction a reorder buffer tag which serves to identify each instruction's relative position in program order. This may advantageously allow instructions to execute out of order. The reorder buffer tags follow each outstanding instruction through central window <b>18</b>, reservation stations <b>22</b>A-C, and functional units <b>24</b>A-C. Reorder buffer <b>32</b> may also reserve a storage location for the result of each instruction. When an instruction completes execution, its results and reorder buffer tag are output by functional units <b>24</b>A-C onto result bus <b>38</b>. Reorder buffer <b>32</b> monitors result bus <b>38</b> and stores the results in the corresponding reserved storage location. Each clock cycle, reorder buffer <b>32</b> may retire up to three instructions. An instruction is retired by copying its results to the architectural register file <b>30</b>, thereby updating the architectural state of microprocessor <b>10</b>. Advantageously, reorder buffer <b>32</b> operates to keep track of the original program sequence for register read and write operations, implements register renaming, allows for speculative instruction execution and branch misprediction recovery, and facilitates precise exceptions.</p><p>Reservation stations <b>22</b>A-C act as buffers for their corresponding functional units <b>24</b>A-C by storing instructions until they are executed. The instructions wait in reservation stations <b>22</b>A-C or load/store unit <b>26</b> until the corresponding functional unit's first execution pipeline stage is available to accept a new instruction. At that point, the instructions may enter functional units <b>24</b>A-C for execution. In one embodiment, each functional unit <b>24</b>A-C is configured to perform integer arithmetic operations of addition and subtraction, as well as shifts, rotates, logical operations, and branch operations. It is noted that a floating point unit (not shown) may also be employed to accommodate floating point operations.</p><p>Register file <b>30</b> comprises two sets of registers. One set comprises the x86 architectural registers, including eight 32-bit real registers (i.e., typically referred to as EAX, EBX, ECX, EDX, EBP, ESI, EDI and ESP). The second set comprises registers for storing the most recent speculative set of values for each architectural register. This \u201cfuture file\u201d of registers provides a convenient place from which to forward speculative register values to pending instructions (e.g., to central window <b>18</b>). If reorder buffer <b>32</b> has a result storage location reserved for a value that will update the desired register, the operand value (or tag thereto) is provided from reorder buffer <b>32</b> rather than from register file <b>30</b>. If there is no location reserved for a required register in reorder buffer <b>32</b>, the value is taken directly from register file <b>30</b>. If the operand corresponds to a memory location, the operand value is provided to central window <b>18</b> from load/store unit <b>26</b>.</p><p>The results of each executed instruction are stored in reorder buffer <b>32</b> until the instruction is \u201cretired\u201d. Retiring an instruction refers to copying the instruction's results to architectural register file <b>30</b> and thereby updating the microprocessor's non-speculative architectural state. As previously noted, reorder buffer tags follow each instruction through reservation stations <b>22</b>A-C and functional units <b>24</b>A-C. Thus, the results may be identified and attributed to the appropriate instruction within reorder buffer <b>32</b>. Once the results are received, reorder buffer <b>32</b> retires instructions in-order in a line-by-line fashion, waiting to retire a line of instructions until the following conditions are met: (1) the line is the oldest line of instructions stored within reorder buffer <b>32</b>, and (2) each instruction in the line has completed execution without an exception or branch misprediction. Note that other variations of reorder buffer <b>32</b> are also possible. For example, in another embodiment reorder buffer <b>32</b> may individually retire instructions as opposed to retiring them in a line-by-line manner. Reorder buffer <b>32</b> may be implemented in a first-in-first-out configuration wherein speculative results move to the \u201cbottom\u201d of the buffer as they are validated and written to register file <b>30</b>, thus making room for new entries at the \u201ctop\u201d of the buffer.</p><p>In the event of a branch misprediction, central window <b>18</b>, reorder buffer <b>32</b>, reservation stations <b>24</b>A-C, and load/store unit <b>26</b> may be configured to flush all pending instructions occurring after the mispredicted branch instruction in program order. Furthermore, the contents of the architectural register file within register/future file <b>28</b> are copied to the future file to replace any erroneous values created by the execution of instructions along the mispredicted branch path. Branch mispredictions may be detected by functional units <b>32</b>A-B, which forward the results of branch instructions to branch prediction unit <b>14</b>.</p><p>Generally speaking, load/store unit <b>34</b> provides an interface between functional units <b>32</b>A-C and data cache <b>36</b>. In one embodiment, load/store unit <b>34</b> is configured with a load/store buffer that has eight storage locations for storing data and address information from pending loads or stores. Load/store unit <b>34</b> also performs dependency checking for load instructions against pending store instructions to ensure that data coherency is maintained. Data cache <b>36</b> is a high speed cache memory provided to temporarily store data being transferred between load/store unit <b>34</b> and the main memory subsystem. In one embodiment, data cache <b>36</b> has a capacity of storing up to 32 kilobytes of data. It is understood that data cache <b>36</b> may be implemented in a variety of sizes and specific memory configurations, including set associative, fully associative, and direct mapped configurations.</p><p>Turning now to FIG. 4, a block diagram of one embodiment of central window <b>18</b> is shown. Central window <b>18</b> is where all instructions are held until they are ready to be issued to reservation stations <b>22</b>A-C and functional units <b>24</b>A-C, respectively. In one embodiment, each central window entry <b>150</b>A-<b>150</b>N is composed of the predecoded instruction with source and destination values or tags, the instruction's reorder buffer tag, and a status field <b>152</b>. Central window <b>18</b> is configured to allocate and retire up to three entries per clock cycle. Central window <b>18</b> is configured as a shiftable/collapsible FIFO, much like load/store unit <b>26</b>. Selection unit <b>154</b> allows each entry <b>150</b>A-<b>150</b>N to be loaded from any of the three issue positions, and each entry <b>150</b>A-<b>150</b>N is capable of issuing to any of the three reservation stations <b>22</b>A-C. Each entry <b>150</b>A-<b>150</b>N is able to independently shift 0, 1, 2, or 3 positions in a single clock cycle. Advantageously, this ability to shift may allow the remaining entries to fill in the gaps created by an instruction that is issued out of order. Issue unit <b>156</b> searches the contents of central window <b>18</b> to find the first instruction ready to be issued to each of the three reservation station/functional unit pipes. While instructions may be executed out of order, preference is given to the oldest instructions outstanding in central window <b>18</b>. Once an instruction is issued to the appropriate functional pipe, deallocation unit <b>158</b> clears the status field of the entry, thereby indicating that the entry is available to store a new instruction. Deallocation unit <b>158</b> also controls the shifting of the entries to fill in any gaps. Advantageously, the vacated entries may be ready to be filled when the next clock cycle occurs.</p><p>Turning now to FIG. 5, detail of the first six entries in one embodiment of central window <b>92</b> are shown. Central window <b>92</b> is configured into rows, each row having three entries. Each entry in the first row (i.e., entries <b>150</b>A-<b>150</b>C) is capable of receiving instructions directly from translation unit <b>86</b> and operands from register file <b>30</b>. Up to three instructions may be written per clock cycle. As previously explained, central window <b>18</b> operates in a FIFO-like manner, with instructions stored in entries <b>150</b>A-<b>150</b>N propagating through each line of central window until being selected for issue to the three reservation stations <b>22</b>A-C. Unlike a FIFO, however, the contents of any entry may be issued, even if there are other instructions before and after it in central window <b>18</b>. Multiplexers <b>170</b>B-<b>170</b>F, which are part of selection unit <b>154</b>, allow entries to be shifted towards the bottom of central window <b>18</b>, thereby filling in any gaps created when an instruction is issued. Each entry's contents are capable of shifting up to three positions in a clock cycle. For example, the contents of entry <b>150</b>A can shift to entry <b>150</b>B (via multiplexer <b>170</b>B), entry <b>150</b>C (via multiplexer <b>170</b>C), or entry <b>150</b>D (via multiplexer <b>170</b>D).</p><p>Multiplexers <b>172</b>A-<b>176</b>B and issue sub-units <b>182</b>A-<b>186</b>B are part of issue unit <b>156</b>, and operate to select and issue up to three instructions each clock cycle, i.e., one for each of the reservation station/functional unit pipelines. Every clock cycle, multiplexers <b>172</b>A-B and issue sub-units <b>182</b>A-B select the oldest instruction ready to be issued to functional unit <b>24</b>A. Similarly, multiplexers <b>174</b>A-B and issue sub-units <b>184</b>A-B select the oldest instruction ready to be issued to functional units <b>24</b>B-C. An instruction is ready to issue when all operand dependencies have been resolved. While only six entries <b>150</b>A-<b>150</b>F are shown in FIG. 5, more entries are contemplated. In addition, while the embodiments illustrated show three reservation station/functional unit pipelines, other numbers are possible. For example, a multiplier functional unit pipeline and a floating point unit pipeline may be added.</p><p>Details of Instruction Cache</p><p>Turning now to FIG. 6A, more details regarding one embodiment of address translation table <b>50</b> and instruction cache <b>16</b> are shown. As the figure illustrates, instructions having variable lengths are reorganized and stored in instruction cache <b>16</b> so as to have a constant length in two dimensions (labeled X and Y in the figure). In the embodiment illustrated, instruction cache <b>16</b> is \u201clogically configured\u201d as a three-dimensional (X, Y, and Z) array, with each instruction having a constant length of six bytes in the X dimension and a constant height of one bit in the Y dimension. As used herein, the term \u201clogically configured\u201d refers to how individual bytes and storage locations within instruction cache <b>16</b> are accessed (addressed). This is in contrast with instruction cache <b>16</b>'s physical configuration, i.e., the positioning of its transistors on a semiconductor substrate (die). In some embodiments, instruction cache <b>16</b> may be logically configured as three-dimensional while being physically configured as two-dimensional. In other embodiments, instruction cache <b>16</b> may be both logically and physically configured as three-dimensional (i.e., with the third physical dimension, indicated by arrow <b>60</b>, referring to the different layers of the wafer or die upon which instruction cache <b>16</b> is formed). For example, each bit or byte of each instruction field may be stored on a different layer. Instructions stored within cache <b>16</b> are configured such that each instruction's fields are stored in the same position in each instruction storage location.</p><p>In one embodiment, each instruction storage location within cache <b>16</b> is configured to store the predecoded and padded x86 instruction fields previously described. Prefix field <b>102</b>, however, may be compressed into one byte to reduce the amount of storage space needed for each instruction. Any fields comprising more than one byte may have their additional bytes stored in the third (Z) dimension. Since each instruction stored within cache <b>16</b> has a corresponding mapping within address translation table <b>50</b>, cache <b>16</b> is effectively fully associative (i.e., any instruction may be stored in any storage location).</p><p>Address translation table <b>50</b> may be direct mapped, set associated, or fully associative. Address translation table <b>50</b> receives requested fetch addresses and translates them into pointers or indexes into instruction cache <b>16</b>. The example embodiment of address translation table <b>50</b> shown in the figure is set associative, and example entry <b>62</b> comprises a fetch address tag <b>64</b> and pointer (or index) <b>66</b>. In this configuration a first portion of the fetch address is used to select the row within address translation table <b>50</b>. Once the row is selected, an entry within the row is selected by comparing a second portion of the fetch address with the fetch address tags stored in each entry. A match selects the desired column or \u201cway\u201d within address translation table <b>50</b>. The entry residing at intersection of the selected row and way is read, and the pointer stored therein is used to select the corresponding instruction storage location in instruction cache <b>16</b>. When the selected instruction is output from instruction cache <b>16</b> to central window <b>18</b>, the multi-dimensional fixed-field format may be retained, or the instruction may be expanded to a single dimensional sequence of instruction bytes (e.g., with padding constants in any unused fields).</p><p>Turning now to FIG. 6B, a variation on the previous instruction format within instruction cache <b>16</b> is shown. In this embodiment, predecode bits <b>114</b> are stored as part of each instruction. As previously noted, predecode bits may store additional information about each instruction. For example, one predecode bit may be used to indicate the presence of a two-byte opcode (i.e., a first opcode byte of 0F (hex) in the x86 instruction set). This may advantageously reduce the maximum length of opcode field <b>104</b> to a single byte.</p><p>Turning now to FIG. 7, another embodiment of an address translation table entry is shown. In this embodiment, example entry <b>62</b> comprises fetch address tag <b>64</b> and pointers <b>66</b>A-N. This configuration may allow \u201cruns\u201d or sequences of instructions to be fetched with a single fetch address. In one embodiment, branch prediction information may also be included in each entry <b>62</b>. For example, assuming the instruction pointed to by pointer <b>66</b>A is a branch instruction, then pointer <b>66</b>B (and any subsequent pointers) may be changed to reflect the predicted outcome of the branch.</p><p>Turning now to FIG. 8, another embodiment of an address translation table entry is shown. In this embodiment, each entry comprises fetch address tag <b>64</b>, pointer <b>62</b>, and a run count value <b>68</b>. Run count value <b>68</b> indicates how many instructions stored sequentially after instruction 1 (i.e., the instruction pointed to by pointer <b>66</b>) are predicted to follow instruction 1. In this embodiment, instruction cache <b>16</b> and address translation table <b>50</b> may be configured to dynamically reorder instructions (and adjust pointers <b>66</b> accordingly) to increase the number of instructions that are stored sequentially.</p><p>Turning now to FIG. 9, yet another embodiment of address translation table <b>50</b> and instruction cache <b>16</b> are shown. In this embodiment, instruction cache <b>16</b> comprises a plurality of two dimensional storage arrays <b>70</b>A-N, each comprising a plurality of storage locations. Each array <b>70</b>A-N is configured to store one type of instruction field for each instruction stored in instruction cache <b>16</b>. For example, prefix array <b>70</b>A is configured with a plurality of instruction field storage locations, each configured to store one prefix instruction field. Similarly, opcode array <b>70</b>B is configured to store the opcode fields of each instruction within instruction cache <b>16</b>. In one embodiment of instruction cache <b>16</b>, each array <b>70</b>A-N may be formed on a different die layer. Other configurations, however, are also possible and contemplated. For example, all arrays may be implemented on a single layer. In another embodiment, entries in address translation table <b>50</b> may store a single pointer to the first field of the corresponding instruction, while each storage location in the arrays (excluding the final array) store a pointer to the next field of the instruction. The final array may optionally store a pointer to the next predicted instruction.</p><p>In the embodiment illustrated in the figure, each entry in address translation table <b>50</b> comprises fetch address tag <b>64</b> and pointers <b>66</b>A-N. Fetch address tag <b>64</b> performs the same function as previously discussed. Pointers <b>66</b>A-N, however, each point to particular field within each array <b>70</b>A-B. This may advantageously allow the size of each array to be varied. For example, code analysis may show that on the average only half of the instructions processed by microprocessor <b>10</b> have prefix bytes. Thus the size of prefix array <b>70</b>A may be configured to have one half the number of entries of opcode array <b>70</b>B. Advantageously, each array size may be tailored to a particular size.</p><p>While the examples above have illustrated specific embodiments, other configurations are possible and contemplated. For example, in one embodiment address translation table <b>50</b> may be direct mapped with each entry no longer having a fetch address tag <b>64</b>.</p><p>Exemplary Computer System</p><p>Turning now to FIG. 10, a block diagram of one embodiment of a computer system <b>200</b> configured to use microprocessor <b>10</b> is disclosed. Computer system <b>200</b> is coupled to a variety of system components through a bus bridge <b>202</b> as shown. Other embodiments are possible and contemplated. In the depicted system, a main memory <b>204</b> is coupled to bus bridge <b>202</b> through a memory bus <b>206</b>, and a graphics controller <b>208</b> is coupled to bus bridge <b>202</b> through an AGP bus <b>210</b>. Finally, a plurality of PCI devices <b>212</b>A-<b>212</b>B are coupled to bus bridge <b>202</b> through a PCI bus <b>214</b>. A secondary bus bridge <b>216</b> may further be provided to accommodate an electrical interface to one or more EISA or ISA devices <b>218</b> through an EISA/ISA bus <b>220</b>. Microprocessor <b>10</b> is coupled to bus bridge <b>202</b> through a CPU bus <b>224</b>.</p><p>Bus bridge <b>202</b> provides an interface between microprocessor <b>10</b>, main memory <b>204</b>, graphics controller <b>208</b>, and devices attached to PCI bus <b>214</b>. When an operation is received from one of the devices connected to bus bridge <b>202</b>, bus bridge <b>202</b> identifies the target of the operation (e.g. a particular device or, in the case of PCI bus <b>214</b>, that the target is on PCI bus <b>214</b>). Bus bridge <b>202</b> routes the operation to the targeted device. Bus bridge <b>202</b> generally translates an operation from the protocol used by the source device or bus to the protocol used by the target device or bus.</p><p>In addition to providing an interface to an ISA/EISA bus for PCI bus <b>214</b>, secondary bus bridge <b>216</b> may further incorporate additional functionality, as desired. For example, in one embodiment, secondary bus bridge <b>216</b> includes a master PCI arbiter (not shown) for arbitrating ownership of PCI bus <b>214</b>. An input/output controller (not shown), either external from or integrated with secondary bus bridge <b>216</b>, may also be included within computer system <b>200</b> to provide operational support for a keyboard and mouse <b>222</b> and for various serial and parallel ports (e.g., a modem port for connecting a modem), as desired. An external cache unit (not shown) may further be coupled to CPU bus <b>224</b> between microprocessor <b>10</b> and bus bridge <b>202</b> in other embodiments. Alternatively, the external cache may be coupled to bus bridge <b>202</b> and cache control logic for the external cache may be integrated into bus bridge <b>202</b>.</p><p>Main memory <b>204</b> is a memory in which application programs are stored and from which microprocessor <b>10</b> primarily executes. A suitable main memory <b>204</b> comprises DRAM (Dynamic Random Access Memory), and preferably a plurality of banks of SDRAM (Synchronous DRAM).</p><p>PCI devices <b>212</b>A-<b>212</b>B are illustrative of a variety of peripheral devices such as, for example, network interface cards, video accelerators, audio cards, hard or floppy disk drives or drive controllers, SCSI (Small Computer Systems Interface) adapters and telephony cards. Similarly, ISA device <b>218</b> is illustrative of various types of peripheral devices, such as a modem, a sound card, and a variety of data acquisition cards such as GPIB or field bus interface cards.</p><p>Graphics controller <b>208</b> is provided to control the rendering of text and images on a display <b>226</b>. Graphics controller <b>208</b> may embody a typical graphics accelerator generally known in the art to render three-dimensional data structures which can be effectively shifted into and from main memory <b>204</b>. Graphics controller <b>208</b> may therefore be a master of AGP bus <b>210</b> in that it can request and receive access to a target interface within bus bridge <b>202</b> to thereby obtain access to main memory <b>204</b>. A dedicated graphics bus accommodates rapid retrieval of data from main memory <b>204</b>. For certain operations, graphics controller <b>208</b> may further be configured to generate PCI protocol transactions on AGP bus <b>210</b>. The AGP interface of bus bridge <b>202</b> may thus include functionality to support both AGP protocol transactions as well as PCI protocol target and initiator transactions. Display <b>226</b> is any electronic display upon which an image or text can be presented. A suitable display <b>226</b> includes a cathode ray tube (\u201cCRT\u201d), a liquid crystal display (\u201cLCD\u201d), etc.</p><p>It is noted that, while the AGP, PCI, and ISA or EISA buses have been used as examples in the above description, any bus architectures may be substituted as desired. It is further noted that computer system <b>200</b> may be a multiprocessing computer system including additional microprocessors (e.g. microprocessor <b>10</b><i>a </i>shown as an optional component of computer system <b>200</b>). Microprocessor <b>10</b><i>a </i>may be similar to microprocessor <b>10</b>. More particularly, microprocessor <b>10</b><i>a </i>may be an identical copy of microprocessor <b>10</b>. Microprocessor <b>10</b><i>a </i>may share CPU bus <b>224</b> with microprocessor <b>10</b> (as shown in FIG. 5) or may be connected to bus bridge <b>202</b> via an independent bus.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Thomas S.", "last_name": "Green", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "ADVANCED MICRO DEVICES, INC."}, {"first_name": "", "last_name": "ADVANCED MICRO DEVICES, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/04"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/38        20060101A I20051008RMEP"}, {"label": "G06F   9/30        20060101A I20051008RMEP"}, {"label": "G06F  12/08        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711125"}, {"primary": false, "label": "711202"}, {"primary": false, "label": "712E09055"}, {"primary": false, "label": "711206"}, {"primary": false, "label": "711118"}, {"primary": false, "label": "712E09051"}, {"primary": false, "label": "712210"}, {"primary": false, "label": "712213"}, {"primary": false, "label": "711E1202"}, {"primary": false, "label": "711205"}, {"primary": false, "label": "712E09029"}, {"primary": false, "label": "711123"}, {"primary": false, "label": "712E09056"}, {"primary": false, "label": "712208"}], "ecla_classes": [{"label": "G06F   9/38C2"}, {"label": "G06F   9/38B2"}, {"label": "G06F   9/30T2A"}, {"label": "G06F   9/38D"}, {"label": "G06F   9/38E"}, {"label": "G06F  12/08B14"}, {"label": "G06F   9/38E2D"}], "cpc_classes": [{"label": "G06F  12/0875"}, {"label": "G06F   9/3857"}, {"label": "G06F   9/30152"}, {"label": "G06F   9/382"}, {"label": "G06F   9/3824"}, {"label": "G06F   9/3855"}, {"label": "G06F   9/3804"}, {"label": "G06F   9/3836"}, {"label": "G06F   9/384"}, {"label": "G06F   9/3844"}, {"label": "G06F   9/3804"}, {"label": "G06F   9/3824"}, {"label": "G06F   9/3857"}, {"label": "G06F   9/3836"}, {"label": "G06F   9/30152"}, {"label": "G06F   9/3844"}, {"label": "G06F   9/3855"}, {"label": "G06F   9/384"}, {"label": "G06F  12/0875"}, {"label": "G06F   9/382"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1998-09-09", "application_date": "1998-09-09", "family_members": [{"ucid": "US-6253287-B1", "titles": [{"lang": "EN", "text": "Using three-dimensional storage to make variable-length instructions appear uniform in two dimensions"}]}]}