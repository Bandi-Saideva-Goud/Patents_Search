{"patent_number": "US-6532528-B1", "publication_id": 73327146, "family_id": 17527816, "publication_date": "2003-03-11", "titles": [{"lang": "EN", "text": "Data processor and data processor system having multiple modes of address indexing and operation"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50460236\"><p>A data processor in which a speed of an address translating operation is raised is disclosed. A translation lookaside buffer is divided into a buffer for data and a buffer for instruction, address translation information for instruction is also stored into a translation lookaside buffer for data, and when a translation miss occurs in a translation lookaside buffer for instruction, new address translation information is fetched from the translation lookaside buffer for data. A high speed of the address translating operation can be realized as compared with that in case of obtaining address translation information from an external address translation table each time a translation miss occurs in the translation lookaside buffer for instruction.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6532528-B1-CLM-00001\" num=\"1\"><claim-text>1. A data processor comprising:</claim-text><claim-text>a central processing unit outputting a virtual address having a plurality of bits; </claim-text><claim-text>a cache memory having a plurality of cache entries accessible by an index address made of partial bits of said virtual address; and </claim-text><claim-text>a selecting circuit for arranging the bit members of said index address by switching between a specified bit of SEL<b>2</b> of said virtual address and an upper bit of said virtual address, said upper bit being a bit other than said specified bit and being higher than said specified bit among said plurality of bits of said virtual address. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6532528-B1-CLM-00002\" num=\"2\"><claim-text>2. A data processor according to <claim-ref idref=\"US-6532528-B1-CLM-00001\">claim 1</claim-ref>, wherein said cache memory is a data cache memory in which a cache entry of data is stored in correspondence to a physical page number, said physical page number which was associatively retrieved by a translation lookaside buffer is supplied to the data cache memory and the data cache memory associatively retrieves a cache entry corresponding to said physical page number.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6532528-B1-CLM-00003\" num=\"3\"><claim-text>3. A data processor according to <claim-ref idref=\"US-6532528-B1-CLM-00001\">claim 1</claim-ref>, wherein said cache memory is an instruction cache memory in which a cache entry of an instruction is stored in correspondence to a physical page number, said physical page number which are associatively retrieved by a translation lookaside buffer is supplied to said instruction cache memory and the instruction cache memory associatively retrieves a cache entry corresponding to said physical page number.</claim-text></claim>"}, {"num": 4, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6532528-B1-CLM-00004\" num=\"4\"><claim-text>4. A data processor comprising:</claim-text><claim-text>a central processing unit outputting a virtual address having a plurality of bits; </claim-text><claim-text>a cache memory having a plurality of cache entries accessible by an index address made of partial bits of said virtual address; and </claim-text><claim-text>a selecting circuit for changing a dividing position of said cache memory depending on a first mode or a second mode, </claim-text><claim-text>wherein the selecting circuit adopts a first bit of said virtual address and put it into said index address according to the first mode and, </claim-text><claim-text>wherein the selecting circuit adopts a second bit of said virtual address and puts said second bit into said index address replacing the first bit according to the second mode. </claim-text></claim>"}, {"num": 5, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6532528-B1-CLM-00005\" num=\"5\"><claim-text>5. A data processor according to <claim-ref idref=\"US-6532528-B1-CLM-00004\">claim 4</claim-ref>, wherein the first bit is a specified bit of said virtual address, and wherein the second bit is an upper bit of said virtual address, said upper bit being a bit other than the specified bit.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53888455\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><p>This is a divisional application of Ser. No. 08/950,668, filed Oct. 15, 1997 now U.S. Pat. No. 6,092,172.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>The invention relates to a data processor having a translation lookaside buffer and, more particularly, a data processing system using such a data processor. For example, the invention relates to a technique which is effective when it is applied to the realization of a high data processing speed.</p><p>In a virtual storage system, a virtual memory space which is sufficiently larger than a physical memory is prepared and a process is mapped into the virtual memory space. Now, \u201cprocess\u201d means a program which is being executed under management of an OS (Operating System). It is, therefore, sufficient to consider only the operation on a virtual memory as for the process. A MMU (Memory Management Unit) is used for mapping from the virtual memory to the physical memory. The MMU is usually managed by the OS (Operating System) and exchanges the physical memory so that the virtual memory which is needed by the process can be mapped into the physical memory. The exchange of the physical memory is performed between the MMU and a secondary storage or the like. The MMU generally also has a function to protect the storage so that a certain process doesn't erroneously access a physical memory of another process.</p><p>When an address translation from an address (virtual address) in the virtual memory to an address (physical address) in the physical memory is performed by using the MMU, there is a case where the address translation information is not registered in the MMU or a virtual memory of another process is erroneously accessed. In this instance, the MMU generates an exception, changes the mapping of the physical memory, and registers new address translation information.</p><p>Although the function of the MMU can be realized even by only software, if the translation is performed by software each time the process accesses to the physical memory, the efficiency thereof is low. To prevent it, a translation lookaside buffer for address translation is prepared on the hardware and address translation information which is frequently used is stored in the translation lookaside buffer. That is, the translation lookaside buffer is constructed as a cache memory for the address translation information. A different point from an ordinary cache memory is that when the address translation fails, the exchange of the address translation information is performed mainly in dependence on software.</p><p>Various cache memories are widely used to realize a high speed of data and instruction access.</p><h4>SUMMARY OF THE INVENTION</h4><p>The present inventors have examined the translation lookaside buffer and cache memory from a viewpoint of realizing a high speed of the memory access. As a processor to divide the translation lookaside buffer into a buffer for an instruction and a buffer for data, for example, there is a processor disclosed in PowerPC 603 RISC Microprocessor User's Manual (MOTOROLA, 1994). The processor further individually has a data cache memory and an instruction cache memory. At pages 7 to 15 of this literature, it will be understood that an instruction TLB miss and a data TLB miss are separately treated in the PowerPC. According to the examination of the present inventors, even if the translation lookaside buffers are separately provided, since there is no interrelation between them, if the address translation fails, necessary address translation information has to be obtained from an external memory and it has been found that there is a limitation in realization of a high memory accessing speed.</p><p>As for the cache memory, when a cache miss occurs, a cache entry is newly read out from the external memory by only an amount of one entry. In this instance, if there is no invalid cache entry, a valid cache entry is swept out from the cache memory in accordance with a logic such as LRU (Least Recently Used) or the like. The cache entry which was swept out as mentioned above may include data or instruction to be subsequently used. Therefore, it is desirable that an instruction to specify a processing routine such that a high speed or the like is required is always held in the cache memory. In such a case, it is also considered to enable the cache memory to be used as a random access memory. However, if all of the areas in the cache memory are constructed as mentioned above, all of the functions as a cache memory are lost, so that a case where an inconvenience is caused in dependence on an application is also presumed.</p><p>It is an object of the invention to provide a data processor which can realize a high memory accessing speed. In more detail, it is an object to provide a technique for realizing a high memory accessing speed from a viewpoint of address translation and to provide a technique for realizing a high memory accessing speed from a viewpoint of a cache memory.</p><p>The above and other objects and novel features of the present invention will be clarified from the description of the specification and the annexed drawings.</p><p>An outline of a typical invention among the inventions disclosed in the present invention will be briefly described as follows.</p><p>That is, according to a first aspect of the invention, a translation lookaside buffer is separately used for data and for an instruction, address translation information for instruction is also stored into the translation lookaside buffer for data, and when a translation miss occurs in the translation lookaside buffer for instruction, new address translation information is fetched from the translation lookaside buffer for data.</p><p>In detail, a data processor (<b>1</b>) comprises: a central processing unit (<b>2</b>); a first translation lookaside buffer (<b>4</b>) in which a part of address translation information to translate a virtual address that is treated by the central processing unit into a physical address is stored and which association-retrieves, from the address translation information, a physical address corresponding to the virtual address that is outputted by the central processing unit; and a second translation lookaside buffer (<b>3</b>) in which address translation information regarding an instruction address in address translation information possessed by the first translation lookaside buffer is stored and which association-retrieves, from the address translation information, a physical address corresponding to the virtual address that is outputted by the central processing unit upon instruction fetching, when a result of the associative retrieval indicates a retrieval miss, association-retrieves the first translation lookaside buffer by a virtual address according to the retrieval miss, and obtains the address translation information retrieved by the associative retrieval.</p><p>Another data processor according to such an aspect comprises: a central processing unit; a first translation lookaside buffer in which a part of address translation information to translate a virtual address that is treated by the central processing unit into a physical address is stored and which associatively retrieves, from the address translation information, a physical page number corresponding to a virtual page number that is outputted by the central processing unit; a second translation lookaside buffer in which address translation information regarding an instruction address in address translation information possessed by the first translation lookaside buffer is stored and which associatively retrieves, from the address translation information, a physical page number corresponding to the virtual page number that is outputted by the central processing unit upon instruction fetching; and a buffer control circuit (<b>320</b>) for, when a result of the associative retrieval by the second translation lookaside buffer indicates a retrieval miss, associatively retrieving the first translation lookaside buffer by a virtual page number according to the retrieval miss, and for supplying the address translation information retrieved by the associative retrieval to the second translation lookaside buffer.</p><p>According to the above means, when the translation miss occurs in the translation lookaside buffer for instruction, since the new address translation information is fetched from the translation lookaside buffer for data. Therefore, a high speed of the address translating operation can be realized as compared with a case of obtaining the address translation information from an external address translation table every time at the time of the translation miss. Thus, a high memory accessing speed is accomplished. Particularly, a reason why the translating speed of the instruction address is made high is because an operand fetch is performed in accordance with a decoding result of the fetched instruction or because a capacity of the translation lookaside buffer for instruction is reduced (the number of entries is small) as compared with that of the translation lookaside buffer for data.</p><p>When the result of the associative retrieval by the second translation lookaside buffer indicates the retrieval miss and the result of the associative retrieval of the first translation lookaside buffer by the virtual page number according to the retrieval miss indicates the retrieval miss, the central processing unit reads out the address translation information including the virtual page number according to the retrieval miss from an external memory provided out of the data processor by an exceptional process and writes the read-out address translation information into the first translation lookaside buffer. After completion of the exceptional process, the interrupted address translating operation is continued.</p><p>According to another aspect of the invention, only a partial area in the cache memory is selectively made operative as a random access memory. In other words, the cache function is suppressed for only the partial area.</p><p>In detail, the data processor further comprises a data cache memory (<b>6</b>) in which a cache entry of data is stored in correspondence to the physical page number and to which the physical page number which was associatively retrieved by the first translation lookaside buffer is supplied and which associatively retrieves the cache entry corresponding to the physical page number. In this instance, a part of the data cache memory is mapped into a predetermined area (E<b>1</b>) that is specified by the virtual address. The data processor further comprises first RAM area discrimination control means (<b>605</b>) for detecting the access to the predetermined area and allowing the data cache memory to perform a random accessing operation.</p><p>The data processor further includes an instruction cache memory (<b>5</b>) in which a cache entry of an instruction is stored in correspondence to the physical page number and to which the physical page number which is associatively retrieved by the second translation lookaside buffer is supplied and which associatively retrieves a cache entry corresponding to the physical page number. In this instance, a part of the instruction cache memory is mapped into the predetermined area (E<b>1</b>) that is specified by the virtual address. The data processor further comprises second RAM area discrimination control means (<b>505</b>) for detecting the access to the predetermined area and for allowing the instruction cache memory to perform a random accessing operation.</p><p>According to the above means, the predetermined areas in the data cache memory and the instruction cache memory are accessed at random and the remaining areas in both of the cache memories are made operative as cache memories to be associatively retrieved. Therefore, particularly, a condition that desired instruction and data which need a high accessing speed are always held in the cache memory and a condition that the instruction and data used recently are held in the cache memory can be satisfied. It contributes to the improvement of a data processing speed.</p><p>According to still another aspect of the invention, as an index address to select a cache line of the cache memory, a bit position of the virtual address is switched to an upper bit position than that in the ordinary operation. Thus, the cache memory is divided every large address space and is allocated to a virtual memory space.</p><p>In more detail, index mode designating means (<b>630</b>) for selectively using a bit on the upper side of the virtual address for the selection of the cache line of the data cache memory is further provided.</p><p>Index mode designating means (<b>530</b>) for selectively using a bit on the upper side of the virtual address for the selection of the cache line of the instruction cache memory is further provided.</p><p>According to the above means, since the bit on the upper side of the virtual address can be used for an index of the cache. Therefore, the cache memory is divided every large address space and can be allocated to the virtual memory space. Thus, the cache of a direct mapping can be falsely treated as a set-associative cache. The invention can contributes to the improvement of the data processing speed.</p><p>Further another aspect of the invention is to improve a use efficiency of the data processor.</p><p>First, an I/O register area (I/O register space) is mapped from a virtual address space (address space on the virtual memory) to a physical address space (address space on the physical memory). That is, there is further provided detecting means (<b>606</b>) for inputting the physical page number which is outputted by an associated hit by the associative retrieval due to the first translation lookaside buffer, for detecting whether the inputted physical page number coincides with the page number allocated to the I/O register space in the data processor or not, for suppressing the associative retrieving operation of the data cache memory by the detection of the coincidence, and for allowing the I/O register to be directly accessed. In this instance, the translation information which is stored into the first translation lookaside buffer has protection information to specify an access privilege to a page and there is provided access protecting means (<b>405</b>) for discriminating an access privilege for the relevant page on the basis of the protection information of translation information according to the associated hit. Thus, the storage protection can be also performed for the I/O register space.</p><p>Second, the translation information which is stored into the first translation lookaside buffer has cache write mode specified information (WT) for specifying which one of write-through and write-back is used as a write control mode for the data cache memory, and there is provided cache write control means (<b>614</b>) for controlling a cache write mode for the relevant page on the basis of the cache write mode information included in the translation information regarding the associated hit. In the write-through mode, the writing operation is performed to both of the cache memory and the external memory in case of a cache hit and is performed to only the external memory in case of a cache miss. In the write-back mode, data is written into a cache entry (cache line) regarding the hit in case of the cache hit, one cache entry is read out from the external memory in case of the cache miss (cache fill), a tag address is updated, and data is written to the cache line. A dirty bit of the cache line which was cache filled as mentioned above is set into a set state. When the cache line which is swept out from the cache memory by the cache fill is dirty, the cache line is written back to the external memory. In this manner, in case of the write-through mode, although the contents in the cache memory and the external memory are always made coincident, the number of times of access to the external memory increases. In the write-back mode, although the number of times of access to the external memory is small, in the case where a period of time during which the contents in the cache memory and the external memory don't coincide exists and a plurality of cache memories unifies the external memory, there is a case where a consistency between the cache memory and the external memory cannot be held. If the write-through mode and the write-back mode can be selected on a page unit basis, the relation between the consistency of the cache memory and the external memory and the accessing speed can be optimized in accordance with the system construction or the contents of the process.</p><p>A data processing apparatus to which the above data processor is applied has an external memory connected to the data processor and its secondary storage.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>FIG. 1 is a block diagram of a data processor according to an example of the invention;</p><p>FIG. 2 is a block diagram showing an example of a unified TLB;</p><p>FIG. 3 is an explanatory diagram of a page size;</p><p>FIG. 4 is a block diagram showing an example of an instruction TLB;</p><p>FIG. 5 is a flowchart showing a procedure of an address translating process in an instruction access;</p><p>FIG. 6 is a flowchart showing an outline of an exceptional process to a TLB miss;</p><p>FIG. 7 is a block diagram showing an example of a data cache memory;</p><p>FIG. 8 is an address map of a virtual address space;</p><p>FIG. 9 is a block diagram showing an example of RAM area discrimination control means and index mode designating means;</p><p>FIG. 10 is an address map of a physical address space;</p><p>FIG. 11 is a block diagram showing an example of an instruction cache memory;</p><p>FIG. 12 is a block diagram showing an example of a self testing circuit; and</p><p>FIG. 13 is a block diagram showing an example of a data processing system to which the data processor of FIG. 1 is applied.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF THE PREFERRED EMBODIMENT</h4><p>[Construction of Data Processor]</p><p>FIG. 1 shows a block diagram of a data processor according to an example of the invention. Although not particularly limited, a data processor <b>1</b> shown in the diagram has a pipeline RISC (Reduced Instruction Set Computer) architecture of 32 bits and an instruction set of the data processor has a fixed length of 16 bits.</p><p>The data processor <b>1</b> individually has TLBs (translation lookaside buffers) <b>3</b> and <b>4</b> for instruction and for data so as to enable an instruction access and a data access by a central processing unit (CPU) <b>2</b> to be performed in parallel. An instruction cache memory <b>5</b> and a data cache memory (operand cache memory) <b>6</b> are also made individual.</p><p>Although not particularly limited, the data processor <b>1</b> handles a virtual address space that is specified by a virtual address of 32 bits and a physical address space that is specified by a physical address of 29 bits. Address translation information to translate the virtual address into the physical address includes a virtual page number and a physical page number corresponding thereto. An address translation table is formed in an external memory of the data processor <b>1</b>. In the address translation information in the address translation table, a part of the information used recently is stored into the translation lookaside buffers <b>3</b> and <b>4</b>. Its control is performed by, for example, the OS of the data processor <b>1</b>.</p><p>The TLB for data (also referred to as a unified TLB) <b>4</b> stores the address translation information of data and instruction by an amount of up to 64 entries. From the address translation information, the unified TLB <b>4</b> associatively retrieves a physical page number corresponding to a virtual page number of a virtual address that is outputted to a signal line <b>111</b> by the CPU <b>1</b> for fetching data and translates the virtual address to the physical address. The virtual address space is divided into units of pages and is translated into physical addresses on a page unit basis.</p><p>The TLB for instruction (hereinafter, also referred to as an instruction TLB) <b>3</b> stores address translation information only for instruction by an amount of up to four entries. Particularly, entries possessed by the instruction TLB <b>3</b> are used as a part of the translation information of instruction addresses possessed by the unified TLB <b>4</b>. The supply of the entries from the unified TLB <b>4</b> to the instruction TLB <b>3</b> is performed through a signal line <b>116</b>. From the address translation information, the instruction TLB <b>3</b> associatively retrieves a physical page number corresponding to a virtual page number of a virtual address that is outputted to a signal line <b>110</b> by the CPU <b>1</b> for fetching the instruction and translates the virtual address to the physical address. In case of a retrieval miss, an operation to obtain the target address translation information from the unified TLB <b>4</b> is instructed through a signal line <b>115</b>.</p><p>The data cache memory <b>6</b> receives the physical address translated by the unified TLB <b>4</b> upon data fetching through a signal line <b>120</b> and performs an association retrieval of the cache entry on the basis of it. When the retrieval result indicates a read hit, data corresponding to the physical address is outputted to a CPU bus <b>117</b> from the cache line according to the hit. When the retrieval result indicates a read miss, data of one cache line including data regarding the miss is read out from the external memory through a bus controller <b>7</b> and a cache fill is performed, so that the data regarding the miss is read out to the CPU bus <b>117</b>. When the retrieval result indicates a write miss, its operation is made different in accordance with a write-through mode or a write-back mode, which will be explained in detail hereinlater.</p><p>The instruction cache memory <b>5</b> receives the physical address translated by the instruction TLB <b>3</b> upon instruction fetching through a signal line <b>125</b> and executes an associative retrieval of the cache entry on the basis of the physical address. When the retrieval result indicates a read hit, an instruction corresponding to the physical address is outputted to a signal line <b>114</b> from a cache line corresponding to the hit. When the retrieval result indicates a read miss, data of one cache line including the instruction corresponding to the miss is read out from the external memory (not shown) through the bus controller <b>7</b> and the cache fill is executed. Thus, the instruction regarding the miss is supplied to the CPU <b>2</b> through the signal line <b>114</b>.</p><p>Although the details will be explained later, it is stated here that partial data areas in the instruction cache memory <b>5</b> and data cache memory <b>6</b> can be selectively accessed at random.</p><p>In addition to the unified TLB <b>4</b> and data cache memory <b>6</b>, a peripheral module <b>8</b> and a self testing circuit <b>9</b> are connected to the CPU bus <b>117</b>. Proper circuits such as timer, serial interface controller, and the like are included in the peripheral module <b>8</b>.</p><p>When a self testing instruction is supplied from the CPU <b>2</b> through a signal line <b>112</b>, the self testing circuit <b>9</b> writes and reads data into/from storing areas in the instruction TLB <b>3</b>, unified TLB <b>4</b>, instruction cache memory <b>5</b>, and data cache memory <b>6</b> and returns a message of the completion of the test to the CPU <b>2</b> through a signal line <b>113</b>. A test result can be read by the CPU <b>2</b> via the CPU bus <b>117</b>. The self testing circuit <b>9</b> supplies an access address signal and write data for tests to the instruction TLB <b>3</b> and the like through a signal line <b>119</b>. Although not particularly limited, the data read out from the instruction TLB <b>3</b>, unified TLB <b>4</b>, instruction cache memory <b>5</b>, and data cache memory <b>6</b> is supplied to the self testing circuit <b>9</b> through dedicated signal lines <b>118</b>, <b>125</b>, <b>126</b>, and <b>127</b>, respectively.</p><p>The instruction cache memory <b>5</b>, data cache memory <b>6</b>, and external bus controller <b>7</b> are connected by a cache address bus <b>121</b>, a cache data bus <b>122</b>, and a control bus (not shown). The external bus controller <b>7</b> controls an activation of an external bus cycle which is necessary to fetch data that is necessary for the cache fill of the instruction cache memory <b>5</b> and the data cache memory <b>6</b> from the external memory and to write back the data of the cache line of the data cache memory <b>6</b> into the external memory. The external bus controller <b>7</b> is connected to an external address bus <b>123</b>, an external data bus, and an external control bus (not shown). The external memory (not shown) is connected to the external address bus <b>123</b>, an external data bus <b>124</b>, and the like. The data processor <b>1</b> shown in FIG. 1 is formed on one semiconductor substrate such as monocrystalline silicon.</p><p>[Unified TLB]</p><p>FIG. 2 shows a block diagram of an example of the unified TLB <b>4</b>. The unified TLB <b>4</b> has a memory cell array to store up to 64 TLB entries. The memory cell array is constructed by an address array <b>400</b> and a data array <b>401</b>. Although not particularly limited, one TLB entry has a virtual page number VPN, a valid bit V, a size bit SZ, a physical page number PPN, a flag FLAG, and a cache write mode bit WT. Areas to store the virtual page number VPN, valid bit V, and size bit SZ are constructed in an address array <b>400</b>. Areas to store the physical page number PPN, flag FLAG, and cache write mode bit WT are constructed in a data array <b>401</b>.</p><p>Although not particularly limited, as shown in FIG. 3, a page size of the virtual address can be selected from sizes of 1 kbytes, 4 kbytes, 64 kbytes, and 1M bytes. The page size is designated by the size bits SZ of two bits.</p><p>The valid bit V indicates a validity of the TLB entry including it and denotes a valid status by the logic value \u201c1\u201d. The flag FLAG includes protection data or the like. The protection data is data of two bits in which an access privilege of the page is shown by a code. For instance, \u201c00\u201d denotes that only the reading is possible in a privilege mode. \u201c01\u201d denotes that the reading and the writing are possible in the privilege mode. \u201c10\u201d denotes that only the reading is possible in the privilege mode and a user mode. \u201c11\u201d denotes that the reading and the writing are possible in the privilege mode and the user mode. The cache write mode bit WT designates in which one of the write-through mode and the write-back mode the data cache memory <b>6</b> is made operative. As will be explained hereinlater, the write-through mode or the write-back mode can be selected on a page unit basis for the data cache memory <b>6</b>.</p><p>Although not particularly limited, the address array <b>400</b> is made up of a CAM (Content Addressable Memory) and a memory cell itself has a comparing function as is well known. In the retrieving operation, memory cells of the address array <b>400</b> are selected in parallel and execute the comparing operation. In FIG. 2, a circuit element which executes the comparing operation is conceptually shown as a comparator. Four comparators <b>402</b>A to <b>402</b>D typically shown indicate elements of comparing circuits corresponding to areas in which the virtual page number of one TLB entry has been stored. Reference numeral <b>402</b>A is intended to be an element for performing the comparator operation of bit <b>10</b> to bit <b>11</b> of the virtual address; <b>402</b>B an element for performing the comparator operation of bit <b>12</b> to bit <b>15</b> of the virtual address; <b>402</b>C an element for performing the comparator operation of bit <b>16</b> to bit <b>19</b> of the virtual address; and <b>402</b>D an element for performing the comparator operation of bit <b>20</b> to bit <b>31</b> of the virtual address. When a comparison result in each of the comparing circuits <b>402</b>A to <b>402</b>D indicates that all bits coincide, a comparison result signal is set to the logic value \u201c1\u201d.</p><p>Comparison targets by the comparators <b>402</b>A to <b>402</b>D are set to corresponding bits of the virtual page numbers which are supplied through the signal line <b>111</b>. Reference numeral <b>403</b> denotes a masking circuit for masking the comparison results by the comparing circuits <b>402</b>A to <b>402</b>C in accordance with the size bit SZ. That is, when the size bit SZ indicates a page of 1 kbytes, the comparison results of the comparing circuits <b>402</b>A to <b>402</b>C are not masked. When the size bit SZ indicates a page of 4 kbytes, the comparison result of the comparing circuit <b>402</b>A is masked. When the size bit SZ indicates a page of 64 kbytes, the comparison results of the comparing circuits <b>402</b>A and <b>402</b>B are masked. When the size bit SZ indicates a page of 1 Mbytes, the comparison results of the comparing circuits <b>402</b>A to <b>402</b>C are masked. According to this example, the masked comparison result is forcedly set to the logic value \u201c1\u201d and is outputted from the masking circuit <b>403</b>. An output of the masking circuit <b>403</b> and the valid bit V of the corresponding TLB entry are supplied to an AND circuit <b>404</b>. An output of the AND circuit <b>404</b> is used as a hit/miss signal <b>420</b> of the TLB entry. Actually, the comparing circuits <b>402</b>A to <b>402</b>D, masking circuit <b>403</b>, and AND circuit <b>404</b> are provided for each of the 64 storing areas of the TLB entry. Therefore, when the TLB entry including the virtual page number which is transferred to the signal line <b>111</b> exists, the output of the AND circuit <b>404</b> corresponding to the TLB entry is set to the logic value \u201c1\u201d.</p><p>The output of each AND circuit <b>404</b> is used as a selection signal of the corresponding TLB entry line in the data section <b>401</b>. The physical page number PPN of the TLB entry line corresponding to the selection signal of the logic value \u201c1\u201d is outputted to the signal line <b>120</b>. The flag FLAG is outputted to an access privilege discriminating circuit <b>405</b>. The cache write mode bit WT is outputted to the data cache memory <b>6</b>.</p><p>The physical page number PPN sent to the signal line <b>120</b> is supplied to the data cache memory <b>6</b> together with an offset (offset of the physical address) of the virtual address transferred to the signal line <b>111</b> and the like.</p><p>A signal (not shown) indicating that the data processor <b>1</b> is at present in which one of the user mode and the privilege mode and indicating which one of the reading and the writing the accessing operation by the CPU <b>2</b> instructs is supplied to the access privilege discriminating circuit <b>405</b>. On the basis of this signal, a check is made to see if the content of the flag FLAG is adapted to the present operating mode of the data processor. When it is not adapted, a protection violation exceptional signal <b>408</b> is supplied to the CPU <b>2</b>.</p><p>The output of each AND circuit <b>404</b> is supplied to TLB miss discriminating circuit <b>406</b>. When it is determined that the outputs of all of the AND circuits <b>404</b> re set to the logic value \u201c0\u201d, the TLB miss discriminating circuit <b>406</b> supplies a TLB miss exceptional signal <b>407</b> to the CPU <b>2</b>. When the TLB miss exception is accepted, the CPU <b>2</b> executes an exceptional process for adding, for example, the TLB entry regarding the TLB miss to the unified TLB <b>4</b> from the address translation table of the external memory by the OS. In the addition of the TLB entry, the TLB entry to be added is fetched to the address section <b>400</b> and data section <b>401</b> from the CPU bus <b>117</b> through a selector <b>409</b>. The selection of the TLB entry line in this instance is performed by fetching an index address which is sent to the signal line <b>111</b> from the CPU <b>2</b> by a selector <b>410</b> and by decoding it by an index decoder <b>411</b>. Although not particularly limited, the selection control for the selectors <b>409</b> and <b>410</b> is executed by the TLB miss discriminating circuit <b>406</b>.</p><p>Although the details will be explained hereinlater, when an instruction of a retrieval reading process is made to the unified TLB <b>4</b> from the instruction TLB <b>3</b>, a virtual page number for a retrieval hit discrimination in the retrieval reading is supplied from the signal line <b>115</b> through a selector <b>412</b>. The output of the TLB entry in the retrieval reading process is executed to the signal line <b>116</b>. An index address for the TLB entry selection in the self test is supplied from the signal line <b>119</b> to the selector <b>410</b>. Write data in the self test is supplied from the signal line <b>119</b> through the selector <b>409</b>.</p><p>[Instruction TLB]</p><p>FIG. 4 shows a block diagram of an example of the instruction TLB <b>3</b>. The instruction TLB <b>3</b> has a memory cell array to store up to four TLB entries. The memory cell array is made up of an address array <b>300</b> and a data array <b>301</b>. Although not particularly limited, one TLB entry has the virtual page number VPN, valid bit V, size bit sz, physical page number PPN, and flag FLAG. Areas to store the virtual page number VPN, valid bit V, and size bit SZ are constructed in the address array <b>300</b>. Areas to store the physical page number PPN and flag FLAG are constructed in the data array <b>301</b>. The contents of the page size, valid bit V, and flag FLAG of the virtual address are not so different from those mentioned above.</p><p>Although not particularly limited, the address array <b>300</b> is constructed by a CAM and the memory cell itself has a comparing function as is well known. In the retrieving operation, the memory cells of the address array <b>300</b> are selected in parallel and execute the comparing operation. In FIG. 4, a circuit element which performs the comparing operation is conceptually shown as a comparator. Four comparators <b>302</b>A to <b>302</b>D typically shown indicate elements of the comparing circuit corresponding to the area in which the virtual page number of one TLB entry has been stored. Reference numeral <b>302</b>A is intended to be an element for performing the comparator operation of bit <b>10</b> and bit <b>11</b> of the virtual address; <b>302</b>B an element for performing the comparator operation of bit <b>12</b> to bit <b>15</b> of the virtual address; <b>302</b>C an element for performing the comparator operation of bit <b>16</b> to bit <b>19</b> of the virtual address; and <b>302</b>D an element for performing the comparator operation of bit <b>20</b> to bit <b>31</b> of the virtual address, respectively. When the comparison result in each of the comparing circuits <b>302</b>A to <b>302</b>D indicates that all of the bits coincide, a comparison result signal is set to the logic value \u201c1\u201d.</p><p>Comparison targets by the comparators <b>302</b>A to <b>302</b>D are set to the corresponding bits of the virtual page number which is supplied through the signal line <b>110</b>. Reference numeral <b>303</b> denotes a masking circuit and masks the comparison results by the comparing circuits <b>302</b>A to <b>302</b>C in accordance with the size bit SZ. That is, when the size bit SZ indicates the page of 1 kbytes, the comparison results of the comparing circuits <b>302</b>A to <b>302</b>C are never masked. When the size bit SZ indicates the page of 4 kbytes, the comparison result of the comparing circuit <b>302</b>A is masked. When the size bit SZ indicates the page of 64 kbytes, the comparison results of the comparing circuits <b>302</b>A and <b>302</b>B are masked. When the size bit SZ indicates the page of 1 Mbytes, the comparison results of the comparing circuits <b>302</b>A to <b>302</b>C are masked. According to this example, the masked comparison result is forcedly set to the logic value \u201c1\u201d and is outputted from the masking circuit <b>303</b>. An output of the masking circuit <b>303</b> and the valid bit V of the corresponding TLB entry are supplied to an AND circuit <b>304</b>. An output of the AND circuit <b>304</b> is used as a hit/miss signal <b>320</b> of the TLB entry. Actually, the comparing circuits <b>302</b>A to <b>302</b>D, masking circuit <b>303</b>, and AND circuit <b>304</b> are provided for each of the four storing areas of the TLB entry. Therefore, when the TLB entry including the virtual page number which is sent to the signal line <b>110</b> exists, the output of the AND circuit <b>304</b> corresponding to the TLB entry is set to the logic value \u201c1\u201d.</p><p>The output of each of the AND circuits <b>304</b> is used as a selection signal of the corresponding TLB entry line in the data section <b>301</b>. The physical page number PPN of the TLB entry line corresponding to the selection signal of the logic value \u201c1\u201d is outputted to the signal line <b>125</b>. The flag FLAG is outputted to the access right discriminating circuit <b>405</b>.</p><p>The physical page number PPN sent to the signal line <b>125</b> is supplied to the instruction cache memory <b>5</b> together with an offset (offset of the physical address) of the virtual address outputted to the signal line <b>110</b>.</p><p>The output of each of the AND circuits <b>304</b> is supplied to a retrieval read control circuit <b>320</b> to instruct a retrieval reading process to the unified TLB <b>4</b>. When it is decided that the outputs of all of the AND circuits <b>304</b> are equal to the logic value \u201c0\u201d (instruction TLB miss), the retrieval read control circuit <b>320</b> starts a control to read the necessary instruction TLB entry from the unified TLB <b>4</b>. That is, the virtual page number and a necessary control signal regarding the instruction TLB miss are supplied to the unified TLB <b>4</b> through the signal line <b>115</b>. Thus, the unified TLB <b>4</b> accesses the address section <b>400</b> in parallel and retrieves the TLB entry which coincides with the virtual page number sent from the signal line <b>115</b>. When a retrieval result indicates the hit, all of the TLB entries (VPN, V, SZ, PPN, FLAG) regarding the hit are supplied in parallel to the instruction TLB <b>3</b> via the signal line <b>116</b> (in this manner, the information that is outputted from the unified TLB <b>4</b> to the outside when the retrieval reading process is instructed from the instruction TLB <b>3</b> also includes the contents of the address section <b>400</b> and differs from that in case of the ordinary retrieval reading operation in the unified TLB <b>4</b>). The instruction TLB <b>3</b> fetches the TLB entries which are supplied from the unified TLB <b>4</b> via a selector <b>309</b>. An index address in this instance is supplied from the retrieval read control circuit <b>320</b> to an index decoder <b>311</b> through a selector <b>310</b>. Although not particularly limited, when adding the TLB entries, the retrieval read control circuit <b>320</b> can replace the TLB entries by the logic of LRU.</p><p>When the retrieval result by the instruction of the retrieval reading process to the unified TLB <b>4</b> from the instruction TLB <b>3</b> indicates the retrieval miss, a TLB miss exception is notified to the CPU <b>2</b> by the TLB miss discriminating circuit <b>406</b>. Thus, the CPU <b>2</b> adds the entries regarding the TLB miss exception to the unified TLB <b>4</b> from the translation lookaside table of the external memory (not shown) as mentioned above. After completion of the exceptional process, the interrupted instruction is again executed, so that a retrieval hit is obtained in the unified TLB <b>4</b>. Consequently, the TLB entries necessary for the instruction TLB <b>3</b> are supplied to the instruction TLB <b>3</b> through the signal line <b>116</b> as mentioned above.</p><p>When the TLB entries are added to the instruction TLB <b>3</b>, the TLB entries to be added are fetched from the signal line <b>116</b> to the address section <b>300</b> and data section <b>301</b> by the selector <b>309</b>. The selection of the TLB entry line at this time is performed by fetching the index address which is supplied from the retrieval read control circuit <b>320</b> by the selector <b>310</b> and by decoding it by the index decoder <b>311</b>. Although not particularly limited, the retrieval read control circuit <b>320</b> performs the selection control for the selectors <b>309</b> and <b>310</b>.</p><p>Although the details will be explained hereinlater, the index address for the TLB entry selection in the self test is transmitted from the signal line <b>119</b> through the selector <b>310</b>. The write data in the self test is supplied from the signal line <b>119</b> via the selector <b>309</b>. The reading operation in the self test is performed to one whole TLB entry which was indexed. All of the indexed TLB entry are supplied to the signal line <b>118</b>.</p><p>[Address Translation in the Instruction Access]</p><p>A procedure of an address translating process in an instruction access will now be described with reference to FIGS. 5 and 6. When the instruction access by the CPU <b>2</b> is activated (start of the instruction fetching by the CPU <b>2</b>), the instruction TLB <b>3</b> retrieves the presence or absence of the TLB entry according to the instruction address and discriminates whether there is a retrieval hit or miss (S<b>1</b>). When there is a retrieval hit, the physical address corresponding to the virtual address is outputted (S<b>2</b>). When there is a retrieval miss in step S<b>1</b>, the unified TLB <b>4</b> retrieves the presence or absence of the TLB entry according to the instruction address in accordance with an instruction from the retrieval read control circuit <b>320</b> (S<b>3</b>), thereby discriminating about the retrieval hit or miss (S<b>4</b>). When there is a retrieval hit, the TLB entry corresponding to the virtual page number of the instruction address is registered into the instruction TLB <b>3</b>. After it was registered, the processing routine is returned to S<b>1</b>. When a discrimination result in step S<b>4</b> indicates the retrieval miss, the TLB miss exception is generated by the TLB miss discriminating circuit <b>406</b>. When the TLB miss exception is generated, the CPU <b>2</b> interrupts the present process and executes a saving process (S<b>10</b>), subsequently registers the TLB entry of the virtual page number it U is initialized to the logic value \u201c0\u201d by a power-on reset.</p><p>Although not particularly limited, the data cache memory <b>6</b> is used for direct mapping. The selection of the cache line is performed by an index decoder <b>602</b>. The index address is supplied from a control circuit <b>603</b> through a selector <b>604</b>. The control circuit <b>603</b> performs a control to distribute, to each section, the virtual addresses which are supplied from the signal line <b>111</b> and the physical page numbers which are supplied from the signal line <b>120</b>. The control circuit <b>603</b> also has RAM area discrimination control means <b>605</b>, index mode designating means <b>630</b>, and I/O register area detecting means <b>606</b>, which will be explained in detail hereinlater.</p><p>The cache tag of the indexed cache line is compared with the corresponding physical page number by a comparator <b>607</b>. The physical page number is supplied from the unified TLB <b>4</b> through the signal line <b>120</b>. When the cache tag CTAG and the physical page number coincide and the valid bit V is equal to the logic value \u201c1\u201d, a cache hit signal <b>608</b> which is outputted from the comparator <b>607</b> is set to the logic value \u201c1\u201d (cache hit). When the cache hit is notified by the cache hit signal <b>608</b>, a gate <b>609</b> allows the data of the indexed cache line to pass to the post stage. A part of the data which was allowed to pass through the gate <b>609</b> by the cache hit is selected by a selector <b>610</b> and is supplied to a bus control circuit <b>611</b>. The selector <b>610</b> executes the selecting operation by using part of the offset address. Such a part of the offset address is extracted by the control circuit <b>603</b> and is supplied via a signal line <b>623</b>.</p><p>The bus control circuit <b>611</b> is connected to an output of the selector <b>610</b>, CPU bus <b>117</b>, cache data bus <b>122</b>, cache address bus <b>121</b>, and the like. Further, the cache hit signal <b>608</b>, the physical address from a signal line <b>616</b>, a read signal and write signal <b>615</b> from the CPU <b>2</b>, and the like are supplied to the bus control circuit <b>611</b>. The bus control circuit <b>611</b> executes a control for outputting the read-out data regarding the cache hit which is outputted from the selector <b>610</b> to the CPU bus <b>117</b>, a control for outputting the physical address for the external memory access to the cache address bus <b>121</b> at the time of the cache miss, a control for writing (cache fill) the data from the external memory through a selector <b>612</b>, a control for writing the cache tag CTAG to the address section of the cache-filled cache line through a selector <b>622</b>, a control for outputting the data to the cache data bus <b>122</b> and for outputting a write-back destination address to the cache address bus <b>121</b> when the data is written back to the external memory, and the like. In addition to the logic for the above controls, the bus control circuit <b>611</b> includes a write-back buffer <b>613</b>. When a necessity to sweep out the dirty cache entry (cache line of U=1) to the external memory occurs due to the cache miss, the write-back buffer <b>613</b> is a data buffer to store the entry to be swept out in order to improve the performance while preferentially performing the cache filling operation. The write-back buffer <b>613</b> has the data as much as one entry of the cache and the storing area of the physical address of the sweep-out destination.</p><p>A cache write control circuit <b>614</b> controls the write-through mode and the write-back mode for the data cache memory <b>6</b>. Any one of the operating modes to control is determined by the cache write mode bit WT included in the TLB entry.</p><p>The control contents by the bus control circuit <b>611</b> and cache write control circuit <b>614</b> will now be separately described with respect to the associated reading operation and the associated writing operation.</p><p>When a reading request of the data is issued from the CPU <b>2</b> to the area which can be cached, the cache line is selected by the index address shown by a part of the virtual addresses. The cache tag CTAG is read out from the selected cache line. The read-out cache tag is compared with the physical page number which is supplied from the unified TLB <b>4</b>. When the cache tag coincides and the valid bit V is equal to the logic value \u201c1\u201d, it is determined that there is the cache hit. For example, data of a long word is outputted from the selector by using a part of the offset of the virtual address. The read-out data is supplied to the CPU bus <b>117</b> by the bus control circuit <b>611</b>. When the tag address doesn't coincide or the valid bit V is equal to the logic value \u201c0\u201d, it is decided that there is the cache miss. The bus control circuit <b>611</b> reads out the data as much as one entry of the cache from the external memory corresponding to the physical address according to the miss via the selector <b>612</b>. Such a data reading operation is called a cache fill. After the necessary data was stored into the data array <b>601</b> by the cache fill, by setting the valid bit V of the cache line to the logic value \u201c1\u201d, the cache tag CTAG is updated. The necessary data is returned to the CPU <b>2</b>. When the cache entry to be swept out from the cache data array <b>601</b> is dirty at the time of the cache fill, the dirty cache entry is swept out to the write-back buffer <b>613</b>. After that, the cache fill is performed. The write-back to the external memory from the write-back buffer <b>613</b> is executed after completion of the cache fill.</p><p>When a writing request of data is generated from the CPU <b>2</b> to the area which can be cached, a discrimination about the cache hit is performed in a manner similar to the reading operation. In case of the cache hit, when the write-back mode is instructed, the data is written into the hit cache entry and the dirty bit U is set to U=1. In the write-through mode, after the data was written into the hit entry, the data is written into the external memory. In this case, the operation for the dirty bit U is not executed. In case of the cache miss, in the write-back mode, the cache fill is executed, the valid bit V is set to V=1, the dirty bit U is set to U=1, the cache tag is updated, and the writing to the data cache memory is performed. In case of the write-through mode, the writing is executed to only the external memory in the cache miss. The cache fill is not executed. When the cache miss occurs in the write-back mode, the process in the case where the entry to be swept out by the cache filling operation is dirty is substantially the same as that in the reading operation.</p><p>The data cache memory <b>6</b> has an RAM mode and an index mode. The RAM mode is an operating mode for enabling the half of the data array <b>601</b> to be accessed at random as an RAM. In the RAM mode, cache entries <b>0</b> to <b>127</b> and <b>256</b> to <b>383</b> are made function as cache memories. Cache entries <b>128</b> to <b>255</b> and <b>384</b> to <b>511</b> are enabled to be accessed at random. The index mode is an operating mode for dividing the cache by switching the bit position of the virtual address to select the cache line and for allocating to a virtual address space. The RAM mode and the index mode are respectively independently selected by setting a predetermined control bit in a control register <b>620</b> to \u201c1\u201d. In a case other than the RAM mode and the index mode, all of the address array <b>600</b> and data array <b>601</b> are used as cache memories.</p><p>As disclosed in FIG. 8 showing the virtual address space of the data processor <b>1</b>, the RAM areas in the data cache memory have been mapped to 0x7C00 0000 to 0x7FFF FFFF. 0x denotes a hexadecimal notation.</p><p>The RAM area discrimination control means <b>605</b> switches the random accessing operation for the RAM areas and the operation as a cache memory. For example, as shown in FIG. 9, an inverter INV<b>1</b> and a 6-input AND gate AND<b>1</b> are provided in order to detect 0x7C by upper six bits s<b>2</b>_a[<b>31</b>] to s<b>2</b>_a[<b>26</b>] of the virtual address. In FIG. 9, s<b>2</b>_a[<b>13</b>] to s<b>2</b>_a[<b>5</b>] are nine bits included in the virtual address and are regarded as an address for an index. Either one of an output of the AND gate and an address bit s<b>2</b>_a[<b>12</b>] is selected by a selector SEL<b>1</b>. The selecting operation of the selector SEL<b>1</b> is controlled by a control signal <b>621</b>. The control signal <b>621</b> is set to the logic value according to one bit in the control register <b>620</b> and this one bit is a control bit to designate the RAM mode. When the RAM mode is designated, the selector SEL<b>1</b> selects the output of the AND gate. The output of the AND<b>1</b> gate is set to the logic value \u201c1\u201d only when the upper six bits s<b>2</b>_a[<b>31</b>] to s<b>2</b>_a[<b>26</b>] of the virtual address are 0x7C and is set to the logic value \u201c0\u201d in the other cases. In the RAM mode, therefore, when the upper six bits s<b>2</b>_a[<b>31</b>] to s<b>2</b>_a[<b>26</b>] of the virtual address are equal to 0x7C, the areas of entries <b>128</b> to <b>255</b> and <b>384</b> to <b>511</b> in the address array <b>600</b> and data array <b>601</b> are used as targets of the index. In the other addresses, the areas of entries <b>0</b> to <b>127</b> and <b>256</b> to <b>383</b> are used as targets of the index. In the RAM mode, when the output signal of the AND<b>1</b> gate is equal to the logic value \u201c1\u201d, the gate <b>609</b> and bus control circuit <b>611</b> mask the cache hit signal <b>608</b>. The selector <b>610</b> and bus control circuit <b>611</b> on the data array <b>601</b> side enables the random reading operation on a 32-bit unit basis from the data array <b>601</b> by using the remaining parts s<b>2</b>_a[<b>23</b>] to s<b>2</b>_[<b>14</b>] and s<b>2</b>_a[<b>4</b>] to s<b>2</b>_a[<b>2</b>] of the virtual address. When the random writing operation is performed to the data array <b>601</b>, the write data is supplied from the CPU bus <b>117</b> through the selector <b>612</b>. The bus control circuit <b>611</b> executes the control of the selector <b>612</b> in the random writing operation by using the partial virtual address s<b>2</b>_a[<b>23</b>] to s<b>2</b>_a[<b>14</b>] and s<b>2</b> a[<b>4</b>] to s<b>2</b>_a[<b>2</b>] in a manner similar to the random reading operation.</p><p>Since the random access in the RAM mode is the direct access to the RAM areas mapped to the virtual space, the access is executed between the CPU bus <b>117</b> and the CPU <b>2</b>. Even when the RAM mode is set, as for the data cache memory <b>6</b>, the caching operation can be still performed by using the half storing areas in the data cache memory <b>6</b> in response to the memory access except for the RAM areas. Whether the operation is the RAM operation or the caching operation is determined on the basis of the output of the AND gate AND<b>1</b> in FIG. 9 in accordance with the above description.</p><p>The index mode designating means <b>630</b> switches the bit position of the virtual address to select the cache line, thereby dividing the cache and allocating to the virtual address space. For example, as shown in FIG. 9, the 25th bit s<b>2</b>_a[<b>25</b>] and the 13th bit s<b>2</b>_a[<b>13</b>] of the virtual address are selected by a selector SEL<b>2</b>. When the RAM mode is not used, an output of the selector SEL<b>2</b> is used together with s<b>2</b>_a[<b>12</b>] to s<b>2</b>_a[<b>5</b>] of the virtual address for the index. The selecting operation of the selector SEL<b>2</b> is controlled by the control signal <b>621</b>. The control signal <b>621</b> is set to the logic value according to one bit in the control register <b>620</b>. This one bit is a control bit to designate the index mode. When the index mode is designated, the selector SEL<b>2</b> selects s<b>2</b>_a[<b>25</b>]. When the index mode is not designated, s<b>2</b>_a[<b>13</b>] is selected. When the index mode is designated, since s<b>2</b> a[<b>25</b>] is used for the index, the upper side and the lower side of the data cache are separately used every 32 Mbytes. By arranging the program to a boundary of 32 Mbytes, the data cache can be falsely handled in a 2-way set-associative manner.</p><p>The I/O register area discriminating means <b>606</b> discriminates whether the physical page number which is supplied from the unified TLB <b>4</b> coincides with the page number allocated to an I/O register area or not. That is, in the data processor <b>1</b>, as shown in an example in FIG. 10, 0x1F00 0000 to 0x1FFF FFFF in the physical address space are allocated to the I/O register area. The I/O register area is an area to which a register included in the peripheral module <b>8</b>, a register such as a control register <b>620</b> included in the data cache memory <b>6</b>, or the like is allocated. The I/O register area denotes that a register such as a general register of the CPU <b>2</b> or a register such as a floating point register in the case where the data processor includes a floating point unit is excluded. As mentioned above, the I/O register area is a register area having a nature such that it is accessed by designating the address to which it was mapped. The I/O register area detecting means <b>606</b> discriminates whether all of the upper five bits of the physical page number which is outputted as a result of the associated-hit due to the unified TLB <b>4</b> are equal to the logic value \u201c1\u201d (0x1F) or not. When it is detected that the access is an access to the I/O register space, they are supplied to the bus control circuit <b>611</b> by a signal <b>624</b>. Thus, the bus control circuit <b>611</b> inhibits the input and output of the data by the caching operation (associative retrieving operation) of the data cache memory <b>6</b> and executes a bus control for directly accessing the I/O register. In this bus control, the physical address which is supplied to the bus control circuit <b>611</b> through the unified TLB <b>4</b> is used. In this instance as well, since the access privilege protecting circuit <b>405</b> described in FIG. 2 monitors protection information (included in the FLAG) included in the TLB entry, the storage protection can be also performed to the I/O register space. As mentioned above, by mapping the I/O register area (I/O register space) from the virtual address space (address space on the virtual memory) to the physical address space (address space on the physical memory), the storage protection can be also performed to the I/O register space.</p><p>In the self testing mode, the write data and the address signal are supplied to the data cache memory <b>6</b> via the signal line <b>119</b>. The address signal is supplied to the index decoder <b>602</b> through the selector <b>604</b>. The write data is supplied to the data array <b>601</b> and address array <b>600</b> via the selectors <b>612</b> and <b>622</b>. The read-out data from the address array <b>600</b> and data array <b>601</b> is supplied to the self testing circuit <b>9</b> through the dedicated signal line <b>127</b>.</p><p>[Instruction Cache Memory]</p><p>FIG. 11 shows an example of the instruction cache memory <b>7</b>. A fundamental construction of the instruction cache memory <b>5</b> is substantially the same as that of the data cache memory <b>6</b> except for a point that it doesn't have the switching function of the write-back/write-through and the direct accessing function to the I/O register area. A point different from the data cache memory <b>6</b> will now be mainly described.</p><p>The instruction cache memory <b>5</b> has a memory cell array to construct up to <b>256</b> cache lines and the memory cell array is made up of an address array <b>500</b> and a data array <b>501</b>. One cache line includes the cache tag (address tag) CTAG constructed by the physical page number, valid bit V, and <b>16</b> instructions ISTs corresponding to it. The cache tag CTAG and valid bit V are arranged in the address array <b>500</b>. The instructions ISTs are arranged in the data array <b>501</b>.</p><p>Although not particularly limited, the instruction cache memory <b>5</b> is used for direct mapping. The selection of the cache line is performed by an index decoder <b>502</b>. The index address is supplied from a control circuit <b>503</b> through a selector <b>504</b>. The control circuit <b>503</b> controls so as to distribute the virtual address which is supplied from the signal line <b>110</b> and the physical page numbers which are supplied from the signal line <b>125</b> to each section. The control circuit <b>503</b> has RAM area discrimination control means <b>505</b> and index mode designating means <b>530</b>.</p><p>The cache tag of the indexed cache line is compared with the corresponding physical page number by a comparator <b>507</b>. The physical page number is supplied from the instruction TLB <b>3</b> through the signal line <b>125</b>. When the cache tag coincides with the physical page number and the valid bit V is equal to the logic value \u201c1\u201d (cache hit), a cache hit signal <b>508</b> which is outputted from the comparator <b>507</b> is set to the logic value \u201c1\u201d. When the cache hit is notified by the cache hit signal <b>508</b>, a gate <b>509</b> allows the data of the indexed cache line to pass to the post stage. A part of the data which was allowed to pass through the gate <b>509</b> due to the cache hit is selected by a selector <b>510</b> and is supplied to a bus control circuit <b>511</b>. The selector <b>510</b> executes the selecting operation by using a part of the offset address. Such a part of the offset address is extracted by the control circuit <b>503</b> and supplied via a signal line <b>523</b>.</p><p>The bus control circuit <b>511</b> is connected to an output of the selector <b>510</b>, CPU bus <b>117</b>, cache data bus <b>122</b>, cache address bus <b>121</b>, signal line <b>114</b>, and the like. Further, the cache hit signal <b>508</b>, the physical address from a signal line <b>516</b>, a read signal and write signal <b>515</b> from the CPU <b>2</b>, and the like are supplied to the bus control circuit <b>511</b>. The bus control circuit <b>511</b> executes a control for outputting the read-out data regarding the cache hit which is outputted from the selector <b>510</b> to the signal line <b>114</b>, a control for outputting the physical address for the external memory access to the cache address bus <b>121</b> at the time of the cache miss, a control for writing (cache fill) the data from the external memory through a selector <b>512</b>, a control for writing the cache tag CTAG to the address section of the cache-filled cache line via a selector <b>522</b>.</p><p>The control contents by the bus control circuit <b>511</b> will now be described. When a reading request of the instruction is generated from the CPU <b>2</b> to the area which can be cached, the cache line is selected by the index address shown by a part of the virtual address. The cache tag is read out from the selected cache line. The read-out cache tag is compared with the physical page number which is supplied from the instruction TLB <b>3</b>. When the cache tag coincides and the valid bit V is equal to the logic value \u201c1\u201d, it is determined that there is a cache hit. For example, the data of a long word is outputted from the selector <b>510</b> by using a part of the offset of the virtual address. The read-out data is supplied to the CPU <b>2</b> via the signal line <b>114</b> by the bus control circuit <b>511</b>. When the tag address doesn't coincide or the valid bit V is equal to the logic value \u201c0\u201d, it is decided that there is a cache miss. The bus control circuit <b>511</b> reads the data as much as one entry of the cache from the external memory corresponding to the physical address regarding the miss via the selector <b>512</b>. This data reading operation is called a cache fill. After the necessary data was stored into the data array <b>501</b> due to the cache fill, by setting the valid bit V of the cache line to the logic value \u201c1\u201d, the cache tag CTAG is updated through the selector <b>522</b> and the necessary data is returned to the CPU <b>2</b>. Since there is no instruction writing upon instruction fetching of the CPU <b>2</b>, even if the old cache entry has been swept out from the instruction cache memory <b>5</b> at the time of the cache fill, there is no need to write back to the external memory.</p><p>The instruction cache memory <b>5</b> also has an RAM mode and an index mode similar to those mentioned above. When the RAM mode is set, the half of the data array <b>501</b> is enabled to be accessed at random as an RAM. In the RAM mode, cache entries <b>0</b> to <b>63</b> and <b>128</b> to <b>191</b> are allowed to function as a cache memory and cache entries <b>64</b> to <b>127</b> and <b>192</b> to <b>255</b> are enabled to be accessed at random. The RAM mode and the index mode are respectively independently selected by setting a predetermined control bit in a control register <b>520</b> to \u201c1\u201d. In the other cases, all of the address array <b>500</b> and data array <b>501</b> are used as cache memories.</p><p>As disclosed in FIG. 8 showing the virtual address space of the data processor <b>1</b>, the RAM areas in the instruction cache memory have been mapped to 0x7C00 0000 to 0x7FFF FFFF. 0x denotes the hexadecimal notation.</p><p>The RAM area discrimination control means <b>505</b> switches the random accessing operation for the RAM areas and the operation as a cache memory. Although its logic can be constructed in a manner similar to FIG. 9, since the number of cache lines is the half of the number of data caches, the position of the index is shifted to a lower position by one bit. The setting of the operating mode is determined by one bit in the control register <b>520</b>. The value of this bit is supplied to the RAM area discrimination control means <b>505</b> by a control signal <b>521</b>. When the RAM mode is designated, in the case where upper six bits of the virtual address are equal to 0x7C, the areas of entries <b>64</b> to <b>127</b> and <b>192</b> to <b>255</b> in the address array <b>500</b> and data array <b>501</b> are set to the targets of the index. In the other address, the areas of entries <b>0</b> to <b>63</b> and <b>128</b> to <b>191</b> are set to the targets of the index. In the RAM mode, so long as the access address indicates the RAM area, the gate <b>509</b> and bus control circuit <b>511</b> mask the cache hit signal <b>508</b> and the selector <b>510</b> and bus control circuit <b>511</b> enable the random reading operation on a 32-bit unit basis from the data array <b>501</b>.</p><p>Since the random access in the RAM mode is the direct instruction access to the RAM areas mapped to the virtual space, the access is executed between the signal line <b>114</b> and the CPU <b>2</b>. Even when the RAM mode is set, as for the instruction cache memory <b>5</b>, the caching operation can be still performed by using the half storing areas in the instruction cache memory <b>5</b> in response to the memory access except for the RAM areas.</p><p>The index mode designating means <b>530</b> switches the bit position of the virtual address to select the cache line, thereby dividing the cache and allocating to the virtual address space. Although its logic can be constructed in a manner similar to FIG. 9, since the number of cache lines is the half of the number of data caches, the position of the index is shifted to a lower position by one bit.</p><p>The selecting operation of the index mode is controlled by the control signal <b>521</b>. The control signal <b>521</b> is set to the logic value according to one bit in the control register <b>520</b> and such one bit is a control bit to designate the index mode. When the index mode is designated, since the 25th bit of the virtual address is used for the index, the upper side and the lower side of the instruction cache are separately used every 32 Mbytes. By arranging the program to a boundary of the 32 Mbytes, the instruction cache can be falsely handled in a 2-way set-associative manner.</p><p>In the self testing mode, the write data and the address signal are supplied to the instruction cache memory via the signal line <b>119</b>. The address signal is supplied to the index decoder <b>502</b> through the selector <b>504</b>. The write data is supplied to the data array <b>501</b> and address array <b>500</b> via the selectors <b>512</b> and <b>522</b>. The read-out data from the address array <b>500</b> and data array <b>501</b> is sent to the self testing circuit <b>9</b> through the dedicated signal line <b>126</b>.</p><p>[Self Testing Circuit]</p><p>FIG. 12 shows a block diagram of the self testing circuit <b>9</b>. According to the self testing circuit <b>9</b>, a test setting circuit <b>900</b> writes test data into the instruction TLB <b>3</b>, unified TLB <b>4</b>, instruction cache memory <b>5</b>, and data cache memory <b>6</b> and supplies the written data to an instruction TLB discriminating circuit <b>903</b>, a unified TLB discriminating circuit <b>904</b>, an instruction cache discriminating circuit <b>905</b>, and a data cache discriminating circuit <b>906</b>, respectively. Each of the discriminating circuits <b>903</b> to <b>906</b> discriminates a coincidence between, for example, the corresponding write data and the read-out data. Discrimination results are held in a result register <b>907</b> and can be read out by the CPU <b>2</b> via a bus control circuit <b>908</b>.</p><p>An activation discriminating circuit <b>909</b> discriminates the activation of the self test by a signal <b>112</b> from the CPU <b>2</b>. When the activation of the self test is instructed, the activation discriminating circuit <b>909</b> activates a state machine <b>910</b> and sequentially repeats a control cycle for a testing operation. The test control circuit <b>900</b> activates a writing cycle and a reading cycle for the instruction TLB <b>3</b>, unified TLB <b>4</b>, instruction cache memory <b>5</b>, and data cache memory <b>6</b> synchronously with the control cycle and controls the discriminating operations of the instruction TLB discriminating circuit <b>903</b>, unified TLB discriminating circuit <b>904</b>, instruction cache discriminating circuit <b>905</b>, and data cache discriminating circuit <b>906</b>, respectively. After the discrimination results of one time were read to the CPU <b>2</b>, the result register <b>907</b> is updated to an initial value by an updating circuit <b>911</b> and those operations are repeated to the end. A test completion discriminating circuit <b>912</b> discriminates the completion of the self test on the basis of an output of the state machine <b>910</b>. A discrimination result is returned to the CPU <b>2</b> by a signal <b>113</b>. A register setting circuit <b>913</b> executes the setting of test conditions such as write data, write address, and the like to the test setting circuit <b>900</b>.</p><p>[Data Processing System]</p><p>FIG. 13 shows an example of a data processing system to which the data processor <b>1</b> is applied. In the diagram, reference numeral <b>1</b> denotes the data processor; <b>11</b> a dynamic random access memory (DRAM); <b>12</b> a DRAM control unit for performing an address multiplexing control and a refreshing control to the DRAM <b>11</b>; and <b>13</b> an SRAM. The RAM <b>13</b> is used as a work area of the data processor <b>1</b>, a temporary storing area of data, or the like. Reference numeral <b>14</b> denotes an ROM having an OS (Operating System) or the like of the data processor <b>1</b>; <b>15</b> a peripheral device control unit to which an external storage device <b>16</b> and a keyboard <b>17</b> which are typically shown are connected; <b>18</b> a display controller which has a frame buffer <b>19</b> and a drawing and display control logic circuit (not shown) and executes a drawing control and a display control to a display <b>20</b>; <b>21</b> a power supply circuit; and <b>22</b> a bus which is typically shown.</p><p>The DRAM <b>11</b>, SRAM <b>13</b>, ROM <b>14</b>, and the like construct external memories of the data processor <b>1</b>. The external storage device <b>16</b> is used as a secondary storage of the external memories. An address translation table is formed in, for example, the SRAM or DRAM.</p><p>[Superiority of Data Processor]</p><p>According to the data processor <b>1</b> described above, the translation lookaside buffers (TLBs) <b>3</b> and <b>4</b> are divided into the buffer for data and the buffer for instruction and the address translation information for instruction is also stored into the translation lookaside buffer <b>4</b> for data. When a translation miss occurs in the translation lookaside buffer <b>3</b> for instruction, new address translation information is fetched from the translation lookaside buffer <b>4</b> for data. Therefore, when a translation miss occurs in the translation lookaside buffer <b>3</b> for instruction, the new address translation information is fetched from the translation lookaside buffer <b>4</b> for data, so that a high speed of the address translating operation can be realized as compared with the case of obtaining the address translation information from the external address translation table each time the translation miss occurs. Thus, a high memory accessing speed can be accomplished.</p><p>Only a partial area in the cache memories <b>5</b> and <b>6</b> can be also selectively made operative as a random access memory. According to this construction, the RAM areas in the data cache memory <b>6</b> and instruction cache memory <b>5</b> are accessed at random. Since the remaining areas in both of the cache memories <b>5</b> and <b>6</b> are made operative as cache memories in which the associative retrieval is performed, particularly, both of a condition such that desired instruction and data which need a high accessing speed are always held in the cache memories <b>5</b> and <b>6</b> and a condition such that the instruction and data which were used recently are held in the cache memories <b>5</b> and <b>6</b> can be satisfied. It is possible to contribute to the improvement of the data processing speed.</p><p>The index addresses to select the lines of the cache memories <b>5</b> and <b>6</b> can be switched. Thus, since the bits on the further upper side of the virtual address can be selectively used for selection of the lines of the cache memories, the cache of the direct mapping can be falsely treated as a set-associative cache. It is possible to contribute to the improvement of the data processing speed.</p><p>The I/O register area is mapped from the virtual address space to the physical address space. In this instance, the TLB entry has protection information to specify an access right to the page. The access right discriminating circuit <b>405</b> discriminates an access right to the relevant page on the basis of the protection information of the translation information regarding the associated hit. Therefore, a storage protection can be performed even for the I/O register space.</p><p>The entry of the unified TLB <b>4</b> has the cache write mode bit WT to specify which one of the write-through and the write-back is used for the data cache memory <b>6</b>. The cache write control circuit <b>614</b> determines a control mode of the cache write with reference to the cache write mode bit WT on a page unit basis. In case of the write-through mode, although the contents in the cache memory and the external memory always coincide, the number of times of access to the external memory increases. In the write-back mode, although the number of external memory accessing times is small, a period of time during which the contents in the cache memory and the external memory don't coincide exists. In the case where a plurality of cache memories unify the external memory, there is a case where a consistency between the cache memory and the external memory cannot be held. Since the write-through mode or the write-back mode can be selected on a page unit basis, the relation between the consistency of the cache memory and the external memory and the accessing speed can be optimized in accordance with the system construction and the contents of the process.</p><p>In the data processing system to which the data processor <b>1</b> is applied, therefore, a data processing efficiency can be improved. In a point of a using mode or the like of the cache memory, the invention can be applied to various systems of different requests.</p><p>Although the invention made by the present inventors has specifically been described above on the basis of the embodiments, the invention is not limited to them but many modifications and variations are possible within the scope of the spirit of the invention.</p><p>For instance, the instruction TLB and unified TLB can be also constructed in a direct mapping mode or a set-associative mode. The set-associative mode can be also used with respect to the data cache memory and the instruction cache memory. The data bus for the self test which is connected to the instruction TLB or the like is not always constructed as a dedicated bus but can be also commonly used by a gate control or the like. The data processor can also has another circuit module such as a floating point unit or the like.</p><p>Effects which are derived by a typical one of the invention disclosed in the present invention will now be briefly described as follows.</p><p>That is, when the translation miss occurs in the translation lookaside buffer for instruction, the new address translation information is fetched from the translation lookaside buffer for data. Therefore, as compared with the case of obtaining the address translation information from the external address translation table each time the translation miss occurs, a higher speed of the address translating operation can be realized. Thus, a high memory accessing speed can be accomplished.</p><p>Partial storing areas in the data cache memory and the instruction cache memory can be accessed at random and the cache memory operation by the associated retrieval can be performed in the remaining storing areas. Therefore, particularly, both of a condition such that desired instruction and data which need the high accessing speed are always held in the cache memory and a condition such that the instruction and data which were used recently are held in the cache memory can be satisfied. It is possible to contribute to the improvement of the data processing speed.</p><p>Since the designated bits of the index address can be switched in the data cache memory and the instruction cache memory, the cache memory can be divided and used every large address space. It is possible to contribute to the improvement of the data processing speed.</p><p>By mapping the I/O register area from the virtual address space to the physical address space, the storage protection can be also performed to the I/O register space.</p><p>Since the write-through mode and the write-back mode can be selected on a page unit basis, the relation between the consistency of the cache memory and the external memory and the accessing speed can be optimized in accordance with the system construction and the contents of the process.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Junichi", "last_name": "Nishimoto", "name": ""}, {"first_name": "Osamu", "last_name": "Nishii", "name": ""}, {"first_name": "Fumio", "last_name": "Arakawa", "name": ""}, {"first_name": "Susumu", "last_name": "Narita", "name": ""}, {"first_name": "Masayuki", "last_name": "Ito", "name": ""}, {"first_name": "Makoto", "last_name": "Toda", "name": ""}, {"first_name": "Kunio", "last_name": "Uchiyama", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "HITACHI, LTD."}, {"first_name": "", "last_name": "ADVANCED PROCESSOR TECHNOLOGIES LLC", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/10"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/10        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711207"}, {"primary": false, "label": "711203"}, {"primary": false, "label": "711200"}, {"primary": false, "label": "711206"}, {"primary": false, "label": "711E12061"}], "ecla_classes": [{"label": "S06F212:652"}, {"label": "G06F  12/10L"}], "cpc_classes": [{"label": "G06F2212/652"}, {"label": "G06F  12/1027"}, {"label": "G06F2212/652"}, {"label": "G06F  12/1027"}, {"label": "G06F2212/652"}, {"label": "G06F  12/1027"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1996-10-16", "application_date": "2000-05-01", "family_members": [{"ucid": "US-6532528-B1", "titles": [{"lang": "EN", "text": "Data processor and data processor system having multiple modes of address indexing and operation"}]}, {"ucid": "TW-359776-B", "titles": [{"lang": "EN", "text": "Data processor and data processing system"}]}, {"ucid": "KR-19980032776-A", "titles": [{"lang": "KO", "text": "\ub370\uc774\ud0c0 \ud504\ub85c\uc138\uc11c \ubc0f \ub370\uc774\ud0c0 \ucc98\ub9ac\uc2dc\uc2a4\ud15c"}, {"lang": "EN", "text": "Data processor and data processing system"}]}, {"ucid": "US-6092172-A", "titles": [{"lang": "EN", "text": "Data processor and data processing system having two translation lookaside buffers"}]}]}