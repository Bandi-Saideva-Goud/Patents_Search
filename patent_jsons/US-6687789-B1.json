{"patent_number": "US-6687789-B1", "publication_id": 73707916, "family_id": 23892412, "publication_date": "2004-02-03", "titles": [{"lang": "EN", "text": "Cache which provides partial tags from non-predicted ways to direct search if way prediction misses"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50615828\"><p>A cache is coupled to receive an input address and a corresponding way prediction. The cache provides output bytes in response to the predicted way (instead of, performing tag comparisons to select the output bytes). Furthermore, a tag may be read from the predicted way and only partial tags are read from the non-predicted ways. The tag is compared to the tag portion of the input address, and the partial tags are compared to a corresponding partial tag portion of the input address. If the tag matches the tag portion of the input address, a hit in the predicted way is detected and the bytes provided in response to the predicted way are correct. If the tag does not match the tag portion of the input address, a miss in the predicted way is detected. If none of the partial tags match the corresponding partial tag portion of the input address, a miss in the cache is determined. On the other hand, if one or more of the partial tags match the corresponding partial tags portion of the input address, the cache searches the corresponding ways to determine whether or not the input address hits or misses in the cache.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00001\" num=\"1\"><claim-text>1. A cache comprising:</claim-text><claim-text>a tag array coupled to receive an index of a read address and a way selection, wherein said tag array comprises a plurality of ways, and wherein said tag array is configured to output a plurality of partial tags, and wherein each of said plurality of partial tags is from one of said plurality of ways; and </claim-text><claim-text>a control circuit coupled to said tag array, wherein said control circuit is configured to generate a search way selection identifying a search way responsive to said read address missing in a first way of said plurality of ways of said tag array, said first way identified by said way selection, and wherein a first partial tag from said search way matches a corresponding portion of said read address. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00002\" num=\"2\"><claim-text>2. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00001\">claim 1</claim-ref> wherein said tag array is further configured to output a first tag from said first way, and wherein said control circuit is configured to determine that said read address is a miss in said first way responsive to said first tag not matching a tag portion of said read address.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00003\" num=\"3\"><claim-text>3. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00002\">claim 2</claim-ref> wherein said tag array is configured to output a partial tag portion of said first tag in response to said index, and wherein said tag array is configured to output a remaining tag portion of said first tag in response to said index and said way selection.</claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00004\" num=\"4\"><claim-text>4. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00003\">claim 3</claim-ref> further comprising:</claim-text><claim-text>a first comparator coupled to receive said remaining tag portion of said first tag and a second corresponding portion of said read address, wherein said first comparator is configured to provide a comparison result signal to said control circuit; and </claim-text><claim-text>a plurality of comparators, wherein a first one of said plurality of comparators is coupled to receive said partial tag portion of said first tag and wherein each other one of said plurality of comparators is coupled to receive one of said plurality of partial tags, and wherein each of said plurality of comparators is coupled to receive said corresponding portion of said read address; and wherein each of said plurality of comparators is coupled to provide a comparison result signal to said control circuit. </claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00005\" num=\"5\"><claim-text>5. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00001\">claim 1</claim-ref> further comprising a multiplexor coupled to receive an input way prediction and said search way selection, wherein said multiplexor is configured to select between said input way prediction and said search way selection and is coupled to provide said way selection to said tag array.</claim-text></claim>"}, {"num": 6, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00006\" num=\"6\"><claim-text>6. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00005\">claim 5</claim-ref> wherein said control circuit is coupled to provide a selection control to said multiplexor and is configured to cause a selection of said input way selection responsive to said control circuit not generating said search way selection.</claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00007\" num=\"7\"><claim-text>7. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00006\">claim 6</claim-ref> wherein said control circuit is configured to cause a selection of said search way selection responsive to generating said search way selection.</claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00008\" num=\"8\"><claim-text>8. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00007\">claim 7</claim-ref> wherein, if said read address misses said search way, said control circuit is configured to generate a second search way selection responsive to a second partial tag from said second search way matching said corresponding portion of said read address.</claim-text></claim>"}, {"num": 9, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00009\" num=\"9\"><claim-text>9. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00001\">claim 1</claim-ref> wherein said control circuit is configured to assert a miss signal responsive to said read address missing in said first way and none of said plurality of partial tags matching said corresponding portion of said read address.</claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00010\" num=\"10\"><claim-text>10. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00009\">claim 9</claim-ref> wherein said control circuit is further configured to assert said miss signal responsive to said read address missing in said first way, searching each of said plurality of ways for which a corresponding partial tag matches said corresponding portion of said read address, and missing in said each of said plurality of ways.</claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00011\" num=\"11\"><claim-text>11. A processor comprising:</claim-text><claim-text>a line predictor configured to provide a way prediction responsive to a fetch address; and </claim-text><claim-text>an instruction cache coupled to receive said way prediction and said fetch address, wherein said instruction cache is set associative and includes a tag array configured to output a plurality of partial tags responsive to an index of said fetch address, and wherein said instruction cache is configured, responsive to said fetch address missing in a first way identified by said way prediction, to search a second way of said tag array for which a corresponding partial tag of said plurality of partial tags matches a corresponding portion of said fetch address. </claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00012\" num=\"12\"><claim-text>12. The processor as recited in <claim-ref idref=\"US-6687789-B1-CLM-00011\">claim 11</claim-ref> wherein, if said instruction cache determines that said fetch address misses in said second way, said instruction cache is configured to search additional ways for which corresponding partial tags match said corresponding portion of said fetch address until either a hit is detected or said instruction cache exhausts ways for which corresponding partial tags match said corresponding portion of said fetch address.</claim-text></claim>"}, {"num": 13, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00013\" num=\"13\"><claim-text>13. The processor as recited in <claim-ref idref=\"US-6687789-B1-CLM-00012\">claim 12</claim-ref> wherein said instruction cache is further configured to assert a miss signal responsive to a miss of said fetch address in said first way and: (i) none of said plurality of partial tags matching said corresponding portion of said fetch address; or (ii) exhausting said ways for which corresponding partial tags match said corresponding portion of said fetch address without detecting a hit of said fetch address.</claim-text></claim>"}, {"num": 14, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00014\" num=\"14\"><claim-text>14. The processor as recited in <claim-ref idref=\"US-6687789-B1-CLM-00012\">claim 12</claim-ref> wherein, responsive to detecting a hit in said second way or said additional ways, said instruction cache is configured to provide an updated way prediction to said line predictor.</claim-text></claim>"}, {"num": 15, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00015\" num=\"15\"><claim-text>15. A computer system comprising:</claim-text><claim-text>a processor including: </claim-text><claim-text>a line predictor configured to provide a way prediction responsive to a fetch address; and </claim-text><claim-text>an instruction cache coupled to receive said way prediction and said fetch address, wherein said instruction cache is set associative and includes a tag array configured to output a plurality of partial tags responsive to an index of said fetch address, and wherein said instruction cache is configured, responsive to said fetch address missing in a first way identified by said way prediction, to search a second way of said tag array for which a corresponding partial tag of said plurality of partial tags matches a corresponding portion of said fetch address; and </claim-text><claim-text>an input/output (I/O) device configured to communicate between said computer system and another computer system to which said I/O device is couplable. </claim-text></claim>"}, {"num": 16, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00016\" num=\"16\"><claim-text>16. The computer system as recited in <claim-ref idref=\"US-6687789-B1-CLM-00015\">claim 15</claim-ref> wherein said I/O device comprises a modem.</claim-text></claim>"}, {"num": 17, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00017\" num=\"17\"><claim-text>17. The computer system as recited in <claim-ref idref=\"US-6687789-B1-CLM-00015\">claim 15</claim-ref> further comprising an audio I/O device.</claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00018\" num=\"18\"><claim-text>18. The computer system as recited in <claim-ref idref=\"US-6687789-B1-CLM-00017\">claim 17</claim-ref> wherein said audio I/O device comprises a sound card.</claim-text></claim>"}, {"num": 19, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00019\" num=\"19\"><claim-text>19. The computer system as recited in <claim-ref idref=\"US-6687789-B1-CLM-00015\">claim 15</claim-ref> further comprising a second processor identical to said processor.</claim-text></claim>"}, {"num": 20, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00020\" num=\"20\"><claim-text>20. The computer system as recited in <claim-ref idref=\"US-6687789-B1-CLM-00015\">claim 15</claim-ref> further comprising a second processor comprising:</claim-text><claim-text>a second line predictor configured to provide a second way prediction responsive to a second fetch address; and </claim-text><claim-text>a second instruction cache coupled to receive said second way prediction and said second fetch address, wherein said second instruction cache is set associative and includes a second tag array configured to output a second plurality of partial tags responsive to an index of said second fetch address, and wherein said second instruction cache is configured, responsive to said second fetch address missing in a third way identified by said second way prediction, to search a fourth way of said second tag array for which a corresponding partial tag of said second plurality of partial tags matches a corresponding portion of said second fetch address. </claim-text></claim>"}, {"num": 21, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00021\" num=\"21\"><claim-text>21. A method comprising:</claim-text><claim-text>reading a plurality of partial tags from a cache responsive to an address; </claim-text><claim-text>determining whether or not said address hits in a predicted way of said cache; </claim-text><claim-text>selecting a second way of said cache to check for a hit responsive to determining that said address does not hit in said predicted way of said cache and further responsive to a first partial tag of said plurality of partial tags matching a corresponding portion of said address, said first partial tag corresponding to said second way; and </claim-text><claim-text>searching said second way of said cache for a hit. </claim-text></claim>"}, {"num": 22, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00022\" num=\"22\"><claim-text>22. The method as recited in <claim-ref idref=\"US-6687789-B1-CLM-00021\">claim 21</claim-ref> wherein said determining comprises:</claim-text><claim-text>reading a tag from said predicted way; and </claim-text><claim-text>comparing said tag to a tag portion of said address. </claim-text></claim>"}, {"num": 23, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00023\" num=\"23\"><claim-text>23. The method as recited in <claim-ref idref=\"US-6687789-B1-CLM-00021\">claim 21</claim-ref> further comprising:</claim-text><claim-text>determining whether or not said address hits in said second way; and </claim-text><claim-text>searching a third way of said cache for a hit in response to said determining said address does not hit in said second way of said cache, said third way different from said predicted way and said second way and a second partial tag of said plurality of partial tags corresponding to said third way matching said corresponding portion of said address. </claim-text></claim>"}, {"num": 24, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00024\" num=\"24\"><claim-text>24. The method as recited in <claim-ref idref=\"US-6687789-B1-CLM-00021\">claim 21</claim-ref> further comprising generating a miss signal responsive to said determining that said address does not hit in said predicted way and none of said plurality of partial tags matching said corresponding portion of said address.</claim-text></claim>"}, {"num": 25, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00025\" num=\"25\"><claim-text>25. The method as recited in <claim-ref idref=\"US-6687789-B1-CLM-00021\">claim 21</claim-ref> further comprising generating a miss signal responsive to said determining that said address does not hit in said predicted way and not detecting a hit in searching each way for which a corresponding partial tag of said plurality of partial tags matches said corresponding portion of said address.</claim-text></claim>"}, {"num": 26, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00026\" num=\"26\"><claim-text>26. The method as recited in <claim-ref idref=\"US-6687789-B1-CLM-00021\">claim 21</claim-ref> further comprising providing an updated way prediction responsive to detecting a hit in a way different from said predicted way.</claim-text></claim>"}, {"num": 27, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00027\" num=\"27\"><claim-text>27. A cache comprising:</claim-text><claim-text>a tag array coupled to receive an index of an address and a way prediction, wherein the tag array comprises a plurality of ways, and wherein the way prediction identifies a predicted way of the plurality of ways, and wherein the tag array is configured to output a plurality of partial tags, and wherein each of the plurality of partial tags corresponds to one of the plurality of ways; and </claim-text><claim-text>a control circuit coupled to the tag array, wherein the control circuit is configured to generate a search way indication identifying a search way responsive to the address missing in the predicted way, and wherein the search way is a non-predicted way of the plurality of ways, and wherein a first partial tag corresponding to the search way matches a corresponding portion of the address. </claim-text></claim>"}, {"num": 28, "parent": 27, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00028\" num=\"28\"><claim-text>28. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00027\">claim 27</claim-ref> wherein the tag array is further configured to output a first full tag from the predicted way, and wherein the control circuit is configured to determine that the address is a miss in the predicted way responsive to the first full tag not matching a tag portion of the address.</claim-text></claim>"}, {"num": 29, "parent": 28, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00029\" num=\"29\"><claim-text>29. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00028\">claim 28</claim-ref> wherein the tag array is configured to output a partial tag portion of the first full tag in response to the index, and wherein the tag array is configured to output a remaining tag portion of the first full tag in response to the index and the way prediction.</claim-text></claim>"}, {"num": 30, "parent": 29, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00030\" num=\"30\"><claim-text>30. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00029\">claim 29</claim-ref> further comprising:</claim-text><claim-text>a first comparator coupled to receive the remaining tag portion of the first full tag and a second corresponding portion of the address, wherein the first comparator is configured to provide a comparison result signal to the control circuit; and </claim-text><claim-text>a plurality of comparators, wherein a first one of the plurality of comparators is coupled to receive the partial tag portion of the first full tag and wherein each other one of the plurality of comparators is coupled to receive a different one of the plurality of partial tags, and wherein each of the plurality of comparators is coupled to receive the corresponding portion of the address, and wherein each of the plurality of comparators is coupled to provide a comparison result signal to the control circuit. </claim-text></claim>"}, {"num": 31, "parent": 27, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00031\" num=\"31\"><claim-text>31. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00027\">claim 27</claim-ref> wherein, if the address misses the search way, the control circuit is configured to generate a second search way indication identifying a second search way of the plurality of ways, where the second search way is a second non-predicted way of the plurality of ways, and wherein the control circuit is configured to generate the second search way indication responsive to a second partial tag corresponding to the second search way matching the corresponding portion of the address.</claim-text></claim>"}, {"num": 32, "parent": 27, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00032\" num=\"32\"><claim-text>32. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00027\">claim 27</claim-ref> wherein the control circuit is configured to assert a miss signal responsive to the address missing in the predicted way and none of the plurality of partial tags matching the corresponding portion of the address.</claim-text></claim>"}, {"num": 33, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6687789-B1-CLM-00033\" num=\"33\"><claim-text>33. The cache as recited in <claim-ref idref=\"US-6687789-B1-CLM-00032\">claim 32</claim-ref> wherein the control circuit is further configured to assert the miss signal responsive to the address missing in the predicted way, searching each of the plurality of ways for which a corresponding partial tag matches the corresponding portion of the address, and missing in the searched ones of the plurality of ways.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54125558\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>This invention is related to the field of processors and, more particularly, to caching mechanisms within processors.</p><p>2. Description of the Related Art</p><p>Superscalar processors achieve high performance by simultaneously executing multiple instructions in a clock cycle and by specifying the shortest possible clock cycle consistent with the design. As used herein, the term \u201cclock cycle\u201d refers to an interval of time during which the pipeline stages of a processor perform their intended functions. At the end of a clock cycle, the resulting values are moved to the next pipeline stage. Clocked storage devices (e.g. registers, latches, flops, etc.) may capture their values in response to a clock signal defining the clock cycle.</p><p>To reduce effective memory latency, processors typically include caches. Caches are high speed memories used to store previously fetched instruction and/or data bytes. The cache memories may be capable of providing substantially lower memory latency than the main memory employed within a computer system including the processor.</p><p>Caches may be organized into a \u201cset associative\u201d structure. In a set associative structure, the cache is organized as a two-dimensional array having rows (often referred to as \u201csets\u201d) and columns (often referred to as \u201cways\u201d). When a cache is searched for bytes residing at an address, a number of bits from the address are used as an \u201cindex\u201d into the cache. The index selects a particular set within the two-dimensional array, and therefore the number of address bits required for the index is determined by the number of sets configured into the cache. The act of selecting a set via an index is referred to as \u201cindexing\u201d. Each way of the cache has one cache line storage location which is a member of the selected set (where a cache line is a number of contiguous bytes treated as a unit for storage in the cache, and may typically be in the range of 16-64 bytes, although any number of bytes may be defined to compose a cache line). The addresses associated with bytes stored in the ways of the selected set are examined to determine if any of the addresses stored in the set match the requested address. If a match is found, the access is said to be a \u201chit\u201d, and the cache provides the associated bytes. If a match is not found, the access is said to be a \u201cmiss\u201d. When a miss is detected, the bytes are transferred from the main memory system into the cache. The addresses associated with bytes stored in the cache are also stored. These stored addresses are referred to as \u201ctags\u201d or \u201ctag addresses\u201d.</p><p>As mentioned above, a cache line storage location from each way of the cache is a member of the selected set (i.e. is accessed in response to selecting the set). Information stored in one of the ways is provided as the output of the cache, and that way is selected by providing a way selection to the cache. The way selection identifies the way to be selected as an output. In a typical set associative cache, the way selection is determined by examining the tags within a set and finding a match between one of the tags and the requested address.</p><p>Unfortunately, set associative caches may be higher latency than a direct mapped cache (which provides one cache line storage location per index) due to the tag comparison to determine the way selection for the output. Furthermore, since the way selection is not known prior to the access, each way is typically accessed and the corresponding way selection is used to late select the output bytes if a hit is detected. Accessing all of the ways may cause undesirably high power consumption. Limiting power consumption is rapidly achieving equal par with increasing operating speed (or frequency) in modem processors. Accordingly, a low latency, low power consuming method for accessing a set associative cache is desired.</p><h4>SUMMARY OF THE INVENTION</h4><p>The problems outlined above are in large part solved by a cache as disclosed herein. The cache is coupled to receive an input address and a corresponding way prediction. The cache provides output bytes in response to the predicted way (instead of performing tag comparisons to select the output bytes), and thus may reduce access latency as compared to performing the tag comparisons. Furthermore, a tag may be read from the predicted way and only partial tags are read from the non-predicted ways. The tag is compared to the tag portion of the input address, and the partial tags are compared to a corresponding partial tag portion of the input address. If the tag matches the tag portion of the input address, a hit in the predicted way is detected and the bytes provided in response to the predicted way are correct. If the tag does not match the tag portion of the input address, a miss in the predicted way is detected. If none of the partial tags match the corresponding partial tag portion of the input address, a miss in the cache is determined. On the other hand, if one or more of the partial tags match the corresponding partial tags portion of the input address, the cache searches the corresponding ways to determine whether or not the input address hits or misses in the cache (e.g. by performing full tag comparisons for the ways in which a partial tag match is detected). Because partial tags are read from the non-predicted ways, power may be conserved as compared to reading the full tags from each of the ways. Advantageously, both access latency and power consumption may be reduced. Furthermore, by providing partial tags, the other ways to be searched may be identified and the number of ways to be searched may be reduced (e.g. each way having a partial tag miss may not be searched).</p><p>Broadly speaking, a cache is contemplated. The cache comprises a tag array and a control circuit coupled to the tag array. The tag array is coupled to receive an index of a read address and a way selection, and comprises a plurality of ways. The tag array is configured to output a plurality of partial tags, each of which is from one of the plurality of ways. The control circuit is configured to generate a search way selection identifying a search way responsive to the read address missing in a first way of the plurality of ways of the tag array. The first way is identified by the way selection. A first partial tag from the search way matches a corresponding portion of the read address.</p><p>Additionally, a processor is contemplated. The processor comprises a line predictor configured to provide a way prediction responsive to a fetch address, and an instruction cache coupled to receive the way prediction and the fetch address. The instruction cache is set associative and includes a tag array configured to output a plurality of partial tags responsive to an index of the fetch address. The instruction cache is configured, responsive to the fetch address missing in a first way identified by the way prediction, to search a second way of the tag array for which a corresponding partial tag of the plurality of partial tags matches a corresponding portion of the fetch address. Still further, a computer system is contemplated including the processor and an input/output (I/O) device configured to communicate between the computer system and another computer system to which the I/O device is couplable.</p><p>Moreover, a method is contemplated. A plurality of partial tags are read from a cache responsive to a fetch address. Whether or not the fetch address hits in a predicted way of the cache is determined. A second way of the cache is searched for a hit in response to determining that the fetch address does not hit in the predicted way of the cache. The second way is different from the predicted way and a first partial tag of the plurality of partial tags corresponding to the second way matches a corresponding portion of the fetch address.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Other objects and advantages of the invention will become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p><p>FIG. 1 is a block diagram of one embodiment of a processor.</p><p>FIG. 2 is a pipeline diagram illustrating one embodiment of a pipeline which may be employed by the processor shown in FIG. <b>1</b>.</p><p>FIG. 3 is a block diagram of a branch prediction/fetch PC generation unit, line predictor, and I-cache shown in FIG. <b>1</b> and an ITLB illustrating interconnection therebetween according to one embodiment of the units.</p><p>FIG. 4 is a block diagram of one embodiment of an I-cache shown in FIGS. 1 and 3.</p><p>FIG. 5 is a block diagram of one embodiment of a tag array shown in FIG. <b>4</b>.</p><p>FIG. 6 is a diagram illustrating fields within an exemplary address.</p><p>FIG. 7 is a state machine diagram illustrating a state machine which may be employed by one embodiment of a control circuit shown in FIG. <b>4</b>.</p><p>FIG. 8 is a flowchart illustration operation of one embodiment of the control circuit shown in FIG. 4 in an access state shown in FIG. <b>7</b>.</p><p>FIG. 9 is a flowchart illustration operation of one embodiment of the control circuit shown in FIG. 4 in a search state shown in FIG. <b>7</b>.</p><p>FIG. 10 is a block diagram of one embodiment of a computer system including the processor shown in FIG. <b>1</b>.</p><p>FIG. 11 is a block diagram of a second embodiment of a computer system including the processor shown in FIG. <b>1</b>.</p><p>While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>Processor Overview</p><p>Turning now to FIG. 1, a block diagram of one embodiment of a processor <b>10</b> is shown. Other embodiments are possible and contemplated. In the embodiment of FIG. 1, processor <b>10</b> includes a line predictor <b>12</b>, an instruction cache (I-cache) <b>14</b>, an alignment unit <b>16</b>, a branch prediction/fetch PC generation unit <b>18</b>, a plurality of decode units <b>24</b>A-<b>24</b>D, a predictor miss decode unit <b>26</b>, a microcode unit <b>28</b>, a map unit <b>30</b>, a retire queue <b>32</b>, an architectural renames file <b>34</b>, a future file <b>20</b>, a scheduler <b>36</b>, an integer register file <b>38</b>A, a floating point register file <b>38</b>B, an integer execution core <b>40</b>A, a floating point execution core <b>40</b>B, a load/store unit <b>42</b>, a data cache (D-cache) <b>44</b>, an external interface unit <b>46</b>, and a PC silo <b>48</b>. Line predictor <b>12</b> is coupled to predictor miss decode unit <b>26</b>, branch prediction/fetch PC generation unit <b>18</b>, PC silo <b>48</b>, and alignment unit <b>16</b>. Line predictor <b>12</b> may also be coupled to I-cache <b>14</b>. I-cache <b>14</b> is coupled to alignment unit <b>16</b> and branch prediction/fetch PC generation unit <b>18</b>, which is further coupled to PC silo <b>48</b>. Alignment unit <b>16</b> is further coupled to predictor miss decode unit <b>26</b> and decode units <b>24</b>A-<b>24</b>D. Decode units <b>24</b>A-<b>24</b>D are further coupled to map unit <b>30</b>, and decode unit <b>24</b>D is coupled to microcode unit <b>28</b>. Map unit <b>30</b> is coupled to retire queue <b>32</b> (which is coupled to architectural renames file <b>34</b>), future file <b>20</b>, scheduler <b>36</b>, and PC silo <b>48</b>. Architectural renames file <b>34</b> is coupled to future file <b>20</b>. Scheduler <b>36</b> is coupled to register files <b>38</b>A-<b>38</b>B, which are further coupled to each other and respective execution cores <b>40</b>A-<b>40</b>B. Execution cores <b>40</b>A-<b>40</b>B are further coupled to load/store unit <b>42</b> and scheduler <b>36</b>. Execution core <b>40</b>A is further coupled to D-cache <b>44</b>. Load/store unit <b>42</b> is coupled to scheduler <b>36</b>, D-cache <b>44</b>, and external interface unit <b>46</b>. D-cache <b>44</b> is coupled to register files <b>38</b>. External interface unit <b>46</b> is coupled to an external interface <b>52</b> and to I-cache <b>14</b>. Elements referred to herein by a reference numeral followed by a letter will be collectively referred to by the reference numeral alone. For example, decode units <b>24</b>A-<b>24</b>D will be collectively referred to as decode units <b>24</b>.</p><p>In the embodiment of FIG. 1, processor <b>10</b> employs a variable byte length, complex instruction set computing (CISC) instruction set architecture. For example, processor <b>10</b> may employ the x86 instruction set architecture (also referred to as IA-32). Other embodiments may employ other instruction set architectures including fixed length instruction set architectures and reduced instruction set computing (RISC) instruction set architectures. Certain features shown in FIG. 1 may be omitted in such architectures.</p><p>Branch prediction/fetch PC generation unit <b>18</b> is configured to provide a fetch address (fetch PC) to I-cache <b>14</b>, line predictor <b>12</b>, and PC silo <b>48</b>. Branch prediction/fetch PC generation unit <b>18</b> may include a suitable branch prediction mechanism used to aid in the generation of fetch addresses. In response to the fetch address, line predictor <b>12</b> provides alignment information corresponding to a plurality of instructions to alignment unit <b>16</b>, and may provide a next fetch address for fetching instructions subsequent to the instructions identified by the provided instruction information. The next fetch address may be provided to branch prediction/fetch PC generation unit <b>18</b> or may be directly provided to I-cache <b>14</b>, as desired. Branch prediction/fetch PC generation unit <b>18</b> may receive a trap address from PC silo <b>48</b> (if a trap is detected) and the trap address may comprise the fetch PC generated by branch prediction/fetch PC generation unit <b>18</b>. Otherwise, the fetch PC may be generated using the branch prediction information and information from line predictor <b>12</b>. Generally, line predictor <b>12</b> stores information corresponding to instructions previously speculatively fetched by processor <b>10</b>. In one embodiment, line predictor <b>12</b> includes 2K entries, each entry locating a group of one or more instructions referred to herein as a \u201cline\u201d of instructions. The line of instructions may be concurrently processed by the instruction processing pipeline of processor <b>10</b> through being placed into scheduler <b>36</b>.</p><p>I-cache <b>14</b> is a high speed cache memory for storing instruction bytes. According to one embodiment I-cache <b>14</b> may comprise, for example, a 128 Kbyte, four way set associative organization employing 64 byte cache lines. However, any I-cache structure may be suitable (including direct-mapped structures).</p><p>Alignment unit <b>16</b> receives the instruction alignment information from line predictor <b>12</b> and instruction bytes corresponding to the fetch address from I-cache <b>14</b>. Alignment unit <b>16</b> selects instruction bytes into each of decode units <b>24</b>A-<b>24</b>D according to the provided instruction alignment information. More particularly, line predictor <b>12</b> provides an instruction pointer corresponding to each decode unit <b>24</b>A-<b>24</b>D. The instruction pointer locates an instruction within the fetched instruction bytes for conveyance to the corresponding decode unit <b>24</b>A-<b>24</b>D. In one embodiment, certain instructions may be conveyed to more than one decode unit <b>24</b>A-<b>24</b>D. Accordingly, in the embodiment shown, a line of instructions from line predictor <b>12</b> may include up to <b>4</b> instructions, although other embodiments may include more or fewer decode units <b>24</b> to provide for more or fewer instructions within a line.</p><p>Decode units <b>24</b>A-<b>24</b>D decode the instructions provided thereto, and each decode unit <b>24</b>A-<b>24</b>D generates information identifying one or more instruction operations (or ROPs) corresponding to the instructions. In one embodiment, each decode unit <b>24</b>A-<b>24</b>D may generate up to two instruction operations per instruction. As used herein, an instruction operation (or ROP) is an operation which an execution unit within execution cores <b>40</b>A-<b>40</b>B is configured to execute as a single entity. Simple instructions may correspond to a single instruction operation, while more complex instructions may correspond to multiple instruction operations. Certain of the more complex instructions may be implemented within microcode unit <b>28</b> as microcode routines (fetched from a read-only memory therein via decode unit <b>24</b>D in the present embodiment). Furthermore, other embodiments may employ a single instruction operation for each instruction (i.e. instruction and instruction operation may be synonymous in such embodiments).</p><p>PC silo <b>48</b> stores the fetch address and instruction information for each instruction fetch, and is responsible for redirecting instruction fetching upon exceptions (such as instruction traps defined by the instruction set architecture employed by processor <b>10</b>, branch mispredictions, and other microarchitecturally defined traps). PC silo <b>48</b> may include a circular buffer for storing fetch address and instruction information corresponding to multiple lines of instructions which may be outstanding within processor <b>10</b>. In response to retirement of a line of instructions, PC silo <b>48</b> may discard the corresponding entry. In response to an exception, PC silo <b>48</b> may provide a trap address to branch prediction/fetch PC generation unit <b>18</b>. Retirement and exception information may be provided by scheduler <b>36</b>. In one embodiment, PC silo <b>48</b> assigns a sequence number (R#) to each instruction to identify the order of instructions outstanding within processor <b>10</b>. Scheduler <b>36</b> may return R#s to PC silo <b>48</b> to identify instruction operations experiencing exceptions or retiring instruction operations.</p><p>Upon detecting a miss in line predictor <b>12</b>, alignment unit <b>16</b> routes the corresponding instruction bytes from I-cache <b>14</b> to predictor miss decode unit <b>26</b>. Predictor miss decode unit <b>26</b> decodes the instruction, enforcing any limits on a line of instructions as processor <b>10</b> is designed for (e.g. maximum number of instruction operations, maximum number of instructions, terminate on branch instructions, etc.). Upon terminating a line, predictor miss decode unit <b>26</b> provides the information to line predictor <b>12</b> for storage. It is noted that predictor miss decode unit <b>26</b> may be configured to dispatch instructions as they are decoded. Alternatively, predictor miss decode unit <b>26</b> may decode the line of instruction information and provide it to line predictor <b>12</b> for storage. Subsequently, the missing fetch address may be reattempted in line predictor <b>12</b> and a hit may be detected.</p><p>In addition to decoding instructions upon a miss in line predictor <b>12</b>, predictor miss decode unit <b>26</b> may be configured to decode instructions if the instruction information provided by line predictor <b>12</b> is invalid. In one embodiment, processor <b>10</b> does not attempt to keep information in line predictor <b>12</b> coherent with the instructions within I-cache <b>14</b> (e.g. when instructions are replaced or invalidate in I-cache <b>14</b>, the corresponding instruction information may not actively be invalidated). Decode units <b>24</b>A-<b>24</b>D may verify the instruction information provided, and may signal predictor miss decode unit <b>26</b> when invalid instruction information is detected. According to one particular embodiment, the following instruction operations are supported by processor <b>10</b>: integer (including arithmetic, logic, shift/rotate, and branch operations), floating point (including multimedia operations), and load/store.</p><p>The decoded instruction operations and source and destination register numbers are provided to map unit <b>30</b>. Map unit <b>30</b> is configured to perform register renaming by assigning physical register numbers (PR#s) to each destination register operand and source register operand of each instruction operation. The physical register numbers identify registers within register files <b>38</b>A-<b>38</b>B. Map unit <b>30</b> additionally provides an indication of the dependencies for each instruction operation by providing R#s of the instruction operations which update each physical register number assigned to a source operand of the instruction operation. Map unit <b>30</b> updates future file <b>20</b> with the physical register numbers assigned to each destination register (and the R# of the corresponding instruction operation) based on the corresponding logical register number. Additionally, map unit <b>30</b> stores the logical register numbers of the destination registers, assigned physical register numbers, and the previously assigned physical register numbers in retire queue <b>32</b>. As instructions are retired (indicated to map unit <b>30</b> by scheduler <b>36</b>), retire queue <b>32</b> updates architectural renames file <b>34</b> and frees any registers which are no longer in use. Accordingly, the physical register numbers in architectural register file <b>34</b> identify the physical registers storing the committed architectural state of processor <b>10</b>, while future file <b>20</b> represents the speculative state of processor <b>10</b>. In other words, architectural renames file <b>34</b> stores a physical register number corresponding to each logical register, representing the committed register state for each logical register. Future file <b>20</b> stores a physical register number corresponding to each logical register, representing the speculative register state for each logical register.</p><p>The line of instruction operations, source physical register numbers, and destination physical register numbers are stored into scheduler <b>36</b> according to the R#s assigned by PC silo <b>48</b>. Furthermore, dependencies for a particular instruction operation may be noted as dependencies on other instruction operations which are stored in the scheduler. In one embodiment, instruction operations remain in scheduler <b>36</b> until retired.</p><p>Scheduler <b>36</b> stores each instruction operation until the dependencies noted for that instruction operation have been satisfied. In response to scheduling a particular instruction operation for execution, scheduler <b>36</b> may determine at which clock cycle that particular instruction operation will update register files <b>38</b>A-<b>38</b>B. Different execution units within execution cores <b>40</b>A-<b>40</b>B may employ different numbers of pipeline stages (and hence different latencies). Furthermore, certain instructions may experience more latency within a pipeline than others. Accordingly, a countdown is generated which measures the latency for the particular instruction operation (in numbers of clock cycles). Scheduler <b>36</b> awaits the specified number of clock cycles (until the update will occur prior to or coincident with the dependent instruction operations reading the register file), and then indicates that instruction operations dependent upon that particular instruction operation may be scheduled. It is noted that scheduler <b>36</b> may schedule an instruction once its dependencies have been satisfied (i.e. out of order with respect to its order within the scheduler queue).</p><p>Integer and load/store instruction operations read source operands according to the source physical register numbers from register file <b>38</b>A and are conveyed to execution core <b>40</b>A for execution. Execution core <b>40</b>A executes the instruction operation and updates the physical register assigned to the destination within register file <b>38</b>A. Additionally, execution core <b>40</b>A reports the R# of the instruction operation and exception information regarding the instruction operation (if any) to scheduler <b>36</b>. Register file <b>38</b>B and execution core <b>40</b>B may operate in a similar fashion with respect to floating point instruction operations (and may provide store data for floating point stores to load/store unit <b>42</b>).</p><p>In one embodiment, execution core <b>40</b>A may include, for example, two integer units, a branch unit, and two address generation units (with corresponding translation lookaside buffers, or TLBs). Execution core <b>40</b>B may include a floating point/multimedia multiplier, a floating point/multimedia adder, and a store data unit for delivering store data to load/store unit <b>42</b>. Other configurations of execution units are possible.</p><p>Load/store unit <b>42</b> provides an interface to D-cache <b>44</b> for performing memory operations and for scheduling fill operations for memory operations which miss D-cache <b>44</b>. Load memory operations may be completed by execution core <b>40</b>A performing an address generation and forwarding data to register files <b>38</b>A-<b>38</b>B (from D-cache <b>44</b> or a store queue within load/store unit <b>42</b>). Store addresses may be presented to D-cache <b>44</b> upon generation thereof by execution core <b>40</b>A (directly via connections between execution core <b>40</b>A and D-Cache <b>44</b>). The store addresses are allocated a store queue entry. The store data may be provided concurrently, or may be provided subsequently, according to design choice. Upon retirement of the store instruction, the data is stored into D-cache <b>44</b> (although there may be some delay between retirement and update of D-cache <b>44</b>). Additionally, load/store unit <b>42</b> may include a load/store buffer for storing load/store addresses which miss D-cache <b>44</b> for subsequent cache fills (via external interface unit <b>46</b>) and re-attempting the missing load/store operations. Load/store unit <b>42</b> is further configured to handle load/store memory dependencies.</p><p>D-cache <b>44</b> is a high speed cache memory for storing data accessed by processor <b>10</b>. While D-cache <b>44</b> may comprise any suitable structure (including direct mapped and set-associative structures), one embodiment of D-cache <b>44</b> may comprise a 128 Kbyte, 2 way set associative cache having 64 byte lines.</p><p>External interface unit <b>46</b> is configured to communicate to other devices via external interface <b>52</b>. Any suitable external interface <b>52</b> may be used, including interfaces to L<b>2</b> caches and an external bus or buses for connecting processor <b>10</b> to other devices. External interface unit <b>46</b> fetches fills for I-cache <b>16</b> and D-cache <b>44</b>, as well as writing discarded updated cache lines from D-cache <b>44</b> to the external interface. Furthermore, external interface unit <b>46</b> may perform non-cacheable reads and writes generated by processor <b>10</b> as well.</p><p>Turning next to FIG. 2, an exemplary pipeline diagram illustrating an exemplary set of pipeline stages which may be employed by one embodiment of processor <b>10</b> is shown. Other embodiments may employ different pipelines, pipelines including more or fewer pipeline stages than the pipeline shown in FIG. <b>2</b>. The stages shown in FIG. 2 are delimited by vertical dashed lines. Each stage is one clock cycle of a clock signal used to clock storage elements (e.g. registers, latches, flops, and the like) within processor <b>10</b>.</p><p>As illustrated in FIG. 2, the exemplary pipeline includes a CAMO stage, a CAMI stage, a line predictor (LP) stage, an instruction cache (IC) stage, an alignment (AL) stage, a decode (DEC) stage, a map<b>1</b> (M<b>1</b>) stage, a map<b>2</b> (M<b>2</b>) stage, a write scheduler (WR SC) stage, a read scheduler (RD SC) stage, a register file read (RF RD) stage, an execute (EX) stage, a register file write (RF WR) stage, and a retire (RET) stage. Some instructions utilize multiple clock cycles in the execute state. For example, memory operations, floating point operations, and integer multiply operations are illustrated in exploded form in FIG. <b>2</b>. Memory operations include an address generation (AGU) stage, a translation (TLB) stage, a data cache <b>1</b> (DC <b>1</b>) stage, and a data cache <b>2</b> (DC<b>2</b>) stage. Similarly, floating point operations include up to four floating point execute (FEX<b>1</b>-FEX<b>4</b>) stages, and integer multiplies include up to four (IM<b>1</b>-IM<b>4</b>) stages.</p><p>During the CAM<b>0</b> and CAM<b>1</b> stages, line predictor <b>12</b> compares the fetch address provided by branch prediction/fetch PC generation unit <b>18</b> to the addresses of lines stored therein. Additionally, the fetch address is translated from a virtual address (e.g. a linear address in the x86 architecture) to a physical address during the CAM<b>0</b> and CAM<b>1</b> stages. In response to detecting a hit during the CAM<b>0</b> and CAM<b>1</b> stages, the corresponding line information is read from the line predictor during the line predictor stage. Also, I-cache <b>14</b> initiates a read (using the physical address) during the line predictor stage. The read completes during the instruction cache stage.</p><p>It is noted that, while the pipeline illustrated in FIG. 2 employs two clock cycles to detect a hit in line predictor <b>12</b> for a fetch address, other embodiments may employ a single clock cycle (and stage) to perform this operation. Moreover, in one embodiment, line predictor <b>12</b> provides a next fetch address for I-cache <b>14</b> and a next entry in line predictor <b>12</b> for a hit, and therefore the CAM<b>0</b> and CAM<b>1</b> stages may be skipped for fetches resulting from a previous hit in line predictor <b>12</b>.</p><p>Instruction bytes provided by <b>1</b>-cache <b>14</b> are aligned to decode units <b>24</b>A-<b>24</b>D by alignment unit <b>16</b> during the alignment stage in response to the corresponding line information from line predictor <b>12</b>. Decode units <b>24</b>A-<b>24</b>D decode the provided instructions, identifying ROPs corresponding to the instructions as well as operand information during the decode stage. Map unit <b>30</b> generates ROPs from the provided information during the map<b>1</b> stage, and performs register renaming (updating future file <b>20</b>). During the map<b>2</b> stage, the ROPs and assigned renames are recorded in retire queue <b>32</b>. Furthermore, the ROPs upon which each ROP is dependent are determined. Each ROP may be register dependent upon earlier ROPs as recorded in the future file, and may also exhibit other types of dependencies (e.g. dependencies on a previous serializing instruction, etc.)</p><p>The generated ROPs are written into scheduler <b>36</b> during the write scheduler stage. Up until this stage, the ROPs located by a particular line of information flow through the pipeline as a unit. However, subsequent to be written into scheduler <b>36</b>, the ROPs may flow independently through the remaining stages, at different times Generally, a particular ROP remains at this stage until selected for execution by scheduler <b>36</b> (e.g. after the ROPs upon which the particular ROP is dependent have been selected for execution, as described above). Accordingly, a particular ROP may experience one or more clock cycles of delay between the write scheduler write stage and the read scheduler stage. During the read scheduler stage, the particular ROP participates in the selection logic within scheduler <b>36</b>, is selected for execution, and is read from scheduler <b>36</b>. The particular ROP then proceeds to read register file operations from one of register files <b>38</b>A-<b>38</b>B (depending upon the type of ROP) in the register file read stage.</p><p>The particular ROP and operands are provided to the corresponding execution core <b>40</b>A or <b>40</b>B, and the instruction operation is performed on the operands during the execution stage. As mentioned above, some ROPs have several pipeline stages of execution. For example, memory instruction operations (e.g. loads and stores) are executed through an address generation stage (in which the data address of the memory location accessed by the memory instruction operation is generated), a translation stage (in which the virtual data address provided by the address generation stage is translated) and a pair of data cache stages in which D-cache <b>44</b> is accessed. Floating point operations may employ up to 4 clock cycles of execution, and integer multiplies may similarly employ up to 4 clock cycles of execution.</p><p>Upon completing the execution stage or stages, the particular ROP updates its assigned physical register during the register file write stage. Finally, the particular ROP is retired after each previous ROP is retired (in the retire stage). Again, one or more clock cycles may elapse for a particular ROP between the register file write stage and the retire stage. Furthermore, a particular ROP may be stalled at any stage due to pipeline stall conditions, as is well known in the art.</p><h4>Cache Access</h4><p>Turning now to FIG. 3, a block diagram illustrating one embodiment of branch prediction/fetch PC generation unit <b>18</b>, line predictor <b>12</b>, I-cache <b>14</b>, an instruction TLB (ITLB) <b>60</b>, and a fetch address mux <b>64</b> is shown. Other embodiments are possible and contemplated. In the embodiment of FIG. 3, branch prediction/fetch PC generation unit <b>18</b> is coupled to receive a trap PC from PC silo <b>48</b>, and is further coupled to ITLB <b>60</b>, line predictor <b>12</b>, I-cache <b>14</b>, and fetch address mux <b>64</b>. ITLB <b>60</b> is further coupled to fetch address mux <b>64</b>, which is coupled to I-cache <b>14</b>. Line predictor <b>12</b> is coupled to I-cache <b>14</b>, and fetch address mux <b>64</b>.</p><p>Generally, branch prediction/fetch PC generation unit <b>18</b> generates a fetch address (fetch PC) for instructions to be fetched. The fetch address is provided to line predictor <b>12</b> and TLB <b>60</b> (as well as PC silo <b>48</b>, as shown in FIG. <b>1</b>). Line predictor <b>12</b> compares the fetch address to fetch addresses stored therein to determine if a line predictor entry corresponding to the fetch address exists within line predictor <b>12</b>. If a corresponding line predictor entry is found, the instruction pointers stored in the line predictor entry are provided to alignment unit <b>16</b>. In parallel with line predictor <b>12</b> searching the line predictor entries, ITLB <b>60</b> translates the fetch address (which is a virtual address in the present embodiment) to a physical address (physical PC) for access to I-cache <b>14</b>. ITLB <b>60</b> provides the physical address to fetch address mux <b>64</b>, and branch prediction/fetch PC generation unit <b>18</b>D controls mux <b>64</b> to select the physical address. The output of mux <b>64</b> is referred to as the cache fetch address (or cache fetch PC) for clarity in the drawings and description. I-cache <b>14</b> reads instruction bytes corresponding to the physical address and provides the instruction bytes to alignment unit <b>16</b>.</p><p>In the present embodiment, each line predictor entry also provides a next fetch address (next fetch PC). The next fetch address is provided to mux <b>64</b>, and branch prediction/fetch PC generation unit <b>18</b> selects the address through mux <b>64</b> as the cache fetch address to access I-cache <b>14</b> in response to line predictor <b>12</b> detecting a hit. In this manner, the next fetch address may be more rapidly provided to I-cache <b>14</b> as long as the fetch addresses continue to hit in the line predictor. The line predictor entry may also include an indication of the next line predictor entry within line predictor <b>12</b> (corresponding to the next fetch address) to allow line predictor <b>12</b> to fetch instruction pointers corresponding to the next fetch address. Accordingly, as long as fetch addresses continue to hit in line predictor <b>12</b>, fetching of lines of instructions may be initiated from the line predictor stage of the pipeline shown in FIG. <b>2</b>. Traps initiated by PC silo <b>48</b> (in response to scheduler <b>36</b>), a disagreement between the prediction made by line predictor <b>12</b> for the next fetch address and the next fetch address generated by branch prediction/fetch PC generation unit <b>18</b> may cause the fetch address generated by branch prediction/fetch PC generation unit <b>18</b> to be searched in line predictor <b>12</b>.</p><p>Additionally, line predictor <b>12</b> provides a way prediction corresponding to the cache fetch address. I-cache <b>14</b> may read the predicted way identified by the way prediction and provide the read instruction bytes to alignment unit <b>16</b>. Advantageously, the latency for accessing I-cache <b>14</b> may be reduced since the tag comparisons are not used to select output data. Furthermore, power consumption may be reduced by idling the non-predicted ways (i.e. not accessing the non-predicted ways), and thus the power that would be consumed by accessing the non-predicted ways is conserved. Still further, I-cache <b>14</b> may access the tag from the predicted way and partial tags from the nonpredicted ways. The partial tags exclude one or more bits of the full tag, and may be used to direct the search of the non-predicted ways if the fetch address misses in the predicted way. Still additional power may be conserved by accessing the full tag from the predicted way but only partial tags from the non-predicted ways. If the fetch address misses the predicted way, I-cache <b>14</b> may search the non-predicted ways for which the corresponding partial tag matches the corresponding portion of the fetch address. If the partial tag does not match the corresponding portion of the fetch address, then the full tag will not match either and thus the way for which a partial tag does not match may be skipped in the search. Accordingly, the search may be more efficient while still allowing power consumption to be reduced as compared to accessing the full tag from each way.</p><p>If a way prediction miss is detected, I-cache <b>14</b> may assert a stall signal to branch prediction/fetch PC generation unit <b>18</b> and line predictor <b>12</b>. The stall signal may cause branch prediction/fetch PC generation unit <b>18</b> and line predictor <b>12</b> to interrupt further generation of fetch addresses to allow I-cache <b>14</b> to search for a hit in the non-predicted ways. Once a hit is detected, I-cache <b>14</b> may provide an updated way prediction to line predictor <b>12</b> and deassert the stall signal. Line predictor <b>12</b> may update the corresponding line predictor entry with the updated way prediction. If a miss is detected (i.e. none of the ways have a matching tag), then I-cache <b>14</b> may select a replacement way and provide the replacement way as an updated way prediction. Alternatively, the replacement way may be selected when the corresponding instruction bytes are provided to I-cache <b>14</b> from external interface unit <b>46</b>.</p><p>Even while next fetch addresses are being generated by line predictor <b>12</b> and are hitting in line predictor <b>12</b>, branch prediction/fetch PC generation unit <b>18</b> continues to generate fetch addresses for logging by PC silo <b>48</b>. Furthermore, branch prediction/fetch PC generation unit <b>18</b> may verify the next fetch addresses provided by line predictor <b>12</b> via one or more branch predictors included therein. The line predictor entries within line predictor <b>12</b> identify the terminating instruction within the line of instructions by type, and line predictor <b>12</b> transmits the type information to branch prediction/fetch PC generation unit <b>18</b> as well as the predicted direction of the terminating instruction (status in FIG. <b>3</b>). Furthermore, for branches forming a target address via a branch displacement included within the branch instruction, line predictor <b>12</b> may provide an indication of the branch displacement. From this information and information stored in the branch predictors, branch prediction/fetch PC generation unit <b>18</b> may generate the virtual next fetch addresses. In one embodiment, the branch predictors include a conditional branch predictor, an indirect branch target address cache, and a return stack.</p><p>It is noted that, in one embodiment, I-cache <b>14</b> may provide a fixed number of instruction bytes per instruction fetch, beginning with the instruction byte located by the fetch address. Since a fetch address may locate a byte anywhere within a cache line, I-cache <b>14</b> may access two cache lines in response to the fetch address (the cache line indexed by the fetch address, and a cache line at the next index in the cache). Other embodiments may limit the number of instruction bytes provided to up to a fixed number or the end of the cache line, whichever comes first. In one embodiment, the fixed number is 16 although other embodiments may use a fixed number greater or less than 16. In embodiments which access two cache lines, two way predictions may be provided. The discussion below with respect to FIGS. 4-9 may refer to a way prediction, but may be extended to multiple concurrent way predictions.</p><p>As used herein, an \u201caddress\u201d is a value which identifies a byte within a memory system to which processor <b>10</b> is couplable. A \u201cfetch address\u201d is an address used to fetch instruction bytes to be executed as instructions within processor <b>10</b>. As mentioned above, processor <b>10</b> may employ an address translation mechanism in which virtual addresses (generated in response to the operands of instructions) are translated to physical addresses (which physically identify locations in the memory system). In the x86 instruction set architecture, virtual addresses may be linear addresses generated according to a segmentation mechanism operating upon logical addresses generated from operands of the instructions. Other instruction set architectures may define the virtual address differently.</p><p>Turning now to FIG. 4, a block diagram of one embodiment of I-cache <b>14</b> is shown. Other embodiments are possible and contemplated. In the embodiment of FIG. 4, I-cache <b>14</b> includes a tag array <b>70</b>, an instruction array <b>72</b>, a remaining tag comparator <b>74</b>, a plurality of partial tag comparators <b>76</b>A-<b>76</b>D, a control circuit <b>78</b>, a way multiplexor (mux) <b>80</b>, an address mux <b>82</b>, and an address register <b>84</b>. Address mux <b>82</b> and address register <b>84</b> are coupled to receive the cache fetch address provided to I-cache <b>14</b>, and address register <b>84</b> is further coupled to address mux <b>82</b>. The output of address mux <b>82</b> is coupled to tag array <b>70</b> and instruction array <b>72</b>, as well as to comparators <b>74</b> and <b>76</b>A-<b>76</b>D. Way mux <b>80</b> is coupled to receive the way prediction provided to I-cache <b>14</b> and is coupled to receive a search way selection from control circuit <b>78</b>. The output of way-mux <b>80</b> is coupled to tag array <b>70</b> and instruction array <b>72</b>. Muxes <b>80</b> and <b>82</b> are coupled to receive selection signals from control circuit <b>78</b>, and address register <b>84</b> is coupled to receive a hold control signal from control circuit <b>78</b>. Remaining tag comparator <b>74</b> is coupled to receive a predicted way remaining tag (RTag) from tag array <b>70</b> and is coupled to provide an output signal to control circuit <b>78</b>. Each of partial tag comparators <b>76</b>A-<b>76</b>D are coupled to receive respective partial tags (PTag<b>0</b>-PTag<b>3</b>) from tag array <b>70</b> and are coupled to provide respective output signals to control circuit <b>78</b>. Control circuit <b>78</b> is coupled to provide a miss signal to external interface unit <b>46</b>, a way prediction update signal to line predictor <b>12</b>, and a stall signal to line predictor <b>12</b> and branch prediction/fetch PC generation unit <b>18</b>. Control circuit <b>78</b> is coupled to receive the way selection from way mux <b>80</b>. It is noted that I-cache <b>14</b> may include additional circuitry (not shown) to manage the transfer of cache lines into I-cache <b>14</b> in response to misses, to manage snoop transactions, etc., as desired.</p><p>Tag array <b>70</b> stores the tags of the cache lines of instruction bytes which are stored in I-cache <b>14</b>, and instruction array <b>72</b> stores the cache lines of instruction bytes. Tag array <b>70</b> and instruction array <b>72</b> may, for example, comprise random access memory (RAM) arrays. There is a one-to-one correspondence between tag storage locations in tag array <b>70</b> and cache line storage locations in instruction array <b>72</b>. More particularly, tag array <b>70</b> and instruction array <b>72</b> may include the same number of sets (and thus be indexed by the same set of index bits) and the same number of ways.</p><p>Generally, while control circuit <b>78</b> is not searching for a hit of a fetch address in a non-predicted way due to a miss in the predicted way, control circuit <b>78</b> selects the cache fetch address through address mux <b>82</b> and the way prediction through way mux <b>80</b>. Accordingly, the cache fetch address and predicted way are provided to tag array <b>70</b> and instruction array <b>72</b>. More particularly, the index portion of the cache fetch address is provided to each array, and the corresponding set is selected. Furthermore, in instruction array <b>72</b>, the predicted way of the set is accessed and the non-predicted ways are held idle to conserve power (in response to the way prediction). Since the power consumed by an array during a read access is generally proportional to the amount of information accessed (e.g. the number of bits or bytes), the power consumed is reduced by idling portions of the array that would otherwise be accessed and then not selected at the output (e.g. the non-predicted ways of instruction array <b>72</b>). The instruction bytes stored in the predicted way are output from instruction array <b>72</b> to alignment unit <b>16</b>. Additionally, a partial tag is read from each way of tag array <b>70</b>, and the remaining tag (i.e. the portion of the tag which is excluded from the partial tag) is read from the predicted way of tag array <b>70</b>. Again, since only partial tags are read from the non-predicted ways, power may be conserved as compared to reading the complete tag from each way. The combination of the partial tag from the predicted way and the remaining tag from the predicted way comprises the tag from the predicted way.</p><p>Remaining tag comparator <b>74</b> compares the remaining tag from the predicted way to a corresponding portion of the fetch address. If the remaining tag and corresponding portion of the fetch address are equal, remaining tag comparator <b>74</b> asserts its output signal to control circuit <b>78</b>. Otherwise, remaining tag comparator <b>74</b> deasserts its output signal to control circuit <b>78</b>. Similarly, each of partial tag comparators <b>76</b>A-<b>76</b>D receives a partial tag from a respective way of tag array <b>70</b> and compares the received partial tag to the corresponding portion of the fetch address. If the received partial tag and corresponding portion of the fetch address are equal, the partial tag comparator <b>76</b>A-<b>76</b>D asserts its output signal. Otherwise, the partial tag comparator <b>76</b>A-<b>76</b>D deasserts its output signal.</p><p>If remaining tag comparator <b>74</b> and the partial tag comparator <b>76</b>A-<b>76</b>D corresponding to the predicted way both assert their output signals, then control circuit <b>78</b> detects a hit in the predicted way and the corresponding instruction bytes provided by instruction array <b>72</b> are correct. Control circuit <b>78</b> does not assert the stall signal or the miss signal, and subsequent accesses may continue. Control circuit <b>78</b> receives the way selection provided to tag array <b>70</b> to determine which of the partial tag comparators <b>76</b>A-<b>76</b>D corresponds to the selected way.</p><p>On the other hand, if either the remaining tag comparator <b>74</b> or the partial tag comparator <b>76</b>A-<b>76</b>D corresponding to the predicted way does not assert its output signal, control circuit <b>78</b> detects a miss in the predicted way. If none of the other partial tag comparators <b>76</b>A-<b>76</b>D asserts its output signal, then the fetch address is a miss in I-cache <b>14</b> and control circuit <b>78</b> asserts the miss signal to external interface unit <b>46</b> to cause the missing instruction bytes to be fetched. Conversely, if at least one of the output signals from the other partial tag comparators <b>76</b>A-<b>76</b>D is asserted and a miss in the predicted way is detected, control circuit <b>78</b> generates a search way selection, provides the search way selection to way mux <b>80</b>, and asserts the stall signal to line predictor <b>12</b> and branch prediction/fetch PC generation unit <b>18</b>. The search way selection identifies a way for which the partial tag matches the corresponding portion of the fetch address. Control circuit <b>78</b> selects the search way selection through way mux <b>80</b> and the value in address register <b>84</b> through address mux <b>84</b>. The search way is accessed, and comparisons to determine if a hit is detected in the search way are preformed in a manner similar to the predicted way access. If a hit is detected, control circuit <b>78</b> transmits the search way selection which hits as the way prediction update to line predictor <b>12</b>.</p><p>Generally, address register <b>84</b> captures the cache fetch address provided to I-cache <b>14</b> each clock cycle. However, address register <b>84</b> includes a hold control which, when asserted, causes address register <b>84</b> to hold its current value. The hold control is asserted during clock cycles in which control circuit <b>78</b> is selecting the search way selection through way mux <b>80</b> and the contents of address register <b>84</b> through address mux <b>82</b>. In this manner, the fetch address which missed in the predicted way is presented again to tag array <b>70</b> and instruction array <b>72</b>, and the search way selection is provided as the way selection to tag array <b>70</b> and instruction array <b>72</b>.</p><p>If control circuit <b>78</b> does not detect a hit in the search way, control circuit <b>78</b> generates additional search ways until each way for which a partial tag hit is detected has been searched. In one embodiment, the first search way may be the lowest numbered way for which a partial tag hit is detected, and the subsequent search ways may be generated in increasing order (e.g. way <b>0</b> is searched first, and way <b>3</b> is searched last, in the present embodiment). Control circuit <b>78</b> continues to assert the stall signal as the search continues. If each of the ways having partial tag hits have been searched for a hit and no hit has been detected, control circuit <b>78</b> asserts the miss signal to external interface unit <b>46</b>. Accordingly, the missing cache line may be fetched and stored into I-cache <b>14</b>. The assertion of the miss signal may cause line predictor <b>12</b> and branch prediction/fetch PC generation unit <b>18</b> to stall until the missing instruction bytes are provided from external interface unit <b>46</b>. Alternatively, control circuit <b>78</b> may continue to assert the stall signal until the missing instruction bytes are provided.</p><p>It is noted that, in addition to accessing only the selected way in instruction array <b>72</b>, instruction array <b>72</b> may employ multiple banks within a cache line. Instruction array <b>72</b> may use one or more cache line offset bits to select a bank (or banks) to access, and the other banks may be held idle. It is further noted that a portion of an array may be held idle by not accessing that portion of the array. For example, the portion may be idle if no storage cells are selected from the portion to output data from the array. In other words, the idled portions may not be \u201cpowered up\u201d. It is further noted that, while an instruction cache is used as an example of the use of way prediction and partial tags, other caches may employ similar structures. For example, a data cache or combined instruction/data cache may employ a similar structure.</p><p>It is noted that alternative configurations are possible. For example, instead of searching each of the ways for which a partial hit is detected, tag array <b>70</b> may be configured to provide a full tag read from each way in addition to reading a full tag from a selected way and partial tags from the other ways. If one or more partial tag hits are detected, control circuit <b>78</b> may perform the full tag read and full tag comparators may be provided for each way. In this manner, the search for a hit may be completed in one read of the tags, rather than one or more reads as described above. It is further noted that address mux <b>82</b> and mux <b>64</b> (shown in FIG. 3) may be merged, if desired, into a single mux to select between address register <b>84</b>, the physical address provided by ITLB <b>60</b>, and the next fetch address provided by line predictor <b>12</b>.</p><p>Turning next to FIG. 5, a block diagram of one embodiment of tag array <b>70</b> is shown. Other embodiments are possible and contemplated. In the embodiment of FIG. 5, tag array <b>70</b> includes a partial tags decoder <b>90</b> coupled to receive an index and a remaining tag decoder <b>92</b> coupled to receive the index and a way selection. Tag array <b>70</b> further includes a set of RAM cells <b>94</b> arranged into sets. An exemplary set <b>96</b> is illustrated between two horizontal dashed lines, and other sets may include an identical configuration receiving different signals from decoders <b>90</b> and <b>92</b>. Set <b>96</b> includes remaining tags cells <b>98</b>A-<b>98</b>D and partial tags cells <b>100</b>. A remaining tags sense amplifier (senseamp) <b>102</b> and a partial tags senseamp <b>104</b> are shown as well. It is noted that additional circuitry may be included to update tag array <b>70</b> (not shown) in any manner suitable for set associative RAM designs.</p><p>In a typical RAM array, a decoder decodes the received index to select a set and activates a word line to the RAM cells within the set. Each RAM cell is coupled to a pair of bit lines (commonly referred to as bit and bit bar), one of which the RAM cell discharges in response to the bit stored in the RAM cell. A senseamp is coupled to the bit lines and is configured to produce a binary zero or binary one for each cell in the set in response to the differential voltage between the pair of bit lines. In the embodiment of tag array <b>70</b> illustrated in FIG. 5, cells <b>98</b>A-<b>98</b>D and cells <b>100</b> are coupled to pairs of bits lines for each cell, represented by vertical lines coupling the cells to one of senseamps <b>102</b> or <b>104</b>. Thus, for each bit in cells <b>98</b>A, a pair of bit lines may be provided and coupled to remaining tags senseamp <b>102</b>. Similarly, for each bit in cells <b>98</b>B, a pair of bit lines may be provided coupled to remaining tags senseamp <b>102</b> (and similarly for cells <b>98</b>C-<b>98</b>D). A pair of bit lines may be provided for each bit in cells <b>100</b>, coupled to partial tags senseamp <b>104</b>. The cells <b>98</b>A-<b>98</b>D and <b>100</b> in other sets (not shown in FIG. 5) are coupled to the same sets of bit lines as the cells <b>98</b>A-<b>98</b>D and <b>100</b>, respectively.</p><p>Generally, partial tags cells <b>100</b> store the partial tags portion of each tag stored in set <b>96</b>. For example, in the illustrated embodiment, tag array <b>70</b> is <b>4</b> way set associative. Thus, partial tags cells <b>100</b> includes cells to store each bit of four partial tags (one for each way) for such an embodiment. Each of remaining tags cells <b>98</b>A-<b>98</b>D includes cells to store each bit of one remaining tag (and thus four tags cells <b>98</b>A-<b>98</b>D are illustrated for the present four way set associative embodiment).</p><p>Partial tags decoder <b>90</b> decodes the index received by the tag array and activates a partial tags word line for the selected set. Each set receives a different word line from partial tags decoder <b>90</b>. For example, FIG. 5 illustrates partial tags <b>100</b> in set <b>96</b> receiving a word line from partial tags decoder <b>90</b>. In response to the activated word line, the partial tags cells <b>100</b> discharge the bit lines coupled thereto and partial tags senseamp <b>104</b> senses the bits of the partial tags via the differential voltages on the bit lines. In this manner, the partial tags from each way are output to comparators <b>76</b>A-<b>76</b>D. It is noted that the word line for the partial tags may actually comprise separate word lines for each way, to allow one partial tags portion to be updated while leaving the remaining partial tags unchanged. For read purposes, each of the word lines is activated to read each of the partial tags.</p><p>Remaining tag decoder <b>92</b> decodes the index to determine the selected set, and also decodes the way selection to select one of the remaining tags stored in the selected set. Remaining tag decoder <b>92</b> provides a plurality of word lines to each set, and activates one of the word lines for the selected set in response to the way selection. For example, FIG. 5 illustrates remaining tag decoder <b>92</b> providing a first word line to remaining tags cells <b>98</b>A, a second word line to remaining tags cells <b>98</b>B, a third word line to remaining tags cells <b>98</b>C, and a fourth word line to remaining tags cells <b>98</b>D. Other sets receive different plurality of word lines. In response to an activated word line, one of remaining tags cells <b>98</b>A-<b>98</b>D discharges the corresponding bit lines and remaining tag senseamp <b>102</b> senses the bits of the remaining tag via the differential voltages. In this manner, the remaining tag from the selected way is output to comparator <b>74</b>. It is noted that, in the illustrated embodiment, remaining tag senseamp <b>102</b> provides one remaining tag from the several remaining tags in the set. Thus, remaining tag senseamp <b>102</b> receives the way selection (or, alternatively, control signals from remaining tag decoder <b>92</b>) to select one set of bit lines corresponding to one of the remaining tags cells <b>98</b>A-<b>98</b>D to sense. In other words, the bit lines from each of remaining tags cells <b>98</b>A-<b>98</b>D may be \u201ccolumn-muxed\u201d prior to being input to the senseamp circuitry. Since senseamps typically consume substantial amounts of the power consumed by an array, limiting the number of bit lines sensed may further reduce power. Alternatively, the remaining tags may be muxed after being sensed by separate senseamps, as desired.</p><p>In an alternative configuration, tag array <b>70</b> may employ a single decoder and a single word line per set, but may provide one remaining tag senseamp <b>102</b> and column mux as described above. In such and embodiment, each remaining tags cells <b>98</b>A-<b>98</b>D in the selected set discharges its corresponding bit lines, but the number of bit lines sensed is still limited and thus the power consumed by the senseamp may be limited. Furthermore, in an embodiment which provides for a full tag read, tag array <b>70</b> may employ senseamps for each bit stored in a set but may only activate the partial tags senseamps and one of the remaining tags senseamps unless a full tag read is performed.</p><p>It is noted that the distribution of cells within a set as shown in FIG. 5 is for convenience and clarity in the drawing only. The cells may be physically arranged in any suitable manner according to design choice.</p><p>Turning now to FIG. 6, an exemplary fetch address <b>110</b> is shown illustrating the various portions of the fetch address as they relate to I-cache <b>14</b>. The most significant bits of the fetch address are to the left in FIG. 6, and the least significant bits are to the right in FIG. <b>6</b>. As shown in FIG. 6, fetch address <b>110</b> is divided into an offset field <b>112</b>, an index field <b>114</b>, and a tag field <b>116</b>. Tag field <b>116</b> is subdivided into a partial tag field <b>118</b> and a remaining tag field <b>120</b>.</p><p>The portion of fetch address <b>110</b> forming offset field <b>112</b> are the bits of fetch address <b>110</b> which define the offset of the addressed byte within the instruction cache line. Accordingly, the number of bits within offset field <b>112</b> is dependent upon the number of bytes within the cache line. For example, in one embodiment I-cache <b>14</b> comprises a 64 byte cache line. For such an embodiment, offset field <b>112</b> is the least significant six bits of the cache line.</p><p>The index field <b>114</b> includes the bits which form the cache index to select the set in I-cache <b>14</b>. In the embodiment shown, index field <b>114</b> comprises the least significant bits of fetch address <b>110</b> exclusive of offset field <b>112</b>. For example, in one embodiment, I-cache <b>14</b> is 128 Kbytes in a four way set associative structure with 64 byte lines. Such an embodiment has 512 sets, and thus the index comprises 9 bits to provide for selection of a set (2<sup>9</sup>=512).</p><p>Tag field <b>116</b> comprises the portion of fetch address <b>110</b> exclusive of the index field <b>114</b> and the offset field <b>112</b>. Tag field <b>116</b> comprises the portion of fetch address <b>110</b> which is stored by I-cache <b>14</b> to uniquely identify a cache line in tag array <b>70</b> (and is referred to as the \u201ctag\u201d). Since offset field <b>112</b> defines a byte within the cache line, the offset field <b>112</b> is not stored. Furthermore, since the index selects the set within I-cache <b>14</b>, the index is inherent in the location of I-cache <b>14</b> storing the tag. Therefore, comparing the tag field <b>116</b> to the tags stored in the selected set identifies a cache line corresponding to the fetch address as being stored in I-cache <b>14</b> (if a match is detected).</p><p>Partial tag field <b>118</b> is, in the illustrated embodiment, the portion of tag field <b>116</b> provided by tag array <b>70</b> for each way to determine which ways to search for a hit if the predicted way misses, and is the portion of the fetch address provided to partial tag comparators <b>76</b>A-<b>76</b>D for comparison to the partial tags provided by tag array <b>70</b>. In the illustrated embodiment, partial tag field <b>118</b> comprises the least significant bits of tag field <b>116</b>. Since many programs exhibit \u201clocality of reference\u201d in which the instructions are physically located in memory near each other, and since I-cache <b>14</b> typically stores the most recently accessed cache lines of instruction bytes, the bits of the tag which may be statistically most likely to be different from the fetch address are the least significant bits of the tag. Accordingly, these bits are selected as the partial tag field <b>118</b> to increase the likelihood of limiting the search of non-predicted ways, thus more quickly locating the hitting way or detecting a miss. Other embodiments may select any suitable portion of tag field <b>116</b> to form the partial tag.</p><p>Remaining tag field <b>120</b> is, in the illustrated embodiment, the portion of the tag field <b>116</b> provided by tag array <b>70</b> for the predicted or selected way to determine if a hit in the way is detected, and is the portion of the fetch address provided to remaining tag comparator <b>74</b> for comparison to the remaining tag provided by tag array <b>70</b>. Remaining tag field <b>120</b> comprises the remaining bits of tag field <b>116</b> exclusive of the partial tag field <b>118</b>.</p><p>The number of bits included in partial tag field <b>118</b> may be selected according to design choice. Generally, the number of bits selected may be a tradeoff between the accuracy with which ways are eliminated or included in the search and the amount of power conserved. The larger the number of bits included in the partial tag, the more likely a way storing a non-matching tag will be eliminated from the search for a hit in a non-predicted way, but the less power will be conserved. For example, in an embodiment having a four way set associative cache, one third of the bits in tag field <b>116</b> may be allocated to partial tag field <b>118</b> while consuming approximately half the power of a full tag read (since one full tag three partial tags, each one third the size of the full tag, are read, equaling two full tags). Any number of bits may be included in the partial tag. Generally, a partial tag excludes at least one bit, and may exclude multiple bits, of the tag.</p><p>Turning next to FIG. 7, an exemplary state machine is shown which may be employed by one embodiment of control circuit <b>78</b>. Other embodiments are possible and contemplated. In the embodiment of FIG. 7, the state machine includes an access state <b>130</b> and a search state <b>132</b>.</p><p>Access state <b>130</b> is the state in which normal fetches from I-cache <b>14</b> are performed. Accordingly, the state machine remains in access state <b>130</b> if a fetch hits in the predicted way. Additionally, the state machine remains in access state <b>130</b> if a fetch misses in the predicted way but also is a miss when compared to the partial tags (i.e. none of the partial tags match the corresponding portion of the fetch address). On the other hand, the state machine transitions from access state <b>130</b> to search state <b>132</b> in response to a fetch which misses in the predicted way and at least one of the partial tags matches the corresponding portion of the fetch address (a \u201cpartial tag hit\u201d).</p><p>Search state <b>132</b> is the state in which I-cache <b>14</b> searches the non-predicted ways which have a partial tag hit for a fetch address which missed the predicted way. The state machine remains in search state <b>132</b> if the fetch address misses the search way of the current search and at least one other way having a partial tag hit is to be searched. The state machine transitions from search state <b>132</b> to access state <b>130</b> if the search way hits or if each of the partial tag hits is exhausted (i.e. has been searched and found to miss).</p><p>Turning next to FIG. 8, a flowchart is shown illustrating operation of one embodiment of control circuit <b>78</b> while the state machine is in access state <b>130</b>. Other embodiments are possible and contemplated. While steps are shown in FIG. 8 in a particular order for ease of understanding, any suitable order may be used. Furthermore, steps may be performed in parallel by the combinatorial logic employed within control circuit <b>78</b>.</p><p>If a fetch address is presented, control circuit <b>78</b> determines if the fetch address misses in the predicted way (decision block <b>140</b>). For example, control circuit <b>78</b> examines the output signals from remaining tag comparator <b>74</b> and the partial tag comparator <b>76</b>A-<b>76</b>D corresponding to the predicted way. If the fetch address hits in the predicted way, control circuit <b>78</b> does nothing additional. On the other hand, if the fetch address misses in the predicted way, control circuit <b>78</b> determines if there is a partial tag hit (decision block <b>142</b>). For example, control circuit <b>78</b> may determine if there is a partial tag hit by examining the output signals from each other partial tag comparator <b>76</b>A-<b>76</b>D. If no partial tag hit is detected, control circuit <b>78</b> asserts the miss signal to external interface unit <b>46</b> to initiate a fetch of the missing cache line (step <b>144</b>). On the other hand, if a partial tag hit is detected, control circuit <b>78</b> asserts the stall signal (step <b>146</b>). Additionally, control circuit <b>78</b> initiates a read of a first way for which a corresponding partial tag hits the fetch address (step <b>148</b>). Control circuit <b>78</b> then transitions the state machine to the search state (step <b>150</b>).</p><p>FIG. 9 is a flowchart illustrating operation of one embodiment of control circuit <b>78</b> while the state machine is in search state <b>132</b>. Other embodiments are possible and contemplated. While steps are shown in FIG. 9 in a particular order for ease of understanding, any suitable order may be used. Furthermore, steps may be performed in parallel by the combinatorial logic employed within control circuit <b>78</b>.</p><p>Control circuit <b>78</b> determines if the fetch address hits in the search way (decision block <b>160</b>). For example, control circuit <b>78</b> examines the output signals from remaining tag comparator <b>74</b> and the partial tag comparator <b>76</b>A-<b>76</b>D corresponding to the search way. If the fetch address hits in the search way, control circuit <b>78</b> transmits the updated way prediction to line predictor <b>12</b> (step <b>162</b>), deasserts the stall signal (step <b>164</b>), and transitions the state machine to the access state (step <b>166</b>).</p><p>On the other hand, if the fetch address does not hit in the search way, control circuit <b>78</b> determines if there are additional partial tag hits to search (decision block <b>168</b>). For example, control circuit <b>78</b> may determine if there is a higher numbered way than the search way for which a partial tag hit is detected. If an additional partial tag hit is detected, control circuit <b>78</b> initiates a read of the next partial tag hit (step <b>170</b>). For example, control circuit <b>78</b> may select the next higher numbered way from the search way for which a partial tag hit is detected. Additionally, control circuit <b>78</b> continues to assert the stall signal to allow for the next way to be searched (step <b>172</b>). If there are no additional partial tag hits to search, control circuit <b>78</b> asserts the miss signal to external interface unit <b>46</b> (step <b>174</b>) and transitions the state machine to the access state (step <b>176</b>).</p><p>It is noted that the term control circuit is used herein to refer to any combination of circuitry (e.g. combinatorial logic gates, data flow elements such as muxes, registers, latches, flops, adders, shifters, rotators, etc., and/or circuits implementing state machines) which operates on inputs and generates outputs in response thereto as described. Additionally, as used herein, the term \u201casserted\u201d refers to providing a logically true value for a signal or a bit. A signal or bit may be asserted if it conveys a value indicative of a particular condition. Conversely, a signal or bit may be \u201cdeasserted\u201d if it conveys a value indicative of a lack of a particular condition. A signal or bit may be defined to be asserted when it conveys a logical zero value or, conversely, when it conveys a logical one value, and the signal or bit may be defined as deasserted when the opposite logical value is conveyed.</p><h4>Computer Systems</h4><p>Turning now to FIG. 10, a block diagram of one embodiment of a computer system <b>200</b> including processor <b>10</b> coupled to a variety of system components through a bus bridge <b>202</b> is shown. Other embodiments are possible and contemplated. In the depicted system, a main memory <b>204</b> is coupled to bus bridge <b>202</b> through a memory bus <b>206</b>, and a graphics controller <b>208</b> is coupled to bus bridge <b>202</b> through an AGP bus <b>210</b>. Finally, a plurality of PCI devices <b>212</b>A-<b>212</b>B are coupled to bus bridge <b>202</b> through a PCI bus <b>214</b>. A secondary bus bridge <b>216</b> may further be provided to accommodate an electrical interface to one or more EISA or ISA devices <b>218</b> through an EISA/ISA bus <b>220</b>. Processor <b>10</b> is coupled to bus bridge <b>202</b> through a CPU bus <b>224</b> and to an optional L<b>2</b> cache <b>228</b>. Together, CPU bus <b>224</b> and the interface to L<b>2</b> cache <b>228</b> may comprise external interface <b>52</b>.</p><p>Bus bridge <b>202</b> provides an interface between processor <b>10</b>, main memory <b>204</b>, graphics controller <b>208</b>, and devices attached to PCI bus <b>214</b>. When an operation is received from one of the devices connected to bus bridge <b>202</b>, bus bridge <b>202</b> identifies the target of the operation (e.g. a particular device or, in the case of PCI bus <b>214</b>, that the target is on PCI bus <b>214</b>). Bus bridge <b>202</b> routes the operation to the targeted device. Bus bridge <b>202</b> generally translates an operation from the protocol used by the source device or bus to the protocol used by the target device or bus.</p><p>In addition to providing an interface to an ISA/EISA bus for PCI bus <b>214</b>, secondary bus bridge <b>216</b> may further incorporate additional functionality, as desired. An input/output controller (not shown), either external from or integrated with secondary bus bridge <b>216</b>, may also be included within computer system <b>200</b> to provide operational support for a keyboard and mouse <b>222</b> and for various serial and parallel ports, as desired. An external cache unit (not shown) may further be coupled to CPU bus <b>224</b> between processor <b>10</b> and bus bridge <b>202</b> in other embodiments. Alternatively, the external cache may be coupled to bus bridge <b>202</b> and cache control logic for the external cache may be integrated into bus bridge <b>202</b>. L<b>2</b> cache <b>228</b> is further shown in a backside configuration to processor <b>10</b>. It is noted that L<b>2</b> cache <b>228</b> may be separate from processor <b>10</b>, integrated into a cartridge (e.g. slot <b>1</b> or slot A) with processor <b>10</b>, or even integrated onto a semiconductor substrate with processor <b>10</b>.</p><p>Main memory <b>204</b> is a memory in which application programs are stored and from which processor <b>10</b> primarily executes. A suitable main memory <b>204</b> comprises DRAM (Dynamic Random Access Memory). For example, a plurality of banks of SDRAM (Synchronous DRAM) or Rambus DRAM (RDRAM) may be suitable.</p><p>PCI devices <b>212</b>A-<b>212</b>B are illustrative of a variety of peripheral devices such as, for example, network interface cards, video accelerators, audio cards, hard or floppy disk drives or drive controllers, SCSI (Small Computer Systems Interface) adapters and telephony cards. Similarly, ISA device <b>218</b> is illustrative of various types of peripheral devices, such as a modem, a sound card, and a variety of data acquisition cards such as GPIB or field bus interface cards.</p><p>Graphics controller <b>208</b> is provided to control the rendering of text and images on a display <b>226</b>. Graphics controller <b>208</b> may embody a typical graphics accelerator generally known in the art to render three-dimensional data structures which can be effectively shifted into and from main memory <b>204</b>. Graphics controller <b>208</b> may therefore be a master of AGP bus <b>210</b> in that it can request and receive access to a target interface within bus bridge <b>202</b> to thereby obtain access to main memory <b>204</b>. A dedicated graphics bus accommodates rapid retrieval of data from main memory <b>204</b>. For certain operations, graphics controller <b>208</b> may further be configured to generate PCI protocol transactions on AGP bus <b>210</b>. The AGP interface of bus bridge <b>202</b> may thus include functionality to support both AGP protocol transactions as well as PCI protocol target and initiator transactions. Display <b>226</b> is any electronic display upon which an image or text can be presented. A suitable display <b>226</b> includes a cathode ray tube (\u201cCRT\u201d), a liquid crystal display (\u201cLCD\u201d), etc.</p><p>It is noted that, while the AGP, PCI, and ISA or EISA buses have been used as examples in the above description, any bus architectures may be substituted as desired. It is further noted that computer system <b>200</b> may be a multiprocessing computer system including additional processors (e.g. processor <b>10</b><i>a </i>shown as an optional component of computer system <b>200</b>). Processor <b>10</b><i>a </i>may be similar to processor <b>10</b>. More particularly, processor <b>10</b><i>a </i>may be an identical copy of processor <b>10</b>. Processor <b>10</b><i>a </i>may be connected to bus bridge <b>202</b> via an independent bus (as shown in FIG. 10) or may share CPU bus <b>224</b> with processor <b>10</b>. Furthermore, processor <b>10</b><i>a </i>may be coupled to an optional L<b>2</b> cache <b>228</b><i>a </i>similar to L<b>2</b> cache <b>228</b>.</p><p>Turning now to FIG. 11, another embodiment of a computer system <b>300</b> is shown. Other embodiments are possible and contemplated. In the embodiment of FIG. 11, computer system <b>300</b> includes several processing nodes <b>312</b>A, <b>312</b>B, <b>312</b>C, and <b>312</b>D. Each processing node is coupled to a respective memory <b>314</b>A-<b>314</b>D via a memory controller <b>316</b>A-<b>316</b>D included within each respective processing node <b>312</b>A-<b>312</b>D. Additionally, processing nodes <b>312</b>A-<b>312</b>D include interface logic used to communicate between the processing nodes <b>312</b>A-<b>312</b>D. For example, processing node <b>312</b>A includes interface logic <b>318</b>A for communicating with processing node <b>312</b>B, interface logic <b>318</b>B for communicating with processing node <b>312</b>C, and a third interface logic <b>318</b>C for communicating with yet another processing node (not shown). Similarly, processing node <b>312</b>B includes interface logic <b>318</b>D, <b>318</b>E, and <b>318</b>F; processing node <b>312</b>C includes interface logic <b>318</b>G, <b>318</b>H, and <b>318</b>I; and processing node <b>312</b>D includes interface logic <b>318</b>J, <b>318</b>K, and <b>318</b>L. Processing node <b>312</b>D is coupled to communicate with a plurality of input/output devices (e.g. devices <b>320</b>A-<b>320</b>B in a daisy chain configuration) via interface logic <b>318</b>L. Other processing nodes may communicate with other I/O devices in a similar fashion.</p><p>Processing nodes <b>312</b>A-<b>312</b>D implement a packet-based link for inter-processing node communication. In the present embodiment, the link is implemented as sets of unidirectional lines (e.g. lines <b>324</b>A are used to transmit packets from processing node <b>312</b>A to processing node <b>312</b>B and lines <b>324</b>B are used to transmit packets from processing node <b>312</b>B to processing node <b>312</b>A). Other sets of lines <b>324</b>C-<b>324</b>H are used to transmit packets between other processing nodes as illustrated in FIG. <b>11</b>. Generally, each set of lines <b>324</b> may include one or more data lines, one or more clock lines corresponding to the data lines, and one or more control lines indicating the type of packet being conveyed. The link may be operated in a cache coherent fashion for communication between processing nodes or in a noncoherent fashion for communication between a processing node and an I/O device (or a bus bridge to an I/O bus of conventional construction such as the PCI bus or ISA bus). Furthermore, the link may be operated in a non-coherent fashion using a daisy-chain structure between I/O devices as shown. It is noted that a packet to be transmitted from one processing node to another may pass through one or more intermediate nodes. For example, a packet transmitted by processing node <b>312</b>A to processing node <b>312</b>D may pass through either processing node <b>312</b>B or processing node <b>312</b>C as shown in FIG. <b>11</b>. Any suitable routing algorithm may be used. Other embodiments of computer system <b>300</b> may include more or fewer processing nodes then the embodiment shown in FIG. <b>11</b>.</p><p>Generally, the packets may be transmitted as one or more bit times on the lines <b>324</b> between nodes. A bit time may be the rising or falling edge of the clock signal on the corresponding clock lines. The packets may include command packets for initiating transactions, probe packets for maintaining cache coherency, and response packets from responding to probes and commands.</p><p>Processing nodes <b>312</b>A-<b>312</b>D, in addition to a memory controller and interface logic, may include one or more processors. Broadly speaking, a processing node comprises at least one processor and may optionally include a memory controller for communicating with a memory and other logic as desired. More particularly, a processing node <b>312</b>A-<b>312</b>D may comprise processor <b>10</b>. External interface unit <b>46</b> may includes the interface logic <b>318</b> within the node, as well as the memory controller <b>316</b>.</p><p>Memories <b>314</b>A-<b>314</b>D may comprise any suitable memory devices. For example, a memory <b>314</b>A-<b>314</b>D may comprise one or more RAMBUS DRAMs (RDRAMs), synchronous DRAMs (SDRAMs), static RAM, etc. The address space of computer system <b>300</b> is divided among memories <b>314</b>A-<b>314</b>D. Each processing node <b>312</b>A-<b>312</b>D may include a memory map used to determine which addresses are mapped to which memories <b>314</b>A-<b>314</b>D, and hence to which processing node <b>312</b>A-<b>312</b>D a memory request for a particular address should be routed. In one embodiment, the coherency point for an address within computer system <b>300</b> is the memory controller <b>316</b>A-<b>316</b>D coupled to the memory storing bytes corresponding to the address. In other words, the memory controller <b>316</b>A-<b>316</b>D is responsible for ensuring that each memory access to the corresponding memory <b>314</b>A-<b>314</b>D occurs in a cache coherent fashion. Memory controllers <b>316</b>A-<b>316</b>D may comprise control circuitry for interfacing to memories <b>314</b>A-<b>314</b>D. Additionally, memory controllers <b>316</b>A-<b>316</b>D may include request queues for queuing memory requests.</p><p>Generally, interface logic <b>318</b>A-<b>318</b>L may comprise a variety of buffers for receiving packets from the link and for buffering packets to be transmitted upon the link. Computer system <b>300</b> may employ any suitable flow control mechanism for transmitting packets. For example, in one embodiment, each interface logic <b>318</b> stores a count of the number of each type of buffer within the receiver at the other end of the link to which that interface logic is connected. The interface logic does not transmit a packet unless the receiving interface logic has a free buffer to store the packet. As a receiving buffer is freed by routing a packet onward, the receiving interface logic transmits a message to the sending interface logic to indicate that the buffer has been freed. Such a mechanism may be referred to as a \u201ccoupon-based\u201d system.</p><p>I/O devices <b>320</b>A-<b>320</b>B may be any suitable I/O devices. For example, I/P devices <b>320</b>A-<b>320</b>B may include network interface cards, video accelerators, audio cards, hard or floppy disk drives or drive controllers, SCSI (Small Computer Systems Interface) adapters and telephony cards, modems, sound cards, and a variety of data acquisition cards such as GPIB or field bus interface cards.</p><p>Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "James B.", "last_name": "Keller", "name": ""}, {"first_name": "Keith R.", "last_name": "Schakel", "name": ""}, {"first_name": "Puneet", "last_name": "Sharma", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "ADVANCED MICRO DEVICES, INC."}, {"first_name": "", "last_name": "GLOBALFOUNDRIES U.S. INC.", "name": ""}, {"first_name": "", "last_name": "GLOBALFOUNDRIES INC.", "name": ""}, {"first_name": "", "last_name": "WILMINGTON TRUST, NATIONAL ASSOCIATION", "name": ""}, {"first_name": "", "last_name": "GLOBALFOUNDRIES INC.", "name": ""}, {"first_name": "", "last_name": "ADVANCED MICRO DEVICES, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMUS"}], "national_classes": [{"primary": true, "label": "711128"}, {"primary": false, "label": "711137"}, {"primary": false, "label": "711E12018"}, {"primary": false, "label": "711156"}, {"primary": false, "label": "711125"}], "ecla_classes": [{"label": "S06F212:6082"}, {"label": "G06F  12/08B10"}], "cpc_classes": [{"label": "G06F   9/3832"}, {"label": "G06F  12/0864"}, {"label": "G06F   9/3857"}, {"label": "G06F   9/384"}, {"label": "G06F2212/6082"}, {"label": "G06F   9/3838"}, {"label": "G06F   9/3824"}, {"label": "G06F   9/30149"}, {"label": "G06F   9/3836"}, {"label": "G06F  12/08"}, {"label": "Y02D  10/00"}, {"label": "G06F2212/6082"}, {"label": "G06F   9/30149"}, {"label": "G06F   9/3857"}, {"label": "G06F   9/3832"}, {"label": "G06F  12/0864"}, {"label": "G06F   9/384"}, {"label": "G06F   9/3838"}, {"label": "Y02D  10/00"}, {"label": "G06F   9/3836"}, {"label": "G06F   9/3824"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "2000-01-03", "application_date": "2000-01-03", "family_members": [{"ucid": "EP-1244970-B1", "titles": [{"lang": "FR", "text": "MEMOIRE CACHE QUI FOURNIT DES ETIQUETTES PARTIELLES DE VOIES NON PREDITES POUR DIRIGER LA RECHERCHE SI LA VOIE PREDITE ECHOUE"}, {"lang": "EN", "text": "CACHE WHICH PROVIDES PARTIAL TAGS FROM NON-PREDICTED WAYS TO DIRECT SEARCH IF WAY PREDICTION MISSES"}, {"lang": "DE", "text": "CACHESPEICHER ZUM BEREITSTELLEN VON PARTIELLEN ETIKETTEN AUS NICHT-VORHERGESAGTEN WEGEN UM DIE SUCHE BEI WEGVORHERSAGEFEHLGRIFFEN ZU LEITEN"}]}, {"ucid": "WO-2001050272-A1", "titles": [{"lang": "FR", "text": "MEMOIRE CACHE QUI FOURNIT DES ETIQUETTES PARTIELLES PARTANT DE VOIES NON PREVUES JUSQU'A LA RECHERCHE DIRECTE SI LES VOIES DE PREVISION MANQUENT"}, {"lang": "EN", "text": "CACHE WHICH PROVIDES PARTIAL TAGS FROM NON-PREDICTED WAYS TO DIRECT SEARCH IF WAY PREDICTION MISSES"}]}, {"ucid": "DE-60003235-D1", "titles": [{"lang": "EN", "text": "CACHE STORAGE FOR PROVIDING PARTIAL LABELS FROM NON-PREPARED WAYS TO DIRECT SEARCH FOR MISSING OUT FORECASTING"}, {"lang": "DE", "text": "CACHESPEICHER ZUM BEREITSTELLEN VON PARTIELLEN ETIKETTEN AUS NICHT-VORHERGESAGTEN WEGEN UM DIE SUCHE BEI WEGVORHERSAGEFEHLGRIFFEN ZU LEITEN"}]}, {"ucid": "US-6687789-B1", "titles": [{"lang": "EN", "text": "Cache which provides partial tags from non-predicted ways to direct search if way prediction misses"}]}, {"ucid": "EP-1244970-A1", "titles": [{"lang": "FR", "text": "MEMOIRE CACHE QUI FOURNIT DES ETIQUETTES PARTIELLES PARTANT DE VOIES NON PREVUES JUSQU'A LA RECHERCHE DIRECTE SI LES VOIES DE PREVISION MANQUENT"}, {"lang": "EN", "text": "CACHE WHICH PROVIDES PARTIAL TAGS FROM NON-PREDICTED WAYS TO DIRECT SEARCH IF WAY PREDICTION MISSES"}, {"lang": "DE", "text": "CACHESPEICHER ZUM BEREITSTELLEN F\u00dcR DIREKTE SUCHE VON PARTIELLEN ETIKETTEN VON NICHT-VORHERGESAGTEN WEGEN BEI WEGVORHERSAGEFEHLGRIFFEN"}]}, {"ucid": "KR-100747127-B1", "titles": [{"lang": "KO", "text": "\uacbd\ub85c \uc608\uce21 \uc2e4\ud328\uc2dc \uc870\ud68c\ub97c \uc9c0\uc2dc\ud558\ub3c4\ub85d \ube44\uc608\uce21 \uacbd\ub85c\ub4e4\ub85c\ubd80\ud130\ubd80\ubd84\uc801\uc778 \ud0dc\uadf8\ub4e4\uc744 \uc81c\uacf5\ud558\ub294 \uce90\uc26c"}, {"lang": "EN", "text": "CACHE WHICH PROVIDES PARTIAL TAGS FROM NON-PREDICTED WAYS TO DIRECT SEARCH IF WAY PREDICTION MISSES"}]}, {"ucid": "KR-20020067596-A", "titles": [{"lang": "KO", "text": "\uacbd\ub85c \uc608\uce21 \uc2e4\ud328\uc2dc \uc870\ud68c\ub97c \uc9c0\uc2dc\ud558\ub3c4\ub85d \ube44\uc608\uce21 \uacbd\ub85c\ub4e4\ub85c\ubd80\ud130\ubd80\ubd84\uc801\uc778 \ud0dc\uadf8\ub4e4\uc744 \uc81c\uacf5\ud558\ub294 \uce90\uc26c"}, {"lang": "EN", "text": "CACHE WHICH PROVIDES PARTIAL TAGS FROM NON-PREDICTED WAYS TO DIRECT SEARCH IF WAY PREDICTION MISSES"}]}, {"ucid": "DE-60003235-T2", "titles": [{"lang": "EN", "text": "CACHE STORAGE FOR PROVIDING PARTIAL LABELS FROM NON-PREPARED WAYS TO DIRECT SEARCH FOR MISSING OUT FORECASTING"}, {"lang": "DE", "text": "CACHESPEICHER ZUM BEREITSTELLEN VON PARTIELLEN ETIKETTEN AUS NICHT-VORHERGESAGTEN WEGEN UM DIE SUCHE BEI WEGVORHERSAGEFEHLGRIFFEN ZU LEITEN"}]}, {"ucid": "JP-2003519835-A", "titles": [{"lang": "JA", "text": "\u30a6\u30a7\u30a4\u4e88\u6e2c\u304c\u30df\u30b9\u3057\u305f\u6642\u306b\u30b5\u30fc\u30c1\u3092\u65b9\u5411\u4ed8\u3051\u308b\u305f\u3081\u306e\u3001\u4e88\u6e2c\u3055\u308c\u306a\u3044\u30a6\u30a7\u30a4\u304b\u3089\u306e\u90e8\u5206\u7684\u306a\u30bf\u30b0\u3092\u63d0\u4f9b\u3059\u308b\u30ad\u30e3\u30c3\u30b7\u30e5"}, {"lang": "EN", "text": "Cache that provides partial tags from unpredicted ways to direct the search when way predictions are missed"}]}, {"ucid": "CN-1208726-C", "titles": [{"lang": "EN", "text": "Cache which provides partial tags from non-predicted ways to direct search if way predition misses"}, {"lang": "ZH", "text": "\u5f53\u9884\u6d4b\u8def\u7ebf\u5931\u8d25\u65f6\u4ece\u975e\u9884\u6d4b\u8def\u7ebf\u63d0\u4f9b\u90e8\u5206\u6807\u8bb0\u4ee5\u6307\u5bfc\u641c\u7d22\u7684\u9ad8\u901f\u7f13\u5b58"}]}, {"ucid": "CN-1415092-A", "titles": [{"lang": "EN", "text": "Cache which provides partial tags from non-predicted ways to direct search if way predition misses"}, {"lang": "ZH", "text": "\u5f53\u9884\u6d4b\u8def\u7ebf\u5931\u8d25\u65f6\u4ece\u975e\u9884\u6d4b\u8def\u7ebf\u63d0\u4f9b\u5c40\u90e8\u6807\u8bb0\u4ee5\u6307\u5bfc\u641c\u7d22\u7684\u9ad8\u901f\u7f13\u5b58"}]}]}