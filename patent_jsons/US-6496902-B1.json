{"patent_number": "US-6496902-B1", "publication_id": 73257867, "family_id": 22838219, "publication_date": "2002-12-17", "titles": [{"lang": "EN", "text": "Vector and scalar data cache for a vector multiprocessor"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11450710\" source=\"national office\"><p>A common scalar/vector data cache apparatus and method for a scalar/vector computer. One aspect of the present invention provides a computer system including a memory. The memory includes a plurality of sections. The computer system also includes a scalar/vector processor coupled to the memory using a plurality of separate address busses and a plurality of separate read-data busses wherein at least one of the sections of the memory is associated with each address bus and at least one of the sections of the memory is associated with each read-data bus. The processor further includes a plurality of scalar registers and a plurality of vector registers and operating on instructions which provide a reference address to a data word. The processor includes a scalar/vector cache unit that includes a cache array, and a FIFO unit that tracks (a.) an address in the cache array to which a read-data value will be placed when the read-data value is returned from the memory, and (b.) a destination code that specifies which of the scalar registers and vector registers into which the read-data value is to be loaded when the read-data value is returned from the memory.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50425035\"><p>A common scalar/vector data cache apparatus and method for a scalar/vector computer. One aspect of the present invention provides a computer system including a memory. The memory includes a plurality of sections. The computer system also includes a scalar/vector processor coupled to the memory using a plurality of separate address busses and a plurality of separate read-data busses wherein at least one of the sections of the memory is associated with each address bus and at least one of the sections of the memory is associated with each read-data bus. The processor further includes a plurality of scalar registers and a plurality of vector registers and operating on instructions which provide a reference address to a data word. The processor includes a scalar/vector cache unit that includes a cache array, and a FIFO unit that tracks (a.) an address in the cache array to which a read-data value will be placed when the read-data value is returned from the memory, and (b.) a destination code that specifies which of the scalar registers and vector registers into which the read-data value is to be loaded when the read-data value is returned from the memory.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00001\" num=\"1\"><claim-text>1. A computer system comprising:</claim-text><claim-text>a common memory, the memory having a plurality of sections; and </claim-text><claim-text>a scalar/vector processor coupled to the memory using a plurality of separate address busses and a plurality of separate read-data busses wherein at least one of the sections of the memory is associated with each address bus and at least one of the sections of the memory is associated with each read-data bus, the processor having a plurality of scalar registers and a plurality of vector registers and operating on instructions which provide a reference address to a data word, the processor comprising: </claim-text><claim-text>a scalar/vector cache unit, the cache unit including: </claim-text><claim-text>a cache array, and </claim-text><claim-text>a FIFO unit that tracks </claim-text><claim-text>an address in the cache array to which a read-data value will be placed when the read-data value is returned from the memory, and </claim-text><claim-text>a destination code that specifies which of the scalar registers and vector registers into which the read-data value is to be loaded when the read-data value is returned from the memory, </claim-text><claim-text>wherein the FIFO unit provides a plurality of FIFOs, each FIFO associated with one or more of the sections of the memory, and wherein the memory includes about eight sections, the FIFO unit includes an equal number of FIFOs, one of the FIFOs associated with each one of the sections, and each FIFO including about forty-eight positions.</claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00002\" num=\"2\"><claim-text>2. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00001\">claim 1</claim-ref>, wherein the scalar/vector processor is further coupled to the memory using one or more separate write-data busses, and wherein the write-data busses are fewer in number than the read-data busses.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00003\" num=\"3\"><claim-text>3. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00001\">claim 1</claim-ref>, wherein the cache unit includes a plurality of caches including a first cache and a second cache, and wherein a first subset of the sections is associated with the first cache and a different subset of the sections is associated with the second cache.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00004\" num=\"4\"><claim-text>4. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00001\">claim 1</claim-ref>, wherein instruction fetches are stored in the cache unit, and wherein a different amount of data is prefetched for instruction fetching than the amount of prefetch data for scalar fetches, and than the amount of prefetch data for vector fetches.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00005\" num=\"5\"><claim-text>5. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00001\">claim 1</claim-ref>, wherein the cache unit can be selectively disabled for instruction fetches and can be selectively disabled for scalar and vector data fetches.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00006\" num=\"6\"><claim-text>6. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00001\">claim 1</claim-ref>, wherein the cache unit fetches a different amount of data based on whether a read-data operation is for a scalar register or a vector register.</claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00007\" num=\"7\"><claim-text>7. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00006\">claim 6</claim-ref>, wherein the cache unit includes a plurality of caches including a first cache and a second cache, and wherein a first subset of the sections is associated with the first cache and a different subset of the sections is associated with the second cache.</claim-text></claim>"}, {"num": 8, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00008\" num=\"8\"><claim-text>8. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00006\">claim 6</claim-ref>, wherein instruction fetches are stored in the cache unit, and wherein a different amount of data is prefetched for instruction fetching than the amount of prefetch data for scalar fetches, and than the amount of prefetch data for vector fetches.</claim-text></claim>"}, {"num": 9, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00009\" num=\"9\"><claim-text>9. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00006\">claim 6</claim-ref>, wherein the cache unit can be selectively disabled for instruction fetches and can be selectively disabled for scalar and vector data fetches.</claim-text></claim>"}, {"num": 10, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00010\" num=\"10\"><claim-text>10. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00006\">claim 6</claim-ref>, wherein the scalar/vector processor is further coupled to the memory using one or more separate write-data busses, and wherein the write-data busses are fewer in number than the read-data busses.</claim-text></claim>"}, {"num": 11, "parent": 10, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00011\" num=\"11\"><claim-text>11. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00010\">claim 10</claim-ref>, wherein the cache unit includes a plurality of caches including a first cache and a second cache, and wherein a first subset of the sections is associated with the first cache and a different subset of the sections is associated with the second cache.</claim-text></claim>"}, {"num": 12, "parent": 10, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00012\" num=\"12\"><claim-text>12. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00010\">claim 10</claim-ref>, wherein instruction fetches are stored in the cache unit, and wherein a different amount of data is prefetched for instruction fetching than the amount of prefetch data for scalar fetches, and than the amount of prefetch data for vector fetches.</claim-text></claim>"}, {"num": 13, "parent": 10, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00013\" num=\"13\"><claim-text>13. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00010\">claim 10</claim-ref>, wherein the cache unit can be selectively disabled for instruction fetches and can be selectively disabled for scalar and vector data fetches.</claim-text></claim>"}, {"num": 14, "parent": 10, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00014\" num=\"14\"><claim-text>14. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00010\">claim 10</claim-ref>, wherein the cache unit includes a plurality of caches including a first cache and a second cache, and wherein a first subset of the sections is associated with the first cache and a different subset of the sections is associated with the second cache, wherein instruction fetches are stored in the cache unit, wherein a different amount of data is prefetched for instruction fetching than the amount of prefetch data for scalar fetches, and than the amount of prefetch data for vector fetches, and wherein the cache unit can be selectively disabled for instruction fetches and can be selectively disabled for scalar and vector data fetches.</claim-text></claim>"}, {"num": 15, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00015\" num=\"15\"><claim-text>15. A method for caching data in a computer system, the system having a common memory having a plurality of sections, and a scalar/vector processor coupled to the memory using a plurality of separate address busses and a plurality of separate read-data busses wherein at least one of the sections of the memory is associated with each address bus and at least one of the sections of the memory is associated with each read-data bus, the processor having a plurality of scalar registers and a plurality of vector registers and operating on instructions which provide a reference address to a data word, the processor having a scalar/vector cache unit, the cache unit including a cache array; the method comprising:</claim-text><claim-text>dividing read requests into groups of requests based on which section each read request is directed towards; </claim-text><claim-text>transmitting a series of addresses on each of the plurality of address busses requesting that a plurality read-data values be placed on each of the plurality of read-data busses; </claim-text><claim-text>for each address on each address bus, tracking both </claim-text><claim-text>an address in the cache array to which a read-data value will be placed when the read-data value is returned from the memory, and </claim-text><claim-text>a destination code that specifies which of the scalar registers and vector registers into which the read-data value is to be loaded when the read-data value is returned from the memory, and </claim-text><claim-text>where in the memory includes about eight sections, one of the group s of requests is associated with each one of the sections, and each group including up to about forty-eight requests.</claim-text></claim>"}, {"num": 16, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00016\" num=\"16\"><claim-text>16. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00015\">claim 15</claim-ref>, further comprising:</claim-text><claim-text>dividing read requests into groups of requests based on which section each read request is directed towards, and </claim-text><claim-text>wherein the step of tracking further comprises</claim-text><claim-text>separately tracking each of the groups of requests. </claim-text></claim>"}, {"num": 17, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00017\" num=\"17\"><claim-text>17. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00015\">claim 15</claim-ref>, wherein the scalar/vector processor is further coupled to the memory using one or more separate write-data busses, and wherein the write-data busses are fewer in number than the read-data busses, wherein the step of transmitting addresses includes transmitting a read request or a write request on each address bus, and wherein the number of write requests which can be transmitted in a given period of time is fewer than the number of read requests.</claim-text></claim>"}, {"num": 18, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00018\" num=\"18\"><claim-text>18. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00015\">claim 15</claim-ref>, wherein the cache unit includes a plurality of caches including a first cache and a second cache, further comprising:</claim-text><claim-text>associating a first subset of the sections with the first cache and a different subset of the sections with the second cache. </claim-text></claim>"}, {"num": 19, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00019\" num=\"19\"><claim-text>19. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00015\">claim 15</claim-ref>, further comprising:</claim-text><claim-text>caching instruction fetches, wherein the amount of prefetch data for instruction fetching differs from the amount of prefetch data for scalar fetches and differs from the amount of prefetch data for vector fetches. </claim-text></claim>"}, {"num": 20, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00020\" num=\"20\"><claim-text>20. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00015\">claim 15</claim-ref>, further comprising:</claim-text><claim-text>selectively disabling caching of instruction fetches; and </claim-text><claim-text>selectively disabling caching of scalar and vector data fetches. </claim-text></claim>"}, {"num": 21, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00021\" num=\"21\"><claim-text>21. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00015\">claim 15</claim-ref>, further comprising:</claim-text><claim-text>fetching a different amount of data based on whether a read-data operation is for a scalar register or a vector register. </claim-text></claim>"}, {"num": 22, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00022\" num=\"22\"><claim-text>22. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00021\">claim 21</claim-ref>, further comprising:</claim-text><claim-text>dividing read requests into groups of requests based on which section each read request is directed towards, and </claim-text><claim-text>wherein the step of tracking further comprises</claim-text><claim-text>separately tracking each of the groups of requests. </claim-text></claim>"}, {"num": 23, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00023\" num=\"23\"><claim-text>23. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00021\">claim 21</claim-ref>, wherein the scalar/vector processor is further coupled to the memory using one or more separate write-data busses, and wherein the write-data busses are fewer in number than the read-data busses, wherein the step of transmitting addresses includes transmitting a read request or a write request on each address bus, and wherein the number of write requests which can be transmitted in a given period of time is fewer than the number of read requests.</claim-text></claim>"}, {"num": 24, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00024\" num=\"24\"><claim-text>24. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00021\">claim 21</claim-ref>, wherein the cache unit includes a plurality of caches including a first cache and a second cache, further comprising:</claim-text><claim-text>associating a first subset of the sections with the first cache and a different subset of the sections with the second cache. </claim-text></claim>"}, {"num": 25, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00025\" num=\"25\"><claim-text>25. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00021\">claim 21</claim-ref>, further comprising:</claim-text><claim-text>caching instruction fetches, wherein the amount of prefetch data for instruction fetching differs from the amount of prefetch data for scalar fetches and differs from the amount of prefetch data for vector fetches. </claim-text></claim>"}, {"num": 26, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00026\" num=\"26\"><claim-text>26. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00021\">claim 21</claim-ref>, further comprising:</claim-text><claim-text>selectively disabling caching of instruction fetches; and </claim-text><claim-text>selectively disabling caching of scalar and vector data fetches. </claim-text></claim>"}, {"num": 27, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00027\" num=\"27\"><claim-text>27. A computer system comprising:</claim-text><claim-text>a common memory, the memory having a plurality of sections; and </claim-text><claim-text>a plurality of scalar/vector processors coupled to the memory using a plurality of separate address busses and a plurality of separate read-data busses wherein at least one of the sections of the memory is associated with each address bus and at least one of the sections of the memory is associated with each read-data bus, the processor having a plurality of scalar registers and a plurality of vector registers and operating on instructions which provide a reference address to a data word, each processor comprising: </claim-text><claim-text>a scalar/vector cache unit, the cache unit including: </claim-text><claim-text>a cache array that caches data for vector fetches, scalar fetches, wherein the cache unit fetches a different amount of data into cache array based on whether a read-data operation is for a scalar register or a vector register, </claim-text><claim-text>a FIFO unit that tracks an address in the cache array to which a read-data value will be placed when the read-data value is returned from the memory, wherein the memory includes a plurality of sections, the FIFO unit includes a number of FIFOs equal to the number of sections, and one of the FIFOs is associated with each one of the sections, </claim-text><claim-text>wherein the memory includes about eight sections and each FIFO of each FIFO unit includes about forty-eight positions.</claim-text></claim>"}, {"num": 28, "parent": 27, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00028\" num=\"28\"><claim-text>28. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00027\">claim 27</claim-ref>, wherein the scalar/vector processor is further coupled to the memory using one or more separate write-data busses, and wherein the write-data busses are fewer in number than the read-data busses.</claim-text></claim>"}, {"num": 29, "parent": 27, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00029\" num=\"29\"><claim-text>29. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00027\">claim 27</claim-ref>, wherein the cache unit includes a plurality of caches including a first cache and a second cache, and wherein a first subset of the sections is associated with the first cache and a different subset of the sections is associated with the second cache.</claim-text></claim>"}, {"num": 30, "parent": 27, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00030\" num=\"30\"><claim-text>30. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00027\">claim 27</claim-ref>, wherein instruction fetches are stored in the cache unit, and wherein a different amount of data is prefetched for instruction fetching than the amount of prefetch data for scalar fetches, and than the amount of prefetch data for vector fetches.</claim-text></claim>"}, {"num": 31, "parent": 27, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00031\" num=\"31\"><claim-text>31. The computer system according to <claim-ref idref=\"US-6496902-B1-CLM-00027\">claim 27</claim-ref>, wherein the cache unit can be selectively disabled for instruction fetches and can be selectively disabled for scalar and vector data fetches.</claim-text></claim>"}, {"num": 32, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00032\" num=\"32\"><claim-text>32. A method for caching data in a computer system, the system having a common memory having a plurality of sections, and a plurality of scalar/vector processors each coupled to the memory using a plurality of separate address busses and a plurality of separate read-data busses wherein at least one of the sections of the memory is associated with each address bus and at least one of the sections of the memory is associated with each read-data bus, each processor having a plurality of scalar registers and a plurality of vector registers and operating on instructions which provide a reference address to a data word, each processor having a scalar/vector cache unit, the cache unit including a cache array; the method comprising:</claim-text><claim-text>dividing read requests into groups of requests based on which section each read request is directed towards; </claim-text><claim-text>from each of the plurality of processors, transmitting a series of addresses on each of the plurality of address busses requesting that a plurality read-data values be placed on each of the plurality of read-data busses; </claim-text><claim-text>for each address on each address bus, tracking both </claim-text><claim-text>an address in the cache array to which a read-data value will be placed when the read-data value is returned from the memory, and </claim-text><claim-text>a destination code that specifies which processor and which of the scalar registers and vector registers in that processor into which the read-data value is to be loaded when the read-data value is returned from the memory, and </claim-text><claim-text>wherein the memory includes about eight sections, one of the groups of requests is associated with each one of the sections, and each group including up to about forty-eight requests.</claim-text></claim>"}, {"num": 33, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00033\" num=\"33\"><claim-text>33. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00032\">claim 32</claim-ref>, further comprising:</claim-text><claim-text>fetching a different amount of data based on whether a read-data operation is for a scalar register or a vector register. </claim-text></claim>"}, {"num": 34, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00034\" num=\"34\"><claim-text>34. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00032\">claim 32</claim-ref>, further comprising:</claim-text><claim-text>dividing read requests into groups of requests based on which section each read request is directed towards, and </claim-text><claim-text>wherein the step of tracking further comprises</claim-text><claim-text>separately tracking each of the groups of requests. </claim-text></claim>"}, {"num": 35, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00035\" num=\"35\"><claim-text>35. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00032\">claim 32</claim-ref>, wherein the scalar/vector processor is further coupled to the memory using one or more separate write-data busses, and wherein the write-data busses are fewer in number than the read-data busses, wherein the step of transmitting addresses includes transmitting a read request or a write request on each address bus, and wherein the number of write requests which can be transmitted in a given period of time is fewer than the number of read requests.</claim-text></claim>"}, {"num": 36, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00036\" num=\"36\"><claim-text>36. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00032\">claim 32</claim-ref>, wherein the cache unit includes a plurality of caches including a first cache and a second cache, further comprising:</claim-text><claim-text>associating a first subset of the sections with the first cache and a different subset of the sections with the second cache. </claim-text></claim>"}, {"num": 37, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00037\" num=\"37\"><claim-text>37. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00032\">claim 32</claim-ref>, further comprising:</claim-text><claim-text>caching instruction fetches, wherein the amount of prefetch data for instruction fetching differs from the amount of prefetch data for scalar fetches and differs from the amount of prefetch data for vector fetches. </claim-text></claim>"}, {"num": 38, "parent": 32, "type": "dependent", "paragraph_markup": "<claim id=\"US-6496902-B1-CLM-00038\" num=\"38\"><claim-text>38. The method according to <claim-ref idref=\"US-6496902-B1-CLM-00032\">claim 32</claim-ref>, further comprising:</claim-text><claim-text>selectively disabling caching of instruction fetches; and</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53684561\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>FIELD OF THE INVENTION</h4><p>The present invention relates to cache memories for high-speed computers and more specifically to cache memories for vector and scalar data in a computer having vector/scalar processors.</p><h4>BACKGROUND OF THE INVENTION</h4><p>A high-speed computer needs fast access to data in memory. The largest and fastest of such computers are known as supercomputers. One method of speeding up a computer is by \u201cpipelining,\u201d wherein the computer's digital logic between an input and an output is divided into several serially connected successive stages. Data are fed into the computer's input stage before data previously input are completely processed through the computer's output stage. There are typically many intermediate stages between the input stage and the output stage. Each stage performs a portion of the overall function desired, adding to the functions performed by previous stages. Thus, multiple pieces of data are in various successive stages of processing at each successive stage of the pipeline between the input and output stages. Preferably, each successive system clock propagates the data one stage further in the pipeline.</p><p>As a result of pipelining, the system clock can operate at a faster rate than the speed of system clocks of non-pipelined machines. In some of today's computers, the system clock cycles in as fast as one nanoseconds (\u201cns\u201d) or less, allowing up to billion operations per second or more though a single functional unit. Parallel functional units within each processor, and parallel processors within a single system, allow even greater throughput. Achieving high-performance throughputs is only possible, however, if data are fed into each pipeline at close to the system clock rate.</p><p>As processor speeds have increased, the size of memory in a typical computer has also increased drastically. In addition, error-correction circuitry is now placed in the memory path to increase reliability. Memory-access speeds have improved over time, but the increased size of memory and the complexity of error-correction circuitry have meant that memory-access time has remained approximately constant. For example, a typical supercomputer system clock rate may have improved from roughly 8 ns to 4 ns to 2 ns to 1 ns over four generations. Over the same time period, memory-access times may have remained at approximately 60 to 100 ns. These times mean that with a 96 ns memory, the 8-ns processor accesses memory in 12 clocks, the 4-ns processor in 24 clocks, and the 2-ns processor in 48 clocks. As a result, a computer which randomly accessed data throughout memory would see almost no overall data-processing-speed improvement even if the system clock rate is increased dramatically.</p><p>One solution has been to organize data into vectors, each including a plurality of data elements, and where, during processing, each element of a vector has similar operations performed on it. Computer designers schedule various portions of the memory to simultaneously fetch various elements of a vector, and these fetched elements are fed into one or more parallel pipelines on successive clock cycles. Within a processor, the vector is held in a vector register having a plurality of vector register elements. Each successive vector-register element holds a successive element of the vector. A \u201cvector-load\u201d operation transfers a vector from memory into a vector register. For example, a vector in memory may be held as a vector image wherein successive elements of the vector are held in successive locations in memory. A vector-load operation moves elements which include a vector into pipelines which couple memory to the vector registers. Overlapped with these vector-load operations, there could be two other pipelines taking data from two other vector registers to feed a vector processor, with the resultant vector fed through a pipeline into a third vector register. Examples of such designs are described in U.S. Pat. No. 4,661,900 issued Apr. 28, 1987 to Chen et al. and U.S. Pat. No. 5,349,667 issued Sept. 20, 1994 to Cray et al., which are hereby incorporated by reference. For example, in a well-tuned system using 2-ns pipeline clocks, the throughput can approach 500 million operations per second for a single vector processor, even with relatively slow memory-access times.</p><p>On the other hand, a scalar processor operating in such a system on somewhat randomly located data must deal with a 48-clock to 70-clock pipelined-memory access time, and must often wait for the results from one operation before determining which data to request next.</p><p>In very-high-speed vector processors, such as the Cray Y-MP C90 manufactured by Cray Research Inc., the assignee of the present invention, a computer system contains a number of central processing units (\u201cCPUs\u201d), each of which may have more than one vector processor and more than one scalar processor. The computer system also contains a number of common memories which store the programs and data used by the CPUs. Vector data are often streamed or pipelined into a CPU from the memories, and so a long access time may be compensated for by receiving many elements on successive cycles as the result of a single request. In contrast, scalar data read by one of the CPUs from one of the common memories may take an inordinate amount of time to access.</p><p>A cache is a relatively fast small storage area inserted between a relatively slow bulk memory and a CPU to improve the average access time for loads and/or stores. Caches are filled with data which, it is predicted, will be accessed more frequently than other data. Accesses from the cache are typically much faster than accesses from the common memories. A \u201ccache hit\u201d is when requested data are found in the data already in the cache. A \u201ccache miss\u201d is when requested data cannot be found in the data already in the cache, and must therefore be accessed more slowly from the common memories. A \u201ccache-hit ratio\u201d is the ratio of requests which result in cache hits divided by the total of cache hits and cache misses. A system or program which has a high cache-hit ratio will usually have better performance than a machine without cache. On the other hand, a poor cache-hit ratio may result in much poorer performance, since much of the memory bandwidth is used up fetching data into the cache which will never be used.</p><h4>SUMMARY OF THE INVENTION</h4><p>A method and apparatus for a common scalar/vector data cache apparatus for a scalar/vector computer.</p><p>One aspect of the present invention provides a computer system. The computer system includes a common memory. The memory includes a plurality of sections. The computer system also includes a scalar/vector processor coupled to the memory using a plurality of separate address busses and a plurality of separate read-data busses wherein at least one of the sections of the memory is associated with each address bus and at least one of the sections of the memory is associated with each read-data bus. The processor further includes a plurality of scalar registers and a plurality of vector registers and operating on instructions which provide a reference address to a data word. The processor includes a scalar/vector cache unit that includes a cache array, and a FIFO unit that tracks (a.) an address in the cache array to which a read-data value will be placed when the read-data value is returned from the memory, and (b.) a destination code that specifies which of the scalar registers and vector registers into which the read-data value is to be loaded when the read-data value is returned from the memory.</p><p>In some embodiments, fetched instructions are also passed through the cache. In some such embodiment, the system allows instruction fetching through the cache to be selectably disabled. In some embodiments the system allows data fetching (i.e., both scalar fetching and vector fetching) through the cache to be selectably disabled. In some embodiments, the selective enabling/disabling of fetches through the cache of instructions and data are separately and independently specified.</p><p>In one embodiment, the cache unit fetches a different amount of data based on whether a read-data operation is for a scalar registers or a vector register. In another embodiment, the FIFO unit provides a plurality of FIFOs, each FIFO associated with one or more of the sections of the memory. In one such embodiment, the memory includes about eight sections, the FIFO unit includes an equal number of FIFOs, one of the FIFOs associated with each one of the sections, and each FIFO including about forty-eight positions.</p><p>In another embodiment, the scalar/vector processor is further coupled to the memory using one or more separate write-data busses, and wherein the write-data busses are fewer in number than the read-data busses.</p><p>In yet another embodiment, the cache unit includes a plurality of caches including a first cache and a second cache, and wherein a first subset of the sections is associated with the first cache and a different subset of the sections is associated with the second cache.</p><p>Another aspect of the present invention provides a method for caching data in a computer system such as that described above. In one embodiment, the method includes transmitting a series of addresses on each of the plurality of address busses requesting that a plurality read-data values be placed on each of the plurality of read-data busses, and for each address on each address bus, tracking both (a.) an address in the cache array to which a read-data value will be placed when the read-data value is returned from the memory, and (b.) a destination code that specifies which of the scalar registers and vector registers into which the read-data value is to be loaded when the read-data value is returned from the memory.</p><p>In one embodiment, the method further includes fetching a different amount of data based on whether a read-data operation is for a scalar register or a vector register. In another embodiment, the method further includes dividing read requests into groups of requests based on which section each read request is directed towards, and the step of tracking further includes separately tracking each of the groups of requests. In one such embodiment, the memory includes about eight sections, one of the groups associated with each one of the sections, and each group including up to about forty-eight requests.</p><p>In one embodiment if the method, the step of transmitting addresses includes transmitting a read request or a write request on each address bus, and wherein the number of write requests which can be transmitted in a given period of time is fewer than the number of read requests.</p><p>In another embodiment, the cache unit includes a plurality of caches including a first cache and a second cache and the method further includes associating a first subset of the sections with the first cache and a different subset of the sections with the second cache.</p><p>Thus the present invention provides a scalar/vector cache that can transmit a series of requests on each of a plurality of busses, each bus connected to a separate section of memory. The address of position in the cache, as well as the destination register for each data value is tracked, for example in a FIFO, such that a plurality of requests can be outstanding at any one time. Different parameters can be used for prefetching based on whether the request is for a scalar register or a vector register, thus optimizing the amount of prefetching done.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>FIG. 1 is a schematic diagram of a scalar/vector computer system <b>10</b> constructed according to the present invention.</p><p>FIG. 2 is a schematic diagram illustrating details of an embodiment of scalar/vector data cache.</p><p>FIG. 3 shows a representation of a portion of multi-processor system <b>20</b>.</p><p>FIG. 4 shows a representation of a multi-processor system <b>20</b> on a card <b>420</b>.</p><p>FIG. 5 shows a representation of a scalar/vector computer system <b>10</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF THE PREFERRED EMBODIMENT</h4><p>In the following detailed description of the preferred embodiments, reference is made to the accompanying drawings which form a part hereof, and in which are shown by way of illustration specific embodiments in which the invention may be practiced. It is to be understood that other embodiments may be utilized and structural changes may be made without departing from the scope of the present invention.</p><p>FIG. 1 is a schematic diagram illustrating an embodiment of a scalar/vector supercomputer <b>10</b> constructed according to the present invention. Scalar/vector supercomputer <b>10</b> includes one or more scalar/vector processor CPUs <b>100</b> connected through a common-memory interface <b>112</b> to one or more sections <b>6020</b> of common memories (labeled <b>602</b>.<b>1</b> through <b>602</b>.M), collectively referred to as common memories <b>602</b>. \u201cMemory\u201d for main memory <b>602</b> in various embodiments includes main memories, such as those implemented with dynamic random access memories (DRAMs), static random access memories (SRAMs) and video random access memories (VRAMs). In one embodiment, system <b>10</b> also includes mass-storage devices <b>601</b>, for example hard magnetic disks, optical disks, and magnetic tape units, each configured to transfer data to and from common memories <b>602</b>. In one embodiment, CPU <b>100</b> is one of a plurality of substantially similar CPUs <b>100</b>, each connected to common memories <b>602</b>. In one such embodiment, common-memory interface <b>112</b> to common memories <b>602</b> includes a plurality of address ports, and a plurality of write-data-out ports and read-data-in ports, each data port being 64 data bits wide, and each address port being 32 bits wide. In one such embodiment, (See FIG. 2) each CPU <b>100</b> includes four 64-bit-wide read-data-in ports <b>121</b> (two coupled to cache-<b>0</b><b>120</b>, and two coupled to cache-<b>1</b><b>130</b>), two 64-bit-wide write-data-out ports <b>122</b> (one coupled to cache-<b>0</b><b>120</b>, and one coupled to cache-<b>1</b><b>130</b>), and four 32-bit-wide address ports <b>123</b> (two coupled to cache-<b>0</b><b>120</b>, and two coupled to cache-<b>1</b><b>130</b>). In one such embodiment, one read-data-in port <b>121</b> to cache-<b>0</b><b>120</b> is connected from sections <b>0</b> and <b>2</b> of common memory <b>602</b>, another read-data-in port <b>121</b> to cache-<b>0</b><b>120</b> is connected from sections <b>5</b> and <b>7</b> of common memory <b>602</b>, and the write-data-out port <b>122</b> from cache-<b>0</b><b>120</b> is connected to sections <b>0</b>, <b>2</b>, <b>5</b> and <b>7</b> of common memory <b>602</b>; one read-data-in port <b>121</b> to cache-<b>1</b><b>120</b> is connected from sections <b>1</b> and <b>3</b> of common memory <b>602</b>, another read-data-in port <b>121</b> to cache-<b>1</b><b>120</b> is connected from sections <b>4</b> and <b>6</b> of common memory <b>602</b>, and the write-data-out port <b>122</b> from cache-<b>0</b><b>120</b> is connected to sections <b>1</b>, <b>3</b>, <b>4</b> and <b>6</b> of common memory <b>602</b>. This facilitates acquiring vector data from frequently used strides in memory, such as strides of <b>1</b>, <b>2</b>, and <b>4</b> (a \u201cstride\u201d is the distance between consecutive elements of data for a vector read or write to memory). In another embodiment, common memories <b>602</b> includes only a single section of common memory <b>602</b>.<b>1</b>.</p><p>In one embodiment of the system shown in FIG. 1, each CPU <b>100</b> includes scalar unit <b>114</b> including S registers <b>102</b>, A registers <b>103</b>, T registers <b>104</b>, B registers <b>105</b>, instruction buffers <b>107</b>, scalar processor <b>125</b>, and address processor <b>135</b>; and vector unit <b>116</b> including V registers <b>160</b> and vector processor <b>165</b>. Common-memory interface <b>112</b> provides access for scalar/vector processor CPU <b>100</b> to common memories <b>602</b>.</p><p>In one embodiment, common memories <b>602</b> are also connected through common-memory interface <b>112</b> to a plurality of other scalar/vector processor CPUs <b>100</b> (for example CPU<b>1</b>, CPU<b>2</b>, and CPU<b>3</b>, each substantially similar to CPU<b>0</b><b>100</b>), forming a multi-processor system <b>20</b>. In one embodiment, common memory interface <b>112</b> is comprised of a plurality of interconnected VA/VB interfaces <b>113</b>, one for each CPU <b>100</b>. In one embodiment, four CPUs <b>100</b> form a multi-processor system <b>20</b> implemented on a single multiprocessor card <b>420</b> having a common memory interface <b>112</b> at a backplane connector <b>1120</b> (see FIG. <b>3</b> and <b>4</b>). In the embodiment, an approximately one-eighth-portion of which is shown in FIG. 3, a four-CPU multi-processor system <b>20</b> includes four vector-scalar processors <b>110</b> (each substantially implemented on a single chip, i.e., PV<b>0</b>, PV<b>1</b>, PV<b>2</b> and PV<b>3</b>), wherein each vector-scalar processor <b>110</b> has two caches <b>120</b> (i.e., vector-scalar processor <b>110</b> labeled PV<b>0</b> connects to caches <b>120</b> labeled CA<b>0</b> and CA<b>1</b>, vector-scalar processor <b>110</b> labeled PV<b>1</b> connects to caches <b>120</b> labeled CA<b>2</b> and CA<b>3</b>, vector-scalar processor <b>110</b> labeled PV<b>2</b> connects to caches <b>120</b> labeled CA<b>4</b> and CAS, and vector-scalar processor <b>110</b> labeled PV<b>3</b> connects to caches <b>120</b> labeled CA<b>6</b> and CA<b>7</b>). The eight caches <b>120</b> (labeled CA<b>0</b> through CA<b>7</b>) are connected to four output-crossbar chips <b>1131</b> labeled VA<b>0</b> through VA<b>3</b>, and to four input-crossbar chips <b>1132</b> labeled VB<b>0</b> through VB<b>3</b>, wherein the four output-crossbar chips <b>1131</b> and the four input-crossbar chips <b>1132</b> together form VA/VB interface <b>113</b> (see FIG. <b>1</b>). In one such embodiment, the output-crossbar chips <b>1131</b> and the input-crossbar chips <b>1132</b> are all designed to be the same chip type, but are programmed by the way they are wired on the board and/or by data loaded into the chips to provide distinct input or output functions as desired (see FIG. <b>4</b>).</p><p>In one embodiment, cache chips CA<b>0</b><b>120</b> and CA<b>1</b><b>130</b> of cache <b>200</b> are implemented in RAM (random-access memory) blocks using IBM 6S technology. In one such embodiment, a pair of identical application-specific integrated circuit (ASIC) chips are used for each cache <b>200</b>. In one embodiment, cache <b>200</b> adds approximately six clock periods of latency to each miss (over what the design would provide without a cache), but reduces the memory latency by approximately sixty-five clock periods for read references that hit in the cache.</p><p>In one embodiment, the two cache chips are entirely independent, each providing a 4-way set associative cache for four of the eight sections <b>6020</b> of memory <b>602</b>. The even port of cache chip CA<b>0</b><b>120</b> handles the references to sections <b>0</b> and <b>2</b> of memory <b>602</b>, the odd port of cache chip CA<b>0</b><b>120</b> handles the references to sections <b>5</b> and <b>7</b> of memory <b>602</b>, the even port of cache chip CA<b>1</b><b>130</b> handles the references to sections <b>4</b> and <b>6</b> of memory <b>602</b>, and the odd port of cache chip CA<b>1</b><b>130</b> handles the references to sections <b>1</b> and <b>3</b> of memory <b>602</b>. in one embodiment, each cache chip (<b>120</b> or <b>130</b>) includes 16,384 words (16 K words) of cache, for a total of 32 K words per processor <b>110</b>, wherein each \u201cword\u201d includes 64-bits of data, and optionally 8 bits of parity or single-error correct, double-error detect (\u201cSECDED\u201d) error correction code.</p><p>FIG. 3 shows a representation of about one-eighth of a multi-processor system <b>20</b>. In the embodiment shown, vector-scalar processor <b>110</b> labeled PV<b>0</b> (about one-half of processor PV<b>0</b> is shown, as indicated by the jagged line at its bottom) is connected to two caches <b>120</b> (CA<b>0</b> shown and CA<b>1</b> not shown) wherein each cache connects via separate and different copies of one write-data bus <b>127</b> (64 bits wide in this embodiment), two address/request busses <b>128</b>, two read-data busses <b>126</b>, plus control lines <b>129</b>. This allows each processor <b>110</b> to send parallel requests to up to four different addresses on each clock cycle, providing up to 2 write requests and/or up to four read requests. As shown, two address busses, two read busses, and one write bus are connected to cache chip CA<b>0</b><b>120</b>, and similarly but not shown, two address busses, two read busses, and one write bus are connected to cache chip CA<b>1</b><b>130</b> in the same manner. In this embodiment, each address bus <b>128</b> includes 30 bits of address, two bits encoding the request, and 10 bits indicating a destination code (i.e., where read data is to be written when it returns). In this embodiment, each read-data bus <b>126</b> includes 64 bits of read data, 2 bits indicating read valid, and 10 bits of destination code. In this embodiment, cache chip CA<b>0</b><b>120</b> adds parity (one bit parity per 8-bit byte) to the write data, thus outputting a 72-bit wide write data bus <b>122</b>. In another embodiment, cache chip CA<b>0</b><b>120</b> adds error checking and correction (ECC) (8 bits ECC per 64-bit word which provides single-error correct, double-error detect (SECDED) protection) to the write data, thus outputting a 72-bit wide write data bus <b>122</b>. This provides error checking and/or correction to the memory data. Similarly, 72 bits of read data are returned on the two read-data busses <b>121</b>, and the parity (or ECC) is checked and stripped off by cache chip CA<b>0</b><b>120</b> to provide 64-bit wide read data on the busses <b>126</b>.</p><p>In one embodiment, one address bus <b>128</b> between processor PV<b>0</b><b>110</b> and cache chip CA<b>0</b><b>120</b> is dedicated to requests from memory sections <b>0</b> and <b>2</b>, and another one address bus <b>128</b> between processor PV<b>0</b><b>110</b> and cache chip CA<b>0</b><b>120</b> is dedicated to requests from memory sections <b>5</b> and <b>7</b>. Each of these address busses <b>128</b> includes 30 bits of address, two bits of request indication, and ten bits of destination code. Cache chip CA<b>0</b> then drives two address busses <b>123</b>, one dedicated to sections <b>0</b> and <b>2</b>, and another dedicated to sections <b>5</b> and <b>7</b>; wherein each bus <b>123</b> includes 29 bits of address, four bits of section indication, and eight bits of request indication. Cache chip <b>120</b> then maintains a 48-position deep first-in-first-out (\u201cFIFO\u201d) <b>520</b> (see FIG. 5 below for more description) for each of the eight memory sections which maintains information to track the cache position and destination code for every memory reference (i.e., for each read operation, FIFO <b>520</b> tracks where in the cache the read data will be placed, and which destination register to load with the read data when in returns).</p><p>In the embodiment shown, each of the four VA crossbar chips <b>1131</b> drives one quarter of the write-data bits and address bits to each of the eight sections of memory <b>602</b> (to even loading and reduce simultaneous-switch problems). That is, the write data bus <b>122</b> and the two address busses <b>123</b> from cache CA<b>0</b> chip <b>120</b> drive all four VA crossbar chips <b>1131</b>, then the VA<b>0</b> crossbar chip <b>1131</b> drives a first <b>17</b> bits (9 bits of WR data, 1 bit of control, and 7 bits of address) on a bus <b>1135</b> to memory <b>602</b> section <b>6020</b> S<b>0</b> (\u201cS<b>0</b>\u201d) while the VA<b>1</b> crossbar chip <b>1131</b> drives a second 17 bits (9 bits of WR data, 1 bit of control, and 7 bits of address) to memory <b>602</b> section <b>0</b> (\u201cS<b>0</b>\u201d), the VA<b>2</b> crossbar chip <b>1131</b> drives a third 17 bits (9 bits of write data, 1 bit of control, and 7 bits of address) to memory <b>602</b> section <b>0</b> (\u201cS<b>0</b>\u201d), and the VA<b>3</b> crossbar chip <b>1131</b> drives a fourth 17 bits (9 bits of WR data, 1 bit of control, and 7 bits of address) to memory <b>602</b> section <b>0</b> (\u201cS<b>0</b>\u201d), for a total of 68 bits (36 bits WR DATA, 4 bits control, and 28 bits address). Similarly, the four VA crossbar chips <b>1131</b> provide separate parallel busses <b>1135</b> to each of the other seven sections of memory <b>602</b> (S<b>1</b> through S<b>7</b>). Thus, a total of thirty-two busses <b>1135</b>, eight from each of four VA chips <b>1131</b>, are passed to memory <b>602</b>.</p><p>In one embodiment, the sections of memory are configured such that consecutive words of data (i.e., words that are stored in consecutive addresses in the address space of system <b>10</b>) are placed in separate sections <b>6020</b> of memory <b>602</b>. In a system having eight sections of memory, for example, words from addresses <b>0</b>, <b>8</b>, <b>16</b>, etc are placed in section <b>0</b>, words from addresses <b>1</b>, <b>9</b>, <b>17</b>, etc are placed in section <b>1</b>, etc.</p><p>In one embodiment, the address busses <b>128</b> from each processor <b>110</b> are clocked at 300 MHz (one request every 3.33 nanoseconds on each of four address busses from each processor <b>110</b> to its cache chips <b>120</b>). In one such embodiment, each processor <b>110</b> will make requests to a given section <b>6020</b> (for example, section S<b>0</b>) of memory <b>602</b> at most on every other clock cycle. Since each bus handles requests for two sections, the requests can be interleaved such that each clock has a request, for example, successive clocks on one of the address busses <b>128</b> would have requests to sections S<b>0</b>, S<b>2</b>, S<b>0</b>, S<b>2</b>, S<b>0</b>, etc. The two cache chips <b>120</b> of each processor <b>100</b> drive their four address busses <b>123</b> at 300 MHz to the VA chips <b>1131</b> (one request every 3.33 nanoseconds on each of four address busses from each pair of cache chips <b>120</b> to the system's four VA chips <b>1131</b>). The four VA chips of system <b>10</b> then drive the memory <b>602</b> at 100 MHz (one request every ten nanoseconds on each of eight address busses from the system's four VA chips <b>1131</b> to memory <b>602</b>).</p><p>In this embodiment, the four VA chips of system <b>10</b> are accepting requests from each of four processors <b>100</b>, and providing a small amount of buffering for these requests. When this buffering is full, handshaking between the VA chips <b>1131</b> and the cache chips <b>120</b> prevent further requests until space is again available (i.e., further requests are held off. Similarly, handshaking between the cache chips <b>120</b> and each processor <b>110</b> will hold off further requests from the processor <b>110</b> if the cache chips <b>120</b> cannot accept the requests and pass them to the VA chips.</p><p>In the embodiment shown, each of the four VB crossbar chips <b>1132</b> receives one quarter of the read-data bits and control bits from each of the eight sections of memory <b>602</b> (to even loading and reduce simultaneous-switch problems). That is, crossbar chip VB<b>0</b><b>1132</b> receives a first 3 bits of control and 18 bits of read data from memory <b>602</b> section S<b>0</b>, chip VB<b>1</b> receives a second 3 bits of control and 18 bits of read data from memory <b>602</b> section S<b>0</b>, chip VB<b>2</b> receives a third 3 bits of control and 18 bits of read data from memory <b>602</b> section S<b>0</b>, and chip VB<b>3</b> receives a fourth 3 bits of control and 18 bits of read data from memory <b>602</b> section S<b>0</b>, for a total of 12 bits of control and 72 bits of data from section S<b>0</b> of memory <b>602</b> into the four VB chips <b>1132</b>. Thus, crossbar chip VB<b>0</b> receives 3 bits of control and 18 bits of read data from each of eight sections (S<b>0</b> through S<b>7</b>) of memory <b>602</b>; crossbar chip VB<b>1</b> receives 3 bits of control and 18 bits of read data from each of eight sections (S<b>0</b> through S<b>7</b>) of memory <b>602</b>; crossbar chip VB<b>2</b> receives 3 bits of control and 18 bits of read data from each of eight sections (S<b>0</b> through S<b>7</b>) of memory <b>602</b>; and crossbar chip VB<b>3</b> receives 3 bits of control and 18 bits of read data from each of eight sections (S<b>0</b> through S<b>7</b>) of memory <b>602</b>. The four VB chips <b>1132</b> then drive a first read-data bus <b>121</b> to cache chip CA<b>0</b><b>120</b> for sections <b>0</b> and <b>2</b> (this bus having 72 bits of read data (with parity or ECC), plus a read-valid bit for section S<b>0</b> and a read-valid bit for section <b>2</b>), and a similar second read-data bus <b>121</b> to cache chip CA<b>0</b><b>120</b> for sections <b>5</b> and <b>7</b> of memory <b>602</b>. The four VB chips <b>1132</b> also drive a first read-data bus <b>121</b> to cache chip CA<b>1</b><b>130</b> for sections <b>1</b> and <b>3</b> and a similar second read-data bus <b>121</b> to cache chip CA<b>1</b><b>130</b> for sections <b>4</b> and <b>6</b> of memory <b>602</b>.</p><p>Since, in the embodiment shown in FIG. 3, each of the eight sections of memory is provided a separate address bus, read-data bus and write data bus, up to eight independent operations to unrelated addresses can be performed substantially simultaneously.</p><p>FIG. 4 shows a representation of a multi-processor system <b>20</b> implemented on a single multiprocessor card <b>420</b> having a common memory interface <b>112</b> at a backplane connector <b>1120</b>. In the embodiment shown, four CPUs <b>100</b> are implemented, each having a vector-scalar processor <b>110</b> and two caches <b>120</b>, and all processors sharing a common VA/VB crossbar interface <b>113</b> connected to backplane connector <b>1120</b>. The card <b>420</b> includes two power connectors <b>1145</b>, a master clock chip MC<b>0</b><b>1142</b> and master clock chip MC<b>1</b><b>1144</b> (chip <b>1144</b> also includes a set of JTAGs that are used to initialize the processors <b>110</b>, caches <b>120</b>, and VA <b>1131</b> and VB <b>1132</b> chips during each initial deadstart of system <b>10</b>), a common channel I/O chip <b>1143</b> used to input and output data through channel adaptor connector <b>1150</b> from and to the I/O subsystem <b>601</b> (see also FIG. <b>1</b>), and a shared variable JS<b>0</b> chip <b>1141</b> (which stores data in a plurality of shared registers used for interprocessor communications and synchronization of processes between processors, the shared registers being read and written by instructions executed in the four processors <b>110</b>).</p><p>FIG. 5 shows a representation of a scalar/vector computer system <b>10</b>. In this embodiment, processor <b>110</b> transmits four parallel address requests on address busses <b>128</b> on each clock cycle. (In the embodiment shown in FIG. 4, only one processor <b>100</b> is implemented.) These four addresses are compared by compare unit <b>520</b> to the addresses of data in cache array <b>530</b>, and if a match is found (\u201ca cache hit\u201d) for any one or more, the corresponding data for those hits is returned directly from cache array <b>530</b> on read-data busses <b>126</b>. Each address bus <b>128</b> is constrained to requests for only two sections of memory <b>602</b> (i.e., a first bus is only for addresses contained in sections <b>0</b> and <b>2</b>, a second bus is only for addresses contained in sections <b>5</b> and <b>7</b>, a third bus is only for addresses contained in sections <b>1</b> and <b>3</b>, and a fourth bus is only for addresses contained in sections <b>4</b> and <b>6</b>). Since each address bus <b>128</b> to each pair of sections only has addresses for those two sections (of eight total sections <b>6020</b> of memory <b>602</b>), each needs two fewer bits than the entire address (for example 30 bits for a 32-bit address space). For example, one address bus <b>128</b> is shared between sections <b>6020</b> labeled S<b>0</b> and S<b>2</b>. In one embodiment, a separate address bus is provided from cache <b>200</b> to each section <b>6020</b> of memory <b>602</b> (i.e., for example eight address busses <b>523</b>, one to each of eight sections <b>6020</b> in system <b>10</b>), along with a separate read-data bus <b>521</b> from cache <b>200</b> to each section <b>6020</b> of memory <b>602</b> (i.e., for example eight read-data busses <b>521</b>, one to each of eight sections <b>6020</b>), and a separate write-data bus <b>522</b> from cache <b>200</b> to each section <b>6020</b> of memory <b>602</b> (i.e., for example eight write-data busses <b>522</b>, one to each of eight sections <b>6020</b>). In one embodiment, each write-data bus is 36 bits wide, and each 72-bit write value is passed serially in two parts. Since the address bus <b>523</b> to each section only has addresses for that section, each needs three fewer bits than the entire address.</p><p>In one embodiment, FIFO unit <b>510</b> includes a separate FIFO for each section <b>6020</b> of memory <b>602</b>. In one such embodiment, each FIFO is <b>48</b> positions deep, thus allowing the tracking of up to 48 times 8 requests, or 384 accesses. In one such embodiment, only accesses that miss in the cache (or when the cache is disabled for that type of access), will be placed in the FIFO which tracks outstanding requests to the memory <b>602</b>. Since each processor <b>110</b> can provide four substantially simultaneous references per clock cycle, each processor can have up to 96 serial references outstanding before it gets data back.</p><p>In a conventional set-associative cache, addresses for memory data references are converted to cache addresses, for example by hashing (exclusive-OR-ing certain memory address bits with others) or dropping some of the memory address bits in order to derive a cache address which has fewer address bits than the memory address. For this reason, there will be a plurality of \u201chash synonyms\u201d for each cache address (i.e., many memory addresses will map to the same cache address, and each such memory address which maps to the same cache address is a \u201chash synonym\u201d to other such addresses). In one embodiment of the present invention, cache <b>200</b> is configured as a 4-way associative cache, wherein for each set of hash synonyms, there are provided four cache entries (also called four \u201cways\u201d), where the four entries all share a common cache base address, called a \u201chash\u201d. In such conventional caches, when a cache miss occurs, the data in one of the ways for that hash (for example, the least-recently-used way) is marked invalid to make space for the new data, and a fetch is initiated to the common memory to read that data. In some processors, processing can continue, and further instructions requesting read data can be processed. However, once the number of references having misses exceeds the number of ways of the cache (e.g., 5 misses to the same hash in a 4-way associative cache), a conventional processor must stop or lock-up, since the processor must obtain and use the data from the first reference before allowing the 5th reference to access one of the ways. This is particularly a problem where accessing the common memory takes a large number of clocks or processor cycles, and where the processor is reading large numbers of data elements as when reading in elements for a vector operand.</p><p>In one embodiment, system <b>10</b> uses only physical addresses within each CPU <b>100</b>, and thus no logical address-to- physical address is performed. Physical addresses are thus used for both cache accesses and accesses to memory <b>602</b>.</p><p>One embodiment of the present invention provides a computer system <b>10</b> that includes a common memory <b>602</b>, the memory <b>602</b> having a plurality of sections <b>6020</b>; and a plurality of scalar/vector processors <b>100</b> coupled to the memory <b>602</b> using a plurality of separate address busses and a plurality of separate read-data busses wherein at least one of the sections of the memory <b>602</b> is associated with each address bus and at least one of the sections of the memory <b>602</b> is associated with each read-data bus, the processor <b>100</b> having a plurality of scalar registers <b>150</b> and a plurality of vector registers <b>160</b> and operating on instructions which provide a reference address to a data word. Each processor <b>110</b> includes a scalar/vector cache unit <b>200</b>. The cache unit <b>200</b> includes a cache array <b>530</b> that caches data for vector fetches, scalar fetches, and wherein the cache unit <b>200</b> fetches a different amount of data into cache array <b>530</b> based on whether a read-data operation is for a scalar register <b>150</b> or a vector register <b>160</b>. In one embodiment, one word of data is fetched into cache <b>200</b> for each vector request (no additional data is prefetched beyond the original request), and eight words of data is fetched into cache <b>200</b> for each scalar request (seven additional words of data are prefetched beyond the original request). In one such embodiment, each instruction fetch is implemented to fetch 32 words of instruction into cache <b>200</b>. Thus, in one embodiment of this computer system <b>10</b>, instruction fetches are run through the cache unit <b>200</b>, and wherein a different amount of data is prefetched for instruction fetching than the amount of prefetch data for scalar fetches, and than the amount of prefetch data for vector fetches. In one such embodiment, the cache unit <b>200</b> can be selectively disabled for instruction fetches and can be selectively disabled for scalar and vector data fetches. In another such embodiment, the cache unit <b>200</b> can be selectively disabled for vector data fetches.</p><p>The prior-art Cray J-90 computer employed a scalar cache having data stored therein for only scalar references (this cache did not keep data for vector references). Standard RISC (reduced-instruction-set computer) cache designs would have trouble if vector data were to be run through the cache stream of references because with a standard 4-way associative cache, lock-up could occur after 4 outstanding misses. Cache <b>200</b> of the present invention permits 48\u00d78 processor outstanding memory references from misses.</p><p>FIFO unit <b>510</b> maintains the order of arrival of data to match the order of requests (i.e., first-in first-out). Thus, data arriving into the cache will be also passed to the destination register (as specified by the destination code maintained by FIFO unit <b>510</b>).</p><p>In one preferred embodiment, the present invention provides a cache bypass for all requests beyond the first four misses to a 4-way cache entry. Thus, a four-way associative cache is provided, a cache miss (either read or write) causes one way of the cache entry to be allocated for the new address. The cache entry includes two status bits: a tag-valid bit and a data-valid bit. The tag for the cache entry includes a value that matches the address of that entry. When a read miss occurs, the tag is loaded from the address to be read, the tag-valid bit is set (indicating tag valid), and the data-valid bit is zeroed (reset, indicating data invalid). When the data from memory <b>602</b> returns and is loaded into the cache <b>200</b>, the data-valid bit is set (indicating data valid). Once a four-way cache entry has four outstanding misses (i.e., all four tags marked valid and all four data marked invalid), further accesses with the same hash synonym will completely bypass the cache. This allows more than four accesses to miss in the cache without stopping the processor <b>100</b>. When a write miss occurs, both the tag and the data are written into the cache, and the data is also written to memory <b>602</b>. When a write is performed, and the tag is found in the cache, but the data-valid bit is not set (indicating an outstanding read is in progress but the data is not returned yet), the tag-valid bit is reset and the data-valid bit is set (thus the read data is not placed into the cache when it returns, but that read data is still forwarded to and loaded into the destination register. Thus, on a read-miss followed by a write miss to the same address, neither the read data nor the write data are placed into the cache, and the tag-valid bit and the data-valid bit are zeroed (marked invalid). The term \u201cstale\u201d as used in this description refers to data which do not reflect the latest and most current values for such data. The data-valid bits are zeroed when a cache miss occurs to prevent reading stale data. Once a tag and its associated data are both loaded into a cache entry (by completing a read operation or write operation), that entry's data-valid bit is set. In one embodiment, in order to invalidate the entire cache (i.e., by a cache invalidate operation), the tag-valid and data-valid bits are reset. The read operation which was performed first is completed and the proper destination register is loaded with the requested (older) data, and the write operation is completed with the memory location loaded with the newer data. The cache entry is thus loaded with neither data value in this embodiment. The tag-valid bit is reset and the data-valid bit is set (to distinguish from an unallocated entry which has both bits reset). This embodiment is described further below.</p><p>In another such embodiment, on a read-miss followed by a write-miss, the write data is loaded into the cache and the data-valid bit is set (marking data valid), and when the original read data returns from memory <b>602</b>, it bypasses the newer data in the cache, and is loaded into the proper destination register as indicated by its respective destination code. Thus the location in memory and the cache location are loaded with the newer data value (the cache marked valid), and the older value is loaded into the proper destination register once it arrives from the memory <b>602</b>.</p><p>In other embodiments, the cache replaces (i.e., invalidates) old data in the cache only when the new replacement data has arrived from the common memory <b>602</b>. This allows the entry (or way) of the cache for that data value to be immediately available for items further back in the FIFO unit <b>510</b>. In one embodiment, FIFO unit <b>510</b> includes 8 separate independent FIFOs, one for each section <b>6020</b> of memory <b>602</b>, each FIFO advancing data reference tracking independently of the others. Thus, in the present invention the FIFO eliminates the processor lock-up that would otherwise occur after the number of cache misses equaled the number of ways in the cache, greatly increasing the number if outstanding references due to misses.</p><p>In one embodiment, the Cache <b>200</b> of the present invention also limits line size to one word for vector references. Therefore, one tag is associated with one word. The typical line in a cache contains 4 to 8 words, where each word is 8 bytes (64 bits, plus parity or ECC if used). The limited line size for vector references is explained by the concept of spacial locality. When accessing scalar references in a cache, the prefetching of approximately 8 words (i.e., where a line size is equal to 8 words) obtains greater spacial locality of the fetched words. This spacial locality is not needed with vector references. However, with scalar references, the cache <b>200</b> of the present invention prefetches approximately 8 words to obtain spacial locality. In other words, the cache has a line size (the number of words that are replaced on a cache miss) that varies between 1 word and 8 words, depending on whether the processor <b>100</b> is performing vector references or scalar references, respectively.</p><p>Each cache line is associated with a tag that stores the common memory address that the data value in cache is from. Increasing the number of words per line decreases the overhead of tag storage and comparison. Increasing the line size also increases the hit ratio for certain patterns of data references (scalar references with a high locality of references). Other reference patterns such as strided vector reads and scatter-gather operations do not benefit for line sizes over one word per line.</p><p>In one embodiment, each cache line is one word long, and has a separate tag. Vector references will fetch only one word. Scalar read references however, cause processor <b>110</b> to issue four parallel read references, one of which is returned to processor <b>110</b> and cache <b>200</b>, and the other three are \u201cprefetch\u201d data that are returned only to cache <b>200</b>. In another such embodiment, for each scalar read request, eight words are fetched (e.g., two sequential groups of four parallel requests, such that one request is made to each of eight sections <b>6020</b>) (seven of which are speculative prefetch data that are placed into cache <b>200</b> on the expectation that at least some of the data will be needed before it disappeared from the cache (overwritten by other data).</p><p>Prior-art Cray machines made up to two references per clock to a common memory, but did not present multiple references per clock to a cache. Cache <b>200</b> of the present invention handles four memory references per clock as compared to one memory reference per clock for standard caches today. This higher bandwidth is to accommodate vector operations.</p><p>Cache <b>200</b> of the present invention invalidates large amounts of data in a relatively few clock cycles (e.g., 256 K bytes in approximately 6 to 8 clock cycles). A conventional cache invalidates approximately one line of data (i.e., about 8 words or 64 bytes) per clock cycle. Invalidating the cache is typically performed when swapping tasks in the computer. One embodiment of the present invention provides an invalidate-cache signal <b>310</b> (see FIG. 3) from processor <b>110</b> to each cache chip <b>120</b>, which is activatable by a program instruction (e.g., a \u201ctest-and-set\u201d instruction) and/or task-swap function. In one such embodiment, the exchange package <b>180</b> (see FIG. 1) for a task includes a disable-cache-invalidate field <b>181</b> which inhibits the operation of the cache invalidate instruction and/or function.</p><p>Cache <b>200</b> of the present invention permits destination cache for data that handles 48 outstanding references per processor to each of eight sections of memory (up to 384 outstanding references for each of four processors) for one embodiment of the present invention. Tracking information for these outstanding references are stored in a FIFO. In the FIFO, a read reference designates 1) the destination code; and 2) information on what to do with data in the cache (whether it goes in cache and where it goes if it does go in cache); and 3) information on fetch vs. store data reference.</p><p>The 48 times 8 outstanding references are possible because there are 8 FIFO's, each having 48 entries deep. Therefore, if misses occur, data is returned from memory in order and information is taken out of the FIFO to tell the CPU what to do with the data. In a normal cache, if you have one miss, the information is buffered by the \u201cways\u201d of the associative cache, so that you can handle more misses. For example, approximately 4 misses are typically buffered in a four-way associative scalar cache. If vector elements were run through (i.e., stored into) such a conventional cache, lock-up and or thrashing would severely degrade performance. Thus, such caches are typically only used for scalar references.</p><p>Buffering of the information necessary to tell the CPU what to do with return data from memory is not new, but the significantly increased buffering using 8 FIFOs each 48-entries-deep for containing this information to handle the 48 times 8 outstanding references is new. The criteria to determine how many FIFO's and how deep the FIFO's should be is determined by how many clocks and buffering covers a round trip to memory and back. Thus, FIFO unit <b>510</b> provides buffering sufficient to cover a round trip to memory and back, and is the necessary buffering to handle the number of outstanding references needed in the vector cache.</p><p>When making vector references to a cache, there are a large number of references. In cache <b>200</b> of the present invention there are 48 times 8=384 outstanding references possible. Since the cache can handle 4 references per clock cycle, cache <b>200</b> of the present invention can make four references on each of 96 clock cycles in a row before it gets data back. In this way, data can be streamed and the processor <b>110</b> does not have to be stopped (waiting for data to arrive or for a cache entry to be made available) and then allowed to proceed. In addition, cache <b>200</b> of one embodiment, the present invention is a write-through cache, to permit a stream of write data without allocation because there is one word per cache line for vector references.</p><p>The term \u201cwrite through\u201d (also called \u201cstore through\u201d) as used in this description is defined as the action of storing a data value from a register into data cache as a part of the operation storing that data value to the corresponding location in the common memories <b>602</b>. In a write-through operation, processor <b>110</b> appears to write the data through data cache <b>200</b> to the common memories <b>602</b>.</p><p>Cache <b>200</b> of the present invention allows four references per clock cycle because cache is divided into four portions and references can be made independently to each portion. In one embodiment described above, two cache chips CA<b>0</b><b>120</b> and CA<b>1</b><b>130</b> implement the cache <b>200</b>, and cache chip CA<b>0</b><b>120</b> includes a first portion for sections <b>0</b> and <b>2</b> of memory <b>602</b> and a second portion for sections <b>5</b> and <b>7</b> of memory <b>602</b>, while cache chip CA<b>1</b><b>130</b> includes a third portion for sections <b>1</b> and <b>3</b> of memory <b>602</b> and a fourth portion for sections <b>4</b> and <b>6</b> of memory <b>602</b>.</p><p>In one embodiment as described hereinafter, each processor <b>110</b> uses 32-bit addresses. Two of the address bits, bits <b>0</b> and <b>2</b>, are consumed by the processor PV chip <b>110</b> when resolving which of the four memory ports <b>128</b> to use. Bits <b>31</b>-<b>3</b> and bit <b>1</b> of the reference address are sent to cache <b>200</b>. Bits <b>12</b>-<b>3</b> and bit <b>1</b> of the address are used as a set index within cache <b>200</b> which address one of the 2 k sets per port. Bits <b>31</b>-<b>13</b> of the address are used as a tag in the cache. These bits are stored on an allocation and are compared to subsequent reference addresses to determine a hit or a miss.</p><p>Set State for Each Cache Entry</p><p>Each set has eight valid bits and a five-bit LRU (least recently used) code. Two of the eight valid bits are assigned to each of the four set elements. A set element includes one tag and one data word. The two bits are labeled TV (tag valid) and DV (data valid). The state is as follows.</p><p><tables id=\"TABLE-US-00001\"><table colsep=\"0\" frame=\"none\" pgwide=\"1\" rowsep=\"0\"><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"14pt\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"21pt\"></colspec><colspec align=\"left\" colname=\"3\" colwidth=\"224pt\"></colspec><thead><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry>TV</entry><entry>DV</entry><entry>Cache state</entry></row><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row></thead><tbody valign=\"top\"><row><entry>0</entry><entry>0</entry><entry>Empty. Line's tag is undefined. Never a tag match.</entry></row><row><entry>1</entry><entry>0</entry><entry>Memory update pending. Set to this state on a read allocation.</entry></row><row><entry></entry><entry></entry><entry>\u2003\u2003Remains in this state until data from memory is written or a</entry></row><row><entry></entry><entry></entry><entry>\u2003\u2003write request is made to this line.</entry></row><row><entry>0</entry><entry>1</entry><entry>Memory update invalid. Set to this state on a write hit to a memory</entry></row><row><entry></entry><entry></entry><entry>\u2003\u2003update pending line. Remains in this state until memory update</entry></row><row><entry></entry><entry></entry><entry>\u2003\u2003data is returned and discarded.</entry></row><row><entry>1</entry><entry>1</entry><entry>Full. Set to this state from memory update pending and memory update</entry></row><row><entry></entry><entry></entry><entry>\u2003\u2003invalid when memory read data returns. Set to this state on a</entry></row><row><entry></entry><entry></entry><entry>\u2003\u2003write allocation.</entry></row><row><entry align=\"center\" nameend=\"3\" namest=\"1\" rowsep=\"1\"></entry></row></tbody></tgroup></table></tables></p><p>If the least-recently-used way is in the memory update pending or invalid state, no allocations of any way will be made on a miss.</p><p><tables id=\"TABLE-US-00002\"><table colsep=\"0\" frame=\"none\" pgwide=\"1\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"259pt\"></colspec><tbody valign=\"top\"><row><entry align=\"center\" nameend=\"1\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry>State Transition Table</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"5\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"1\" colwidth=\"35pt\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"28pt\"></colspec><colspec align=\"center\" colname=\"3\" colwidth=\"14pt\"></colspec><colspec align=\"center\" colname=\"4\" colwidth=\"21pt\"></colspec><colspec align=\"left\" colname=\"5\" colwidth=\"161pt\"></colspec><tbody valign=\"top\"><row><entry>Type of</entry><entry>Tag</entry><entry></entry><entry></entry><entry></entry></row><row><entry>Access</entry><entry>Match</entry><entry>TV</entry><entry>DV</entry></row><row><entry align=\"center\" nameend=\"5\" namest=\"1\" rowsep=\"1\"></entry></row><row><entry></entry><entry></entry><entry></entry><entry></entry><entry>Result (Matching Way)</entry></row><row><entry>READ</entry><entry>Y</entry><entry>1</entry><entry>1</entry><entry>Normal Read Hit. Only LRU changes.</entry></row><row><entry>READ</entry><entry>Y</entry><entry>1</entry><entry>0</entry><entry>Read Miss no allocate. Only LRU changes.</entry></row><row><entry>READ</entry><entry>Y</entry><entry>0</entry><entry>1</entry><entry>Read Hit. Only LRU changes.</entry></row><row><entry>WRITE</entry><entry>Y</entry><entry>1</entry><entry>1</entry><entry>Write hit. Data and LRU change.</entry></row><row><entry>WRITE</entry><entry>Y</entry><entry>1</entry><entry>0</entry><entry>Write hit. Data, TV, DV and LRU change.</entry></row><row><entry>WRITE</entry><entry>Y</entry><entry>0</entry><entry>1</entry><entry>Write hit. Data and LRU change.</entry></row><row><entry></entry><entry></entry><entry></entry><entry></entry><entry>Result (LRU Way)</entry></row><row><entry>READ</entry><entry>N</entry><entry>1</entry><entry>1</entry><entry>Re-allocate LRU way. Tag, DV, and LRU change.</entry></row><row><entry>READ</entry><entry>N</entry><entry>1</entry><entry>0</entry><entry>Miss no allocate. No LRU changes.</entry></row><row><entry>READ</entry><entry>N</entry><entry>0</entry><entry>1</entry><entry>Miss no allocate. No LRU changes.</entry></row><row><entry>READ</entry><entry>N</entry><entry>0</entry><entry>0</entry><entry>Allocate LRU. Tag, TV, DV, and LRU change.</entry></row><row><entry>WRITE</entry><entry>N</entry><entry>1</entry><entry>1</entry><entry>Re-allocate LRU way. Tag, DV, and LRU change.</entry></row><row><entry>WRITE</entry><entry>N</entry><entry>1</entry><entry>0</entry><entry>Miss no allocate. No LRU changes.</entry></row><row><entry>WRITE</entry><entry>N</entry><entry>0</entry><entry>1</entry><entry>Miss no allocate. No LRU changes.</entry></row><row><entry>WRITE</entry><entry>N</entry><entry>0</entry><entry>0</entry><entry>Write Allocate. Tag, Data, TV, DV and LRU.</entry></row><row><entry align=\"center\" nameend=\"5\" namest=\"1\" rowsep=\"1\"></entry></row></tbody></tgroup></table></tables></p><p>The LRU code is an eight bit entity that keeps track of the order of set element use from most recently used to least recently used. Bits <b>4</b> and <b>3</b> contain the most recently used element, bits <b>2</b> and <b>1</b> contain the next most recently used element. Of the two elements that are not in bits <b>4</b> through <b>1</b>, bit <b>0</b> is clear if the smaller element number is the least recently used element, and is set otherwise.</p><p>Read Operation</p><p>For a read, the cache is checked for a hit. A hit is indicated by a tag compare and an associated data-valid bit set. If there is a hit, the cache data is placed on the port read data path. If there is a tag compare and the tag-valid bit is set but the data-valid bit is clear, the cache reference results in a miss but no allocation is made. If there is a miss and the LRU element has both TV set or DV set but not both, no allocation is made. If there is a miss, and the LRU element has both TV and DV clear or both TV and DV set, an allocation is made. On a read allocation, the tag portion of the address is stored into the allocated element, the TV bit is set while the DV bit is clear, and a reference is made to memory. The LRU code is updated after a hit or an allocation, with the hit or allocated element becoming the new most recently used element.</p><p>Section Queue for Reads</p><p>Each of the four memory sections referenced by a cache chip has its own section queue for reads. The section queue operation depends on memory read data being returned in order of reference for each memory section. When a read reference is made, a flag indicating returned data is to be written to the cache, the set index, and an element pointer are stored in the section queue. Other information stored is the destination queue and a maintenance bit for maintenance reads.</p><p>Write Operation</p><p>For a write, all references are checked against the cache and are sent to memory as well. In the case of a write hit, if both the tag- and data-valid flags are set, the cache is updated with the write data. In the case of a write miss, the LRU set element is allocated, provided its TV and DV bits are both set or both clear. The tag and data are stored, and the associated TV and DV bits are set. If there is a tag compare on a write to a location that has its tag-valid bit set and data-valid bit clear, the new data is stored into the set element, the TV bit is cleared, and the data-valid bit is set. This will happen after a write to a memory address that has been previously read and allocated, but the read data has not made it back from memory. The TV clear and DV set are used to indicate that the read data should not be stored in the cache. After the read data is returned, the TV bit gets set.</p><p>Back-to-back Same Set Accesses</p><p>Any reference to an index potentially changes the set's state. A subsequent reference to the same index within 6 clock periods will read the old valid set state rather than the modified state that is stored after completion of the read/modify/write cycle. For this reason, special accommodations are required for back to back accesses of the same index. The following table indicates the changes made to the state in response to the previous set state. In the case of a tag match, the valid bits correspond to the set element that matches. If there is no match, the valid bits are taken from the LRU set element.</p><p>In all of these cases, simply treating a subsequent read message as a miss will always assure a correct result since the memory <b>602</b> will always be coherent and since a set does not require a state change due to a read access. A state change will generally result from a read operation, but skipping a cache access on a read does not make any cache contents invalid. For a write, a state change may be required. A cached address that is written too must be updated or invalidated in the cache in order to maintain cache coherence with system memory.</p><p>On a write, it is necessary to update the cached data on a hit. For a cache element that is in the process of being updated, as indicated by a TV bit without the DV bit, it is also necessary to clear TV and set DV to prevent the cache data from being written by the update data. If a location has a TV clear and DV set condition, it is necessary to prevent the update from happening. There is no apparent way to avoid a cache access on a write. Even an invalidate set cannot be done unless there is a way to stop data updates from a previous read allocation on a set basis. The solution is to compare the index of a previous access and use the write data from the previous access as the new set state.</p><p>Cache Array <b>530</b> Contention</p><p>There are three access paths into the cache array (i.e., RAMs) <b>530</b> which store cached data. The cache array data RAMs <b>530</b> are read on a speculative basis and in parallel with tag reads on a PV read reference from a processor <b>110</b>. Of the data read, one word will be returned to a PV processor <b>110</b> if there is a hit in the cache, none of the data read will be used if there is a cache miss. A second access source is a write of PV reference data on a PV write reference (from a processor <b>110</b>) that results in a hit or an allocate. The final method of access is a write to the cache array <b>530</b> of memory data which completes the read allocation process. Writes of PV data from processor <b>110</b> to the cache array <b>530</b> are put into a delayed PV reference queue. This queue can also hold PV read references for data which was unavailable when a PV read reference first occurred.</p><p>The priorities for access types are: first\u2014memory, second\u2014the PV reference queue, and finally\u2014speculative PV reads. The memory data writes get first priority since the memory path can have the most outstanding references, and the path from memory <b>602</b> assumes that a memory read reference, once initiated, can go to completion. The memory path also gets highest priority since it is the highest latency reference type. PV read references (for a processor <b>110</b>) continue as normal even if a data word is unavailable due to a conflict with a memory write. The cache control functions as normal in this case, and may result in a miss or a hit to one of the data words that was read. If a hit occurs to the word that was not read due to a conflict, the cache reference is placed in a delayed reference queue. The reference will complete, and read data will be returned to the PV, as soon as the RAM location becomes available. If there is a conflict between a delayed reference and a memory data write, the memory data write gets preference. If there is a conflict between a delayed PV and a speculative read, the delayed PV reference gets priority. A conflict with a delayed PV reference from an earlier reference may subsequently cause another PV read reference to go into the delayed reference queue. All writes of PV data to the cache are placed in the delayed PV reference queue. This assures that prior reads in the delayed PV reference queue are completed in order with respect to the PV cache write. The particular data ram that is the destination of the PV write reference is flagged as unavailable for speculative read references until the PV write to cache completes. A hit to a data ram that is flagged as unavailable will result in a reference going to the delayed reference queue. This assures that subsequent reads to the same address as a prior write will be performed in order.</p><p>Cache Chip <b>120</b> Data Path Contention</p><p>There are a number of sources of data path contention internal to the cache <b>200</b>. For each port, the cache is subdivided into two sections. Each section's RAM block for data and for tags have a two-clock-period access. Therefore, consecutive references to the same section must have a clock of delay between them. Other sources of cache contention are a write to a cache line. There are two write sources. The write data may come from the PV on a write hit or allocate, and the write data from memory on a read allocate. Tags will always be written from the PV address path.</p><p>Another source of contention occurs on the data path on the PV. Memory read data and cache read data on a read hit are transmitted across the same read data path on the PV. It is expected that the cache will stall to accommodate memory data since memory data cannot be stopped.</p><p>The contention between reads and writes to a cache location, both tag updates and writes of data, can be eliminated by using two-port RAMs, RAMs with separate read and write addressing. Since two-port RAMs take more area than single-port RAMs, the cache size would be reduced. There would still be contention between the multiple sources of write data, the PV and the memory.</p><p>Cache Data Protection</p><p>Tags and cache data will be protected by a single parity bit. A parity error on a tag or data read will be treated as a cache miss. Data will not be protected on the cache chip outside of the tag and data RAMs, and will not be protected when passed to or received from the PV. SECDED will be generated on the cache as data is written to memory, and will be checked on memory read data immediately after it arrives.</p><p>Cache Invalidation</p><p>The PV raises the pv_inval signal to invalidate the cache. A cache invalidate clears all valid bits, essentially clearing the cache of all data. All LRU codes are returned to an initial starting point. Any memory read references which are outstanding at the time of invalidation will not be used to update the cache.</p><p>Maintenance References</p><p>There are two maintenance signals, one for a read and one for a write. A write reference coincident with the maintenance write bit causes destination code bits [<b>7</b>:<b>0</b>] to be substituted for generated check bits. A read reference coincident with the maintenance read bit causes check bits returned form memory to be substituted for eight of the data bits.</p><p>Check bit <b>0</b> is returned in data bit <b>0</b>.</p><p>Check bit <b>1</b> is returned in data bit <b>8</b>.</p><p>Check bit <b>2</b> is returned in data bit <b>16</b>.</p><p>Check bit <b>3</b> is returned in data bit <b>24</b>.</p><p>Check bit <b>4</b> is returned in data bit <b>32</b>.</p><p>Check bit <b>5</b> is returned in data bit <b>40</b>.</p><p>Check bit <b>6</b> is returned in data bit <b>48</b>.</p><p>Check bit <b>7</b> is returned in data bit <b>56</b>.</p><p>Maintenance writes do not allocate in the cache and maintenance reads bypass the cache for memory.</p><p>SECDED Error Reporting</p><p>Bit <b>1</b> of the read-data-valid signals, cae_rvld for the even port and cao_rvld for the odd port, indicate that a SECDED error is being reported. Bit <b>1</b> and <b>0</b> will both assert to report a double-bit error, and bit <b>1</b> will assert without bit <b>0</b> to report a single-bit error. Error data will be reported on the data lines instead of read data when an error is indicated.</p><p>In one embodiment, double-bit errors on memory data that go only to the cache are not reported. Any data with a double-bit error will not be stored in the cache. This causes the cache location to wait for memory data (and miss on reads) indefinitely, or at least until there is a cache invalidate. Single-bit errors on memory data that is going only to the cache will be reported and the data will be stored in cache. A cache error, with err_data bits <b>23</b>-<b>20</b> all ones, can be both correctable and uncorrectable. A correctable cache error is an error that was detected in time to undo a cache hit and miss instead. The bad cache location is simply not used. An uncorrectable cache error is an error that is detected too late to cause a cache miss and make a read reference.</p><p>Error-data bits [<b>23</b>:<b>20</b>] indicate a reference type as follows.</p><p>Err_data [<b>23</b>:<b>20</b>]=0010=SECDED error on exchange</p><p>0001=SECDED error on fetch</p><p>0110=SECDED error on B reg</p><p>0101=SECDED error on T reg</p><p>1000=SECDED error on V reg</p><p>1110=SECDED error on A/S reg</p><p>1111=cache error</p><p>Err_data [<b>19</b>]=uncorrectable error</p><p>Err_data [<b>18</b>]=correctable error</p><p>Err_data [<b>17</b>:<b>10</b>]=bits [<b>9</b>:<b>2</b>] of the reference address for PV <b>110</b> to CA <b>120</b> (Note this is not the same as the system address bits [<b>9</b>:<b>2</b>])</p><p>Err_data [<b>9</b>:<b>8</b>]=0 and will be set to system address bits [<b>1</b>:<b>0</b>] by the PV</p><p>Err_data [<b>7</b>:<b>0</b>]=syndrome</p><p>The bad address bits <b>8</b> and <b>9</b> are sent as zeroes by the cache and defined in the PV chip.</p><p>Selective Data, Instruction Caching</p><p>The read reference type is determined by bits [<b>9</b>:<b>6</b>] of the destination code as follows:</p><p>If dest_code [<b>9</b>:<b>6</b>]=1xxx, it is a vector read reference</p><p>If dest\u2014code [<b>9</b>:<b>6</b>]=01xx, it is a fetch reference</p><p>If dest\u2014code [<b>9</b>:<b>6</b>]=001x, it is a B or T read reference</p><p>If dest\u2014code [<b>9</b>:<b>6</b>]=0001, it is an A or S read reference</p><p>If dest\u2014code [<b>9</b>:<b>6</b>]=0000, it is a scalar read ahead</p><p>If pv_dce (data cache enable) is asserted, the cache will check for read hits and make read allocation on all reference types except fetch references. It will also check for hits and make allocation on all write references (for both scalars and vectors).</p><p>If pv_ice (instruction cache enable) is asserted, the cache will check for read hits and make read allocations on fetch references. It will check for write hits (although there shouldn't be any) but will not make any allocations on write references.</p><p>It is to be understood that the above description is intended to be illustrative, and not restrictive. Many other embodiments will be apparent to those of skill in the art upon reviewing the above description. The scope of the invention should, therefore, be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Gregory J.", "last_name": "Faanes", "name": ""}, {"first_name": "Eric P.", "last_name": "Lundberg", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "CRAY INC."}, {"first_name": "", "last_name": "WELLS FARGO BANK, N.A.", "name": ""}, {"first_name": "", "last_name": "CRAY INC.", "name": ""}, {"first_name": "", "last_name": "FOOTHILL CAPITAL CORPORATION (A CALIFORNIA CORPORATION)", "name": ""}, {"first_name": "", "last_name": "CRAY, INC.", "name": ""}, {"first_name": "", "last_name": "TERA COMPUTER COMPANY", "name": ""}, {"first_name": "", "last_name": "SILICON GRAPHICS, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/100"}, {"primary": false, "label": "G06F  13/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMUS"}, {"label": "G06F  15/78        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711118"}, {"primary": false, "label": "712011"}, {"primary": false, "label": "712003"}, {"primary": false, "label": "711E12051"}, {"primary": false, "label": "712004"}, {"primary": false, "label": "711E12057"}, {"primary": false, "label": "712207"}, {"primary": false, "label": "711E12021"}], "ecla_classes": [{"label": "G06F  15/80V4"}, {"label": "G06F   9/30R5D"}, {"label": "G06F   9/30A2"}, {"label": "G06F   9/30R5X"}, {"label": "G06F  12/08B6P4"}, {"label": "G06F  12/08B18"}, {"label": "S06F12:08B6P2"}, {"label": "G06F  12/08B8"}], "cpc_classes": [{"label": "G06F  12/0859"}, {"label": "G06F   9/3013"}, {"label": "G06F  15/8076"}, {"label": "G06F   9/3013"}, {"label": "G06F   9/3004"}, {"label": "G06F  12/0857"}, {"label": "G06F   9/30138"}, {"label": "G06F  12/0862"}, {"label": "G06F  12/0857"}, {"label": "G06F  12/0862"}, {"label": "G06F  12/0859"}, {"label": "G06F  12/0888"}, {"label": "G06F  12/0888"}, {"label": "G06F   9/3004"}, {"label": "G06F  15/8076"}, {"label": "G06F   9/30138"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1998-12-31", "application_date": "1998-12-31", "family_members": [{"ucid": "US-6496902-B1", "titles": [{"lang": "EN", "text": "Vector and scalar data cache for a vector multiprocessor"}]}, {"ucid": "US-20020144061-A1", "titles": [{"lang": "EN", "text": "Vector and scalar data cache for a vector multiprocessor"}]}, {"ucid": "US-6665774-B2", "titles": [{"lang": "EN", "text": "Vector and scalar data cache for a vector multiprocessor"}]}]}