{"patent_number": "US-6304961-B1", "publication_id": 72817106, "family_id": 25471319, "publication_date": "2001-10-16", "titles": [{"lang": "EN", "text": "Computer system and method for fetching a next instruction"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72627206\"><p>The invention relates to a computer system and method for fetching a next instruction. In one embodiment, a computer system includes an instruction cache, a next fetch address register, and a fetch unit. The instruction cache includes an instruction array for storing a plurality of processor instructions and a next address fetch array for storing at least one next fetch address. Each next fetch address associated with at least one of the processor instructions stored in the instruction array and indicating a location of a processor instruction to be fetched. In another embodiment, an apparatus includes a first device configured to fetch a first instruction stored in an instruction cache, a second device configured to unconditionally store a next fetch address associated with the first instruction, and a third device configured to unconditionally fetch a second instruction stored at a location indicated by the stored next fetch address.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00001\" num=\"1\"><claim-text>1. A computer system comprising:</claim-text><claim-text>an instruction cache for storing a plurality of processor instructions, the instruction cache including </claim-text><claim-text>an instruction array for storing the plurality of processor instructions; </claim-text><claim-text>an instruction class array for storing an instruction class for each of the plurality of processor instructions; and </claim-text><claim-text>a predictive annotation array for storing at least one prediction indicator for each of the plurality of processor instructions, wherein the instruction cache selectively refers to the predictive annotative array based upon the instruction class; </claim-text><claim-text>a next address fetch array for storing at least one next fetch address, each next fetch address associated with at least one of the processor instructions stored in the instruction array and indicating a location of a processor instruction to be fetched; </claim-text><claim-text>a next fetch address register, coupled to the instruction cache, for receiving a next fetch address associated with each processor instruction previously fetched from the instruction cache; and </claim-text><claim-text>a fetch unit, coupled to the instruction cache and the next fetch address register, for fetching a processor instruction stored at a location indicated by each next fetch address received at the next fetch address register. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00002\" num=\"2\"><claim-text>2. The computer system of claim <b>1</b>, further comprising an execution unit, coupled to the prefetch and dispatch unit, for executing the first instruction after the prefetch and dispatch unit dispatches the first instruction to the execution unit.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00003\" num=\"3\"><claim-text>3. The computer system of claim <b>1</b>, wherein the next fetch address is a sequential address following the first instruction.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00004\" num=\"4\"><claim-text>4. The computer system of claim <b>1</b>, wherein a first instruction is a branch instruction and the instruction cache further comprises a branch prediction field for storing branch prediction information.</claim-text></claim>"}, {"num": 5, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00005\" num=\"5\"><claim-text>5. The computer system of claim <b>4</b>, wherein the branch prediction information in the branch prediction field is for predicting branch not taken, and the next address fetch field is for storing the sequential address after the first instruction.</claim-text></claim>"}, {"num": 6, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00006\" num=\"6\"><claim-text>6. The computer system of claim <b>4</b>, wherein the branch prediction information in the branch prediction field is for predicting branch taken, and the next address fetch field is for storing a target address of the first instruction.</claim-text></claim>"}, {"num": 7, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00007\" num=\"7\"><claim-text>7. The computer system of claim <b>4</b>, wherein the branch prediction information is dynamically updated in accordance with a branch prediction algorithm.</claim-text></claim>"}, {"num": 8, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00008\" num=\"8\"><claim-text>8. The computer system of claim <b>1</b>, further comprising an execution unit, coupled to the instruction cache, for executing a first instruction, the execution unit further including a resolve unit for resolving an actual next fetch address of the first instruction, and a compare circuit for comparing the actual next fetch address with the next fetch address contained in the next address fetch field associated with the first instruction.</claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00009\" num=\"9\"><claim-text>9. The computer system of claim <b>8</b>, further comprising a prefetch unit, coupled to the execution unit and the instruction cache, for fetching a second instruction corresponding to the actual next fetch address and providing the second instruction to the execution unit for execution if the compare circuit determines that the actual next fetch address of the first instruction and the next fetch address contained in the next address fetch field associated with the first instruction are different.</claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00010\" num=\"10\"><claim-text>10. The computer system of claim <b>9</b>, wherein execution of the computer program resumes at the second instruction if the compare circuit determines that the actual next fetch address of the first instruction and the next fetch address contained in the next address fetch field associated with the first instruction are different.</claim-text></claim>"}, {"num": 11, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00011\" num=\"11\"><claim-text>11. The computer system of claim <b>9</b>, wherein the next address fetch field associated with the first instruction is updated with the actual next fetch address corresponding to the second instruction if the compare circuit determines that the actual next fetch address of the first instruction and the next fetch address contained in the next address fetch field associated with the first instruction are different.</claim-text></claim>"}, {"num": 12, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00012\" num=\"12\"><claim-text>12. The computer system of claim <b>1</b>, wherein the fetched instruction is fetched from the instruction cache.</claim-text></claim>"}, {"num": 13, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00013\" num=\"13\"><claim-text>13. The computer system of claim <b>1</b>, wherein the fetched instruction is fetched from memory if the fetched instruction is not in the instruction cache.</claim-text></claim>"}, {"num": 14, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00014\" num=\"14\"><claim-text>14. The computer system of claim <b>1</b>, wherein the fetched instruction is fetched and speculatively dispatched into an execution unit for execution.</claim-text></claim>"}, {"num": 15, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00015\" num=\"15\"><claim-text>15. The computer system of claim <b>1</b>, wherein the plurality of instructions stored in the instruction cache are arranged in a number of cache lines, each one of the number of cache lines including (n) of the plurality of instructions stored in the cache.</claim-text></claim>"}, {"num": 16, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00016\" num=\"16\"><claim-text>16. The computer system of claim <b>15</b>, wherein each of the number of cache lines includes (k) of the next address fetch field addresses, where (k) is equal to or less than (n).</claim-text></claim>"}, {"num": 17, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00017\" num=\"17\"><claim-text>17. The computer system of claim <b>15</b>, wherein each of the number of cache lines includes (m) branch prediction elements, where (m) is equal to or less than (n).</claim-text></claim>"}, {"num": 18, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00018\" num=\"18\"><claim-text>18. The computer system of claim <b>15</b>, wherein each of the number of cache lines further includes a branch prediction field for storing branch prediction information for a dominating instruction among the (n) instructions in each of the number of cache lines in the instruction cache.</claim-text></claim>"}, {"num": 19, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00019\" num=\"19\"><claim-text>19. The computer system of claim <b>1</b>, wherein the next fetch address stored in a next address fetch field associated with the first instruction is initialized in accordance with a predetermined policy when the first instruction is placed into the instruction cache from memory.</claim-text></claim>"}, {"num": 20, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00020\" num=\"20\"><claim-text>20. The computer system of claim <b>1</b>, wherein the instruction cache further includes an instruction decode field for storing instruction decode information for a first instruction.</claim-text></claim>"}, {"num": 21, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00021\" num=\"21\"><claim-text>21. The computer system of claim <b>20</b>, wherein the instruction decode information includes a class identifier for identifying the class of a first instruction.</claim-text></claim>"}, {"num": 22, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00022\" num=\"22\"><claim-text>22. The computer system of claim <b>1</b>, wherein the next fetch address is a predicted next fetch address.</claim-text></claim>"}, {"num": 23, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00023\" num=\"23\"><claim-text>23. A method of providing a computer system comprising the steps of:</claim-text><claim-text>providing an instruction cache for storing a plurality of processor instructions, the instruction cache including </claim-text><claim-text>an instruction field for storing the plurality of processor instructions; </claim-text><claim-text>an instruction class array for storing an instruction class for each of the plurality of processor instructions; </claim-text><claim-text>a predictive annotation array for storing a prediction indicator for each of the plurality of processor instructions; and </claim-text><claim-text>a next address fetch array for storing at least one next fetch address, each next fetch address associated with at least one of the processor instructions stored in the instruction array and indicating a location of a processor instructions to be fetched; </claim-text><claim-text>selectively referring to the predictive annotative array based upon the instruction class; </claim-text><claim-text>providing a next fetch address register, coupled to the instruction cache, for receiving a next fetch address associated with each processor instruction previously fetched from the instruction cache; and </claim-text><claim-text>providing a fetch unit, coupled to the instruction cache and the next fetch address register, for fetching a processor instruction stored at a location indicated by each next fetch address received at the next fetch address register. </claim-text></claim>"}, {"num": 24, "parent": 23, "type": "dependent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00024\" num=\"24\"><claim-text>24. The method of claim <b>23</b>, further comprising the step of providing an execution unit, coupled to the prefetch and dispatch unit, for executing the first instruction after the prefetch and dispatch unit dispatches the first instruction to the execution unit.</claim-text></claim>"}, {"num": 25, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6304961-B1-CLM-00025\" num=\"25\"><claim-text>25. A method of fetching instructions, comprising the steps of:</claim-text><claim-text>fetching a first instruction stored in an instruction cache; </claim-text><claim-text>providing an instruction class for the first instruction; </claim-text><claim-text>selectively referring to a predictive annotative array based upon the instruction class of the first instruction; </claim-text><claim-text>unconditionally storing, in a next fetch address register coupled to the instruction cache, a next fetch address associated with the first instruction; and </claim-text><claim-text>unconditionally fetching a second instruction stored at a location indicated by the next fetch address stored in the next fetch address register.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54740146\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><p>This is a continuation of application Ser. No. 08/363,107, filed Dec. 22, 1994 now abandoned.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Related Application</p><p>The present application which is a continuation application of, commonly assigned application Ser. No. 07/938,371 entitled \u201cA Computer System Having a Minimum Latency Cache Which Stores Partially Decoded Instructions, Branch Prediction and Next Fetch Address Prediction Information,\u201d filed Aug. 31, 1992, now abandoned and incorporated in its entirety by reference herein.</p><p>2. Field Of Invention</p><p>The present invention relates to the field of computer systems. More specifically, the present invention relates to a computer system having a minimum latency cache which stores instructions decoded to determine class, branch prediction and next fetch address prediction information.</p><h4>BACKGROUND</h4><p>Historically, when a branch instruction was dispatched in a computer system, instruction fetching and dispatching were stalled until the branch direction and the target address were resolved. Since this approach results in lower system performance, it is rarely used in modern high performance computers. To obtain higher system performance, various techniques have been developed to allow instruction fetching and dispatching to continue in an efficient manner without waiting for the resolution of the branch direction. Central to the efficiency of continuing instruction prefetching and dispatching is the ability to predict the correct branch direction. There are several common approaches to predicting branch direction:</p><p>1. Static prediction: Under this approach, the higher probability direction for a particular branch instruction is ascertained. When the branch instruction is fetched, the ascertained direction is always taken. For example, a direction for a branch instruction maybe set to \u201cBranch Taken\u201d, or alternatively, set to \u201cBranch Not Taken\u201d.</p><p>2. Dynamic software prediction: Under this approach, a branch prediction algorithm predicts the branch direction.</p><p>3. Dynamic hardware prediction: Under this approach, a branch prediction algorithm predicts the branch direction based on the branch history information maintained in a branch prediction table.</p><p>The static prediction approach is simple to implement, however, its prediction hit rate is generally less than 75%. Such a prediction hit rate is generally too low for high performance computers. The dynamic software prediction approach generally works quite well when used in conjunction with a compilation technique known as trace scheduling. Without trace scheduling, the prediction hit rate is generally very low. Unfortunately, trace scheduling is difficult to apply to some programs and implementations. The dynamic hardware prediction generally provides an adequate prediction hit rate. However, it increases the complexity of the processor design and requires additional hardware to maintain the separate branch prediction table. Further, if the size of a cache is enlarged in a redesign, the size of the table would also have to be increased, complicating the redesign process.</p><h4>SUMMARY OF THE INVENTION</h4><p>The present invention relates to a novel computer system. The computer system includes a low latency cache that stores instructions decoded to determine class, branch prediction information, and next address fetch information.</p><p>The present invention includes a cache having a plurality of cache lines. Each cache line includes (n) instructions and (n) instruction class (ICLASS) fields for storing the decoded class information of the instructions respectively. Each cache line also includes one or more branch prediction (BRPD) fields and one or more next fetch address prediction (NFAPD) fields.</p><p>When an instruction is fetched, the corresponding ICLASS field, BRPD field information and the NFAPD information are all provided to the prefetch and dispatch unit of the computer system. The ICLASS information informs the prefetch unit if the fetched instruction is a branch. Since the instruction has already been partially decoded, the need to perform a partial decode in the prefetch and dispatch unit to determine if an instruction is a branch instruction is avoided. If the instruction is a branch instruction, the BRPD field provides a prediction of either \u201cBranch Taken\u201d or \u201cBranch Not Taken\u201d. For non-branch instructions, the BRPD field is ignored. For non-branch instructions, the NFAPD typically contains the next sequential address. For branch instructions, the NFAPD contains either the next sequential address or the target address of the branch instruction. If the BRPD field contains a \u201cBranch Taken\u201d prediction, the corresponding NFAPD field typically contains the target address for the branch instruction. Alternatively, if the BRPD field contains a \u201cBranch Not Taken\u201d status, the corresponding NFAPD field typically contains the next sequential address. In any event, the NFAPD information is used to define the next line from the cache to be fetched, thereby avoiding the need to calculate the next fetch address in the prefetch unit. The prefetch and dispatch unit needs to calculate the next fetch address only when a misprediction of a branch instruction occurs. An update policy is used to correct the BRPD and the NFAPD values in the event the predictions turn out to be wrong.</p><p>The number of BRPD fields and NFAPD fields per cache line varies depending on the specific embodiment of the present invention. In one embodiment, a specific BRPD field and an NFAPD field is provided for each instruction per cache line. If there is more than one branch instruction per cache line, each branch instruction enjoys the benefit of a dedicated branch prediction and next fetch address prediction. In a simplified embodiment, one BRPD field and one NFAPD field is shared among all the instructions per cache line. Under these circumstances, only a dominant instruction in the cache line makes use of the BRPD and the NFAPD information. A dominant instruction is defined as the first branch instruction with a \u201cBranch Taken\u201d status in the cache line. For example, with a dominant instruction, the BRPD field is set to \u201cBranch Taken\u201d, and the NFAPD typically contains the target address for the dominant branch instruction. When the instruction is fetched, control is typically transferred to the target address of the dominant instruction. Since the dominant instruction is the first instruction in a cache line to cause a control transfer, it is not necessary for the other instructions in the cache line to have their own BRPD fields and NFAPD fields respectively.</p><p>The present invention represents a significant improvement over the prior art. The need to perform a partial decode or a next fetch address calculation in the prefetch and dispatch unit is eliminated with a vast majority of the fetched instructions. As such, fetch latency is significantly reduced and processor throughput is greatly enhanced.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>DESCRIPTION OF THE DRAWINGS</h4><p>The objects, features and advantages of the system of the present invention will be apparent from the following detailed description of the invention with references to the drawings in which:</p><p>FIG. 1 is a block diagram of a computer system according to the present invention.</p><p>FIG. 2 illustrates a block diagram of an instruction cache in the computer system of the present invention.</p><p>FIG. 3 illustrates a block diagram of an instruction prefetch and dispatch unit used in the computer system of the present invention.</p><p>FIGS. 4<i>a</i>-<b>4</b><i>b </i>are two flow diagrams illustrating the operation of the instruction prefetch and dispatch unit.</p><p>FIG. 5 is a flow diagram illustrating the operation of the instruction cache.</p><p>FIG. 6 illustrates exemplary line entries in the instruction cache used in the computer system of the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF THE PREFERRED EMBODIMENT</h4><p>Referring to FIG. 1, a functional block diagram illustrating a computer system of the present invention is shown. The computer system <b>10</b> includes an instruction prefetch and dispatch unit <b>12</b>, execution units <b>14</b>, an instruction cache <b>16</b>, a data cache <b>18</b>, a memory unit <b>20</b> and a memory management unit <b>22</b>. The instruction cache <b>16</b> and data cache <b>18</b> are coupled to the instruction prefetch and dispatch unit <b>12</b>, the execution units <b>14</b>, and the memory management unit <b>22</b> respectively. The prefetch and dispatch unit <b>12</b> is coupled to the execution units <b>14</b> and the memory management unit <b>22</b>. The data cache <b>18</b> is coupled to memory <b>20</b>. The instruction cache <b>16</b> is coupled to memory <b>20</b>.</p><p>Cooperatively, the memory management unit <b>22</b> and the prefetch and dispatch unit <b>12</b> fetch instructions from instruction cache <b>16</b> and data from the data cache <b>18</b> respectively and dispatch them as needed to the execution units <b>14</b>. The results of the executed instructions are then stored in the data cache <b>18</b> or main memory <b>20</b>. Except for the instruction prefetch and dispatch unit <b>12</b>, and the instruction cache <b>16</b>, the other elements, <b>14</b> and <b>18</b> through <b>22</b>, are intended to represent a broad category of these elements found in most computer systems. The components and the basic functions of these elements <b>14</b>, and <b>18</b> through <b>22</b> are well known and will not be described further. It will be appreciated that the present invention may be practiced with other computer systems having different architectures. In particular, the present invention may be practiced with a computer system having no memory management unit <b>22</b>. Furthermore, the present invention may be practiced with a unified instruction/data cache or an instruction cache only.</p><p>Referring now to FIG. 2, a block diagram illustrating the instruction cache <b>16</b> of the present invention is shown. The instruction cache <b>16</b> includes an instruction array <b>24</b>, a tag array <b>26</b>, an ICLASS array <b>27</b>, a predictive annotation array <b>28</b>, and selection logic <b>30</b>. The cache is segmented into a plurality of cache lines <b>34</b><sub>1 </sub>through <b>34</b><sub>x</sub>. Each cache line <b>34</b> includes (n) instructions in the instruction array <b>24</b>, (m) branch prediction BRPD fields <b>40</b>, (k) next address prediction NFAPD fields <b>42</b> in the predictive annotation array <b>28</b>, (n) ICLASS fields <b>44</b> in the ICLASS array <b>27</b>, and (n) tags in the tag array <b>26</b>. It also should be noted that the instruction cache <b>16</b> may be set associative. With such an embodiment, individual arrays <b>24</b> through <b>29</b> are provided for each set in the instruction cache <b>16</b>.</p><p>Each of the (n) instructions per cache line <b>34</b> contained in the instruction cache <b>16</b> are decoded to determine their class. In one embodiment, the instructions are decoded by decoder <b>17</b> and the instruction class encodings are stored in the appropriate ICLASS field <b>44</b>, when the cache line <b>34</b> is being brought into the instruction cache <b>16</b>. In an alternative embodiment, the instruction class encodings are stored before the cache line <b>34</b> is brought into the instruction cache <b>16</b>. Examples of instruction classes are the program counter (PC) relative branch, register indirect branch, memory access, arithmetic and floating point.</p><p>When the instruction cache <b>16</b> receives a next fetch address from the instruction prefetch and dispatch unit <b>12</b>, the appropriate cache line <b>34</b> is accessed. The (n) instructions, the (m) BRPD fields <b>40</b>, the (k) NFAPD fields <b>42</b>, the (n) ICLASS fields <b>44</b>, and the corresponding tag information, of the cache line are provided to the selection logic <b>30</b>. In the event the instruction cache <b>16</b> includes more than one set, then the selection logic <b>30</b> selects the proper line from the plurality of sets. With embodiments having only a single set, the selection logic <b>30</b> simply passes the accessed line <b>34</b> to the instruction prefetch and dispatch unit <b>12</b>. The set selection logic <b>30</b> is intended to represent a broad category of selection logic found in most computer systems, including the selection logic described in U.S. patent application Ser. No. 07/906,699, filed on Jun. 30, 1992, entitled Rapid Data Retries From A Data Storage Using Prior Access Predictive Annotation, assigned to the same assignee of the present invention now U.S. Pat. No. 5,392,414.</p><p>The BRPD fields <b>40</b> and NFAPD fields <b>42</b> are initialized in accordance with a pre-established policy when a cache line <b>34</b> is brought into the cache <b>16</b>. When an instruction is fetched, the corresponding ICLASS field <b>44</b> information, BRPD field <b>40</b> information and the NFAPD field <b>42</b> information are all provided to the prefetch and dispatch unit <b>12</b>. Since the instruction has already been decoded to determine class, the need to perform a full decode in the prefetch and dispatch unit <b>12</b> to determine if an instruction is a branch instruction is avoided. If the instruction is a non-branch instruction, the BRPD information is ignored. The NFAPD information, however, provides the next address to be fetched, which is typically the sequential address of the next line in the instruction cache <b>16</b>. If a predecoded instruction is a branch instruction, the corresponding BRPD field <b>40</b> contains either a \u201cBranch Taken\u201d or a \u201cBranch Not Taken\u201d prediction and the NFAPD field <b>42</b> contains a prediction of either the target address of the branch instruction or the sequential address of the next line <b>34</b> in the instruction cache <b>16</b>. Regardless of the type of instruction, the predicted next address is used to immediately fetch the next instruction.</p><p>After a branch instruction is fetched, an update policy is used to update the entries in the corresponding BRPD field <b>40</b> and the NFAPD field <b>42</b> when the actual direction of the branch instruction and the actual next fetch address is resolved in the execution units <b>14</b>. If the branch prediction and next fetch address prediction were correct, execution continues and the BRPD field <b>40</b> or the NFAPD field <b>42</b> are not altered. On the other hand, if either prediction is wrong, the BRPD field <b>40</b> and the NFAPD field <b>42</b> are updated as needed by the prefetch and dispatch unit <b>12</b>. If the misprediction caused the execution of instructions down an incorrect branch path, execution is stopped and the appropriate execution units <b>14</b> are flushed. Execution of instructions thereafter resumes along the correct path. The next time the same instruction is fetched, a branch prediction decision is made based on the updated branch prediction information in the BRPD field <b>40</b> and the next prefetch address is based on the updated contents of NFAPD field <b>42</b>.</p><p>During operation, the BRPD fields <b>40</b> and NFAPD fields <b>42</b> are updated in accordance with a specified update policy. For the sake of simplicity, only a single bit of information is used for the BRPD field <b>40</b>. This means that the BRPD field <b>40</b> can assume one of two states, either \u201cBranch Taken\u201d or \u201cBranch Not Taken\u201d. One possible update policy is best described using a number of examples, as provided below.</p><p>1. If the BRPD predicts \u201cBranch Taken\u201d and the NFAPD field contains the target address, and the actual branch is not taken, then the BRPD is updated to \u201cBranch Not Taken\u201d and the NFAPD is updated to the next sequential address.</p><p>2. If the BRPD predicts \u201cBranch Taken\u201d, and the actual branch is taken, but the the NFAPD misses, then the NFAPD is updated to the target address of the branch instruction.</p><p>3. If the BRPD predicts \u201cBranch Not Taken\u201d and the NFAPD field contains the next sequential address, and the actual branch is taken, then the BRPD is updated to \u201cBranch Taken\u201d and the NFAPD is updated to the target address of the branch instruction.</p><p>4. If the BRPD predicts \u201cBranch Not Taken\u201d, and the actual branch is not taken, but the NFAPD misses, the NFAPD is updated to the sequential address.</p><p>5. If the BRPD predicts \u201cBranch Not Taken\u201d, and the actual branch is not taken, and the NFAPD provides the next sequential address, then the BRPD and NFAPD fields are not updated.</p><p>6. If the BRPD predicts \u201cBranch Taken\u201d and the actual branch is taken and the NFAPD provides the target address, then the BRPD and NFAPD fields are not updated.</p><p>In summary, the BRPD field and the NFAPD field are updated to the actual branch taken and actual next fetch address. In alternative embodiments, more sophisticated branch prediction algorithms may be used. For example, multiple bits may be used for the BRPD field <b>42</b>, thereby providing finer granularity and more information about each branch prediction.</p><p>In one embodiment, a specific BRPD field <b>40</b> and a corresponding NFAPD field <b>42</b> is provided for each instruction per cache line <b>34</b> (i.e., n=m=k). As such, each branch instruction per cache line <b>34</b> enjoys the benefit of a dedicated branch prediction and next fetch address prediction as stored in BRPD field <b>40</b> and corresponding NFAPD field <b>42</b> respectively. In a simplified embodiment, one BRPD field <b>40</b> (i.e., m=1) and one NFAPD field <b>42</b> (i.e., k=1) is shared among all the instructions per cache line <b>34</b>. With this embodiment, only the dominant instruction in the cache line <b>34</b> makes use of the branch prediction information and the next fetch address information. A dominant instruction is defined as the first branch instruction with a \u201cBranch Taken\u201d status in the cache line <b>34</b>. Therefore, the BRPD contains a \u201cBranch Taken\u201d prediction and the corresponding NFAPD typically contains the target address for the dominating instruction. Since the dominant instruction is the first instruction in the cache line to cause a control transfer, it is not necessary for the other instructions to have their own BRPD fields <b>40</b> and NFAPD fields <b>42</b>.</p><p>It will be appreciated that the number of BRPD fields <b>40</b> and NFAPD fields <b>42</b> is design dependent. As the number of BRPD fields <b>40</b> (m) and NFAPD fields <b>42</b> (k) increases toward the number of instructions (n) per cache line <b>34</b>, the likelihood of branch and next fetch address prediction hits will increase. In contrast, as the number of BRPD fields <b>40</b> and NFAPD fields <b>42</b> approaches one, the likelihood of mispredictions increases, but the structure of cache <b>16</b> is simplified.</p><p>Referring to FIG. 3, a block diagram of the pertinent sections of the prefetch and dispatch unit <b>12</b> are shown. The prefetch and dispatch unit <b>12</b> includes a comparator <b>68</b>, a next fetch address (NFA) register <b>70</b>, an instruction queue <b>72</b>, an update unit <b>74</b>, and a dispatch unit <b>76</b>. For each instruction, the comparator <b>68</b> is coupled to receive the BRPD field <b>40</b> and the NFAPD field <b>42</b> information from instruction cache <b>16</b> and the actual branch direction and next fetch address from the execution units <b>14</b>. It should be noted that the actual branch and next fetch address typically arrive at the comparator <b>68</b> at a later point in time since a certain period of time is needed for the actual branch to resolve in the execution units <b>14</b>. The comparator <b>68</b> determines if the BRPD and the NFAPD are respectively correct, i.e., a hit. If the comparison yields a miss, the BRPD field and/or the NFAPD field <b>42</b> information is updated by update circuit <b>74</b> in accordance with the update policy described above. The updated BRPD and/or NFAPD information is then returned to the instruction cache <b>16</b>. The actual NFA also is placed in the NFA register <b>70</b>.</p><p>Referring now to FIG. 4<i>a </i>and FIG. 4<i>b, </i>two flow diagrams illustrating the operation of the prefetch and dispatch until <b>12</b> are shown. In FIG. 4<i>a, </i>the instruction prefetch and dispatch unit <b>12</b> determines if a fetch/prefetch should be initiated (block <b>94</b>). If a fetch/prefetch should be initiated, the instruction prefetch and dispatch unit <b>12</b> uses the address stored in the NFA register <b>70</b> to fetch the next instruction from instruction cache <b>16</b> (block <b>96</b>). In response, the instruction cache <b>16</b> provides the instruction prefetch and dispatch unit <b>12</b> with the requested instruction. The instruction is then placed into the instruction queue <b>72</b>. Thereafter, the instruction is dispatched by dispatch unit <b>76</b>. It should be noted that with each fetched instruction, the corresponding NFAPD value is placed in the NFA register <b>70</b> and is used to fetch the next instruction. When the comparator <b>68</b> determines that the NFAPD is incorrect, the actual NFA is placed into the NFA register <b>70</b>, and the fetching of instructions resumes at the actual NFA. The instruction prefetch and dispatch unit repeats the above process steps until the instruction queue <b>72</b> is empty or the computer system is shut down.</p><p>As shown in FIG. 4<i>b, </i>the instruction prefetch and dispatch unit <b>12</b> also receives a branch resolution signal <b>200</b> (actual branch) as the branch instruction completes execution in the execution units <b>14</b> (block <b>108</b>). The instruction prefetch and dispatch unit <b>12</b> then determines if the branch prediction is correct (diamond <b>110</b>). If the predicted branch is incorrect, the instruction prefetch and dispatch unit <b>12</b> updates the selected BRPD field <b>40</b> and the NFAPD field <b>42</b> in accordance with the above-defined update policy (block <b>114</b>). If the selected BRPD predicted the branch direction correctly, the instruction prefetch and dispatch unit <b>12</b> determines if the next address in the NFAPD field is correct (block <b>112</b>). If the selected NFAPD predicted the next fetch address incorrectly, the instruction prefetch and dispatch unit <b>12</b> updates the NFAPD (block <b>116</b>). If the NFAPD is correct, its status remains unchanged.</p><p>Referring now to FIG. 5, a flow diagram illustrating the operation of the instruction cache <b>16</b> is shown. The instruction cache <b>16</b> receives the fetch address from the instruction prefetch and dispatch unit <b>12</b> (block <b>74</b>). In response, the instruction cache <b>16</b> determines if there is a cache hit (block <b>76</b>). If there is a cache hit, selection logic <b>30</b>, if necessary, selects and provides the appropriate set of instructions and the corresponding ICLASS field <b>44</b>, BRPD field <b>40</b> and NFAPD field <b>42</b> information to the instruction prefetch and dispatch unit <b>12</b>.</p><p>If there is a cache miss, the instruction cache <b>16</b> initiates a cache fill procedure (block <b>80</b>). In one embodiment, the instructions accessed from memory <b>20</b> are provided directly to prefetch and dispatch unit <b>12</b>. Alternatively, the instructions may be provided to the instruction prefetch and dispatch unit <b>12</b> after the cache line is filled in cache <b>16</b>. As described earlier, the instructions are decoded to determine their class prior to being stored in the instruction cache <b>16</b>. Additionally, the BRPD field <b>40</b> and NFAPD field <b>42</b> are initialized in accordance with the initialization policy of the branch and next fetch address prediction algorithm (block <b>86</b>).</p><h4>OPERATION</h4><p>For the purpose of describing the operation of the present invention, several examples are provided. In the provided examples, there is only one (1) BRPD field <b>40</b> and NFAPD field <b>42</b> provided per cache line (i.e., m=k=1). For the purpose of simplifying the examples, the BRPD field <b>42</b> contains only 1 bit of information, and therefore can assume only two states; \u201cBranch Taken\u201d and \u201cBranch Not Taken\u201d.</p><p>Referring to FIG. 6, several lines <b>34</b><sub>1</sub>-<b>34</b><sub>7 </sub>of the instruction cache <b>16</b> is shown. In this example, there are four instructions (n=4) per cache line <b>34</b>. The four instructions are labeled, from left to right 4, 3, 2, 1, respectively, as illustrated in column <b>101</b> of the cache <b>16</b>. A \u201c1\u201d bit indicates that the instruction in that position is a branch instruction. A \u201c0\u201d bit indicates that the instruction is some other type of instruction, but not a branch instruction. In column <b>103</b>, the BRPD fields <b>40</b> for the cache lines <b>34</b> are provided. A single BRPD field <b>40</b> (m=1) is provided for the four instructions per cache line <b>34</b>. In the BRPD field <b>40</b>, a \u201c0\u201d value indicates a \u201cBranch Not Taken\u201d prediction and a \u201c1\u201d value indicates \u201cBranch Taken\u201d prediction. With this embodiment, the BRPD information provides the branch prediction only for the dominant instruction in the cache line. The column <b>105</b> contains the next fetch address in the NFAPD field <b>42</b>. A single NFAPD field <b>42</b> (k=1) is provided for the four instructions per cache line <b>34</b>. If the BRPD field <b>40</b> is set to \u201c0\u201d, then the corresponding NFAPD field <b>42</b> contains the address of the next sequential instruction. On the other hand, if the BRPD field <b>40</b> contains a \u201c1\u201d, then the corresponding NFAPD field <b>42</b> contains the target address of the dominant instruction in the cache line <b>34</b>.</p><p>In the first cache line <b>34</b><sub>1</sub>, the four instructions are all non-branch instructions, as indicated by the four \u201c0\u201d in column <b>101</b>. As such, the corresponding BRPD field <b>40</b> is set to \u201c0\u201d \u201cBranch Not Taken\u201d and the NFAPD field <b>42</b> is set to the sequential address.</p><p>The second and third cache lines <b>34</b><sub>2 </sub>and <b>34</b><sub>3 </sub>each include one branch instruction respectively. In the cache line <b>34</b><sub>2</sub>, the branch instruction is located in the first position, as indicated by the \u201c1\u201d in the first position of column <b>101</b>. The corresponding BRPD field is set to \u201c0\u201d, and NFAPD is set to \u201cnext sequ addr 1\u201d. Accordingly, the branch prediction is \u201cBranch Not Taken\u201d, and the NFAPD is the next sequential address (i.e., <b>34</b><sub>3</sub>). In the third cache line <b>34</b><sub>3</sub>, the first instruction is a branch instruction. The corresponding BRPD field is set to \u201c1\u201d, and NFAPD is set to \u201ctarget addr 1\u201d. The branch prediction algorithm thus predicts \u201cBranch Taken\u201d, and the next fetch address is the \u201ctarget address 1\u201d of the first instruction.</p><p>The fourth cache line <b>34</b><sub>4 </sub>and fifth cache line <b>35</b><sub>5 </sub>provide examples of cache lines <b>34</b> having two branch instructions. In both lines <b>34</b><sub>4 </sub>and <b>34</b><sub>5</sub>, the branch instructions are located in the first and third positions in column <b>101</b>. With cache line <b>34</b><sub>4</sub>, both instructions have a branch prediction set to \u201cBranch Not Taken\u201d, i.e., there are no dominant instructions. The corresponding field BRPD is therefore set to \u201c0\u201d, and NFAPD is set to \u201cnext sequ addr\u201d.</p><p>In contrast, with the fifth cache line <b>35</b><sub>5</sub>, the branch prediction algorithm predicts \u201cBranch Taken\u201d for the first branch instruction. The first instruction in the cache <b>35</b><sub>5</sub>is therefore the dominant instruction of the cache line. The corresponding BRPD field is set to \u201c1\u201d, and NFAPD is set to \u201ctarget addr 1\u201d. Since the dominant instruction will cause a control transfer, the branch prediction and next fetch address for the third instruction are not necessary.</p><p>The sixth <b>34</b><sub>6 </sub>and seventh <b>34</b><sub>7 </sub>cache lines provide two more examples of cache lines having two branch instructions. In both cache lines, the first and third instruction are branch instructions. In the sixth cache line <b>34</b><sub>6</sub>, the branch prediction is \u201cBranch Not Taken\u201d, but the prediction for the second branch instruction is, \u201cBranch Taken\u201d. Accordingly, the third instruction is considered the dominant instruction and the NFAPD field contains the target address for the third instruction of the line. Thus, BRPD is set to \u201c1\u201d, and NFAPD is set to \u201ctarget address 3\u201d. In the seventh cache line <b>34</b><sub>7</sub>, the branch prediction for both branch instructions is \u201cBranch Taken\u201d. Since the first instruction is the dominant instruction of the line, the BRPD field is set to \u201cBranch Taken\u201d \u201c1\u201d and the NFAPD field is set to \u201ctarget addr 1\u201d.</p><p>In embodiments where the number of BRPD fields <b>40</b> and NFAPD fields <b>42</b> equals the number of instructions per cache line <b>34</b> (i.e., m=n), the operation of the present invention is straight forward. The BRPD field <b>40</b> and the NFAPD field <b>42</b> for each branch instruction are used to predict the \u201cBranch Taken\u201d and next fetch address. Further, the BRPD field <b>40</b> and the NFAPD field <b>42</b> are updated in accordance with the outcome of the respective branch instruction when executed.</p><p>While the invention has been described in relationship to the embodiments shown in the accompanying figures, other alternatives, embodiments and modifications will be apparent to those skilled in the art. It is intended that the specification be only exemplary, and that the true scope and spirit of the invention be indicated by the following claims.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Robert", "last_name": "Yung", "name": ""}, {"first_name": "Kit Sang", "last_name": "Tam", "name": ""}, {"first_name": "Alfred K. W.", "last_name": "Yeung", "name": ""}, {"first_name": "William N.", "last_name": "Joy", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "SUN MICROSYSTEMS, INC."}], "ipc_classes": [{"primary": true, "label": "G06F   9/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101ALI20051220RMJP"}, {"label": "G06F   9/38        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "712238"}, {"primary": false, "label": "712E09051"}], "ecla_classes": [{"label": "G06F   9/38E2D"}], "cpc_classes": [{"label": "G06F  12/08"}, {"label": "G06F   9/3844"}, {"label": "G06F   9/3844"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1992-08-31", "application_date": "1997-02-14", "family_members": [{"ucid": "US-20020124162-A1", "titles": [{"lang": "EN", "text": "Computer system and method for fetching a next instruction"}]}, {"ucid": "EP-0586057-A2", "titles": [{"lang": "FR", "text": "(Pr\u00e9)extraction rapide et distribution d'instructions utilisant des annotations pr\u00e9dictives de (pr\u00e9)extractions ant\u00e9rieures"}, {"lang": "EN", "text": "Rapid instruction (pre)fetching and dispatching using prior (pre)fetch predictive annotations"}, {"lang": "DE", "text": "Schnelles Vorausholen und Zuteilung von Befehlen mittels vorausschauender Anmerkungen von fr\u00fcher vorausgeholten"}]}, {"ucid": "DE-69327927-D1", "titles": [{"lang": "EN", "text": "Quickly prefetch and assign commands using predictive annotations from previous ones"}, {"lang": "DE", "text": "Schnelles Vorausholen und Zuteilung von Befehlen mittels vorausschauender Anmerkungen von fr\u00fcher vorausgeholten"}]}, {"ucid": "US-6304961-B1", "titles": [{"lang": "EN", "text": "Computer system and method for fetching a next instruction"}]}, {"ucid": "EP-0586057-A3", "titles": [{"lang": "EN", "text": "RAPID INSTRUCTION (PRE)FETCHING AND DISPATCHING USING PRIOR (PRE)FETCH PREDICTIVE ANNOTATIONS"}]}, {"ucid": "KR-100287628-B1", "titles": [{"lang": "EN", "text": "Method and apparatus for preempting and naming a command at high speed using a preemptive predictive annotation"}, {"lang": "KO", "text": "\uc6b0\uc120 \uc120\ucde8 \uc608\uce21\uc801 \uc8fc\uc11d\uc744 \uc774\uc6a9\ud558\uc5ec \uace0\uc18d\uc73c\ub85c \uba85\ub839\uc744 \uc120\ucde8 \ubc0f \uc9c0\uba85\ud558\ub294 \ubc29\ubc95 \ubc0f \uc7a5\uce58"}]}, {"ucid": "JP-H06208463-A", "titles": [{"lang": "EN", "text": "METHOD AND EQUIPMENT FOR PROMPTLY DISPATCHING COMMND TO AT LEAST ONE EXECUTION DEVICE"}, {"lang": "JA", "text": "\u547d\u4ee4\u3092\u305d\u306e\u5c11\u306a\u304f\u3068\u3082\uff11\u3064\u306e\u5b9f\u884c\u88c5\u7f6e\u3078\u8fc5\u901f\u306b\u30c7\u30a3\u30b9\u30d1\u30c3\u30c1\u3059\u308b\u65b9\u6cd5\u304a\u3088\u3073\u88c5\u7f6e"}]}, {"ucid": "JP-3518770-B2", "titles": [{"lang": "JA", "text": "\u547d\u4ee4\u3092\u305d\u306e\u5c11\u306a\u304f\u3068\u3082\uff11\u3064\u306e\u5b9f\u884c\u88c5\u7f6e\u3078\u8fc5\u901f\u306b\u30c7\u30a3\u30b9\u30d1\u30c3\u30c1\u3059\u308b\u65b9\u6cd5\u304a\u3088\u3073\u88c5\u7f6e"}, {"lang": "EN", "text": "Method and apparatus for quickly dispatching instructions to at least one execution unit"}]}, {"ucid": "KR-940004436-A", "titles": [{"lang": "EN", "text": "Method and apparatus for preempting and naming a command at high speed using a preemptive predictive annotation"}, {"lang": "KO", "text": "\uc6b0\uc120 \uc120\ucde8 \uc608\uce21\uc801 \uc8fc\uc11d\uc744 \uc774\uc6a9\ud558\uc5ec \uace0\uc18d\uc73c\ub85c \uba85\ub839\uc744 \uc120\ucde8 \ubc0f \uc9c0\uba85\ud558\ub294 \ubc29\ubc95 \ubc0f \uc7a5\uce58"}]}, {"ucid": "DE-69327927-T2", "titles": [{"lang": "EN", "text": "Quickly prefetch and assign commands using predictive annotations from previous ones"}, {"lang": "DE", "text": "Schnelles Vorausholen und Zuteilung von Befehlen mittels vorausschauender Anmerkungen von fr\u00fcher vorausgeholten"}]}, {"ucid": "EP-0586057-B1", "titles": [{"lang": "FR", "text": "(Pr\u00e9)extraction rapide et distribution d'instructions utilisant des annotations pr\u00e9dictives de (pr\u00e9)extractions ant\u00e9rieures"}, {"lang": "EN", "text": "Rapid instruction (pre)fetching and dispatching using prior (pre)fetch predictive annotations"}, {"lang": "DE", "text": "Schnelles Vorausholen und Zuteilung von Befehlen mittels vorausschauender Anmerkungen von fr\u00fcher vorausgeholten"}]}]}