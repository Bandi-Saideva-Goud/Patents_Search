{"patent_number": "US-6202129-B1", "publication_id": 72607529, "family_id": 21983863, "publication_date": "2001-03-13", "titles": [{"lang": "EN", "text": "Shared cache structure for temporal and non-temporal information using indicative bits"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72525161\"><p>A method and system for providing cache memory management. The system comprises a main memory, a processor coupled to the main memory, and at least one cache memory coupled to the processor for caching of data. The at least one cache memory has at least two cache ways, each comprising a plurality of sets. Each of the plurality of sets has a bit which indicates whether one of the at least two cache ways contains non-temporal data. The processor accesses data from one of the main memory or the at least one cache memory.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00001\" num=\"1\"><claim-text>1. A computer system having cache memory management, the computer system comprising:</claim-text><claim-text>a main memory; </claim-text><claim-text>a processor coupled to said main memory, said processor to execute instructions to process non-temporal data and temporal data; </claim-text><claim-text>at least one cache memory coupled to said processor, said at least one cache memory having at least two cache ways, each of said at least two cache ways comprising a plurality of sets of data stored in said at least one cache memory, each of said plurality of sets of data having a first bit and a second bit in said at least one cache memory, said first bit of each of said plurality of sets of data to indicate whether one of said at least two cache ways in the associated set of data contains said non-temporal data that can be replaced first, said non-temporal data being infrequently used by the processor, said second bit indicative of an order of data entry in a corresponding way; and </claim-text><claim-text>wherein said processor accesses data from one of said main memory or said at least one cache memory. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00002\" num=\"2\"><claim-text>2. The computer system of claim <b>1</b>, wherein the associated set of data does not contain said non-temporal data and said order is indicative of whether said data entry is a least recently used entry by the processor with respect to other data entries in the associated set of data.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00003\" num=\"3\"><claim-text>3. The computer system of claim <b>1</b>, wherein said first bit is set to indicate that one of said at least two cache ways in the associated set of data contains said non-temporal data which will be replaced upon a cache miss.</claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00004\" num=\"4\"><claim-text>4. The computer system of claim <b>3</b>, wherein the associated set of data contains said non-temporal data and said second bit points to said non-temporal data in one of said at least two cache ways in the associated set of data.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00005\" num=\"5\"><claim-text>5. The computer system of claim <b>1</b>, wherein said first bit is cleared to indicate that none of said at least two cache ways in the associated set of data contains said non-temporal data.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00006\" num=\"6\"><claim-text>6. The computer system of claim <b>1</b>, further comprising:</claim-text><claim-text>cache control logic coupled to said at least one cache memory and said processor, said cache control logic to control said at least one cache memory. </claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00007\" num=\"7\"><claim-text>7. The computer system of claim <b>6</b>, wherein said processor receives an instruction for accessing data, said processor determining if said data is located in said at least one cache memory, if so, accessing said data from said at least one cache memory, otherwise, accessing said data from said main memory.</claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00008\" num=\"8\"><claim-text>8. The computer system of claim <b>7</b>, wherein if said data is accessed from said at least one cache memory, said cache control logic determines if said data is temporal, and, if said data is temporal and if said first bit is not set to indicate non-temporal data then updating an order of said second bit corresponding to said way that is being accessed, otherwise leaving said order unchanged.</claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00009\" num=\"9\"><claim-text>9. The computer system of claim <b>8</b>, wherein said first bit associated with the set of data of said way being accessed is unchanged.</claim-text></claim>"}, {"num": 10, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00010\" num=\"10\"><claim-text>10. The computer system of claim <b>7</b>, wherein if said data accessed from said at least one cache memory is non-temporal, said cache control logic configures said first bit to indicate that said accessed data is non-temporal, said cache control logic further updating said order of said second bit.</claim-text></claim>"}, {"num": 11, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00011\" num=\"11\"><claim-text>11. The computer system of claim <b>7</b>, wherein if said data is accessed from said main memory, said cache control logic determines if said data is non-temporal, if so, configuring said first bit to indicate that said accessed data is non-temporal, said cache control logic leaving unchanged said order of said second bit.</claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00012\" num=\"12\"><claim-text>12. The computer system of claim <b>11</b>, wherein if said cache control logic determines that said data is temporal, said cache control logic configures said first bit to indicate that said accessed data is temporal, said cache control logic updating said order of said second bit.</claim-text></claim>"}, {"num": 13, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00013\" num=\"13\"><claim-text>13. The computer system of claim <b>1</b>, wherein said first bit of each of said plurality of sets of data is a lock bit.</claim-text></claim>"}, {"num": 14, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00014\" num=\"14\"><claim-text>14. The computer system of claim <b>1</b>, wherein said non-temporal data is a type of data that is predetermined to be infrequently used by the processor.</claim-text></claim>"}, {"num": 15, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00015\" num=\"15\"><claim-text>15. The computer system of claim <b>1</b>, wherein said non-temporal data is a type of data that is streaming data that need not be cached.</claim-text></claim>"}, {"num": 16, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00016\" num=\"16\"><claim-text>16. A method in a computer system of allocating cache memory for replacement, the method comprising:</claim-text><claim-text>providing a main memory; </claim-text><claim-text>providing a processor coupled to said main memory, said processor to execute instructions to process non-temporal data and temporal data; </claim-text><claim-text>providing at least one cache memory coupled to said processor, said at least one cache memory having at least two cache ways, each of said at least two cache ways comprising a plurality of sets of data stored in said at least one cache memory, each of said plurality of sets of data having a first bit and a second bit in said at least one cache memory, said first bit of each of said plurality of sets of data to indicate whether one of said at least two cache ways in the associated set of data contains said non-temporal data that can be replaced first, said non-temporal data being infrequently used by the processor, said second bit indicative of an order of data entry in a corresponding way; and </claim-text><claim-text>accessing, by said processor, data from one of said main memory or said at least one cache memory. </claim-text></claim>"}, {"num": 17, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00017\" num=\"17\"><claim-text>17. The method of claim <b>16</b>, wherein the associated set of data does not contain said non-temporal data and said order is indicative of whether said data entry is a least recently used entry by the processor with respect to other data entries in the associated set of data.</claim-text></claim>"}, {"num": 18, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00018\" num=\"18\"><claim-text>18. The method of claim <b>16</b>, wherein said first bit is set to indicate that one of said at least two cache ways in the associated set of data contains said non-temporal data which will be replaced upon a cache miss.</claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00019\" num=\"19\"><claim-text>19. The method of claim <b>18</b>, wherein the associated set of data contains said non-temporal data and said second bit points to said non-temporal data in one of said at least two cache ways in the associated set of data.</claim-text></claim>"}, {"num": 20, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00020\" num=\"20\"><claim-text>20. The method of claim <b>16</b>, wherein said first bit is cleared to indicate that none of said at least two cache ways in the associated set of data contains said non-temporal data.</claim-text></claim>"}, {"num": 21, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00021\" num=\"21\"><claim-text>21. The method of claim <b>16</b>, further comprising:</claim-text><claim-text>providing a cache control logic coupled to said at least one cache memory and said processor, said cache control logic controlling said at least one cache memory. </claim-text></claim>"}, {"num": 22, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00022\" num=\"22\"><claim-text>22. The method of claim <b>21</b>, wherein said processor receives an instruction for accessing data, said processor determining if said data is located in said at least one cache memory, if so, accessing said data from said at least one cache memory, otherwise, accessing said data from said main memory.</claim-text></claim>"}, {"num": 23, "parent": 22, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00023\" num=\"23\"><claim-text>23. The method of claim <b>22</b>, wherein if said data is accessed from said at least one cache memory, said cache control logic determines if said data is temporal, and, if said data is temporal and if said first bit is not set to indicate non-temporal data then updating an order of said second bit corresponding to said way that is being accessed, otherwise leaving said order unchanged.</claim-text></claim>"}, {"num": 24, "parent": 22, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00024\" num=\"24\"><claim-text>24. The method of claim <b>22</b>, wherein said first bit associated with the set of data of said way being accessed is unchanged.</claim-text></claim>"}, {"num": 25, "parent": 22, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00025\" num=\"25\"><claim-text>25. The method of claim <b>22</b>, wherein if said data accessed from said at least one cache memory is non-temporal, said cache control logic configures said first bit to indicate that said accessed data is non-temporal, said cache control logic further updating said order of said second bit.</claim-text></claim>"}, {"num": 26, "parent": 22, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00026\" num=\"26\"><claim-text>26. The method of claim <b>22</b>, wherein if said data is accessed from said main memory, said cache control logic determines if said data is non-temporal, if so, configuring said first bit to indicate that said accessed data is non-temporal, said cache control logic leaving unchanged said order of said second bit.</claim-text></claim>"}, {"num": 27, "parent": 26, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00027\" num=\"27\"><claim-text>27. The method of claim <b>26</b>, wherein if said cache control logic determines that said data is temporal, said cache control logic configures said first bit to indicate that said accessed data is temporal, said cache control logic updating said order of said second bit.</claim-text></claim>"}, {"num": 28, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00028\" num=\"28\"><claim-text>28. The method of claim <b>16</b>, wherein said first bit of each of said plurality of sets of data is a lock bit.</claim-text></claim>"}, {"num": 29, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00029\" num=\"29\"><claim-text>29. The method of claim <b>16</b>, wherein said non-temporal data is a type of data that is predetermined to be infrequently used by the processor.</claim-text></claim>"}, {"num": 30, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00030\" num=\"30\"><claim-text>30. The method of claim <b>16</b>, wherein said non-temporal data is a type of data that is streaming data that need not be cached.</claim-text></claim>"}, {"num": 31, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00031\" num=\"31\"><claim-text>31. A data cache memory for storing temporal and non-temporal data therein, the data cache memory comprising:</claim-text><claim-text>one or more cache sets of data, each of the one or more cache sets of data comprising </claim-text><claim-text>at least two ways, each of the at least two ways to store data for the associated cache set of data; </claim-text><claim-text>one or more least recently used bits, the least recently used bits in one case to indicate the least recently accessed way of the associated cache set of data by a processor; and </claim-text><claim-text>a lock bit, the lock bit to indicate whether any one of the at least two ways within the associated cache set of data contains the non-temporal data that can be replaced first. </claim-text></claim>"}, {"num": 32, "parent": 31, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00032\" num=\"32\"><claim-text>32. The data cache memory of claim <b>31</b>, wherein the non-temporal data is a type of data that is predetermined to be infrequently used by the processor.</claim-text></claim>"}, {"num": 33, "parent": 31, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00033\" num=\"33\"><claim-text>33. The data cache memory of claim <b>31</b>, wherein the non-temporal data is a type of data that is streaming data that need not be cached.</claim-text></claim>"}, {"num": 34, "parent": 31, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00034\" num=\"34\"><claim-text>34. The data cache memory of claim <b>31</b>, wherein the lock bit is set for one cache set of data of the one or more cache sets of data and,</claim-text><claim-text>the lock bit indicating that the one cache set of data contains the non-temporal data </claim-text><claim-text>and </claim-text><claim-text>the one or more least recently used bits are updated to indicate which way of the at least two ways contains the non-temporal data. </claim-text></claim>"}, {"num": 35, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00035\" num=\"35\"><claim-text>35. A method for a data cache memory storing temporal and non-temporal data of indicating a non-temporal data entry that can be replaced with other data, the method comprising:</claim-text><claim-text>providing a cache memory having, </claim-text><claim-text>one or more sets of data, each of the one or more sets of data comprising </claim-text><claim-text>at least two ways, each of the at least two ways to store data for the associated set of data, </claim-text><claim-text>one or more least recently used bits, the least recently used bits in one case to indicate the least recently accessed way of the associated set of data by a processor, and </claim-text><claim-text>a lock bit, the lock bit to indicate whether any one of the at least two ways within the associated set of data contains the non-temporal data that can be replaced first; </claim-text><claim-text>determining whether data is temporal or non-temporal from a locality hint associated with an instruction processing the data; and </claim-text><claim-text>setting the lock bit for an associated set of data in the case that data stored into a way of the associated set of data is the non-temporal data. </claim-text></claim>"}, {"num": 36, "parent": 35, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00036\" num=\"36\"><claim-text>36. The method of claim <b>35</b>, wherein the non-temporal data is a type of data that is predetermined to be infrequently used.</claim-text></claim>"}, {"num": 37, "parent": 35, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00037\" num=\"37\"><claim-text>37. The method of claim <b>35</b>, wherein the non-temporal data is a type of data that is streaming data that need not be cached.</claim-text></claim>"}, {"num": 38, "parent": 35, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00038\" num=\"38\"><claim-text>38. The method of claim <b>35</b>, further comprising:</claim-text><claim-text>updating the one or more least recently used bits to indicate which way of the at least two ways contains the non-temporal data. </claim-text></claim>"}, {"num": 39, "parent": 35, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00039\" num=\"39\"><claim-text>39. The method of claim <b>35</b>, further comprising:</claim-text><claim-text>clearing the lock bit for an associated set of data in the case that data stored into a way of the associated set upon a cache miss is temporal. </claim-text></claim>"}, {"num": 40, "parent": 35, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00040\" num=\"40\"><claim-text>40. The method of claim <b>35</b>, wherein the instruction processing the data with the locality hint is a non-temporal instruction.</claim-text></claim>"}, {"num": 41, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00041\" num=\"41\"><claim-text>41. A method of clearing non-temporal data pollution in a data cache memory storing temporal and non-temporal data, the method comprising:</claim-text><claim-text>dynamically setting a first bit in a cache set of data in the data cache memory in response to a cache miss to the cache set during execution of a non-temporal instruction, the first bit indicating one of at least two ways within the cache set of data contains the non-temporal data, a second bit in the cache set of data indicating an order of data entry in the at least two ways within the cache set of data; </claim-text><claim-text>replacing the non-temporal data in one of the at least two ways with the temporal data in response to the first bit being set and a cache miss to the cache set during execution of a temporal instruction; and </claim-text><claim-text>dynamically clearing the first bit in the cache set of data in response to replacing the non-temporal data in the one of the at least two ways with the temporal data. </claim-text></claim>"}, {"num": 42, "parent": 41, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00042\" num=\"42\"><claim-text>42. The method of claim <b>41</b>, wherein the non-temporal data is a type of data that is predetermined to be infrequently used.</claim-text></claim>"}, {"num": 43, "parent": 41, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00043\" num=\"43\"><claim-text>43. The method of claim <b>41</b>, wherein the non-temporal data is a type of data that is streaming data that need not be cached.</claim-text></claim>"}, {"num": 44, "parent": 41, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00044\" num=\"44\"><claim-text>44. The method of claim <b>41</b>, wherein,</claim-text><claim-text>the data cache memory includes a plurality of cache sets of data and the dynamically setting of the first bit, the dynamically clearing of the first bit and the replacing non temporal data is performed one cache set at a time during instruction access to each cache set of the plurality of cache sets of data within the data cache memory. </claim-text></claim>"}, {"num": 45, "parent": 41, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00045\" num=\"45\"><claim-text>45. The method of claim <b>41</b>, further comprising:</claim-text><claim-text>updating the second bit of the cache set of data to point to which way of the at least two ways contains the non-temporal data in response to the cache miss to the cache set during execution of the non-temporal instruction. </claim-text></claim>"}, {"num": 46, "parent": 45, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00046\" num=\"46\"><claim-text>46. The method of claim <b>45</b>, further comprising:</claim-text><claim-text>maintaining the setting of the first bit and maintaining the second bit to point to which way of the at least two ways contains the non-temporal data in response to the cache hit to the cache set during execution of the temporal instruction. </claim-text></claim>"}, {"num": 47, "parent": 45, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00047\" num=\"47\"><claim-text>47. The method of claim <b>45</b>, further comprising:</claim-text><claim-text>maintaining the first bit set, and the second bit is not updated so as to remain pointing to which way of the at least two ways contains the non-temporal data in response to a cache hit to the cache set during execution of the non-temporal instruction and a mode bit being cleared. </claim-text></claim>"}, {"num": 48, "parent": 45, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00048\" num=\"48\"><claim-text>48. The method of claim <b>45</b>, further comprising:</claim-text><claim-text>setting the first bit in response to a cache hit to the cache set during execution of the non-temporal instruction and a mode bit being set. </claim-text></claim>"}, {"num": 49, "parent": 48, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00049\" num=\"49\"><claim-text>49. The method of claim <b>48</b>, further comprising:</claim-text><claim-text>updating the second bit of the cache set of data to point to which way of the at least two ways was hit in response to the cache hit to the cache set during execution of the non-temporal instruction and the mode bit being set. </claim-text></claim>"}, {"num": 50, "parent": 41, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00050\" num=\"50\"><claim-text>50. The method of claim <b>41</b>, further comprising:</claim-text><claim-text>maintaining the setting of the first bit in the cache set of data in response to a cache hit to the cache set during execution of the temporal instruction. </claim-text></claim>"}, {"num": 51, "parent": 41, "type": "dependent", "paragraph_markup": "<claim id=\"US-6202129-B1-CLM-00051\" num=\"51\"><claim-text>51. The method of claim <b>41</b>, wherein,</claim-text><claim-text>the first bit is a lock bit.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54510781\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>The present invention relates in general to the field of processors, and in particular, to a technique of providing a shared cache structure for temporal and non-temporal instructions.</p><p>2. Description of the Related Art</p><p>The use of a cache memory with a processor facilitates the reduction of memory access time. The fundamental idea of cache organization is that by keeping the most frequently accessed instructions and data in the fast cache memory, the average memory access time will approach the access time of the cache. To achieve the maximum possible speed of operation, typical processors implement a cache hierarchy, that is, different levels of cache memory. The different levels of cache correspond to different distances from the processor core. The closer the cache is to the processor, the faster the data access. However, the faster the data access, the more costly it is to store data. As a result, the closer the cache level, the faster and smaller the cache.</p><p>The performance of cache memory is frequently measured in terms of its hit ratio. When the processor refers to memory and finds the word in cache, it is said to produce a hit. If the word is not found in cache, then it is in main memory and it counts as a miss. If a miss occurs, then an allocation is made at the entry indexed by the access. The access can be for loading data to the processor or storing data from the processor to memory. The cached information is retained by the cache memory until it is no longer needed, made invalid or replaced by other data, in which instances the cache entry is de-allocated.</p><p>In processors implementing a cache hierarchy, such as the Pentium Pro\u2122 processors which have an L1 and an L2 cache, the faster and smaller L1 cache is located closer to the processor than the L2 cache. When the processor requests cacheable data, for example, a load instruction, the request is first sent to the L1 cache. If the requested data is in the L1 cache, it is provided to the processor. Otherwise, there is an L1 miss and the request is transferred to the L2 cache. Likewise, if there is an L2 cache hit, the data is passed to the L1 cache and the processor core. If there is an L2 cache miss, the request is transferred to main memory. The main memory responds to the L2 cache miss by providing the requested data to the L2 cache, the L1 cache, and to the processor core.</p><p>The type of data that is typically stored in cache includes active portions of programs and data. When the cache is full, it is necessary to replace existing lines of stored data in the cache memory to make room for newly requested lines of data. One such replacement technique involves the use of the least recently used (LRU) algorithm, which replaces the least recently used line of data with the newly requested line. In the Pentium Pro\u2122 processors, since the L2 cache is larger than the L1 cache, the L2 cache typically stores everything in the L1 cache and some additional lines that have been replaced in the L1 cache by the LRU algorithm.</p><p>U.S. Pat. application Ser. No. 08/767,950 filed Dec. 17, 1996 now U.S. Pat. No. 5,829,025, entitled \u201cComputer System and Method of Allocating Cache Memories in a Multilevel Cache Hierarchy utilizing a Locality Hint within an Instruction\u201d by Milland Mittal discloses a technique for allocating cache memory through the use of a locality hint associated with an instruction. When a processor accesses memory for transfer of data between the processor and the memory, that access can be allocated to the various levels of cache, or not allocated to cache memory at all, according to the locality hint associated with the instruction. Certain instructions are used infrequently. For example, non-temporal prefetch instructions preload data which the processor does not require immediately, but which are anticipated to be required in the near future. Such data is typically used only once or will not be reused in the immediate future, and is termed \u201cnon-temporal data\u201d. Instructions that are frequently used are termed \u201ctemporal data\u201d. For non-temporal data, since the data is used infrequently, optimal performance dictates that the cached data not be overwritten by this infrequently used data. U.S. Pat. No. 5,829,025 solves this problem by providing a buffer, separate from the cache memory, for storing the infrequently used data, such as non-temporal prefetched data. However, the use of an extra, separate buffer is expensive both in terms of cost and space.</p><p>Accordingly, there is a need in the technology for providing a shared cache structure for temporal and non-temporal instructions, which eliminates the use of a separate buffer.</p><h4>BRIEF SUMMARY OF THE INVENTION</h4><p>A method and system for providing cache memory management. The system comprises a main memory, a processor coupled to the main memory, and at least one cache memory coupled to the processor for caching of data. The at least one cache memory has at least two cache ways, each comprising a plurality of sets. Each of the plurality of sets has a bit which indicates whether one of the at least two cache ways contains non-temporal data. The processor accesses data from one of the main memory or the at least one cache memory.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The invention is illustrated by way of example, and not limitation, in the figures. Like reference indicate similar elements.</p><p>FIG. 1 illustrates a circuit block diagram of one embodiment of a computer system which implements the present invention, in which a cache memory is used for data accesses between a main memory and a processor of the computer system.</p><p>FIG. 2 is a circuit block diagram of a second embodiment of a computer system which implements the present invention, in which two cache memories are arranged into cache memory levels for accessing data between a main memory and a processor(s) of the computer system.</p><p>FIG. 3 is a block diagram illustrating one embodiment of the organizational structure of the cache memory in which the technique of the present invention is implemented.</p><p>FIG. 4 is a table illustrating the cache management technique, according to one embodiment of the present invention.</p><p>FIGS. 5A and 5B illustrate one example of the organization of a cache memory prior to and after temporal instruction hits way 2 of cache set 0, according to one embodiment of the present invention.</p><p>FIGS. 6A and 6B illustrate another example of the organization of a cache memory prior to and after temporal instruction hits way 2 of cache set 0, according to one embodiment of the present invention.</p><p>FIGS. 7A-7D illustrate a example of the organization of a cache memory prior to and after a non-temporal instruction hits way 2 of cache set 0, according to one embodiment of the present invention.</p><p>FIGS. 8A-8D illustrate another example of the organization of a cache memory prior to and after a non-temporal instruction hits way 2 of cache set 0, according to one embodiment of the present invention.</p><p>FIGS. 9A and 9B illustrate one example of the organization of a cache memory prior to and after a temporal instruction miss to cache set 0, according to one embodiment of the present invention.</p><p>FIGS. 10A and 10B illustrate an example of the organization of a cache memory prior to and after a non-temporal instruction miss to cache set 0, according to one embodiment of the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF THE INVENTION</h4><p>A technique is described for providing management of cache memories, in which cache allocation is determined by data utilization. In the following description, numerous specific details are set forth, such as specific memory devices, circuit diagrams, processor instructions, etc., in order to provide a thorough understanding of the present invention. However, it will be appreciated by one skilled in the art that the present invention may be practiced without these specific details. In other instances, well known techniques and structures have not been described in detail in order not to obscure the present invention. It is to be noted that a particular implementation is described as a preferred embodiment of the present invention, however, it is readily understood that other embodiments can be designed and implemented without departing from the spirit and scope of the present invention. Furthermore, it is appreciated that the present invention is described in reference to a serially arranged cache hierarchy system, but it need not be limited strictly to such a hierarchy.</p><p>Referring to FIG. 1, a typical computer system is shown, wherein a processor <b>10</b>, which forms the central processing unit (CPU) of the computer system is coupled to a main memory <b>11</b> by a bus <b>14</b>. The main memory <b>11</b> is typically comprised of a random-access-memory and is usually referred to as RAM. Subsequently, the main memory <b>11</b> is generally coupled to a mass storage device <b>12</b>, such as a magnetic or optical memory device, for mass storage (or saving) of information. A cache memory <b>13</b> (hereinafter also referred simply as cache) is coupled to the bus <b>14</b> as well. The cache <b>13</b> is shown located between the CPU <b>11</b> and the main memory <b>11</b>, in order to exemplify the functional utilization and transfer of data associated with the cache <b>13</b>. It is appreciated that the actual physical placement of the cache <b>13</b> can vary depending on the system and the processor architecture. Furthermore, a cache controller <b>15</b> is shown coupled to the cache <b>13</b> and the bus <b>14</b> for controlling the operation of the cache <b>13</b>. The operation of a cache controller, such as the controller <b>15</b>, is known in the art and, accordingly, in the subsequent Figures, cache controllers are not illustrated. It is presumed that some controller(s) is/are present under control of the CPU <b>10</b> to control the operation of cache(s) shown.</p><p>In operation, information transfer between the memory <b>11</b> and the CPU <b>10</b> is achieved by memory accesses from the CPU <b>10</b>. When cacheable data is currently or shortly to be accessed by the CPU <b>10</b>, that data is first allocated in the cache <b>13</b>. That is, when the CPU <b>10</b> accesses a given information from the memory <b>11</b>, it seeks the information from the cache <b>13</b>. If the accessed data is in the cache <b>13</b>, a \u201chit\u201d occurs. Otherwise, a \u201cmiss\u201d results and cache allocation for the data is sought. As currently practiced, most accesses (whether load or store) require the allocation of the cache <b>13</b>. Only uncacheable accesses are not allocated in the cache.</p><p>Referring to FIG. 2, a computer system implementing a multiple cache arrangement is shown. The CPU <b>10</b> is still coupled to the main memory <b>11</b> by the bus <b>14</b> and the memory <b>11</b> is then coupled to the mass storage device <b>12</b>. However, in the example of FIG. 2, two separate cache memories <b>21</b> and <b>22</b> are shown. The caches <b>21</b>-<b>22</b> are shown arranged serially and each is representative of a cache level, referred to as Level <b>1</b> (L1) cache and Level <b>2</b> (L2) cache, respectively. Furthermore, the L1 cache <b>21</b> is shown as part of the CPU <b>10</b>, while the L2 cache <b>22</b> is shown external to the CPU <b>10</b>. This structure exemplifies the current practice of placing the L1 cache on the processor chip while lower level caches are placed external to it, where the lower level caches are further from the processor core. The actual placement of the various cache memories is a design choice or dictated by the processor architecture. Thus, it is appreciated that the L1 cache could be placed external to the CPU <b>10</b>.</p><p>Generally, CPU <b>10</b> includes an execution unit <b>23</b>, register file <b>24</b> and fetch/decoder unit <b>25</b>. The execution unit <b>23</b> is the processing core of the CPU <b>10</b> for executing the various arithmetic (or non-memory) processor instructions. The register file <b>24</b> is a set of general purpose registers for storing (or saving) various information required by the execution unit <b>23</b>. There may be more than one register file in more advanced systems. The fetch/decoder unit <b>25</b> fetches instructions from a storage location (such as the main memory <b>11</b>) holding the instructions of a program that will be executed and decodes these instructions for execution by the execution unit <b>23</b>. In more advanced processors utilizing pipelined architecture, future instructions are prefetched and decoded before the instructions are actually needed so that the processor is not idle waiting for the instructions to be fetched when needed.</p><p>The various units <b>23</b>-<b>25</b> of the CPU <b>10</b> are coupled to an internal bus structure <b>27</b>. A bus interface unit (BIU) <b>26</b> provides an interface for coupling the various units of CPU <b>10</b> to the bus <b>14</b>. As shown in FIG. 2, the L1 cache is coupled to the internal bus <b>27</b> and functions as an internal cache for the CPU <b>10</b>. However, again it is to be emphasized that the L1 cache could reside outside of the CPU <b>10</b> and coupled to the bus <b>14</b>. The caches can be used to cache data, instructions or both. In some systems, the L1 cache is actually split into two sections, one section for caching data and one section for caching instructions. However, for simplicity of explanation, the various caches described in the Figures are shown as single caches with data, instructions and other information all referenced herein as data. It is appreciated that the operations of the units shown in FIG. 2 are known. Furthermore it is appreciated that the CPU <b>10</b> actually includes many more components than just the components shown. Thus, only those structures pertinent to the understanding of the present invention are shown in FIG. <b>2</b>. In one embodiment, the invention is utilized in systems having data caches. However, the invention is applicable to any type of cache.</p><p>It is also to be noted that the computer system may be comprised of more than one CPU (as shown by the dotted line in FIG. <b>2</b>). In such a system, it is typical for multiple CPUs to share the main memory <b>11</b> and/or mass storage unit <b>12</b>. Accordingly, some or all of the caches associated with the computer system may be shared by the various processors of the computer system. For example, with the system of FIG. 2, L1 cache <b>21</b> of each processor would be utilized by its processor only, but the main memory <b>11</b> would be shared by all of the CPUs of the system. In addition, each CPU has an associated external L2 cache <b>22</b>.</p><p>The invention can be practiced in a single CPU computer system or in a multiple CPU computer system. It is further noted that other types of units (other than processors) which access memory can function equivalently to the CPUs described herein and, therefore, are capable of performing the memory accessing functions similar to the described CPUs. For example, direct memory accessing (DMA) devices can readily access memory similar to the processors described herein. Thus, a computer system having one processor (CPU), but one or more of the memory accessing units would function equivalent to the multiple processor system shown and described herein.</p><p>As noted, only two caches <b>21</b>-<b>22</b> are shown. However, the computer system need not be limited to only two levels of cache. It is now a practice to utilize a third level (L3) cache in more advanced systems. It is also the practice to have a serial arrangement of cache memories so that data cached in the L1 cache is also cached in the L2 cache. If there happens to be an L3 cache, then data cached in the L2 cache is typically cached in the L3 cache as well. Thus, data cached at a particular cache level is also cached at all higher levels of the cache hierarchy.</p><p>FIG. 3 is a block diagram illustrating one embodiment of the organizational structure of the cache memory in which the technique of the present invention is implemented. In general, there are \u201cx\u201d sets in a cache structure, \u201cy\u201d ways per set (where y\u22672), and where each way contains one data entry or one cache line. The invention provides an LRU lock bit per cache set which indicates whether any one of the ways within that set contains non-temporal (NT) data. If so, the regular or pseudo LRU bits will be updated to point to the NT data. There are also \u201cz\u201d regular or pseudo LRU bits per set. Unless the LRU lock bit is set, the regular or pseudo LRU bits point to the way within the set in accordance with the least recently used technique implemented. The number of regular or pseudo-LRU bits per set varies depending on the number of ways per set and the LRU (regular or pseudo) technique implemented.</p><p>In the embodiment as shown, the cache <b>50</b> is organized as a four-way set associative cache. In the example of FIG. 3, each page is shown as being equal to one-fourth the cache size. In particular, the cache <b>50</b> is divided into four ways (for example, way 0 (<b>52</b>), way 1 (<b>54</b>), way 2 (<b>56</b>) and way 3 (<b>58</b>)) of equal size and main memory <b>11</b> (see also FIGS. 1 and 2) is viewed as divided into pages (e.g., page 0-page n). In another embodiment, each page may be larger or smaller than the cache size. The organizational structure of cache <b>50</b> (as shown in FIG. 3) may be implemented within the cache <b>13</b> of FIG. 1, the L1 cache <b>21</b> and/or L2 cache <b>22</b> of FIG. <b>2</b>.</p><p>The cache <b>50</b> also includes an array of least recently used (LRU) bits <b>60</b><sub>0</sub>-<b>60</b><sub>n </sub>each of which points to the way within a set with the least recently used data (or NT data, if a biased LRU technique is implemented). Such listing is performed in accordance with an LRU technique under the control of the cache controller <b>15</b>, to determine which cache entry to overwrite in the event that a cache set is full. The LRU logic (not shown) keeps track of the cache locations within a set that have been least recently used. In one embodiment, an LRU technique that strictly keeps track of the least-recently used directory algorithm may be implemented. In one alternate embodiment, a pseudo-LRU algorithm which makes a best attempt at keeping track of the least recently used directory element, is implemented. For discussion purposes, the bits <b>60</b><sub>0</sub>-<b>60</b><sub>n </sub>will be referred to as LRU bits <b>60</b><sub>0</sub>-<b>60</b><sub>n</sub>, while the array of LRU bits <b>60</b><sub>0</sub>-<b>60</b><sub>n </sub>will be referred to herein as LRU bit <b>60</b>.</p><p>The cache <b>50</b> further includes an array of LRU lock bits <b>70</b><sub>0</sub>-<b>70</b><sub>n </sub>which indicates whether any of the ways <b>52</b>, <b>54</b>, <b>56</b>, <b>58</b> within a given set contains data that should not pollute the cache <b>50</b> (i.e., data with infrequent usage), as described in detail in the following sections.</p><p>FIG. 4 is a table illustrating the cache management technique in accordance with the principles of the present invention. The invention utilizes the array of LRU lock bits <b>70</b><sub>0</sub>-<b>70</b><sub>n </sub>to indicate whether any of the corresponding cached data is streaming or non-temporal, and as such, would be the first entry to be replaced upon a cache miss to the corresponding set. In one embodiment, the LRU lock bit <b>70</b>, when set to 1, indicates that the corresponding set has an entry that is non-temporal. If the LRU lock bit <b>70</b> is cleared, upon a cache hit by a temporal instruction, the corresponding LRU bit(s) <b>60</b> is(are) updated in accordance with the LRU technique implemented (see item <b>1</b> of FIG. 4) and the associated LRU lock bit is not updated. However, if the LRU lock bit <b>70</b> is already set to 1 (indicating that the corresponding set has a non-temporal instruction), the LRU lock bit <b>70</b> is not updated, and the LRU bit <b>60</b> is not updated (see item <b>2</b> ).</p><p>In the case of a cache hit by a non-temporal instruction, the LRU bit <b>60</b> and the LRU lock bit <b>70</b> are not updated, regardless of the status of the LRU lock bit <b>70</b> (see item <b>3</b>). In an alternate embodiment, as controlled through a mode bit in a control register in the L1 cache controller, cache hits by a streaming or non-temporal instructions force the LRU bits to the way that was hit (see item <b>4</b>). In addition, the LRU lock bit <b>70</b> is set to 1. In this embodiment, the data hit by the streaming or non-temporal instruction will be the first to be replaced upon a cache miss to the corresponding set.</p><p>Upon a cache miss by a temporal instruction, the LRU lock bit is cleared and the LRU bit <b>60</b> is updated (item <b>5</b>) based on a pseudo LRU technique. However, upon a cache miss by a streaming or non-temporal instruction, the LRU lock bit <b>70</b> is set to 1 and the corresponding LRU bit <b>60</b> is not updated (item <b>6</b>).</p><p>Examples of each of the items provided in the table of FIG. 4 will now be discussed. FIGS. 5A and 5B illustrate one example of the organization of a cache memory prior to and after temporal instruction hits way 2 of cache set 0. This example corresponds to item <b>1</b> of FIG. <b>4</b>. Here, LRU lock bit <b>70</b><sub>0 </sub>had been previously cleared for cache set 0, and since the cache set 0 was hit by a temporal instruction, the LRU lock bit <b>70</b><sub>0 </sub>is not updated. However, the LRU bit <b>60</b><sub>0 </sub>is updated in accordance with the LRU technique implemented. In the example, it is assumed that the pseudo LRU technique indicates that way 3 is the least recently used entry.</p><p>FIGS. 6A and 6B illustrate another example of the organization of a cache memory prior to and after temporal instruction hits way 2 of cache set 0. This example corresponds to item <b>2</b> of FIG. <b>4</b>. Here, LRU lock bit <b>70</b><sub>0 </sub>had been previously set for cache set 0, indicating that the corresponding set contains non-temporal data. Accordingly, neither the LRU lock bit <b>70</b><sub>0 </sub>nor the LRU bit <b>60</b><sub>0 </sub>is updated.</p><p>FIGS. 7A-7D illustrate an example of the organization of a cache memory prior to and after a non-temporal instruction hits way 2 of cache set 0. This example corresponds to item <b>3</b> of FIG. <b>4</b> and may be implemented by setting a mode bit located in the L1 cache controller to zero (see FIG. <b>4</b>). In the first case (FIGS. <b>7</b>A and <b>7</b>B), LRU lock bit <b>70</b><sub>0 </sub>had been previously cleared for cache set 0. In this embodiment, a non-temporal cache hit does not update the LRU lock bit <b>70</b>. Accordingly, since the cache set 0 was hit by a non-temporal instruction, neither the LRU lock bit <b>70</b><sub>0 </sub>nor the LRU bit <b>60</b><sub>0 </sub>is updated. In the second case (FIGS. <b>7</b>C and <b>7</b>D), LRU lock bit <b>70</b><sub>0 </sub>had been previously set for cache set 0, indicating that the corresponding set contains non-temporal data. Accordingly, neither the LRU lock bit <b>70</b><sub>0 </sub>nor the LRU bit <b>60</b><sub>0 </sub>is updated.</p><p>FIGS. 8A-8D illustrate another example of the organization of a cache memory prior to and after a non-temporal instruction hits way 2 of cache set 0. This example corresponds to item <b>4</b> of FIG. <b>4</b> and may be implemented by setting the mode bit located in the L1 cache controller to one (see FIG. <b>4</b>). In the first case (FIGS. <b>8</b>A and <b>8</b>B), LRU lock bit <b>70</b><sub>0 </sub>had been previously cleared for cache set 0. In this example of an alternate embodiment to that example shown in FIGS. 7A-7D, a non-temporal cache hit updates the LRU lock bit <b>70</b>. Accordingly, as shown in FIG. 8A, since the cache set 0 was hit by a non-temporal instruction, the LRU lock bit <b>70</b><sub>0 </sub>is updated (set to 1), as shown in FIG. <b>8</b>B. In addition, the LRU bits <b>60</b><sub>0 </sub>are updated to indicate the way that was hit. In the case where LRU lock bit <b>70</b><sub>0 </sub>had been previously set for cache set 0 (FIGS. <b>8</b>C and <b>8</b>D), the LRU lock bit <b>70</b><sub>0 </sub>remains set to 1. In addition, the LRU bits <b>60</b><sub>0 </sub>are forced to point to the way within the set that was hit.</p><p>FIGS. 9A and 9B illustrate one example of the organization of a cache memory prior to and after a temporal instruction miss to cache set 0. This example corresponds to item <b>5</b> of FIG. <b>4</b>. Here, LRU lock bit <b>70</b><sub>0 </sub>had been previously set for cache set 0, and since there is a miss by a temporal instruction targeting set 0, the LRU lock bit <b>70</b><sub>0 </sub>is cleared for that set, upon replacing the temporal miss in the cache. However, the LRU bit <b>60</b><sub>0 </sub>is updated in accordance with the LRU technique implemented. In the example, the pseudo LRU technique indicates that way 3 is the least recently used entry.</p><p>FIGS. 10A-10B illustrate an example of the organization of a cache memory prior to and after a non-temporal instruction miss to cache set 0. This example corresponds to item <b>6</b> of FIG. <b>4</b>. In this case, LRU lock bit <b>70</b><sub>0 </sub>had been previously cleared for cache set 0. Since there is a non-temporal miss to cache set 0, the LRU lock bit <b>70</b><sub>0 </sub>is set and the LRU bits <b>60</b><sub>0 </sub>remain the same, in order to point to the non-temporal data in the corresponding set 0.</p><p>By implementing the apparatus and method of the present invention, a shared cache structure for managing temporal and non-temporal instructions, which minimizes data pollution in cache or cache hierarchy is provided. Implementation of the present invention also eliminates the use of a separate buffer, making its implementation both cost effective and efficient.</p><p>The present invention may be embodied in other specific forms without departing from its spirit or essential characteristics. The described embodiments are to be considered in all respects only as illustrative and not restrictive. The scope of the invention is, therefore, indicated by the appended claims rather than the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Salvador", "last_name": "Palanca", "name": ""}, {"first_name": "Niranjan L.", "last_name": "Cooray", "name": ""}, {"first_name": "Angad", "last_name": "Narang", "name": ""}, {"first_name": "Vladimir", "last_name": "Pentkovski", "name": ""}, {"first_name": "Steve", "last_name": "Tsai", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTEL CORPORATION"}, {"first_name": "", "last_name": "INTEL CORPORATION", "name": ""}, {"first_name": "", "last_name": "INTEL CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMUS"}, {"label": "G06F  12/12        20060101A I20051008RMJP"}], "national_classes": [{"primary": true, "label": "711133"}, {"primary": false, "label": "711145"}, {"primary": false, "label": "711160"}, {"primary": false, "label": "711136"}, {"primary": false, "label": "711E1202"}, {"primary": false, "label": "711128"}, {"primary": false, "label": "711159"}, {"primary": false, "label": "711E12075"}, {"primary": false, "label": "711E12018"}], "ecla_classes": [{"label": "G06F  12/12B6"}, {"label": "S06F12:08B8"}, {"label": "G06F  12/08B14"}, {"label": "G06F  12/08B10"}], "cpc_classes": [{"label": "G06F  12/126"}, {"label": "G06F  12/0875"}, {"label": "G06F  12/0862"}, {"label": "G06F  12/0864"}, {"label": "G06F  12/0875"}, {"label": "G06F  12/126"}, {"label": "G06F  12/0862"}, {"label": "G06F  12/0864"}, {"label": "G06F  12/08"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1998-03-31", "application_date": "1998-03-31", "family_members": [{"ucid": "RU-2212704-C2", "titles": [{"lang": "RU", "text": "\u0421\u0422\u0420\u0423\u041a\u0422\u0423\u0420\u0410 \u0421\u041e\u0412\u041c\u0415\u0421\u0422\u041d\u041e \u0418\u0421\u041f\u041e\u041b\u042c\u0417\u0423\u0415\u041c\u041e\u0413\u041e \u041a\u042d\u0428\u0410 \u0414\u041b\u042f \u0412\u0420\u0415\u041c\u0415\u041d\u041d\u042b\u0425 \u0418 \u041d\u0415\u0412\u0420\u0415\u041c\u0415\u041d\u041d\u042b\u0425 \u041a\u041e\u041c\u0410\u041d\u0414"}, {"lang": "EN", "text": "SHARED CACHE STRUCTURE FOR TIMING AND NON-TIMING COMMANDS"}]}, {"ucid": "US-20020007441-A1", "titles": [{"lang": "EN", "text": "Shared cache structure for temporal and non-temporal instructions"}]}, {"ucid": "TW-573252-B", "titles": [{"lang": "EN", "text": "Shared cache structure for temporal and non-temporal instructions"}]}, {"ucid": "KR-100389549-B1", "titles": [{"lang": "EN", "text": "SHARED CACHE STRUCTURE FOR TEMPORAL AND NON-TEMPORAL INSTRUCTIONS"}, {"lang": "KO", "text": "\ud15c\ud3ec\ub7f4 \ubc0f \ub10c\ud15c\ud3ec\ub7f4 \uba85\ub839\uc5b4\uc5d0 \ub300\ud55c \uacf5\uc720 \uce90\uc2dc \uad6c\uc870"}]}, {"ucid": "US-6202129-B1", "titles": [{"lang": "EN", "text": "Shared cache structure for temporal and non-temporal information using indicative bits"}]}, {"ucid": "CN-1295687-A", "titles": [{"lang": "EN", "text": "Shared cache structure for temporal and non-temporal instructions"}, {"lang": "ZH", "text": "\u4e34\u65f6\u6307\u4ee4\u4e0e\u975e\u4e34\u65f6\u6307\u4ee4\u5171\u4eab\u7684\u9ad8\u901f\u7f13\u5b58\u7ed3\u6784"}]}, {"ucid": "EP-1066566-B1", "titles": [{"lang": "FR", "text": "STRUCTURE A ANTEMEMOIRE PARTAGEE POUR INSTRUCTIONS TEMPORELLES ET NON-TEMPORELLES ET METHODE CORRESPONDANTE"}, {"lang": "EN", "text": "SHARED CACHE STRUCTURE FOR TEMPORAL AND NON-TEMPORAL INSTRUCTIONS AND CORRESPONDING METHOD"}, {"lang": "DE", "text": "GEMEINSAMER CACHE-SPEICHER F\u00dcR TEMPORALE UND NICHT-TEMPORALE BEFEHLE UND VERFAHREN HIERF\u00dcR"}]}, {"ucid": "US-6584547-B2", "titles": [{"lang": "EN", "text": "Shared cache structure for temporal and non-temporal instructions"}]}, {"ucid": "JP-2002510085-A", "titles": [{"lang": "JA", "text": "\u30c6\u30f3\u30dd\u30e9\u30ea\u547d\u4ee4\u53ca\u3073\u975e\u30c6\u30f3\u30dd\u30e9\u30ea\u547d\u4ee4\u7528\u306e\u5171\u7528\u30ad\u30e3\u30c3\u30b7\u30e5\u69cb\u9020"}, {"lang": "EN", "text": "Shared cache structure for temporary and non-temporary instructions"}]}, {"ucid": "JP-4486750-B2", "titles": [{"lang": "JA", "text": "\u30c6\u30f3\u30dd\u30e9\u30ea\u547d\u4ee4\u53ca\u3073\u975e\u30c6\u30f3\u30dd\u30e9\u30ea\u547d\u4ee4\u7528\u306e\u5171\u7528\u30ad\u30e3\u30c3\u30b7\u30e5\u69cb\u9020"}, {"lang": "EN", "text": "Shared cache structure for temporary and non-temporary instructions"}]}, {"ucid": "WO-1999050752-A1", "titles": [{"lang": "FR", "text": "STRUCTURE A ANTEMEMOIRE PARTAGEE POUR INSTRUCTIONS TEMPORELLES ET NON-TEMPORELLES"}, {"lang": "EN", "text": "SHARED CACHE STRUCTURE FOR TEMPORAL AND NON-TEMPORAL INSTRUCTIONS"}]}, {"ucid": "WO-1999050752-A9", "titles": [{"lang": "FR", "text": "STRUCTURE A ANTEMEMOIRE PARTAGEE POUR INSTRUCTIONS TEMPORELLES ET NON-TEMPORELLES"}, {"lang": "EN", "text": "SHARED CACHE STRUCTURE FOR TEMPORAL AND NON-TEMPORAL INSTRUCTIONS"}]}, {"ucid": "BR-9909295-A", "titles": [{"lang": "PT", "text": "Estrutura de cache compartilhado para instru\u00e7\u00f5es temporais e n\u00e3o-temporais"}, {"lang": "EN", "text": "Shared cache structure for temporal and non-temporal statements"}]}, {"ucid": "CN-1230750-C", "titles": [{"lang": "EN", "text": "Shared cache structure for temporal and non-temporal instructions"}, {"lang": "ZH", "text": "\u4e34\u65f6\u6307\u4ee4\u4e0e\u975e\u4e34\u65f6\u6307\u4ee4\u5171\u4eab\u7684\u9ad8\u901f\u7f13\u5b58\u7ed3\u6784"}]}, {"ucid": "EP-1066566-A1", "titles": [{"lang": "DE", "text": "GEMEINSAMER CACHE-SPEICHER F\u00dcR TEMPORALE UND NICHT-TEMPORALE BEFEHLE"}, {"lang": "FR", "text": "STRUCTURE A ANTEMEMOIRE PARTAGEE POUR INSTRUCTIONS TEMPORELLES ET NON-TEMPORELLES"}, {"lang": "EN", "text": "SHARED CACHE STRUCTURE FOR TEMPORAL AND NON-TEMPORAL INSTRUCTIONS"}]}, {"ucid": "EP-1066566-A4", "titles": [{"lang": "FR", "text": "STRUCTURE A ANTEMEMOIRE PARTAGEE POUR INSTRUCTIONS TEMPORELLES ET NON-TEMPORELLES"}, {"lang": "DE", "text": "GEMEINSAMER CACHE-SPEICHER F\u00dcR TEMPORALE UND NICHT-TEMPORALE BEFEHLE"}, {"lang": "EN", "text": "SHARED CACHE STRUCTURE FOR TEMPORAL AND NON-TEMPORAL INSTRUCTIONS"}]}, {"ucid": "AU-3364599-A", "titles": [{"lang": "EN", "text": "Shared cache structure for temporal and non-temporal instructions"}]}, {"ucid": "KR-20010042262-A", "titles": [{"lang": "KO", "text": "\ud15c\ud3ec\ub7f4 \ubc0f \ub10c\ud15c\ud3ec\ub7f4 \uba85\ub839\uc5b4\uc5d0 \ub300\ud55c \uacf5\uc720 \uce90\uc2dc \uad6c\uc870"}, {"lang": "EN", "text": "SHARED CACHE STRUCTURE FOR TEMPORAL AND NON-TEMPORAL INSTRUCTIONS"}]}]}