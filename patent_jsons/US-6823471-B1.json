{"patent_number": "US-6823471-B1", "publication_id": 74059732, "family_id": 33434737, "publication_date": "2004-11-23", "titles": [{"lang": "EN", "text": "Method for providing high availability within a data processing system via a reconfigurable hashed storage subsystem"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50753791\"><p>A processor includes execution resources, data storage, and an instruction sequencing unit, coupled to the execution resources and the data storage, that supplies instructions within the data storage to the execution resources. At least one of the execution resources, the data storage, and the instruction sequencing unit is implemented with a plurality of hardware partitions of like function for processing a respective one of a plurality of data streams. If an error is detected in a particular hardware partition, the data stream assigned to that hardware partition is reassigned to another of the plurality of hardware partitions, thus preventing an error in one of the hardware partitions from resulting in a catastrophic failure.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00001\" num=\"1\"><claim-text>1. A processor, comprising:</claim-text><claim-text>execution resources; </claim-text><claim-text>data storage; and </claim-text><claim-text>an instruction sequencing unit, coupled to said execution resources and said data storage, that supplies instructions within said data storage to said execution resources; </claim-text><claim-text>wherein of said execution resources, said data storage, and said instruction sequencing unit, at least said execution resources are implemented with a plurality of hardware partitions of like function for processing a respective one of a plurality of data streams, and wherein said instruction sequencing unit includes a hashing circuit that assigns said plurality of data streams to said plurality of hardware partitions based upon an address hash of addresses associated with instructions within said plurality of data streams, said hash being selected by a hash selection circuit within said processor, and wherein if an error is detected in a particular hardware partition among said plurality of hardware partitions that is assigned a particular data stream among said plurality of data streams to process, said hashing selection circuit reassigns said particular data stream to at least one other of said plurality of hardware partitions by changing the address hash implemented by the hashing circuit. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00002\" num=\"2\"><claim-text>2. The processor of <claim-ref idref=\"US-6823471-B1-CLM-00001\">claim 1</claim-ref>, wherein said processor reassigns said particular data stream in response to detection of a double-bit ECC error.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00003\" num=\"3\"><claim-text>3. The processor of <claim-ref idref=\"US-6823471-B1-CLM-00001\">claim 1</claim-ref>, wherein said processor reassigns said particular data stream dynamically during operation of said processor.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00004\" num=\"4\"><claim-text>4. The processor of <claim-ref idref=\"US-6823471-B1-CLM-00001\">claim 1</claim-ref>, wherein said data storage and said execution resources are implemented with a same number of hardware partitions.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00005\" num=\"5\"><claim-text>5. The processor of <claim-ref idref=\"US-6823471-B1-CLM-00001\">claim 1</claim-ref>, wherein following reassignment of said particular data stream, said particular hardware partition is idle.</claim-text></claim>"}, {"num": 6, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00006\" num=\"6\"><claim-text>6. A data processing system, comprising:</claim-text><claim-text>at least one interconnect; </claim-text><claim-text>at least one memory coupled to said interconnect; and </claim-text><claim-text>at least one processor coupled to said interconnect, wherein said processing includes: </claim-text><claim-text>execution resources; </claim-text><claim-text>data storage; and </claim-text><claim-text>an instruction sequencing unit, coupled to said execution resources and said data storage, that supplies instructions within said data storage to said execution resources; </claim-text><claim-text>wherein of said execution resources, said data storage, and said instruction sequencing unit, at least said execution resources are implemented with a plurality of hardware partitions of like function for processing a respective one of a plurality of data streams, and wherein said instruction sequencing unit includes a hashing circuit that assigns said plurality of data streams to said plurality of hardware partitions based upon an address hash of addresses associated with instructions within said plurality of data streams, said hash being selected by a hash selection circuit within said processor, and wherein if an error is detected in a particular hardware partition among said plurality of hardware partitions that is assigned a particular data stream among said plurality of data streams to process, said hashing selection circuit reassigns said particular data stream to at least one other of said plurality of hardware partitions by changing the address hash implemented by the hashing circuit. </claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00007\" num=\"7\"><claim-text>7. The data processing system of <claim-ref idref=\"US-6823471-B1-CLM-00006\">claim 6</claim-ref>, wherein said processor reassigns said particular data stream in response to detection of a double-bit ECC error.</claim-text></claim>"}, {"num": 8, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00008\" num=\"8\"><claim-text>8. The data processing system of <claim-ref idref=\"US-6823471-B1-CLM-00006\">claim 6</claim-ref>, wherein said processor reassigns said particular data stream dynamically during operation of said processor.</claim-text></claim>"}, {"num": 9, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00009\" num=\"9\"><claim-text>9. The data processing system of <claim-ref idref=\"US-6823471-B1-CLM-00006\">claim 6</claim-ref>, wherein said data storage and said execution resources are implemented with a same number of hardware partitions.</claim-text></claim>"}, {"num": 10, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00010\" num=\"10\"><claim-text>10. The data processing system of <claim-ref idref=\"US-6823471-B1-CLM-00006\">claim 6</claim-ref>, wherein following reassignment of said particular data stream, said particular hardware partition is idle.</claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00011\" num=\"11\"><claim-text>11. A method of operating a processor, including execution resources, data storage, and an instruction sequencing unit, coupled to said execution resources and said data storage, that supplies instructions within said data storage to said execution resources, wherein of said execution resources, said data storage, and said instruction sequencing unit, at least said execution resources are implemented with a plurality of hardware partitions of like function, said method comprising:</claim-text><claim-text>assigning a plurality of data streams to said plurality of hardware partitions based upon an address hash of addresses associated with instructions within said plurality of data streams; </claim-text><claim-text>processing each of the plurality of data streams within a respective one of the plurality of hardware partitions of like function; and </claim-text><claim-text>in response to detection of an error in a particular hardware partition among said plurality of hardware partitions that is processing a particular data stream among said plurality of data streams, reassigning said particular data stream to at least one other of said plurality of hardware partitions by changing said address hash. </claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00012\" num=\"12\"><claim-text>12. The method of <claim-ref idref=\"US-6823471-B1-CLM-00011\">claim 11</claim-ref>, wherein reassigning said particular data stream comprises reassigning said particular data stream in response to detection of a double-bit ECC error.</claim-text></claim>"}, {"num": 13, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00013\" num=\"13\"><claim-text>13. The method of <claim-ref idref=\"US-6823471-B1-CLM-00011\">claim 11</claim-ref>, wherein reassigning said particular data stream comprises reassigning said particular data stream dynamically during operation of said processor.</claim-text></claim>"}, {"num": 14, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00014\" num=\"14\"><claim-text>14. The method of <claim-ref idref=\"US-6823471-B1-CLM-00011\">claim 11</claim-ref>, wherein processing each of a plurality of data streams comprises processing said plurality of data streams within a corresponding plurality of hardware partitions in said data storage and within a corresponding plurality of hardware partitions in said execution resources.</claim-text></claim>"}, {"num": 15, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6823471-B1-CLM-00015\" num=\"15\"><claim-text>15. The method of <claim-ref idref=\"US-6823471-B1-CLM-00011\">claim 11</claim-ref>, and further comprising following reassignment of said particular data stream, idling said particular hardware partition.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54338614\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>CROSS REFERENCE TO RELATED APPLICATIONS</h4><p>The present application is related to the following copending applications, which are filed on even date herewith and incorporated herein by reference:</p><p>(1) U.S. application Ser. No. 09/364,284;</p><p>(2) U.S. application Ser. No. 09/364,283;</p><p>(3) U.S. application Ser. No. 09/364,282;</p><p>(4) U.S. application Ser. No. 09/364,287;</p><p>(5) U.S. application Ser. No. 09/364,288;</p><p>(6) U.S. application Ser. No. 09/364,285;</p><p>(7) U.S. application Ser. No. 09/364,286.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Technical Field</p><p>The present invention relates in general to data processing and, in particular, to hardware faults in a data processing system. Still more particularly, the present invention relates to a processor and data processing system having redundant hardware partitions that provide repair capability.</p><p>2. Description of the Related Art</p><p>In order to capitalize on the high performance processing capability of a state-of-the-art processor core, the storage subsystem of a data processing system must efficiently supply the processor core with large amounts of instructions and data. Conventional data processing systems attempt to satisfy the processor core's demand for instructions and data by implementing deep cache hierarchies and wide buses capable of operating at high frequency. Although heretofore such strategies have been somewhat effective in staying apace of the demands of the core as processing frequency has increased, such strategies, because of their limited scalability, are by themselves inadequate to meet the data and instruction consumption demands of state-of-the-art and future processor technologies operating at 1 GHz and beyond.</p><h4>SUMMARY OF THE INVENTION</h4><p>To address the above and other shortcomings of conventional processor and data processing system architectures, the present invention introduces a processor having a hashed and partitioned storage subsystem. A processor includes execution resources, data storage, and an instruction sequencing unit, coupled to the execution resources and the data storage, that supplies instructions within the data storage to the execution resources. At least one of the execution resources, the data storage, and the instruction sequencing unit is implemented with a plurality of hardware partitions of like function for processing a respective one of a plurality of data streams. If an error is detected in a particular hardware partition, the data stream assigned to that hardware partition is reassigned to another of the plurality of hardware partitions, thus preventing an error in one of the hardware partitions from resulting in a catastrophic failure.</p><p>All objects, features, and advantages of the present invention will become apparent in the following detailed written description.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The novel features believed characteristic of the invention are set forth in the appended claims. The invention itself however, as well as a preferred mode of use, further objects and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein:</p><p>FIG. 1 depicts an illustrative embodiment of a multiprocessor data processing system in accordance with the present invention;</p><p>FIG. 2 illustrates a more detailed block diagram of a processor in the multiprocessor data processing system of FIG. 1;</p><p>FIG. 3A depicts a circuit that can implement an exemplary hashing algorithm on selected address bits;</p><p>FIG. 3B illustrates the bit positions of the address bits forming inputs to the exemplary hashing algorithm shown in FIG. 3A;</p><p>FIGS. 4A and 4B respectively depict more detailed block diagrams of the general purpose register file (GPRF) and floating-point register file (FPRF) of the processor of FIG. 2;</p><p>FIG. 5 is a block diagram of an exemplary embodiment of a compiler in accordance with the present invention;</p><p>FIG. 6 illustrates an exemplary embodiment of an instruction within the instruction set architecture (ISA) of the processor depicted in FIG. 2; and</p><p>FIG. 7 depicts a block diagram of an illustrative embodiment of a hash selection circuit in accordance with the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF ILLUSTRATIVE EMBODIMENT</h4><p>With reference now to the figures and in particular with reference to FIG. 1, there is illustrated a high level block diagram of a multiprocessor data processing system in accordance with the present invention. As depicted, data processing system <b>8</b> includes a number of processors <b>10</b><i>a</i>-<b>10</b><i>d</i>, which each comprise a single integrated circuit including a processor core and an on-chip cache subsystem, as discussed further below. Processors <b>10</b><i>a</i>-<b>10</b><i>d </i>are all connected to each of system interconnects <b>12</b><i>a</i>-<b>12</b><i>d</i>, which are in turn each coupled to a respective one of system memories <b>16</b><i>a</i>-<b>16</b><i>d </i>through an associated one of memory controllers <b>14</b><i>a</i>-<b>14</b><i>d. </i></p><p>According to an important aspect of the present invention, data processing system <b>8</b> implements a hashed and partitioned storage subsystem. That is, instead of the single memory controller and system memory implemented in many conventional data processing systems, the present invention partitions the system memory hardware into multiple memory controllers <b>14</b><i>a</i>-<b>14</b><i>d </i>and multiple system memories <b>16</b><i>a</i>-<b>16</b><i>d</i>. System memories <b>16</b><i>a</i>-<b>16</b><i>d </i>can each contain only a respective subset of all memory addresses, such that the disjoint subsets contained in all of system memories <b>16</b> together form the system memory data set. For example, each of system memories <b>16</b> may have a storage capacity of 2 GB for a total collective storage capacity of 8 GB. The subset of memory addresses assigned to each system memory <b>16</b> is determined by a hash algorithm implemented by each of processors <b>10</b><i>a</i>-<b>10</b><i>d</i>, as discussed further below.</p><p>System interconnects <b>12</b><i>a</i>-<b>12</b><i>d </i>serve as conduits for transactions between processing units <b>10</b>, transactions between processing units <b>10</b> and memory controllers <b>14</b>, and transactions between processors <b>10</b> or memory controllers <b>14</b> and other snoopers (e.g., I/O controllers) that may be coupled to system interconnects <b>12</b>. By virtue of the fact that each system interconnect <b>12</b> is connected to less than all of memory controllers <b>14</b> (and in the illustrated embodiment only one), each system interconnect <b>12</b> conveys only transactions that pertain to the addresses subset(s) assigned to the attached memory controller(s) <b>14</b>. Advantageously, system interconnects <b>12</b>, which may each be implemented as one or more buses or as a cross-point switch, can be implemented with the same or different architectures, bandwidths, and communication protocols, as will become apparent.</p><p>The hashing and partitioning of the storage subsystem of data processing system <b>8</b> is not limited in application to memory controllers <b>14</b> and system memories <b>16</b>, but preferably extends to the instruction fetch units (IFUs), load-store units (LSUs), register files, and cache subsystems of processors <b>10</b>. Referring now to FIG. 2, there is illustrated a high level block diagram of a processor <b>10</b> within data processing system <b>8</b> of FIG. <b>1</b>. As shown, processor <b>10</b> includes three principal collections of circuitry: instruction sequencing unit <b>20</b>, execution units <b>22</b>, <b>24</b><i>a</i>-<b>24</b><i>d </i>and <b>26</b>, and data storage including register files <b>28</b> and <b>30</b> and cache subsystem <b>32</b>.</p><p>In the illustrative embodiment, cache subsystem <b>32</b>, which provides low latency storage for data and instructions likely to be processed by the execution of processor <b>10</b>, includes level two (L<b>2</b>) caches <b>34</b><i>a</i>-<b>34</b><i>d </i>and bifurcated level one (L<b>1</b>) instruction and data caches <b>36</b><i>a</i>-<b>36</b><i>d </i>and <b>38</b><i>a</i>-<b>38</b><i>d</i>, respectively. In the illustrative embodiment, L<b>1</b> instruction caches <b>36</b> may be 32 kB each, L<b>1</b> data caches <b>38</b> may be 16 kB each, and L<b>2</b> caches <b>34</b> may be 512 kB each, for combined cache capacities of 128 kB of L<b>1</b> instruction cache, 64 kB of L<b>1</b> data cache, and 2 MB of L<b>2</b> cache. Of course, if desired, cache subsystem <b>32</b> may also include additional levels of on-chip or off-chip in-line or lookaside caches.</p><p>As indicated by the interconnection of L<b>1</b> caches <b>36</b> and <b>38</b> to respective L<b>2</b> caches <b>34</b><i>a</i>-<b>34</b><i>d </i>and the interconnection of L<b>2</b> caches <b>34</b><i>a</i>-<b>34</b><i>d </i>to respective system interconnects <b>12</b><i>a</i>-<b>12</b><i>d</i>, each L<b>1</b> cache <b>36</b>, <b>38</b> and each L<b>2</b> cache <b>34</b> can store only data and instructions having addresses within the subset of addresses contained in system memories <b>16</b> coupled to the associated interconnect. Thus, in the illustrated example, L<b>1</b> caches <b>36</b><i>a </i>and <b>38</b><i>a </i>and L<b>2</b> cache <b>34</b><i>a </i>can only cache data and instructions residing in system memory <b>16</b><i>a</i>, L<b>1</b> caches <b>36</b><i>b </i>and <b>38</b><i>b </i>and L<b>2</b> cache <b>34</b><i>b </i>can only cache data and instructions residing in system memory <b>16</b><i>b</i>, etc.</p><p>Instruction sequencing unit <b>20</b> contains a number of instruction fetch units (IFUs) <b>40</b><i>a</i>-<b>40</b><i>d </i>that are each coupled to a respective one of L<b>1</b> instruction cache <b>36</b><i>a</i>-<b>36</b><i>d</i>. Thus, each IFU <b>40</b> has an affinity to a particular address subset. IFUs <b>40</b> independently fetch instructions from the associated L<b>1</b> instruction caches <b>36</b> and pass fetched instructions to either branch unit <b>42</b> or dispatch unit <b>44</b>, depending upon whether the instructions are branch or sequential instructions, respectively. Branch instructions are processed directly by branch unit but sequential instructions are opportunistically assigned by dispatch unit <b>44</b> to one of execution units <b>22</b>, <b>24</b><i>a</i>-<b>24</b><i>d </i>and <b>26</b> as execution resources (e.g., registers and a slot in completion buffer <b>46</b>) become available. Dispatch unit <b>44</b> assigns instructions to execution units <b>22</b>, <b>24</b><i>a</i>-<b>24</b><i>d </i>and <b>26</b> according to instruction type and, if a load or store instruction, the target address of the instruction. In other words, integer and floating point instructions are dispatched to integer unit (IU) <b>22</b> and floating-point unit (FPU) <b>26</b>, respectively, while load and store instructions are dispatched to particular ones of LSUs <b>24</b><i>a</i>-<b>24</b><i>d </i>after dispatch unit <b>44</b> hashes the target address specified by the instruction to determine which L<b>1</b> data cache <b>38</b> contains the target data. Thus, each of LSUs <b>24</b> executes only those load and store instructions targeting addresses within the particular address subset with which the associated L<b>1</b> cache has affinity.</p><p>The hash algorithm implemented by dispatch unit <b>44</b>, which is programmable and can be altered dynamically during operation of data processing system <b>8</b> as discussed below, can be based on any type of address (e.g., effective address, virtual address, or real (physical) address) or any combination of address types. Referring now to FIG. 3A, there is illustrated a block diagram of exemplary hashing circuit that utilizes five low order bits, which are present in effective, virtual, and real addresses, to hash an input address into one of four address subsets A-D. As shown in FIG. 3B, the five input bits, designated bits <b>52</b>-<b>56</b>, form the high order bits of the 12-bit page offset within both the N-bit (e.g., 64-bit) effective addresses <b>60</b> utilized by processors <b>10</b> and the 42-bit real addresses <b>62</b> utilized cache subsystem <b>32</b>, memory controllers <b>14</b>, and other snoopers coupled to system interconnects <b>12</b>. In addition, the five selected bits form the low order bits of the index portion of the 42-bit real address <b>64</b> utilized to select a congruence class within L<b>2</b> caches <b>34</b>. As depicted in FIG. 3A, the exemplary hashing algorithm performs an exclusive-OR of bits <b>52</b>, <b>54</b> and <b>56</b> (e.g., with an XOR gate <b>52</b>) and an exclusive-OR of bits <b>53</b> and <b>55</b> (e.g., with an XOR gate <b>54</b>) and decodes the two-bit result with a decoder <b>56</b> to select one of the four address subsets.</p><p>In the illustrative embodiment, dispatch unit <b>44</b> is the only point of centralization or interaction between the different instruction and data pipelines. As a consequence, if operations such as synchronizing instructions (e.g., SYNC) must be made visible to all caches or all system interconnects, dispatch unit <b>44</b> broadcasts such operations to all LSUs <b>24</b>. The synchronizing instructions are thereafter made visible on all system interconnects <b>12</b>.</p><p>Referring again to FIG. 2, general purpose register file (GPRF) <b>28</b> and floating-point register file (FPRF) <b>30</b> are utilized to temporarily store integer and floating-point operands consumed by and resulting from instruction execution. Thus, IU <b>22</b> is coupled to GPRF <b>28</b>, FPU <b>26</b> is coupled to FPRF <b>30</b>, and GPRF <b>28</b> and FPRF <b>30</b> are each coupled to one or more (and possibly all) of LSUs <b>24</b>. As shown in FIGS. 4A and 4B, which respectively illustrate more detailed views of GPRF <b>28</b> and FPRF <b>30</b>, each register file contains a respective set of rename registers <b>70</b>, <b>72</b> for temporarily storing result data produced by the execution of instructions and a set of architected registers <b>74</b>, <b>76</b> for storing operand and result data. Result data is transferred from rename registers <b>70</b>, <b>72</b> to the associated set of architected registers <b>74</b>, <b>76</b> following execution of an instruction under the direction of completion unit <b>46</b> within ISU <b>20</b>.</p><p>In accordance with the present invention, each of rename registers <b>70</b>, <b>72</b> and architected registers <b>74</b>, <b>76</b> may be partitioned between the various hashes so that only result data from instructions residing at and/or targeting addresses within the subset defined by a hash are stored in rename and architected registers associated with that hash. It is important to note that the number of registers allocated to each hash within each of register sets <b>70</b>, <b>72</b>, <b>74</b> and <b>76</b> can differ and the number of rename and architected registers allocated to each hash may be programmable or dynamically alterable during operation of processor <b>10</b>, for example, in response to an on-chip performance monitor <b>60</b> detecting a threshold number of dispatch stalls for instructions having addresses within a particular address subset.</p><p>There are several ways in which the enhanced parallelism of the hashed and partitioned storage subsystem of the present invention can be exploited. For example, a compiler can be optimized to allocate different classes of data, for example, instructions, data, and the instruction page table entries and data page table entries utilized for address translation, to different address subsets. Alternatively, the classes of data assigned to each address subset may be data for different types of applications, for example, technical or commercial. The compiler can also distribute variables accessed by software among the various address to maximize utilization of LSUs <b>24</b>.</p><p>With reference now to FIG. 5, there is depicted a block diagram of an illustrative embodiment of a compiler that implements the optimizations described above. In the illustrative embodiment, compiler <b>80</b> includes a scanner/parser <b>82</b> that, in response to receipt of an instruction set architecture (ISA) source program as an input, tokenizes the ISA source program and verifies program syntax according to a defined context-free grammar. Scanner/parser <b>82</b> outputs a syntactic structure representing the program to translator <b>84</b>. Translator <b>84</b> receives the output of scanner/parser <b>82</b> and generates either an intermediate code representation or target machine code after verifying that the constructs parsed by scanner/parser <b>82</b> are legal and meaningful in context. According to the illustrative embodiment, an optimizer <b>86</b> receives an intermediate code representation produced by translator <b>84</b> and optimizes the location of variables in memory, register utilization, etc., as described above by reference to a hashing algorithm known to be implemented by dispatch unit <b>44</b>. The optimized intermediate code output by optimizer <b>86</b> is then utilized by machine code generator <b>88</b> to produce a target machine code executable by a processor <b>10</b>.</p><p>Alternatively, or in addition to such compiler optimizations, the hashed and partitioned subsystem of the present invention can be exploited by incorporating an awareness of the hashing of memory addresses into the instruction set architecture (ISA) of processors <b>10</b>. For example, FIG. 6 illustrates an ISA instruction <b>90</b> that, in addition to conventional opcode and operand fields <b>92</b> and <b>94</b>, includes optional source and destination hash fields <b>96</b> and <b>98</b>. Thus, a programmer could be permitted, by supplying value(s) within hash fields <b>96</b> and <b>98</b>, to explicitly direct the compiler as to which address subset source data is drawn and the address subset to which result data is stored.</p><p>The above compiler and ISA mechanisms for directing data to selected address subsets are particularly advantageous when the hardware partitions having affinity with the various address subsets are individually tailored for the type and amount of data anticipated to be within each address subset. In other words, to enhance performance each hardware partition can be implemented differently from the others. For example, the hardware of some of LSUs <b>24</b> can be devoted to execution of only integer loads and stores (i.e., be connected to only GPRF <b>28</b>), while the hardware of other LSUs <b>24</b> can be capable of executing only floating-point loads and stores (i.e., be connected to only FPRF <b>30</b>). In addition, certain of LSUs <b>24</b> be implemented with duplicate hardware such that multiple load and store instructions targeting addresses within the address subset associated with those LSUs <b>24</b> by the hash algorithm can be executed in parallel.</p><p>Each level of cache can also be heterogeneous. For example, caches of the same type (e.g., L<b>1</b> instruction cache, L<b>1</b> data cache, and L<b>2</b> cache) can be designed or configured with differing sizes, associativities, coherence protocols, inclusivities, sectoring, replacement policies, and prefetch behaviors. Such diversity among caches is particularly useful if different data types are allocated to different address subsets. For example, if the compiler is optimized to assign all locks to a small address subset, the caches having affinity to that address subset can be limited to a small size to reduce access latency and therefore improve system performance on updates to shared data. The \u201clock\u201d caches may also exhibit a different behavior from caches associated with other address subsets, for example, a store-through (or store-with-update) rather than a write-back protocol, to make the release of a lock visible to other processors <b>10</b> via a particular system interconnect <b>12</b> in response to execution of a store-conditional instruction.</p><p>As noted above, the implementation of diverse hardware components of the same type can also extend to system interconnects <b>12</b>, and can also extend to memory controllers <b>14</b> and system memories <b>16</b>. For example, a particular memory controller <b>14</b> in FIG. 1 can be implemented with duplicate memory controller hardware operating in parallel, and different memory controllers <b>14</b> can access the associated system memory <b>16</b> differently to retrieve a requested cache line of data (e.g., horizontal versus vertical slicing of memory). In addition, different system memories <b>16</b> can be implemented with differing memory technologies, for example, synchronous dynamic access memory (SDRAM) versus DRAM, differing module sizes, etc.</p><p>The hashed and partitioned storage subsystem of the present invention also preferably supports dynamic hash optimization and dynamic repair capability. In a conventional processor having only one cache at each level in a cache hierarchy and single instances of other storage subsystem circuitry, the occurrence of a double-bit bit ECC error in a particular cache or circuit would disable the processor. In contrast, if a double-bit ECC error (which is not correctable) is detected within a particular hardware partition of a processor <b>10</b> in accordance with the present invention, the hashing algorithm implemented by dispatch unit <b>44</b> can be altered dynamically to redistribute all addresses within the address subset associated with the defective partition to one or more of the other address subsets, thus idling the defective hardware (which may also be disabled). The hashing algorithm implemented by dispatch unit <b>44</b> can also be modified to redistribute the subsets to which memory addresses belong while retaining the full number of subsets, for example, to maximize LSU utilization, to improve address bus and/or data bus utilization or to reduce single-bit (soft) errors.</p><p>With reference now to FIG. 7, there is depicted an exemplary embodiment of a hash selection circuit that supports dynamic changes to the hashing algorithm implemented by dispatch unit <b>44</b>. Hash selection circuit <b>100</b> includes a number of hashing circuits <b>102</b> (one of which may be hashing circuit <b>50</b> of FIG. 3A) that each receive certain of the bits of effective address <b>60</b> as inputs and provide a hash output designating one of the hardware partitions. As noted above, the hashing algorithms implemented by hashing circuits <b>102</b> preferably differ, such that some of hashing circuits <b>102</b> hash addresses to fewer than all of the hardware partitions and others of hashing circuits <b>102</b> provide different hashes but still distribute addresses among all hardware partitions. The hash output of each hashing circuit <b>102</b> forms an input of multiplexer <b>104</b>, which selects one of the hash outputs as its output in response to select signal <b>106</b>. As illustrated, select signal <b>106</b> is derived from the contents of control register <b>108</b>, which may in turn be set by either or both of monitoring software and monitoring hardware (e.g., performance monitor <b>60</b>).</p><p>Once a dynamic update has been made to the control register <b>108</b> of a processor <b>10</b> within data processing system <b>8</b>, coherent operation of data processing system <b>8</b> requires that a similar update be performed at each of the other processors <b>10</b>. These updates can be handled by sourcing special transactions from the updated processor <b>10</b> on system interconnects <b>12</b> or by execution of a software exception handler that writes a new value to each other control register <b>108</b>. To minimize the performance penalty associated with a dynamic hash update, L<b>2</b> caches <b>34</b> are preferably implemented such that full addresses are utilized and such that each L<b>2</b> cache <b>34</b> snoops all address transactions regardless of the address subset to which the address transactions belong. With this arrangement, a dynamic change in the address hash implemented by dispatch unit <b>44</b> would require only caches in a disabled hardware partition to be flushed. However, if each L<b>2</b> cache <b>34</b> only snoops address transactions for its assigned address subset, all LSUs <b>24</b> and caches within each hardware partition from which any address is reassigned would have to be flushed prior to enforcing a dynamic change in the hash.</p><p>As has been described, the present invention provides an improved processor and data processing system architecture having a hashed and partitioned storage subsystem. The present invention not only enhances performance through increased hardware parallelism, but also permits the various hardware partitions to be individually optimized for the type of data contained in each address subset. Advantageously, the address subset assigned to each hardware partition can be changed dynamically by updating the hash, thus permitting runtime optimization and dynamic repair capability. The hashed and partitioned architecture of the present invention is also highly scalable and supports future increases in processor operating frequency through the addition of more hardware partitions.</p><p>While the invention has been particularly shown and described with reference to a preferred embodiment, it will be understood by those skilled in the art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention. For example, although a compiler in accordance with the present invention can reside within the volatile and non-volatile storage of an operating data processing system, the compiler may alternatively be implemented as a program product for use with a data processing system. Such a program product can be delivered to a data processing system via a variety of signal-bearing media, which include, without limitation, non-rewritable storage media (e.g., CD-ROM), rewritable storage media (e.g., a floppy diskette or hard disk drive), and communication media, such as digital and analog networks. It should be understood, therefore, that such signal-bearing media, when carrying or encoding computer readable instructions that direct the functions of the present invention, represent alternative embodiments of the present invention.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Ravi Kumar", "last_name": "Arimilli", "name": ""}, {"first_name": "Leo James", "last_name": "Clark", "name": ""}, {"first_name": "John Steve", "last_name": "Dodson", "name": ""}, {"first_name": "Guy Lynn", "last_name": "Guthrie", "name": ""}, {"first_name": "Jerry Don", "last_name": "Lewis", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTERNATIONAL BUSINESS MACHINES CORPORATION"}, {"first_name": "", "last_name": "INTERNATIONAL BUSINESS MACHINES CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  11/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  11/20        20060101A I20070721RMEP"}, {"label": "G06F  11/00        20060101A I20051110RMEP"}], "national_classes": [{"primary": true, "label": "714010"}, {"primary": false, "label": "714011"}, {"primary": false, "label": "714E11071"}, {"primary": false, "label": "714013"}, {"primary": false, "label": "714003"}, {"primary": false, "label": "71400511"}], "ecla_classes": [{"label": "G06F  11/20"}], "cpc_classes": [{"label": "G06F  11/20"}, {"label": "G06F  11/20"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1999-07-30", "application_date": "1999-07-30", "family_members": [{"ucid": "US-6823471-B1", "titles": [{"lang": "EN", "text": "Method for providing high availability within a data processing system via a reconfigurable hashed storage subsystem"}]}]}