{"patent_number": "US-6233657-B1", "publication_id": 72680826, "family_id": 24492362, "publication_date": "2001-05-15", "titles": [{"lang": "EN", "text": "Apparatus and method for performing speculative stores"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11192124\" source=\"national office\"><p>An apparatus for performing speculative stores is provided. The apparatus reads the original data from a cache line being updated by a speculative store, storing the original data in a restore buffer. The speculative store data is then stored into the affected cache line. Should the speculative store later be canceled, the original data may be read from the restore buffer and stored into the affected cache line. The cache line is thereby returned to a pre-store state. In one embodiment, the cache is configured into banks. The data read and restored comprises the data from one of the banks which comprise the affected cache line. Instead of forwarding store data to subsequent load memory accesses, the store is speculatively performed to the data cache and the loads may subsequently access the data cache. Dependency checking between loads and stores prior to the speculative performance of the store may stall the load memory access until the corresponding store memory access has been performed. Similar functionality to forwarding of store data is obtained through the performance of load memory accesses to the data cache. Additionally, speculative load memory accesses which are partially overlapped by a prior speculative store memory access may be performed more efficiently.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72558488\"><p>An apparatus for performing speculative stores is provided. The apparatus reads the original data from a cache line being updated by a speculative store, storing the original data in a restore buffer. The speculative store data is then stored into the affected cache line. Should the speculative store later be canceled, the original data may be read from the restore buffer and stored into the affected cache line. The cache line is thereby returned to a pre-store state. In one embodiment, the cache is configured into banks. The data read and restored comprises the data from one of the banks which comprise the affected cache line. Instead of forwarding store data to subsequent load memory accesses, the store is speculatively performed to the data cache and the loads may subsequently access the data cache. Dependency checking between loads and stores prior to the speculative performance of the store may stall the load memory access until the corresponding store memory access has been performed. Similar functionality to forwarding of store data is obtained through the performance of load memory accesses to the data cache. Additionally, speculative load memory accesses which are partially overlapped by a prior speculative store memory access may be performed more efficiently.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00001\" num=\"1\"><claim-text>1. A microprocessor comprising:</claim-text><claim-text>a load/store unit configured to select a speculative store memory access to convey to a data cache; and </claim-text><claim-text>said data cache coupled to said load/store unit, wherein said data cache is configured to speculatively update a cache line stored therein in response to said speculative store memory access, and wherein said data cache is configured to read first data from said cache line prior to updating said cache line in response to said speculative store memory access and to store said first data in a buffer, and wherein said first data is restored to said cache line in response to a snoop hit. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00002\" num=\"2\"><claim-text>2. The microprocessor as recited in claim <b>1</b> wherein said first data includes one or more bytes updated by said speculative store memory access.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00003\" num=\"3\"><claim-text>3. The microprocessor as recited in claim <b>2</b> wherein said data cache comprises a plurality of banks, and wherein said first data is stored in one of said plurality of banks.</claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00004\" num=\"4\"><claim-text>4. The microprocessor as recited in claim <b>3</b> wherein different portions of said cache line are stored in each one of said plurality of banks.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00005\" num=\"5\"><claim-text>5. The microprocessor as recited in claim <b>1</b> wherein said data cache is further configured to restore said first data to said cache line if said speculative store memory access is subsequent to a mispredicted branch or an instruction experiencing an exception.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00006\" num=\"6\"><claim-text>6. The microprocessor as recited in claim <b>1</b> wherein load/store unit is configured to perform a plurality of speculative store memory accesses.</claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00007\" num=\"7\"><claim-text>7. The microprocessor as recited in claim <b>6</b> wherein, in response to said snoop hit, first data corresponding to each of said plurality of speculative store memory accesses to corresponding cache lines updated by said each of said plurality of speculative store memory accesses.</claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00008\" num=\"8\"><claim-text>8. The microprocessor as recited in claim <b>7</b> wherein said plurality of speculative store memory accesses are restored in reverse program order.</claim-text></claim>"}, {"num": 9, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00009\" num=\"9\"><claim-text>9. The microprocessor as recited in claim <b>1</b> wherein said load/store unit is configured to convey an address of said speculative store memory access to said data cache to select said cache line to restore said first data in response to said snoop hit.</claim-text></claim>"}, {"num": 10, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00010\" num=\"10\"><claim-text>10. The microprocessor as recited in claim <b>1</b> wherein said first data is restored only if said snoop hit causes a writeback from said data cache.</claim-text></claim>"}, {"num": 11, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00011\" num=\"11\"><claim-text>11. The microprocessor as recited in claim <b>1</b> wherein said load/store unit is configured to reselect said speculative store memory access to be conveyed to said data cache subsequent to said snoop hit and said first data being restored.</claim-text></claim>"}, {"num": 12, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00012\" num=\"12\"><claim-text>12. A method comprising:</claim-text><claim-text>reading first data from a data cache prior to speculatively storing second data into said data cache responsive to a speculative store memory access, said first data including bytes replaced by said second data; </claim-text><claim-text>detecting a snoop hit in said data cache; and </claim-text><claim-text>restoring said first data to said data cache responsive to said detecting. </claim-text></claim>"}, {"num": 13, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00013\" num=\"13\"><claim-text>13. The method as recited in claim <b>12</b> further comprising storing said first data in a restore buffer.</claim-text></claim>"}, {"num": 14, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00014\" num=\"14\"><claim-text>14. The method as recited in claim <b>12</b> further comprising determining that said speculative store is subsequent to a mispredicted branch or an instruction experiencing an exception.</claim-text></claim>"}, {"num": 15, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00015\" num=\"15\"><claim-text>15. The method as recited in claim <b>14</b> further comprising restoring said first data to said data cache responsive to said determining.</claim-text></claim>"}, {"num": 16, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00016\" num=\"16\"><claim-text>16. The method as recited in claim <b>12</b> wherein said data cache comprises a plurality of banks, and wherein said first data is stored in one of said plurality of banks.</claim-text></claim>"}, {"num": 17, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00017\" num=\"17\"><claim-text>17. The method as recited in claim <b>12</b> further comprising performing a plurality of speculative store memory accesses, said performing including reading said first data from said data cache prior to speculatively storing for each of said plurality of speculative store memory accesses.</claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00018\" num=\"18\"><claim-text>18. The method as recited in claim <b>17</b> further comprising restoring said first data corresponding to each of said plurality of speculative store memory accesses to said data cache responsive to said detecting.</claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00019\" num=\"19\"><claim-text>19. The method as recited in claim <b>18</b> wherein said restoring is performed for each of said plurality of speculative store memory accesses in reverse program order.</claim-text></claim>"}, {"num": 20, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00020\" num=\"20\"><claim-text>20. The method as recited in claim <b>12</b> further comprising determining whether or not said snoop hit causes a writeback of a cache line from said data cache, wherein said restoring is performed only if said snoop hit does cause said writeback.</claim-text></claim>"}, {"num": 21, "parent": 12, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00021\" num=\"21\"><claim-text>21. The method as recited in claim <b>12</b> further comprising performing said speculative store memory access again subsequent to said restoring.</claim-text></claim>"}, {"num": 22, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00022\" num=\"22\"><claim-text>22. A computer system comprising:</claim-text><claim-text>a microprocessor comprising: </claim-text><claim-text>a load/store unit configured to select a speculative store memory access to convey to a data cache; and </claim-text><claim-text>said data cache coupled to said load/store unit, wherein said data cache is configured to speculatively update a cache line stored therein in response to said speculative store memory access, and wherein said data cache is configured to read first data from said cache line prior to updating said cache line in response to said speculative store memory access and to store said first data in a buffer, and wherein said first data is restored to said cache line in response to a snoop hit; and </claim-text><claim-text>an input/output (I/O) device coupled to said microprocessor and to another computer system, wherein said I/O device is configured to communicate between said computer system and said another computer system. </claim-text></claim>"}, {"num": 23, "parent": 22, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00023\" num=\"23\"><claim-text>23. The computer system as recited in claim <b>22</b> further comprising a second microprocessor.</claim-text></claim>"}, {"num": 24, "parent": 22, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00024\" num=\"24\"><claim-text>24. The computer system as recited in claim <b>22</b> wherein said I/O device comprises a modem.</claim-text></claim>"}, {"num": 25, "parent": 22, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00025\" num=\"25\"><claim-text>25. The computer system as recited in claim <b>22</b> further comprising an audio I/O device.</claim-text></claim>"}, {"num": 26, "parent": 25, "type": "dependent", "paragraph_markup": "<claim id=\"US-6233657-B1-CLM-00026\" num=\"26\"><claim-text>26. The computer system as recited in claim <b>25</b> wherein said audio I/O device comprises a sound card.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54547430\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><p>This application is a continuation of U.S. patent application Ser. No. 09/181,407, filed on Oct. 28, 1998, now U.S. Pat. No. 6,006,317 which is a continuation of U.S. patent application Ser. No. 08/621,958, filed on Mar. 26, 1996, now U.S. Pat. No. 5,838,943.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>This invention relates to microprocessors and, more particularly, to an apparatus for performing store memory accesses in a microprocessor.</p><p>2. Description of the Relevant Art</p><p>Superscalar microprocessors achieve high performance by simultaneously executing multiple instructions during a clock cycle and by specifying the shortest possible clock cycle consistent with the design. As used herein, the term \u201cclock cycle\u201d refers to an interval of time during which the pipeline stages of a microprocessor perform their intended functions. Storage devices (e.g. registers or arrays) capture their values in response to a clock signal defining the clock cycle. For example, storage devices may capture a value in response to a rising or falling edge of the clock signal.</p><p>Since superscalar microprocessors execute multiple instructions per clock cycle and the clock cycle is short, a high bandwidth memory system is required to provide instructions and data to the superscalar microprocessor (i.e. a memory system that can provide a large number of bytes in a short period of time). However, superscalar microprocessors are ordinarily configured into computer systems with a large main memory composed of dynamic random access memory (DRAM) cells. DRAM cells are characterized by access times which are significantly longer than the clock cycle of modern superscalar microprocessors. Also, DRAM cells typically provide a relatively narrow output bus to convey the stored bytes to the superscalar microprocessor. Therefore, DRAM cells provide a memory system that provides a relatively small number of bytes in a relatively long period of time, and do not form a high bandwidth memory system.</p><p>Because superscalar microprocessors are typically not configured into a computer system with a memory system having sufficient bandwidth to continuously provide instructions and data, superscalar microprocessors are often configured with caches. Caches are storage devices containing multiple blocks of storage locations, configured on the same silicon substrate as the microprocessor or coupled nearby. The blocks of storage locations are used to hold previously fetched instruction or data bytes. Each block of storage locations stores a set of contiguous bytes, and is referred to as a cache line. Typically, cache lines are transferred to and from the main memory as a unit. Bytes can be transferred from the cache to the destination (a register or an instruction processing pipeline) quickly; commonly one or two clock cycles are required as opposed to a large number of clock cycles to transfer bytes from a DRAM main memory.</p><p>Caches may be organized into an \u201cassociative\u201d structure (also referred to as \u201cset associative\u201d). In a set associative structure, the cache lines are accessed as a two-dimensional array having rows and columns. When a cache is searched for bytes residing at an address, a number of bits from the address are used as an \u201cindex\u201d into the cache. The index selects a particular row within the two-dimensional array, and therefore the number of address bits required for the index is determined by the number of rows configured into the cache. The act of selecting a row via an index is referred to as \u201cindexing\u201d. The addresses associated with bytes stored in the multiple cache lines of a row are examined to determine if any of the addresses match the requested address. If a match is found, the access is said to be a \u201chit\u201d, and the cache provides the associated bytes. If a match is not found, the access is said to be a \u201cmiss\u201d. When a miss is detected, the bytes are transferred from the memory system into the cache. The addresses associated with bytes stored in the cache are also stored. These stored addresses are referred to as \u201ctags\u201d or \u201ctag addresses\u201d.</p><p>The cache lines within a row form the columns of the row. Columns may also be referred to as \u201cways\u201d. The column is selected by examining the tags from a row and finding a match between one of the tags and the requested address. A cache designed with one column per row is referred to as a \u201cdirect-mapped cache\u201d. In a direct-mapped cache, the tag must be examined to determine if an access is a hit, but the tag examination is not required to select which bytes are transferred to the outputs of the cache.</p><p>In addition to employing caches, superscalar microprocessors often employ speculative execution to enhance performance. An instruction may be speculatively executed if the instruction is executed prior to determination that the instruction is actually to be executed within the current instruction stream. Other instructions which precede the instruction in program order may cause the instruction not to be actually executed (i.e. a mispredicted branch instruction or an instruction which causes an exception). If an instruction is speculatively executed and later determined to not be within the current instruction stream, the results of executing the instruction are discarded. Unfortunately, store memory accesses are typically not performed speculatively. As used herein, a \u201cmemory access\u201d refers to a transfer of data between one or more main memory storage locations and the microprocessor. A transfer from memory to the microprocessor (a \u201cread\u201d) is performed in response to a load memory access. A transfer from the microprocessor to memory (a \u201cwrite\u201d) is performed in response to a store memory access. Memory accesses may be a portion of executing an instruction, or may be the entire instruction. A memory access may be completed internal to the microprocessor if the memory access hits in the data cache therein. As used herein, \u201cprogram order\u201d refers to the sequential order of instructions specified by a computer program.</p><p>While speculative load memory accesses are often performed, several difficulties typically prevent implementation of speculative store memory accesses. As opposed to registers which are private to the microprocessor, memory may be shared with other microprocessors or devices. Although the locations being updated may be stored in the data cache, the data cache is required to maintain coherency with main memory. In other words, an update performed to the data cache is recognized by other devices which subsequently access the updated memory location. Other devices must not detect the speculative store memory access, which may later be canceled from the instruction processing pipeline due to incorrect speculative execution. However, once the store becomes non-speculative, external devices must detect the corresponding update. Additionally, speculative loads subsequent to the speculative store within the microprocessor must detect the updated value even while the store is speculative.</p><p>Instead of speculatively performing store memory accesses, many superscalar microprocessors place the store memory accesses in a buffer. When the store memory accesses become non-speculative, they are performed. Load memory accesses which access memory locations updated by a prior store memory access may be stalled until the store memory access completes, or may receive forwarded data from the store memory access within the buffer. Even when forwarding is implemented, the load memory access is stalled for cases in which the load memory access is not completely overlapped by the store memory access (i.e. the load memory access also reads bytes which are not updated by the store memory access). Buffer locations occupied by stores and loads which depend upon those stores are not available to subsequent memory accesses until the store is performed. Performance of the microprocessor is thereby decreased due to the inability to perform speculative store memory accesses. An apparatus allowing speculative performance of store memory accesses while ensuring correct operation is desired.</p><h4>SUMMARY OF THE INVENTION</h4><p>The problems outlined above are in large part solved by an apparatus for performing speculative stores. The apparatus reads the original data from a cache line being updated by a speculative store, storing the original data in a restore buffer. The speculative store data is then stored into the affected cache line. Should the speculative store later be canceled, the original data may be read from the restore buffer and stored into the affected cache line. The cache line is thereby returned to a pre-store state. In one embodiment, the cache is configured into banks. The data read and restored comprises the data from one of the banks which comprise the affected cache line. Advantageously, store memory accesses are performed speculatively. Since the store memory access has already been performed, the store memory access may immediately be discarded by the load/store unit when the corresponding instruction is retired. Performance may be increased by more efficient release of load/store buffer space. Additionally, the reorder buffer may retire subsequent instructions more quickly. Reorder buffer efficiency may thereby be increased. Still further, store throughput may be increased due to the speculative performance of the cache access. Cache access and hit determination need not be performed between receipt of a retirement indication from the reorder buffer and a signal from the load/store unit that the store memory access is completed. Subsequent stores may then be indicated as ready to retire earlier. The ability to restore the original data to the cache line enables correct operation in the case of incorrect execution of the speculative store or a snoop hit.</p><p>As opposed to many prior load/store units, the load/store unit described herein does not perform forwarding of store data to subsequent load memory accesses. Instead, since the store is speculatively performed to the data cache, the loads may access the data cache. Dependency checking between loads and stores prior to the speculative performance of the store may stall the load memory access until the corresponding store memory access has been performed. Advantageously, forwarding logic is not employed by the load/store unit. Similar functionality is obtained through the performance of load memory accesses to the data cache. Additionally, speculative load memory accesses which are partially overlapped by a prior speculative store memory access may be performed more efficiently. The data cache, subsequent to the speculative store, contains each of the bytes accessed by the load memory access.</p><p>Broadly speaking, the present invention contemplates an apparatus for performing speculative stores in a microprocessor comprising a first buffer, a first control unit, a cache and a second buffer. The first buffer is configured to store a plurality of store memory accesses. Coupled to the first buffer, the first control unit is configured to select at least one of the plurality of store memory accesses for cache access, and wherein the selected store memory access is speculative. The cache is coupled to receive the selected store memory access, and is configured to read first data from a cache line accessed by the selected store memory access. Additionally, the cache is configured to store second data corresponding to the selected store memory access into the cache line subsequent to reading the first data. The second buffer is coupled to the cache and is configured to store the first data. The first control unit is configured to receive an indication that the selected store memory access is incorrectly executed. In response to the indication, the first control unit is configured to convey the selected store memory access to the cache. The cache is configured to store the first data into the cache line in response to the indication, whereby the first data is restored to the cache line when the selected store memory access is incorrectly executed.</p><p>The present invention further contemplates a method for performing speculative store memory accesses in a microprocessor, comprising several steps. First data is read from a cache line accessed by a store memory access. Second data corresponding to the store memory access is stored into the cache line subsequent to the reading. The first data is restored to the cache line in response to an indication that the store memory access is incorrectly executed.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Other objects and advantages of the invention will become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p><p>FIG. 1 is a block diagram of one embodiment of a superscalar microprocessor.</p><p>FIG. 2 is a block diagram of a pair of decode units shown in FIG. 1, according to one embodiment of the microprocessor.</p><p>FIG. 3 is a block diagram of a load/store unit and a data cache shown in FIG. 1, according to one embodiment of the microprocessor.</p><p>FIG. 3A is a timing diagram showing events for a load memory access which hits in an unpredicted column.</p><p>FIG. 3B is a timing diagram showing events for a store memory access which hits in a predicted column.</p><p>FIG. 3C is a timing diagram showing events for a store memory access which hits in an unpredicted column.</p><p>FIG. 3D is a timing diagram showing events for a store memory access which misses.</p><p>FIG. 4A is a diagram showing information stored in a load/store buffer within the load/store unit shown in FIG. 3, according to one embodiment of the load/store unit.</p><p>FIG. 4B is a diagram showing information stored in a restore buffer within the data cache shown in FIG. 3, according to one embodiment of the data cache.</p><p>FIG. 5 is a flow chart illustrating operation of one embodiment of the load/store unit.</p><p>FIG. 6 is a block diagram of a computer system including the microprocessor shown in FIG. <b>1</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><p>While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><h4>DETAILED DESCRIPTION OF THE INVENTION</h4><p>Turning now to FIG. 1, a block diagram of one embodiment of a microprocessor <b>10</b> is shown. Microprocessor <b>10</b> includes a prefetch/predecode unit <b>12</b>, a branch prediction unit <b>14</b>, an instruction cache <b>16</b>, an instruction alignment unit <b>18</b>, a plurality of decode units <b>20</b>A-<b>20</b>C, a plurality of reservation stations <b>22</b>A-<b>22</b>C, a plurality of functional units <b>24</b>A-<b>24</b>C, a load/store unit <b>26</b>, a data cache <b>28</b>, a register file <b>30</b>, a reorder buffer <b>32</b>, and an MROM unit <b>34</b>. Blocks referred to herein with a reference number followed by a letter will be collectively referred to by the reference number alone. For example, decode units <b>20</b>A-<b>20</b>C will be collectively referred to as decode units <b>20</b>.</p><p>Prefetch/predecode unit <b>12</b> is coupled to receive instructions from a main memory subsystem (not shown), and is coupled to instruction cache <b>16</b>. Similarly, branch prediction unit <b>14</b> is coupled to instruction cache <b>16</b>. Still further, branch prediction unit <b>14</b> is coupled to decode units <b>20</b> and functional units <b>24</b>. Instruction cache <b>16</b> is further coupled to MROM unit <b>34</b> and instruction alignment unit <b>18</b>. Instruction alignment unit <b>18</b> is in turn coupled to decode units <b>20</b>. Each decode unit <b>20</b>A-<b>20</b>C is coupled to load/store unit <b>26</b> and to respective reservation stations <b>22</b>A-<b>22</b>C. Reservation stations <b>22</b>A-<b>22</b>C are further coupled to respective functional units <b>24</b>A-<b>24</b>C. Additionally, decode units <b>20</b> and reservation stations <b>22</b> are coupled to register file <b>30</b> and reorder buffer <b>32</b>. Functional units <b>24</b> are coupled to load/store unit <b>26</b>, register file <b>30</b>, and reorder buffer <b>32</b> as well. Data cache <b>28</b> is coupled to load/store unit <b>26</b> and to the main memory subsystem. Finally, MROM unit <b>34</b> is coupled to decode units <b>20</b>.</p><p>Generally speaking, load/store unit <b>26</b> and data cache <b>28</b> operate together to perform speculative store memory accesses. Load/store unit <b>26</b> selects a speculative store memory access for performance based upon a predetermined set of criteria, and conveys the store memory access to data cache <b>28</b>. Prior to performing the store, data cache <b>28</b> copies the corresponding data within the affected cache line into a restore buffer. Load/store unit <b>26</b> retains the speculative store memory access until either the store memory access retires or until an indication to restore the speculative store memory accesses is received. Speculative stores are restored (i.e. the cache line is returned to the pre-store state) if an instruction prior to the speculative stores is a mispredicted branch or an instruction which experiences an exception. Additionally, a restore is performed if a snoop is detected which requires data cache <b>28</b> to write a cache line to main memory. It is noted that, in the case of a restore in response to a snoop, the store memory access is retained by load/store unit <b>26</b> and performed to data cache <b>28</b> again after the snoop is serviced. Load/store unit <b>26</b> conveys the address of a store requiring restoration, and asserts a restore indication to data cache <b>28</b>. Data cache <b>28</b> accesses the restore buffer and writes the data stored therein to the cache line accessed by the speculative store address. The original data is thereby restored to the cache line. Advantageously, store memory accesses are performed speculatively. Since the store memory access has already been performed, the store memory access may immediately be cleared from the load/store buffer when reorder buffer <b>32</b> indicates that the store is ready to be retired. Performance may be increased by more efficient release of load/store buffer space. Additionally, reorder buffer <b>32</b> may progress to retiring subsequent instructions more quickly. Reorder buffer efficiency may thereby be increased. Still further, store throughput may be increased. Previously, stores were indicated to be non-speculative during a clock cycle. The store then accessed data cache <b>28</b> and determined if the store hit the cache before indicating to reorder buffer <b>32</b> that the store is complete and clearing the corresponding load/store buffer storage location. Subsequent stores were not indicated as non-speculative until the cache access and hit determination were made. Because cache access and hit determination are performed prior to indication that the store is non-speculative in load/store unit <b>26</b>, the store may be completed immediately upon non-speculative indication. The ability to restore the original data to the cache line enables correct operation in the case of incorrect execution of the speculative store or a snoop hit.</p><p>As opposed to many prior load/store units, load/store unit <b>26</b> does not perform forwarding of store data to subsequent load memory accesses. Instead, since the store is speculatively performed to data cache <b>28</b>, the loads may access data cache <b>28</b>. Dependency checking between loads and stores prior to the speculative performance of the store stalls the load memory access until the corresponding store memory access has been performed. Advantageously, forwarding logic is not employed by load/store unit <b>26</b>. Similar functionality is obtained through the performance of load memory accesses to data cache <b>28</b>. Additionally, speculative loads which are partially overlapped by a prior speculative store may be performed more efficiently. Data cache <b>28</b>, subsequent to the speculative store, contains each of the bytes accessed by the load.</p><p>Instruction cache <b>16</b> is a high speed cache memory provided to store instructions. Instructions are fetched from instruction cache <b>16</b> and dispatched to decode units <b>20</b>. In one embodiment, instruction cache <b>16</b> is configured to store up to 32 kilobytes of instructions in an 8 way set associative structure having 16 byte lines (a byte comprises 8 binary bits). Instruction cache <b>16</b> may additionally employ a way prediction scheme in order to speed access times to the instruction cache. Instead of accessing tags identifying each line of instructions and comparing the tags to the fetch address to select a way, instruction cache <b>16</b> predicts the way that is accessed. In this manner, the way is selected prior to accessing the instruction storage. The access time of instruction cache <b>16</b> may be similar to a direct-mapped cache. A tag comparison is performed and, if the way prediction is incorrect, the correct instructions are fetched and the incorrect instructions are discarded. It is noted that instruction cache <b>16</b> may be implemented as a fully associative, set associative, or direct mapped configuration.</p><p>Instructions are fetched from main memory and stored into instruction cache <b>16</b> by prefetch/predecode unit <b>12</b>. Instructions may be prefetched prior to instruction cache <b>16</b> recording a miss for the instructions in accordance with a prefetch scheme. A variety of prefetch schemes may be employed by prefetch/predecode unit <b>12</b>. As prefetch/predecode unit <b>12</b> transfers instructions from main memory to instruction cache <b>16</b>, prefetch/predecode unit <b>12</b> generates three predecode bits for each byte of the instructions: a start bit, an end bit, and a functional bit. The predecode bits form tags indicative of the boundaries of each instruction. The predecode tags may also convey additional information such as whether a given instruction can be decoded directly by decode units <b>20</b> or whether the instruction is executed by invoking a microcode procedure controlled by MROM unit <b>34</b>, as will be described in greater detail below.</p><p>One encoding of the predecode tags for an embodiment of microprocessor <b>10</b> employing the x86 instruction set will next be described. If a given byte is the first byte of an instruction, the start bit for that byte is set. If the byte is the last byte of an instruction, the end bit for that byte is set. Instructions which may be directly decoded by decode units <b>20</b> are referred to as \u201cfast path\u201d instructions. The remaining x86 instructions are referred to as MROM instructions, according to one embodiment. For fast path instructions, the functional bit is set for each prefix byte included in the instruction, and cleared for other bytes. Alternatively, for MROM instructions, the functional bit is cleared for each prefix byte and set for other bytes. The type of instruction may be determined by examining the functional bit corresponding to the end byte. If that functional bit is clear, the instruction is a fast path instruction. Conversely, if that functional bit is set, the instruction is an MROM instruction. The opcode of an instruction may thereby be located within an instruction which may be directly decoded by decode units <b>20</b> as the byte associated with the first clear functional bit in the instruction. For example, a fast path instruction including two prefix bytes, a Mod R/M byte, and an SIB byte would have start, end, and functional bits as follows:</p><p><tables id=\"TABLE-US-00001\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"49PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"49PT\"></colspec><colspec align=\"center\" colname=\"2\" colwidth=\"119PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></thead><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Start bits</entry><entry morerows=\"0\" valign=\"top\">10000</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">End bits</entry><entry morerows=\"0\" valign=\"top\">00001</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Functional bits</entry><entry morerows=\"0\" valign=\"top\">11000</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row></tbody></tgroup></table></tables></p><p>MROM instructions are instructions which are determined to be too complex for decode by decode units <b>20</b>. MROM instructions are executed by invoking MROM unit <b>34</b>. More specifically, when an MROM instruction is encountered, MROM unit <b>34</b> parses and issues the instruction into a subset of defined fast path instructions to effectuate the desired operation. MROM unit <b>34</b> dispatches the subset of fast path instructions to decode units <b>20</b>. A listing of exemplary x86 instructions categorized as fast path instructions will be provided further below.</p><p>Microprocessor <b>10</b> employs branch prediction in order to speculatively fetch instructions subsequent to conditional branch instructions. Branch prediction unit <b>14</b> is included to perform branch prediction operations. In one embodiment, up to two branch target addresses are stored with respect to each cache line in instruction cache <b>16</b>. Prefetch/predecode unit <b>12</b> determines initial branch targets when a particular line is predecoded. Subsequent updates to the branch targets corresponding to a cache line may occur due to the execution of instructions within the cache line. Instruction cache <b>16</b> provides an indication of the instruction address being fetched, so that branch prediction unit <b>14</b> may determine which branch target addresses to select for forming a branch prediction. Decode units <b>20</b> and functional units <b>24</b> provide update information to branch prediction unit <b>14</b>. Because branch prediction unit <b>14</b> stores two targets per cache line, some branch instructions within the line may not be stored in branch prediction unit <b>14</b>. Decode units <b>20</b> detect branch instructions which were not predicted by branch prediction unit <b>14</b>. Functional units <b>24</b> execute the branch instructions and determine if the predicted branch direction is incorrect. The branch direction may be \u201ctaken\u201d, in which subsequent instructions are fetched from the target address of the branch instruction. Conversely, the branch direction may be \u201cnot taken\u201d, in which subsequent instructions are fetched from memory locations consecutive to the branch instruction. When a mispredicted branch instruction is detected, instructions subsequent to the mispredicted branch are discarded from the various units of microprocessor <b>10</b>. A variety of suitable branch prediction algorithms may be employed by branch prediction unit <b>14</b>.</p><p>Instructions fetched from instruction cache <b>16</b> are conveyed to instruction alignment unit <b>18</b>. As instructions are fetched from instruction cache <b>16</b>, the corresponding predecode data is scanned to provide information to instruction alignment unit <b>18</b> (and to MROM unit <b>34</b>) regarding the instructions being fetched. Instruction alignment unit <b>18</b> utilizes the scanning data to align an instruction to each of decode units <b>20</b>. In one embodiment, instruction alignment unit <b>18</b> aligns instructions from three sets of eight instruction bytes to decode units <b>20</b>. Instructions are selected independently from each set of eight instruction bytes into preliminary issue positions. The preliminary issue positions are then merged to a set of aligned issue positions corresponding to decode units <b>20</b>, such that the aligned issue positions contain the three instructions which are prior to other instructions within the preliminary issue positions in program order. Decode unit <b>20</b>A receives an instruction which is prior to instructions concurrently received by decode units <b>20</b>B and <b>20</b>C (in program order). Similarly, decode unit <b>20</b>B receives an instruction which is prior to the instruction concurrently received by decode unit <b>20</b>C in program order.</p><p>Decode units <b>20</b> are configured to decode instructions received from instruction alignment unit <b>18</b>. Register operand information is detected and routed to register file <b>30</b> and reorder buffer <b>32</b>. Additionally, if the instructions require one or more memory operations to be performed, decode units <b>20</b> dispatch the memory operations to load/store unit <b>26</b>. Each instruction is decoded into a set of control values for functional units <b>24</b>, and these control values are dispatched to reservation stations <b>22</b> along with operand address information and displacement or immediate data which may be included with the instruction.</p><p>Microprocessor <b>10</b> supports out of order execution, and thus employs reorder buffer <b>32</b> to keep track of the original program sequence for register read and write operations, to implement register renaming, to allow for speculative instruction execution and branch misprediction recovery, and to facilitate precise exceptions. A temporary storage location within reorder buffer <b>32</b> is reserved upon decode of an instruction that involves the update of a register to thereby store speculative register states. If a branch prediction is incorrect, the results of speculatively-executed instructions along the mispredicted path can be invalidated in the buffer before they are written to register file <b>30</b>. Similarly, if a particular instruction causes an exception, instructions subsequent to the particular instruction may be discarded. In this manner, exceptions are \u201cprecise\u201d (i.e. instructions subsequent to the particular instruction causing the exception are not completed prior to the exception). It is noted that a particular instruction is speculatively executed if it is executed prior to instructions which precede the particular instruction in program order. Preceding instructions may be a branch instruction or an exception-causing instruction, in which case the speculative results may be discarded by reorder buffer <b>32</b>.</p><p>The instruction control values and immediate or displacement data provided at the outputs of decode units <b>20</b> are routed directly to respective reservation stations <b>22</b>. In one embodiment, each reservation station <b>22</b> is capable of holding instruction information (i.e., instruction control values as well as operand values, operand tags and/or immediate data) for up to three pending instructions awaiting issue to the corresponding functional unit. It is noted that for the embodiment of FIG. 1, each reservation station <b>22</b> is associated with a dedicated functional unit <b>24</b>. Accordingly, three dedicated \u201cissue positions\u201d are formed by reservation stations <b>22</b> and functional units <b>24</b>. In other words, issue position <b>0</b> is formed by reservation station <b>22</b>A and functional unit <b>24</b>A. Instructions aligned and dispatched to reservation station <b>22</b>A are executed by functional unit <b>24</b>A. Similarly, issue position <b>1</b> is formed by reservation station <b>22</b>B and functional unit <b>24</b>B; and issue position <b>2</b> is formed by reservation station <b>22</b>C and functional unit <b>24</b>C.</p><p>Upon decode of a particular instruction, if a required operand is a register location, register address information is routed to reorder buffer <b>32</b> and register file <b>30</b> simultaneously. Those of skill in the art will appreciate that the x86 register file includes eight 32 bit real registers (i.e., typically referred to as EAX, EBX, ECX, EDX, EBP, ESI, EDI and ESP). In embodiments of microprocessor <b>10</b> which employ the x86 microprocessor architecture, register file <b>30</b> comprises storage locations for each of the 32 bit real registers. Additional storage locations may be included within register file <b>30</b> for use by MROM unit <b>34</b>. Reorder buffer <b>32</b> contains temporary storage locations for results which change the contents of these registers to thereby allow out of order execution. A temporary storage location of reorder buffer <b>32</b> is reserved for each instruction which, upon decode, is determined to modify the contents of one of the real registers. Therefore, at various points during execution of a particular program, reorder buffer <b>32</b> may have one or more locations which contain the speculatively executed contents of a given register. If following decode of a given instruction it is determined that reorder buffer <b>32</b> has a previous location or locations assigned to a register used as an operand in the given instruction, the reorder buffer <b>32</b> forwards to the corresponding reservation station either: 1) the value in the most recently assigned location, or 2) a tag for the most recently assigned location if the value has not yet been produced by the functional unit that will eventually execute the previous instruction. If reorder buffer <b>32</b> has a location reserved for a given register, the operand value (or tag) is provided from reorder buffer <b>32</b> rather than from register file <b>30</b>. If there is no location reserved for a required register in reorder buffer <b>32</b>, the value is taken directly from register file <b>30</b>. If the operand corresponds to a memory location, the operand value is provided to the reservation station through load/store unit <b>26</b>.</p><p>In one particular embodiment, reorder buffer <b>32</b> is configured to store and manipulate concurrently decoded instructions as a unit. This configuration will be referred to herein as \u201cline-oriented\u201d. By manipulating several instructions together, the hardware employed within reorder buffer <b>32</b> may be simplified. For example, a line-oriented reorder buffer included in the present embodiment allocates storage sufficient for instruction information pertaining to three instructions (one from each decode unit <b>20</b>) whenever one or more instructions are dispatched by decode units <b>20</b>. By contrast, a variable amount of storage is allocated in conventional reorder buffers, dependent upon the number of instructions actually dispatched. A comparatively larger number of logic gates may be required to allocated the variable amount of storage. When each of the concurrently decoded instructions has executed, the instruction results are stored into register file <b>30</b> simultaneously. The storage is then free for allocation to another set of concurrently decoded instructions. Additionally, the amount of control logic circuitry employed per instruction is reduced because the control logic is amortized over several concurrently decoded instructions. A reorder buffer tag identifying a particular instruction may be divided into two fields: a line tag and an offset tag. The line tag identifies the set of concurrently decoded instructions including the particular instruction, and the offset tag identifies which instruction within the set corresponds to the particular instruction. It is noted that storing instruction results into register file <b>30</b> and freeing the corresponding storage is referred to as \u201cretiring\u201d the instructions. It is further noted that any reorder buffer configuration may be employed in various embodiments of microprocessor <b>10</b>.</p><p>As noted earlier, reservation stations <b>22</b> store instructions until the instructions are executed by the corresponding functional unit <b>24</b>. An instruction is selected for execution if: (i) the operands of the instruction have been provided; and (ii) the operands have not yet been provided for instructions which are within the same reservation station <b>22</b>A-<b>22</b>C and which are prior to the instruction in program order. It is noted that when an instruction is executed by one of the functional units <b>24</b>, the result of that instruction is passed directly to any reservation stations <b>22</b> that are waiting for that result at the same time the result is passed to update reorder buffer <b>32</b> (this technique is commonly referred to as \u201cresult forwarding\u201d). An instruction may be selected for execution and passed to a functional unit <b>24</b>A-<b>24</b>C during the clock cycle that the associated result is forwarded. Reservation stations <b>22</b> route the forwarded result to the functional unit <b>24</b> in this case.</p><p>In one embodiment, each of the functional units <b>24</b> is configured to perform integer arithmetic operations of addition and subtraction, as well as shifts, rotates, logical operations, and branch operations. The operations are performed in response to the control values decoded for a particular instruction by decode units <b>20</b>. It is noted that a floating point unit (not shown) may also be employed to accommodate floating point operations. The floating point unit may be operated similar to load/store unit <b>26</b> in that any of decode units <b>20</b> may dispatch instructions to the floating point unit.</p><p>Each of the functional units <b>24</b> also provides information regarding the execution of conditional branch instructions to the branch prediction unit <b>14</b>. If a branch prediction was incorrect, branch prediction unit <b>14</b> flushes instructions subsequent to the mispredicted branch that have entered the instruction processing pipeline, and causes fetch of the required instructions from instruction cache <b>16</b> or main memory. It is noted that in such situations, results of instructions in the original program sequence which occur after the mispredicted branch instruction are discarded, including those which were speculatively executed and temporarily stored in load/store unit <b>26</b> and reorder buffer <b>32</b>.</p><p>Results produced by functional units <b>24</b> are sent to reorder buffer <b>32</b> if a register value is being updated, and to load/store unit <b>26</b> if the contents of a memory location are changed. If the result is to be stored in a register, reorder buffer <b>32</b> stores the result in the location reserved for the value of the register when the instruction was decoded. Results may be conveyed upon a plurality of result buses <b>38</b>.</p><p>Load/store unit <b>26</b> provides an interface between functional units <b>24</b> and data cache <b>28</b>. In one embodiment, load/store unit <b>26</b> is configured with a load/store buffer having eight storage locations for data and address information for pending load or store memory accesses. Decode units <b>20</b> arbitrate for access to the load/store unit <b>26</b>. When the buffer is full, a decode unit must wait until load/store unit <b>26</b> has room for the pending load or store request information. Load/store unit <b>26</b> also performs dependency checking for load memory accesses against pending store memory accesses to ensure that data coherency is maintained. Additionally, load/store unit <b>26</b> may include a special register storage for special registers such as the segment registers and other registers related to the address translation mechanism defined by the x86 microprocessor architecture.</p><p>Load/store unit <b>26</b> is configured to perform load memory accesses speculatively. Store memory accesses are performed in program order, but may be speculatively stored into the predicted way. If the predicted way is incorrect, the data prior to the store memory access is subsequently restored to the predicted way and the store memory access is performed to the correct way. Stores may be executed speculatively as well. Speculatively executed stores are placed into a restore buffer, along with a copy of the cache line prior to the update. If the speculatively executed store is later discarded due to branch misprediction, exception, or snoop, the cache line may be restored to the value stored in the buffer.</p><p>Data cache <b>28</b> is a high speed cache memory provided to temporarily store data being transferred between load/store unit <b>26</b> and the main memory subsystem. In one embodiment, data cache <b>28</b> has a capacity of storing up to sixteen kilobytes of data in an eight way set associative structure. Similar to instruction cache <b>16</b>, data cache <b>28</b> may employ a way prediction mechanism. It is understood that data cache <b>28</b> may be implemented in a variety of specific memory configurations, including a set associative configuration.</p><p>In one particular embodiment of microprocessor <b>10</b> employing the x86 microprocessor architecture, instruction cache <b>16</b> and data cache <b>28</b> are linearly addressed. The linear address is formed from the offset specified by the instruction and the base address specified by the segment portion of the x86 address translation mechanism. Linear addresses may optionally be translated to physical addresses for accessing a main memory. The linear to physical translation is specified by the paging portion of the x86 address translation mechanism. It is noted that a linear addressed cache stores linear address tags. A set of physical tags (not shown) may be employed for mapping the linear addresses to physical addresses and for detecting translation aliases. Additionally, the physical tag block may perform linear to physical address translation.</p><p>Turning now to FIG. 2, a block diagram of one embodiment of decode units <b>20</b>A and <b>20</b>B are shown. Each decode unit <b>20</b> receives an instruction from instruction alignment unit <b>18</b>. Additionally, MROM unit <b>34</b> is coupled to each decode unit <b>20</b> for dispatching fast path instructions corresponding to a particular MROM instruction. Decode unit <b>20</b>A comprises early decode unit <b>40</b>A, multiplexor <b>42</b>A, and opcode decode unit <b>44</b>A. Similarly, decode unit <b>20</b>B includes early decode unit <b>40</b>B, multiplexor <b>42</b>B, and opcode decode unit <b>44</b>B.</p><p>Certain instructions in the x86 instruction set are both fairly complicated and frequently used. In one embodiment of microprocessor <b>10</b>, such instructions include more complex operations than the hardware included within a particular functional unit <b>24</b>A-<b>24</b>C is configured to perform. Such instructions are classified as a special type of MROM instruction referred to as a \u201cdouble dispatch\u201d instruction. These instructions are dispatched to a pair of opcode decode units <b>44</b>. It is noted that opcode decode units <b>44</b> are coupled to respective reservation stations <b>22</b>. Each of opcode decode units <b>44</b>A-<b>44</b>C forms an issue position with the corresponding reservation station <b>22</b>A-<b>22</b>C and functional unit <b>24</b>A-<b>24</b>C. Instructions are passed from an opcode decode unit <b>44</b> to, the corresponding reservation station <b>22</b> and further to the corresponding functional unit <b>24</b>.</p><p>Multiplexor <b>42</b>A is included for selecting between the instructions provided by MROM unit <b>34</b> and by early decode unit <b>40</b>A. During times in which MROM unit <b>34</b> is dispatching instructions, multiplexor <b>42</b>A selects instructions provided by MROM unit <b>34</b>. At other times, multiplexor <b>42</b>A selects instructions provided by early decode unit <b>40</b>A. Similarly, multiplexor <b>42</b>B selects between instructions provided by MROM unit <b>34</b>, early decode unit <b>40</b>A, and early decode unit <b>40</b>B. The instruction from MROM unit <b>34</b> is selected during times in which MROM unit <b>34</b> is dispatching instructions. During times in which early decode unit <b>40</b>A detects a fast path instruction having an SIB byte, the instruction from early decode unit <b>40</b>A is selected by multiplexor <b>42</b>B. Otherwise, the instruction from early decode unit <b>40</b>B is selected. When early decode unit <b>40</b>A detects a fast path instruction having an SIB byte, an instruction for calculating the address specified by the SIB byte is dispatched to opcode decode unit <b>44</b>A. Opcode decode unit <b>44</b>B receives the fast path instruction.</p><p>According to one embodiment employing the x86 instruction set, early decode units <b>40</b> perform the following operations:</p><p>(i) merge the prefix bytes of the instruction into an encoded prefix byte;</p><p>(ii) decode unconditional branch instructions (which may include the unconditional jump, the CALL, and the RETURN) which were not detected during branch prediction;</p><p>(iii) decode source and destination flags;</p><p>(iv) decode the source and destination operands which are register operands and generate operand size information; and</p><p>(v) determine the displacement and/or immediate size so that displacement and immediate data may be routed to the opcode decode unit.</p><p>Opcode decode units <b>44</b> are configured to decode the opcode of the instruction, producing control values for functional unit <b>24</b>. Displacement and immediate data are routed with the control values to reservation stations <b>22</b>.</p><p>Since early decode units <b>40</b> detect operands, the outputs of multiplexors <b>42</b> are routed to register file <b>30</b> and reorder buffer <b>32</b>. Operand values or tags may thereby be routed to reservation stations <b>22</b>. Additionally, memory operands are detected by early decode units <b>40</b>. Therefore, the outputs of multiplexors <b>42</b> are routed to load/store unit <b>26</b>. Memory operations corresponding to instructions having memory operands are stored by load/store unit <b>26</b>.</p><p>Turning next to FIG. 3, a block diagram of one embodiment of load/store unit <b>26</b> and data cache <b>28</b> is shown. Load/store unit <b>26</b> includes a load/store buffer <b>50</b>, an allocation control unit <b>52</b>, an access selection control unit <b>54</b>, an update control unit <b>56</b>, and a selection device <b>58</b>. Data cache <b>28</b> includes a cache storage and control block <b>60</b>, a restore control unit <b>62</b>, and a restore buffer <b>64</b>. Load memory accesses and store memory accesses may be referred to as loads and stores, respectively, for brevity.</p><p>Generally speaking, speculative store requests may be conveyed from load/store buffer <b>50</b> to cache storage and control block <b>60</b>. Cache storage and control block <b>60</b> reads the original data (i.e. data prior to performing the store operation) from the affected cache line. Subsequently, cache storage and control block <b>60</b> stores the data associated with the store into the affected cache line. The original data is stored in restore buffer <b>64</b>. Update control unit <b>56</b> receives indications from reorder buffer <b>32</b> of mispredicted branches, exceptions, and loads and stores which are ready to be retired. If a mispredicted branch is detected, speculative store operation is halted. When the mispredicted branch instruction is retired, those stores which have been speculatively performed and which were in the buffer when the mispredicted branch instruction was detected are restored. If an exception or snoop access requiring a writeback response is detected, speculative stores are also restored.</p><p>When speculative stores are restored, load/store unit <b>26</b> conveys the most recently performed speculative store as a restore operation to data cache <b>28</b>. Data cache <b>28</b> retrieves the most recent data from restore buffer <b>64</b> and stores the data into the corresponding cache line within cache storage and control block <b>60</b>. Load/store unit <b>26</b> continues restoring with the next most recently performed speculative store, until all outstanding speculative stores have been restored. Restore buffer <b>64</b> provides next most recently stored data for each restore operation, thereby providing the original data prior to each speculative store. The stores are restored in the reverse order (i.e. most recently performed to least recently performed) from the order in which they were performed. The reverse order ensures that multiple speculative stores to the same cache line are restored correctly, such that the cache line is returned to the value it contained prior to the multiple speculative stores.</p><p>Load/store buffer <b>50</b> is coupled to a plurality of decode request buses <b>66</b> from decode units <b>20</b>, and to a plurality of functional unit result buses <b>68</b> from functional units <b>24</b>. Functional unit result buses <b>68</b> may form a portion of result buses <b>38</b> (shown in FIG. <b>1</b>). Decode units <b>20</b> decode instructions, conveying indications of loads and/or stores associated with the instructions to load/store unit <b>26</b> upon decode request buses <b>66</b> (one from each decode unit <b>20</b>A-<b>20</b>C). Loads and stores are accepted into load/store buffer <b>50</b> under the control of allocation control unit <b>52</b>. If not all of the requests conveyed upon request buses <b>66</b> may be accepted (i.e. due to a lack of free storage locations within load/store buffer <b>50</b>), requests from decode unit <b>20</b>A are given highest priority. Requests from decode unit <b>20</b>B are given next highest priority, followed by requests from decode unit <b>20</b>C. Memory accesses are thereby stored into load/store buffer <b>50</b> in program order, according to the embodiment shown. The program order of a pair of accesses within load/store buffer <b>50</b> may be determined via their respective storage locations. Load and store memory access addresses are calculated by functional units <b>24</b>, and the addresses are forwarded to load/store unit <b>26</b> upon functional unit result buses <b>68</b> (one from each functional unit <b>24</b>). Allocation unit <b>52</b> directs the received addresses to the respective load and store memory operations, as identified by a reorder buffer tag for the associated instruction. Data for stores is also provided by functional units <b>24</b> upon functional unit result buses <b>68</b>.</p><p>In the embodiment shown, allocation unit <b>52</b> maintains a pointer to a storage location within load/store buffer <b>50</b>. The pointer indicates the location to be allocated to the next load/store request from decode units <b>20</b>. When operations are removed from load/store buffer <b>50</b>, the remaining operations are shifted down such that the operation which is prior to each other operation within load/store buffer <b>50</b> is at the bottom of the buffer. Remaining operations are stored in order up to the storage location indicated by the pointer. It is noted that load/store buffer <b>50</b> may be operated in any suitable fashion.</p><p>Load/store buffer <b>50</b> comprises a plurality of storage locations for storing memory operations (i.e. load and store memory accesses). In one embodiment, eight storage locations are included. The operations stored within load/store buffer <b>50</b> are conveyed to selection device <b>58</b>. Under the control of access selection control unit <b>54</b>, selection device <b>58</b> selects up to two memory accesses to be performed to data cache <b>28</b> during a clock cycle. Cache access buses <b>70</b> emanate from selection device <b>58</b> and are coupled to cache storage and control block <b>60</b>. It is noted that selection device <b>58</b> may comprise a plurality of multiplexor circuits for selecting operations from the set of operations conveyed thereto.</p><p>Access selection control unit <b>54</b> selects memory accesses from load/store buffer <b>50</b> which fulfill the selection criteria for cache access. In one embodiment, a load memory access may be selected for cache access if the following criteria are met: (i) the load memory address has been calculated; (ii) store memory accesses prior to the load memory access in load/store buffer <b>50</b> have calculated addresses; and (iii) the stores are not to the same index within data cache <b>28</b> as the load memory address. Criterion (ii) ensures that a load does not speculatively bypass a store upon which the load depends prior to the store receiving an address. Criterion (iii) ensures that a load does not bypass a store upon which it may depend after the address is calculated. It is noted that, by comparing indexes as specified by criterion (iii), loads do not bypass a store to the same physical address even if the linear addresses differ. Loads thereby bypass misses in load/store buffer <b>50</b>, as well as stores to dissimilar indexes.</p><p>For this embodiment, store memory accesses may be selected for cache access if the following criteria are met: (i) the store memory address has been calculated and the store data has been provided; (ii) the store memory address is aligned or the store is non-speculative; (iii) memory accesses prior to the store memory access have been performed and have not missed data cache <b>28</b>; and (iv) no more than three speculative store memory accesses are outstanding. Store memory accesses are thereby performed in program order with respect to other memory accesses. In particular, the store selection criteria does not include an indication that the store is indicated as ready for retirement by reorder buffer <b>32</b>. Therefore, stores may be selected for cache access while the stores are speculative. It is noted that, in cases in which more than two memory accesses fulfill the above listed criteria, the qualifying memory accesses which are nearest the bottom of load/store buffer <b>50</b> are selected. In other words, the qualifying memory accesses which are foremost in program order are selected.</p><p>Cache storage and control block <b>60</b> comprises a cache storage for data and tag addresses, as well as associated control circuitry. In one embodiment, data cache <b>28</b> is a set-associative cache. Cache storage and control block <b>60</b> receives the cache accesses upon cache access buses <b>70</b>, and performs the appropriate actions (read or write) dependent upon the type of cache access. Among the information conveyed to cache storage and control block <b>60</b> upon cache accesses bus <b>70</b> is an indication of the load or store (read or write) nature of the request, the address, the number of bytes to be accessed, the store data (for stores), and a restore indication. For load memory accesses, the corresponding data is conveyed upon result buses <b>72</b> to reservation stations <b>22</b> and reorder buffer <b>32</b>. Additionally, result buses <b>72</b> are coupled to restore buffer <b>64</b> for speculative store memory accesses. Data corresponding to each speculative store, prior to the store being performed, is conveyed upon result buses <b>72</b>. Restore control unit <b>62</b> causes the data to be stored into storage locations within restore buffer <b>64</b>. Restore control unit <b>62</b> maintains a restore pointer (stored in storage location <b>74</b>) which indicates the storage location to be allocated to data from a store memory access. For a set associative cache, restore buffer <b>64</b> additionally stores the column (or way) of the cache in which the affected line is stored. When data is stored into restore buffer <b>64</b>, restore control unit <b>62</b> increments the pointer. Subsequent store data may thereby be stored into restore buffer <b>64</b>.</p><p>Cache storage and control block <b>60</b> conveys status information regarding each cache access upon status bus <b>76</b> to update control unit <b>56</b>. The status information includes a hit/miss status for each access. Update control unit <b>56</b> updates the control information for each corresponding operation within load/store buffer <b>50</b> in response to the status information. If a load operation is a hit, the load operation is deleted from load/store buffer <b>50</b>. If a load operation is a miss, the operation is marked as a miss. Load/store unit <b>26</b> retains the load miss until the miss is non-speculative (i.e. the load's reorder buffer tag is conveyed upon retire bus <b>78</b> from reorder buffer <b>32</b>). The miss is then serviced by fetching the corresponding cache line from main memory. Non-speculative stores (i.e. stores which are indicated upon retire bus <b>78</b>) operate similar to loads except that misses are not be fetched from main memory, according to the present embodiment. Instead, the store data is written to main memory. Other embodiments may fetch the cache line, similar to loads. Speculative stores are retained within load/store buffer <b>50</b> until the stores become non-speculative. In this manner, if the speculative stores are performed incorrectly, restore actions may be performed using the speculative store address. When operations are deleted from load/store buffer <b>50</b>, update control unit <b>56</b> informs allocation control unit <b>52</b> such that the pointer indicating the next storage location to be allocated is adjusted accordingly.</p><p>It is noted that in one embodiment, retire bus <b>78</b> is configured to convey reorder buffer tags of up to two memory accesses which are ready to be retired (i.e. the memory accesses are non-speculative) Update control unit <b>56</b> compares the reorder buffer tags to operations within the buffer and interprets the operations as non-speculative. Once any required actions are completed, update control unit <b>56</b> informs reorder buffer <b>32</b> that the corresponding operations are complete and the associated instructions may be retired. For speculative stores, an indication upon retire bus <b>78</b> may be immediately followed by a response from load/store unit <b>26</b>. The corresponding speculative stores may be deleted from load/store buffer <b>50</b> upon retirement.</p><p>Restore control unit <b>62</b> receives status bus <b>76</b> as well. If an operation is a store and the operation is a hit, then restore control unit <b>62</b> increments the restore buffer pointer stored in storage location <b>74</b>. If the operation is not a store or is a miss, then restore control unit <b>62</b> does not increment the pointer. In this manner, original data for store accesses is stored into restore buffer <b>64</b>.</p><p>Update control unit <b>56</b> maintains a store pointer (in storage location <b>80</b>) indicative of the storage location within load/store buffer <b>50</b> which stores the most recent speculatively performed store. When store memory operations are performed, the pointer is incremented to indicate the storage location of the store operation. As operations are deleted from load/store buffer <b>50</b>, the pointer is updated to continue indicating the most recently performed store.</p><p>A mispredicted branch conductor <b>82</b> and a retire mispredicted branch conductor <b>84</b> are coupled between reorder buffer <b>32</b> and update control unit <b>56</b>. When a mispredicted branch instruction is detected, a signal is asserted upon mispredicted branch conductor <b>82</b>. In response to the signal, load/store unit <b>26</b> halts speculative store execution. When the mispredicted branch instruction is retired (as indicated by an asserted signal upon retire mispredicted branch conductor <b>84</b>), the operations which are subsequent to the mispredicted branch instruction along the mispredicted path of the instruction stream still remain within load/store buffer <b>50</b>. These speculatively performed stores are restored to data cache <b>28</b>.</p><p>The restore operations are handled by update control unit <b>56</b>, in concert with access selection control unit <b>54</b> and restore control unit <b>62</b>. Update control unit <b>56</b> selects the store indicated by the store pointer within storage location <b>80</b>, and indicates the selection to access selection control unit <b>54</b>. Access selection control unit <b>54</b> selects the operation for conveyance to cache storage and control block <b>60</b>. Additionally, the restore indication is asserted for the access upon cache access buses <b>70</b>. Cache storage and control block <b>60</b> receives the restore indication, and indexes the cache with the associated address. The data for the restore is read from restore buffer <b>64</b> according to the restore pointer stored in storage location <b>74</b>. For set associative caches, the column of the affected cache line is read from restore buffer <b>64</b> as well. Accordingly, the original data corresponding to the store is restored to the affected cache line. Both the speculative store pointer and the restore pointer are updated to indicate the next most recent speculative store and corresponding original data, and the restored speculative store may be discarded from load/store buffer <b>50</b>. These restore actions are repeated by load/store buffer <b>50</b> until each speculative store has been restored (i.e. until the speculative store pointer points to the bottom of load/store buffer <b>50</b>). It is noted that microprocessor <b>10</b> may fetch instructions from the correct instruction stream immediately upon detection of a mispredicted branch. Load/store unit <b>26</b> halts speculative store operation upon such detection in order to simplify speculative store restoration. Upon retirement of a mispredicted branch, speculative stores within load/store buffer <b>50</b> are from the mispredicted path and therefore are restored. If speculative store operation continued after mispredicted branch detection, some of the speculative stores within load/store buffer <b>50</b> upon retirement of the mispredicted branch might be from the correct path. A mechanism for discerning which speculative stores to restore would be employed. Such embodiments are within the spirit and scope of the present invention. Upon completion of the restoration process, speculative store operation resumes.</p><p>When an instruction which causes an exception is retired, reorder buffer <b>32</b> asserts a signal upon exception conductor <b>86</b>. In response to the signal, update control unit <b>56</b> performs restore operations for all outstanding speculative stores. The operations performed are similar to the mispredicted branch retirement case. When an exception is signalled, all outstanding loads and stores are discarded (following any restoring actions). Similarly, if a snoop access which causes a writeback is detected by the physical tags (not shown), the speculative stores are restored prior to performing the writeback. It is noted that a snoop access occurs when an external device coupled to the main memory subsystem accesses memory. The address being accessed is checked against addresses stored in the caches, to ensure that any updates performed by microprocessor <b>10</b> to the accessed memory locations are visible to the accessing device. Otherwise, the device may read data from main memory which does not include the updates, violating memory coherency rules. Additionally, if a device updates a memory location, then caches upon microprocessor <b>10</b> invalidate the corresponding cache line. In this manner, subsequent accesses to the memory location by microprocessor <b>10</b> receive the updated data.</p><p>In addition to storing data and tag addresses, cache storage and control block <b>60</b> stores the state of each cache line. The state indicates the validity of the line, as well as whether or not the line has been modified. A particularly popular state encoding, the MESI state encoding, is employed according to one embodiment. The MESI state encoding includes four states: Modified, Exclusive, Shared, and Invalid. Modified indicates that the cache line is valid in the cache and is modified with respect to main memory. Exclusive indicates that the cache line is valid in the cache and not modified. Additionally, no other device which shares main memory with microprocessor <b>10</b> stores a copy of the cache line. Shared indicates that the cache line is valid in the cache and not modified. Additionally, other devices which share main memory with microprocessor <b>10</b> may store a copy of the cache line. Invalid indicates that the cache line is not valid in the cache. According to this embodiment, stores are not performed to the cache if the cache line is in the exclusive state. If the cache line is exclusive, the store is not performed and the cache line is changed to the modified state in accordance with MESI coherency rules. The store may then be performed. If the cache line is in the modified state, the store is performed. If the cache line is in the shared state, non-speculative stores are performed to the cache as well as to the main memory subsystem. Other caches which may be storing the cache line (i.e. in other microprocessors) invalidate the line when the main memory subsystem is updated. Additionally, the cache line within cache storage and control block <b>60</b> may be changed to the exclusive state. In this manner, microprocessor <b>10</b> ensures that no other device is storing a copy of the affected cache line. Speculative stores to a shared cache line are held until they are non-speculative. Additionally, for embodiments in which data cache <b>28</b> is linearly addressed, the physical tags are updated to the modified state prior to the store for cache snooping purposes.</p><p>In the embodiment shown, restore buffer <b>64</b> is operated as a \u201ccircular buffer\u201d. A circular buffer is a buffer in which the storage locations are used in a rotating fashion, such that each storage location is used prior to the reuse of the first storage location used. For example, a circular buffer having four storage locations numbered zero through three (consecutively), may use storage location zero, then storage location one, then storage location two, then storage location three, then storage location zero, etc. If values are stored into a circular buffer by incrementing through the storage locations, the values may be retraced in the reverse order by decrementing through the storage locations. Due to this reverse order retracing ability, the circular buffer is ideal for use as restore buffer <b>64</b>. Data is added to restore buffer <b>64</b>, and may be retraced in reverse order to perform restore activities. In one embodiment, restore buffer <b>64</b> includes four storage locations. Load/store unit <b>26</b> ensures that no more than four speculative stores are simultaneously outstanding via criterion (iv) of the store selection criteria.</p><p>It is noted that restore buffer <b>64</b> may be operated in fashions other than a circular buffer. For example, restore buffer <b>64</b> may be operated as a stack in which the most recently stored data is always within a particular storage location identified as the top of restore buffer <b>64</b>. Previous entries are shifted down (i.e. away from the top) as new entries are added. Other configurations of restore buffer <b>64</b> may also be used, as long as data may be stored into restore buffer <b>64</b> and retraced in the reverse order for performing restores of speculative stores.</p><p>In one particular embodiment, data cache <b>28</b> comprises a \u201cbanked\u201d cache having eight banks. In a banked cache, the bytes of a particular cache line are stored in a plurality of banks. When a cache access is performed, only the bank including the requested bytes is accessed. By configuring the cache into banks, multiple accesses may be performed concurrently without constructing a full dual ported array, which is more complex and larger than the banked cache. Speculative store memory accesses affect only one bank. Restore buffer <b>64</b> is configured with sufficient storage in each storage location for storing bytes corresponding to a single bank. It is noted that, for this embodiment, loads and stores are defined to be unaligned if they access more than one bank.</p><p>Continuing with the particular embodiment, cache storage and control block <b>60</b> employs way prediction for selecting a column for a particular access. Data is routed upon result buses <b>72</b> in response to cache access upon cache access buses <b>70</b> during a clock cycle. For stores, data is speculatively stored into the predicted column. However, the corresponding hit/miss information is not available until the following clock cycle. Additionally, hit/miss information may indicate a hit in the predicted column, a hit in one of the unpredicted columns, or a miss. Misses are handled as described previously, and hits in the predicted column are similar to the cache hit case described above. However, a hit in an unpredicted column causes corrective actions to be taken. The way prediction is updated to indicate the column which hits. Additionally, the data from the correct way is conveyed during the third clock cycle for loads. Similarly, the original data from the predicted column is restored to the predicted column during the third clock cycle. Original data from the unpredicted column which hits is conveyed to restore buffer <b>64</b> during the second clock cycle. Original data from the predicted column is captured internally by cache storage and control block <b>60</b> in order to perform restoration due to a hit in an unpredicted column or a miss. For a store miss, original data for the predicted column is restored internally by cache storage and control block <b>60</b> and the store is retained in load/store buffer <b>50</b> for performance when the store becomes non-speculative. It is noted that load/store unit <b>26</b> cancels an access to data cache <b>28</b> during a third clock cycle for load or store hits in unpredicted columns and for store misses during a first clock cycle prior to the third clock cycle. Cache storage and control block <b>60</b> uses the idle port(s) on the cache to perform the restoration actions.</p><p>Exemplary timing diagrams <b>85</b>, <b>87</b>, <b>88</b>, and <b>89</b> showing a hit in an unpredicted column for a load, hit in a predicted column for a store, hit in an unpredicted column for a store, and store miss are shown as FIGS. 3A, <b>3</b>B, <b>3</b>C, and <b>3</b>D, respectively. Clock cycles are separated by vertical dashed lines. It is noted that, for embodiments employing timing similar to the timing diagrams of FIGS. 3A-3D, a separate bus from result buses <b>72</b> may be used to convey data to restore buffer <b>64</b>.</p><p>Turning now to FIG. 4A, a diagram of an exemplary storage location <b>90</b> from load/store buffer <b>50</b> is shown according to one embodiment of load/store buffer <b>50</b>. Storage location <b>90</b> includes a valid field <b>92</b>, a type field <b>94</b>, an address valid field <b>96</b>, an address field <b>98</b>, a data valid field <b>100</b>, a data field <b>102</b>, and a control field <b>106</b>.</p><p>Valid field <b>92</b> comprises a bit indicative, when set, that storage location <b>90</b> is storing a load or store memory access. When clear, the bit indicates that storage location <b>90</b> is not storing a load or store memory access (i.e. storage location <b>90</b> is empty). Address valid field <b>96</b> and data valid field <b>100</b> comprise bits as well. Address valid field <b>96</b> indicates the validity of address field <b>98</b>, while data valid field <b>100</b> indicates the validity of data field <b>102</b>. Type field <b>94</b> identifies the type of memory access stored in location <b>90</b>. Type field <b>94</b> includes a pair of bits in one embodiment. The first of the pair of bits is indicative, when set, of a load memory access. The second of the pair of bits is indicative, when set, of a store memory access. This encoding allows a load and a store memory access which are derived from the same instruction to be stored in the same storage location <b>90</b>.</p><p>Address field <b>98</b> stores the address associated with the memory access. The address is provided by a functional unit <b>24</b>, and may be provided during a different clock cycle than the memory access is placed into load/store buffer <b>50</b>. In one embodiment, address field <b>98</b> includes 32 bits for storing a 32 bit address. Data field <b>102</b> stores the data associated with a store memory access. In one embodiment, data field <b>102</b> comprises 32 bits for storing up to 32 bits of store data.</p><p>Control field <b>106</b> stores additional control information regarding memory accesses. In one embodiment, control field <b>106</b> includes a reorder buffer tag identifying a storage location within reorder buffer <b>32</b> which stores an instruction corresponding to the memory access. Additionally, an accessed bit is included indicating that the memory access was selected for cache access during the previous clock cycle. The memory access is thereby prevented from being selected by access selection control unit <b>54</b> during the clock cycle, in which cache hit information is provided upon status bus <b>76</b>. A size is included for indicating the number of bytes operated upon by the memory access. Still further, a pair of miss bits are included for indicating that cache misses were detected for an access. Two bits are included for the possibility of unaligned accesses missing in either one of their two access to data cache <b>28</b>. A serialize bit indicates that the memory access should not be performed speculatively. Certain memory accesses, such as I/O accesses, are not performed speculatively because the access storage locations outside of microprocessor <b>10</b>. A dependent bit indicates that a memory access is dependent upon a prior memory access. If the dependent bit is set, the memory access is not selected by address selection control unit <b>54</b>. An unaligned bit indicates, when set, that the memory access is unaligned. Finally, a physical address bit indicates, when set, that the address in address field <b>98</b> is a physical address which must be checked against the physical tags to determine a cache hit or miss.</p><p>Turning next to FIG. 4B, a storage location <b>110</b> within restore buffer <b>64</b> is shown according to one embodiment of restore buffer <b>64</b>. Storage location <b>110</b> includes a data field <b>112</b> and a column field <b>114</b>. Data field <b>112</b> stores the original data (i.e. the data prior to performance of the store) from the cache line affected by the corresponding speculative store. In one embodiment, data field <b>112</b> is configured to store data from one bank of data cache <b>28</b>. In one particular embodiment, one bank of data comprises 32 bits. Column field <b>114</b> stores an indication of the column within data cache <b>28</b> from which the data in data field <b>112</b> was taken. Column field <b>114</b> is used to select the column to receive the data stored in data field <b>112</b> when a restore operation is performed using that data. In embodiments of data cache <b>28</b> employing a direct-mapped cache, column field <b>114</b> is eliminated.</p><p>Turning to FIG. 5, a flow chart is shown indicating the steps employed for restoration of speculative stores following retirement of a mispredicted branch. Similar steps are performed for exceptions and snoop indications. Decision box <b>120</b> indicates whether or not a restoration should be performed. If retirement of a mispredicted branch is not detected, no restoration is performed. If retirement of a mispredicted branch is detected, restoration is performed.</p><p>The actions indicated by boxes <b>122</b> and <b>124</b> are performed for each speculative store within load/store buffer <b>50</b>, beginning with the most recently performed speculative store (i.e. the speculative store indicated by the store pointer). Data is read from the restore buffer storage location indicated by the restore buffer pointer. The data is stored into data cache <b>28</b> in the row indicated by the address of the store and the column indicated by the restore buffer storage location. The store memory access may then be discarded from load/store buffer <b>50</b>. It is noted that, in one embodiment, the store memory access is discarded if reorder buffer <b>32</b> indicates that the store memory access is discarded and the restore operation has been performed for the store memory access.</p><p>After completion of the restore for the current store memory access, load/store unit <b>26</b> determines if additional speculative stores remain to be restored. If the store pointer is indicating the bottom storage location of load/store buffer <b>50</b>, then the restoration is completed. If the store pointer is indicating a storage location other than the bottom of load/store buffer <b>50</b>, the restoration is not complete. The decision step is shown as decision box <b>126</b> in FIG. <b>5</b>. If additional action is required, the store pointer and restore buffer pointer are both decremented to indicate the next most recent speculative store and corresponding original data (step <b>128</b>). Load/store unit <b>26</b> returns to step <b>122</b> in the following clock cycle to continue the restoration process.</p><p>Turning now to FIG. 6, a computer system <b>200</b> including microprocessor <b>10</b> is shown. Computer system <b>200</b> further includes a bus bridge <b>202</b>, a main memory <b>204</b>, and a plurality of input/output (I/O) devices <b>206</b>A-<b>206</b>N. Plurality of I/O devices <b>206</b>A-<b>206</b>N will be collectively referred to as I/O devices <b>206</b>. Microprocessor <b>10</b>, bus bridge <b>202</b>, and main memory <b>204</b> are coupled to a system bus <b>208</b>. I/O devices <b>206</b> are coupled to an I/O bus <b>210</b> for communication with bus bridge <b>202</b>.</p><p>Bus bridge <b>202</b> is provided to assist in communications between I/O devices <b>206</b> and devices coupled to system bus <b>208</b>. I/O devices <b>206</b> typically require longer bus clock cycles than microprocessor <b>10</b> and other devices coupled to system bus <b>208</b>. Therefore, bus bridge <b>202</b> provides a buffer between system bus <b>208</b> and input/output bus <b>210</b>. Additionally, bus bridge <b>202</b> translates transactions from one bus protocol to another. In one embodiment, input/output bus <b>210</b> is an Enhanced Industry Standard Architecture (EISA) bus and bus bridge <b>202</b> translates from the system bus protocol to the EISA bus protocol. In another embodiment, input/output bus <b>108</b> is a Peripheral Component Interconnect (PCI) bus and bus bridge <b>202</b> translates from the system bus protocol to the PCI bus protocol. It is noted that many variations of system bus protocols exist. Microprocessor <b>10</b> may employ any suitable system bus protocol.</p><p>I/O devices <b>206</b> provide an interface between computer system <b>200</b> and other devices external to the computer system. Exemplary I/O devices include a modem, a serial or parallel port, a sound card, etc. I/O devices <b>206</b> may also be referred to as peripheral devices. Main memory <b>204</b> stores data and instructions for use by microprocessor <b>10</b>. In one embodiment, main memory <b>204</b> includes at least one Dynamic Random Access Memory (DRAM) and a DRAM memory controller.</p><p>It is noted that although computer system <b>200</b> as shown in FIG. 6 includes one microprocessor, other embodiments of computer system <b>200</b> may include multiple microprocessors. Similarly, computer system <b>200</b> may include multiple bus bridges <b>202</b> for translating to multiple dissimilar or similar I/O bus protocols. Still further, a cache memory for enhancing the performance of computer system <b>200</b> by storing instructions and data referenced by microprocessor <b>10</b> in a faster memory storage may be included. The cache memory may be inserted between microprocessor <b>10</b> and system bus <b>208</b>, or may reside on system bus <b>208</b> in a \u201clookaside\u201d configuration.</p><p>It is noted that the present discussion may refer to the assertion of various signals. As used herein, a signal is \u201casserted\u201d if it conveys a value indicative of a particular condition. Conversely, a signal is \u201cdeasserted\u201d if it conveys a value indicative of a lack of a particular condition. A signal may be defined to be asserted when it conveys a logical zero value or, conversely, when it conveys a logical one value. Additionally, various values have been described as being discarded in the above discussion. A value may be discarded in a number of manners, but generally involves modifying the value such that it is ignored by logic circuitry which receives the value. For example, if the value comprises a bit, the logic state of the value may be inverted to discard the value. If the value is an n-bit value, one of the n-bit encodings may indicate that the value is invalid. Setting the value to the invalid encoding causes the value to be discarded. Additionally, an n-bit value may include a valid bit indicative, when set, that the n-bit value is valid. Resetting the valid bit may comprise discarding the value. Other methods of discarding a value may be used as well.</p><p>It is further noted that the terms most recently performed, next most recently performed, and least recently performed have been used herein to describe speculative store memory accesses. A speculative store is most recently performed among a set of speculative stores if it accessed the cache subsequent to each of the other speculative stores within the set. The next most recently performed speculative store is the speculative store which accessed the cache subsequent to each other speculative store within the set except for the most recently performed speculative store. The least recently performed speculative store access the cache prior to each other speculative store within the set.</p><p>Table 1 below indicates fast path, double dispatch, and MROM instructions for one embodiment of microprocessor <b>10</b> employing the x86 instruction set:</p><p><tables id=\"TABLE-US-00002\"><table colsep=\"0\" frame=\"none\" rowsep=\"0\"><tgroup align=\"left\" cols=\"1\" colsep=\"0\" rowsep=\"0\"><colspec align=\"center\" colname=\"1\" colwidth=\"217PT\"></colspec><thead valign=\"bottom\"><row><entry morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\">TABLE 1</entry></row></thead><tbody valign=\"top\"><row><entry align=\"center\" morerows=\"0\" nameend=\"1\" namest=\"1\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\">x86 Fast Path, Double Dispatch, and MROM Instructions</entry></row></tbody></tgroup><tgroup align=\"left\" cols=\"3\" colsep=\"0\" rowsep=\"0\"><colspec align=\"left\" colname=\"OFFSET\" colwidth=\"35PT\"></colspec><colspec align=\"left\" colname=\"1\" colwidth=\"77PT\"></colspec><colspec align=\"left\" colname=\"2\" colwidth=\"105PT\"></colspec><tbody valign=\"top\"><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">x86 Instruction</entry><entry morerows=\"0\" valign=\"top\">Instruction Category</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AAA</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AAD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AAM</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AAS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ADC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ADD</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">AND</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ARPL</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BOUND</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BSF</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BSR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BSWAP</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BT</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BTC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BTR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">BTS</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CALL</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CBW</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CWDE</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CLC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CLD</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CLI</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CLTS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMP</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPXCHG</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CMPXCHG8B</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CPUID</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CWD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">CWQ</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">DDA</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">DAS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">DEC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">DIV</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ENTER</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">HLT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">IDIV</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">IMUL</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">IN</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INTO</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INVD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">INVLPG</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">IRET</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">IRETD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">Jcc</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">JCXZ</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">JECXZ</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">JMP</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LAHF</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LAR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LDS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LES</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LFS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LGS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LSS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LEA</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LEAVE</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LGDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LIDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LLDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LMSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LODS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LODSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LODSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LODSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LOOP</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LOOPcond</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LSL</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">LTR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOV</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVCC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOV CR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOV DR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVSX</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOVZX</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MUL</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">NEG</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">MOP</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">NOT</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OUT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OUTS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OUTSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OUTSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">OUTSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">POP</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">POPA</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">POPAD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">POPF</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">POPFD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">PUSH</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">PUSHA</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">PUSHAD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">PUSHF</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">PUSHFD</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">RCL</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">RCR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ROL</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">ROR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">RDMSR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">REPE</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">REPZ</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">REPNE</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">REPNZ</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">RET</entry><entry morerows=\"0\" valign=\"top\">double dispatch</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">RSM</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SAHF</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SAL</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SAR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SHL</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SHR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SBB</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SCAS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SCASB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SCASW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SCASD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SETcc</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SGDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SIDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SHLD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SHRD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SLDT</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SMSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STC</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STD</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STI</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STOS</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STOSB</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STOSW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STOSD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">STR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">SUB</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">TEST</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">VERR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">VERW</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">WBINVD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">WRMSR</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">XADD</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">XCHG</entry><entry morerows=\"0\" valign=\"top\">MROM</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">XLAT</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">XLATB</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry morerows=\"0\" valign=\"top\">XOR</entry><entry morerows=\"0\" valign=\"top\">fast path</entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"center\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" rowsep=\"1\" valign=\"top\"></entry></row><row><entry morerows=\"0\" valign=\"top\"></entry><entry align=\"left\" morerows=\"0\" nameend=\"2\" namest=\"OFFSET\" valign=\"top\">Note: Instructions including an SIB byte are also considered fast path instructions. </entry></row></tbody></tgroup></table></tables></p><p>It is noted that a superscalar microprocessor in accordance with the foregoing may further employ the latching structures as disclosed within the co-pending, commonly assigned patent application entitled \u201cConditional Latching Mechanism and Pipelined Microprocessor Employing the Same\u201d, Ser. No. 08/400,608 filed Mar. 8, 1995, by Pflum et al, now abandoned and continued in application Ser. No. 08/744,707, filed Oct. 31, 1996, now U.S. Pat. No. 5,831,462. The disclosure of this patent application is incorporated herein by reference in its entirety.</p><p>It is further noted that aspects regarding array circuitry may be found in the co-pending, commonly assigned patent application entitled \u201cHigh Performance Ram Array Circuit Employing Self-Time Clock Generator for Enabling Array Access\u201d, Ser. No. 08/473,103 filed Jun. 7, 1995 by Tran, now U.S. Pat. No. 5,619,464. The disclosure of this patent application is incorporated herein by reference in its entirety.</p><p>It is additionally noted that other aspects regarding superscalar microprocessors may be found in the following co-pending, commonly assigned patent applications:</p><p>\u201cLinearly Addressable Microprocessor Cache\u201d, Ser. No. 08/146,381, filed Oct. 29, 1993 by Witt, now abandoned and continued in application Ser. No. 08/506,509 filed Jul. 24, 1995, now U.S. Pat. No. 5,623,619; \u201cSuperscalar Microprocessor Including a High Performance Instruction Alignment Unit\u201d, Ser. No. 08/377,843, filed Jan. 25, 1995 by Witt, et al, now abandoned and continued in application Ser. No. 08/884,818 filed Jun. 30, 1997, now U.S. Pat. No. 5,819,057; \u201cA Way Prediction Structure\u201d, Ser. No. 08/522,181, filed Aug. 31, 1995 by Roberts, et al, now abandoned and continued in application Ser. No. 08/884,819 filed Jun. 30, 1997, now U.S. Pat. No. 5,845,323; \u201cA Data Cache Capable of Performing Store Accesses in a Single Clock Cycle\u201d, Ser. No. 08/521,627, filed Aug. 31, 1995 by Witt, et al, now U.S. Pat. No. 5,860,104; \u201cA Parallel and Scalable Instruction Scanning Unit\u201d, Ser. No. 08/475,400, filed Jun. 7, 1995 by Narayan, now abandoned and continued in application Ser. No. 08/915,092 filed Aug. 20, 1997, now U.S. Pat. No. 5,875,315; and \u201cAn Apparatus and Method for Aligning Variable-Byte Length Instructions to a Plurality of Issue Positions\u201d, Ser. No. 08/582,473, filed Jan. 2, 1996 by Narayan, et al, now U.S. Pat. No. 5,822,559. The disclosure of these patent applications are is incorporated herein by reference in their entirety.</p><p>In accordance with the above disclosure, an apparatus for performing speculative stores has been provided. The apparatus may advantageously improve the performance of a microprocessor employing the apparatus. Instead of waiting for an indication that the stores are non-speculative, the apparatus performs the stores speculatively. When the stores become non-speculative, they may retire without having to perform the store at that time. Additionally, store forwarding within the load/store unit of the microprocessor may be eliminated. Loads may access the data cache in order to retrieve the speculative store information.</p><p>Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "H. S.", "last_name": "Ramagopal", "name": ""}, {"first_name": "Rajiv M.", "last_name": "Hattangadi", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "ADVANCED MICRO DEVICES, INC."}], "ipc_classes": [{"primary": true, "label": "G06F  12/08"}, {"primary": false, "label": "G06F   9/312"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/38        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711146"}, {"primary": false, "label": "712E09061"}, {"primary": false, "label": "712225"}, {"primary": false, "label": "711137"}, {"primary": false, "label": "712248"}, {"primary": false, "label": "712E09048"}, {"primary": false, "label": "711140"}, {"primary": false, "label": "712E0905"}], "ecla_classes": [{"label": "G06F   9/38H2"}, {"label": "G06F   9/38E2"}, {"label": "G06F   9/38D4"}], "cpc_classes": [{"label": "G06F   9/3842"}, {"label": "G06F   9/3834"}, {"label": "G06F   9/3834"}, {"label": "G06F   9/3863"}, {"label": "G06F   9/3842"}, {"label": "G06F   9/3863"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1996-03-26", "application_date": "1999-09-17", "family_members": [{"ucid": "US-6233657-B1", "titles": [{"lang": "EN", "text": "Apparatus and method for performing speculative stores"}]}, {"ucid": "US-6006317-A", "titles": [{"lang": "EN", "text": "Apparatus and method performing speculative stores"}]}, {"ucid": "US-5838943-A", "titles": [{"lang": "EN", "text": "Apparatus for speculatively storing and restoring data to a cache memory"}]}]}