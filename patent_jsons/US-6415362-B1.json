{"patent_number": "US-6415362-B1", "publication_id": 73058329, "family_id": 23171753, "publication_date": "2002-07-02", "titles": [{"lang": "EN", "text": "Method and system for write-through stores of varying sizes"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50341500\"><p>A method and system for performing write-through store operations of valid data of varying sizes in a data processing system, where the data processing system includes multiple processors that are coupled to an interconnect through a memory hierarchy, where the memory hierarchy includes multiple levels of cache, where at least one lower level of cache of the multiple of levels of cache requires store operations of all valid data of at least a predetermined size. First, it is determined whether or not a write-through store operation is a cache hit in a higher level of cache of the multiple levels of cache. In response to a determination that cache hit has occurred in the higher level of cache, the write-through store operation is merged with data read from the higher level of cache to provide a merged write-through operation of all valid data of at least the predetermined size to a lower level of cache. The merged write-through operation is performed in the lower level of cache, such that write-through operations of varying sizes to a lower level of cache which requires write operations of all valid data of at least a predetermined size are performed with data merged from a higher level of cache.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00001\" num=\"1\"><claim-text>1. A method of performing write-through store operations of valid data of varying sizes in a data processing system, where said data processing system includes a plurality of processors that are coupled to an interconnect through a memory hierarchy, where said memory hierarchy includes a plurality of levels of cache, where at least one lower level of cache of said plurality of levels of cache requires write operations of all valid data of at least a predetermined size, said method comprising the steps of:</claim-text><claim-text>detecting whether or not a write-through store operation is a cache hit in a higher level of cache of said plurality of levels of cache; </claim-text><claim-text>merging said write-through store operation with data read from said higher level of cache to provide a merged write-through operation of said all valid data of at least said predetermined size to said lower level of cache, in response to detecting a cache hit in said higher level of cache; and </claim-text><claim-text>writing said merged write-through operation in said lower level of cache, in response to detecting a cache hit in said lower level of cache for said write-through operation, such that the write-through operations of valid data of varying sizes to said lower level of cache which requires write operations of said all valid data of at least said predetermined size are performed with data merged from said higher level of cache. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00002\" num=\"2\"><claim-text>2. The method of performing write-through store operations according to <claim-ref idref=\"US-6415362-B1-CLM-00001\">claim 1</claim-ref>, said method further comprising the step of:</claim-text><claim-text>passing said write-through operation to said lower level of cache without merging data, in response to not detecting a cache hit in said higher level of cache; </claim-text><claim-text>determining whether or not said write-through operation is said all valid data of at least said predetermined size; and </claim-text><claim-text>setting a valid byte indication flag for said write-through operation, in response to determining that said write-through operation is said all valid data of at least said predetermined size. </claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00003\" num=\"3\"><claim-text>3. The method of performing write-through store operations according to <claim-ref idref=\"US-6415362-B1-CLM-00002\">claim 2</claim-ref>, said method further comprising the steps of:</claim-text><claim-text>storing said write-through store operation in a store queue until issued within said lower level of cache; </claim-text><claim-text>determining whether said valid byte indication flag is set for said write-through operation issued from said store queue, in response to a detecting a cache hit in said lower level of cache for said write-through operation; and </claim-text><claim-text>in response to determining that said valid byte indication flag is not set for said write-through operation: </claim-text><claim-text>storing said write-through store operation in a register; </claim-text><claim-text>flushing a portion of said cache of said predetermined size to which said write-through store operation is targeted; said </claim-text><claim-text>passing said write-through store operation from said register to a next level of cache of said plurality of levels of cache. </claim-text></claim>"}, {"num": 4, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00004\" num=\"4\"><claim-text>4. The method of performing write-through store operations according to <claim-ref idref=\"US-6415362-B1-CLM-00002\">claim 2</claim-ref>, said method further comprising the steps of:</claim-text><claim-text>storing said write-through store operation in a store queue until issued within said lower level of cache; </claim-text><claim-text>determining whether said valid byte indication flag is set for said write-through operation issued from said store queue, in response to a detecting a cache hit in said lower level of cache for said write-through operation; and </claim-text><claim-text>in response to determining that said valid byte indication flag is set for said write-through operation: </claim-text><claim-text>writing said write-through store operation in a lower level of cache; and </claim-text><claim-text>passing said write-through store operation to said next level of cache of said plurality of levels of cache. </claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00005\" num=\"5\"><claim-text>5. The method of performing write-through store operations according to <claim-ref idref=\"US-6415362-B1-CLM-00001\">claim 1</claim-ref>, said method further comprising the step of:</claim-text><claim-text>storing said merged write-through operation in a store queue until issued within said lower level cache. </claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00006\" num=\"6\"><claim-text>6. The method of performing write-through store operations according to <claim-ref idref=\"US-6415362-B1-CLM-00001\">claim 1</claim-ref>, said method further comprising the step of:</claim-text><claim-text>indicating that said write-through store operation is said all valid data of at least a predetermined size by setting said valid byte indication flag. </claim-text></claim>"}, {"num": 7, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00007\" num=\"7\"><claim-text>7. A data processing system comprising:</claim-text><claim-text>an interconnect; </claim-text><claim-text>a plurality of processors coupled to said interconnect through a memory hierarchy; </claim-text><claim-text>said memory hierarchy which includes a plurality of levels of cache; </claim-text><claim-text>at least one lower level of cache of said plurality of levels of cache which requires store operations of valid data of at least a predetermined size; </claim-text><claim-text>means for detecting whether or not a write-through store operation is a cache hit in a higher level of cache of said plurality of levels of cache; </claim-text><claim-text>means for merging said write-through store operation with data read from said higher level of cache to provide a merged write-through operation of all said valid data of at least said predetermined size to a lower level of cache, in response to detecting a cache hit in said higher level of cache; and </claim-text><claim-text>means for writing said merged write-through operation in said lower level of cache, in response to detecting a cache hit in said lower level for said write-through operation, such that write-through operations of varying sizes to said lower level of cache which requires write operations of said valid data of at least said predetermined size are performed with data merged from a higher level of cache. </claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00008\" num=\"8\"><claim-text>8. The data processing system according to <claim-ref idref=\"US-6415362-B1-CLM-00007\">claim 7</claim-ref>, said system further comprising:</claim-text><claim-text>means for passing said write-through operation to said lower level of cache without merging data, in response to not detecting a cache hit in said higher level of cache; </claim-text><claim-text>means for determining whether or not said write-through operation is all said valid data of at least said predetermined size; and </claim-text><claim-text>means for setting a valid byte indication flag for said write-through operation, in response to determining that said write-through operation is all said valid data of at least said predetermined size. </claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00009\" num=\"9\"><claim-text>9. The data processing system according to <claim-ref idref=\"US-6415362-B1-CLM-00008\">claim 8</claim-ref>, said system further comprising:</claim-text><claim-text>a store queue for storing said write-through store operation until said write-through store operation is issued within said lower level of cache; </claim-text><claim-text>means for determining whether said valid byte indication flag is set for said write-through operation issued from said store queue, in response to detecting a cache hit for said write-through operation in said lower level cache; </claim-text><claim-text>means for storing said write-through store operation in a register, in response to determining that said valid byte indication flag is not set for said write-through operation; </claim-text><claim-text>means for flushing a portion of said cache of said predetermined size to which said write-through store operation is targeted; and </claim-text><claim-text>means for passing said write-through store operation from said register to next level of cache of said plurality of levels of cache. </claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00010\" num=\"10\"><claim-text>10. The data processing system according to <claim-ref idref=\"US-6415362-B1-CLM-00009\">claim 9</claim-ref>, wherein said means for storing said write-through operation comprises:</claim-text><claim-text>a register for holding said write-through store operation during flushing, said register for issuing said write-through store operation after a flushing operations is issued. </claim-text></claim>"}, {"num": 11, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00011\" num=\"11\"><claim-text>11. The data processing system according to <claim-ref idref=\"US-6415362-B1-CLM-00008\">claim 8</claim-ref>, said system further comprising:</claim-text><claim-text>a store queue for storing said write-through store operation until said write-through store operation is issued within said lower level of cache; </claim-text><claim-text>means for determining whether said valid byte indication flag is set for a write-through operation issued from said store queue, in response to detecting a cache hit for said write-through operation in said lower level cache; </claim-text><claim-text>means for writing said write-through store operation in said lower level of cache, in response to determining that said valid byte indication flag is set for said write-through operation; and </claim-text><claim-text>means for passing a write-through store operation to a next level of cache of said plurality of levels of cache. </claim-text></claim>"}, {"num": 12, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00012\" num=\"12\"><claim-text>12. The data processing system according to <claim-ref idref=\"US-6415362-B1-CLM-00007\">claim 7</claim-ref>, said system further comprising:</claim-text><claim-text>means for determining whether or not a write-through store operation is all said valid data of at least said predetermined size; and </claim-text><claim-text>a size indication flag which is set in response to determining that said write-through store operation is all said valid data of at least said predetermined size. </claim-text></claim>"}, {"num": 13, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00013\" num=\"13\"><claim-text>13. The data processing system according to <claim-ref idref=\"US-6415362-B1-CLM-00007\">claim 7</claim-ref>, said system further comprising:</claim-text><claim-text>a store queue for holding write-through operations at said lower level of cache until said write-through operations are issued. </claim-text></claim>"}, {"num": 14, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00014\" num=\"14\"><claim-text>14. The data processing system according to <claim-ref idref=\"US-6415362-B1-CLM-00007\">claim 7</claim-ref>, wherein said means for merging said write-through store operation with data read from said higher level of cache to provide a merged write-through operation of all valid data of at least said predetermined size to a lower level of cache, in response to detecting a cache hit in said higher level of cache further comprises:</claim-text><claim-text>a sense amp for reading data from said higher level cache; </claim-text><claim-text>a write driver for writing data to said higher level cache; </claim-text><claim-text>a multiplexer for selecting from said data read from said higher level cache and said data being written to said higher level cache; and </claim-text><claim-text>a write-enable signal for enabling said multiplexer for selection. </claim-text></claim>"}, {"num": 15, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00015\" num=\"15\"><claim-text>15. The data processing system according to <claim-ref idref=\"US-6415362-B1-CLM-00007\">claim 7</claim-ref>, said means for writing said merged write-through operation in said lower level of cache further comprising:</claim-text><claim-text>a store queue for holding said merged write-through operation until said merged write-through operation is issued; </claim-text><claim-text>a multi-level pipeline for passing said write-through operation to said lower level cache to be performed. </claim-text></claim>"}, {"num": 16, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00016\" num=\"16\"><claim-text>16. The data processing system according to <claim-ref idref=\"US-6415362-B1-CLM-00007\">claim 7</claim-ref>, wherein said lower level of cache which requires store operations of at least said predetermined size is an L3 cache.</claim-text></claim>"}, {"num": 17, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6415362-B1-CLM-00017\" num=\"17\"><claim-text>17. The data processing system according to <claim-ref idref=\"US-6415362-B1-CLM-00007\">claim 7</claim-ref>, wherein said higher level of cache is an L2 cache.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53590805\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Technical Field</p><p>The present invention relates in general to an improved method and system for data processing and in particular to an improved method and system for performing write-through stores in a multiprocessor data processing system. Still more particularly, the present invention relates to a method and system for maintaining cache coherency for write-through store operations in a multiprocessor system where the stores are of varying sizes.</p><p>2. Description of the Related Art</p><p>In a conventional symmetric multiprocessor (SMP) data processing system, all of the processors are generally identical, that is, the processors all utilize common instruction sets and communication protocols, have similar hardware architectures, and are generally provided with similar memory hierarchies. For example, a conventional SMP data processing system may comprise a system memory, a plurality of processing elements that each include a processor and one or more levels of cache memory and a system bus coupling the processing elements to each other and to the system memory. To obtain valid execution results in a SMP data processing system, it is important to maintain a coherent memory hierarchy, that is, to provide a single view of the contents of memory to all of the processors.</p><p>A coherent memory hierarchy is maintained through the use of a selected memory coherency protocol, such as the MESI protocol. In the MESI protocol, an indication of a coherency state is stored in association with each coherency granule (e.g. cache line or sector) of at least all upper level (cache) memories. Each coherency granule can have one of four states, modified (M), exclusive (E), shared (S), or invalid (I), which is indicated by two bits in the cache directory. The modified state indicates that a coherency granule is valid only in the cache storing the modified coherency granule and that the value of the modified coherency granule has not been written to system memory. When a coherency granule is indicated as exclusive, the coherency granule is resident in, of all caches, at that level of the memory hierarchy, only the cache having the coherency granule in the exclusive state. The data in the exclusive state is consistent with system memory, however. If a coherency granule is marked as shared in a cache directory, the coherency granule is resident in the associated cache and in at least one other cache at the same level of the memory hierarchy, all of the copies of the coherency granule being consistent with system memory. Finally, the invalid state indicates that the data and address tag associated with a coherency granule are both in invalid.</p><p>A write-through store updates data at each valid level in the cache hierarchy as well as main memory which corresponds to the address accessed. In particular, a write-through or store-through cache operates to provide a write operation to both the cache memory and the main memory during processor write operations, thus insuring consistency between the data in the cache memory and the main memory. To maintain cache coherency, a coherent write-through store must either flush valid matching cache lines on a processor or modify the data associated with a particular line to reflect the update caused by the write-through store. This ensures that subsequent loads from all processors obtain the newly updated data. Typically, a bus \u201csnooping\u201d technique is utilized to initiate invalidation of cache lines.</p><p>Preferably, a write-through store can be valid data of varying sizes from one byte up to the largest data type for a particular processor. In certain caches, handling write-through stores of varying size poses problems. In particular some caches do not provide a capability to write valid data less than a particular size. Therefore, to write-through data in these caches a cache line must first be flushed and invalidated, utilizing processor cycles. Thereby, the write-through operation passes by the cache onto the next memory without being written. However, it is preferable to maintain data in the caches for reduced latency accessing. In addition, since flushing for each write-through store is inefficient, it is therefore desirable to provide for the write-through of varying sizes of valid data to caches which do not provide the capability to write valid data less than a particular width.</p><h4>SUMMARY OF THE INVENTION</h4><p>It is therefore one object of the present invention to provide an improved method and system for data processing.</p><p>It is therefore another object of the present invention to provide an improved method and system for performing write-through stores in a multiprocessor data processing system</p><p>It is yet another object of the present invention to provide an improved method and system for maintaining cache coherency for write-through store operations in a multiprocessor system where the stores are of varying sizes.</p><p>The foregoing objects are achieved as is now described. A method and system for performing write-through store operations of valid data of varying sizes in a data processing system are provided, where the data processing system includes multiple processors that are coupled to an interconnect through a memory hierarchy, where the memory hierarchy includes multiple levels of cache, where at least one lower level of cache of the multiple of levels of cache requires store operations of all valid data of at least a predetermined size. First, it is determined whether or not a write-through store operation is a cache hit in a higher level of cache of the multiple levels of cache. In response to a determination that a cache hit has occurred in the higher level of cache, the write-through store operation is merged with data read from the higher level of cache to provide a merged write-through operation of all valid data of at least the predetermined size to a lower level of cache. The merged write-through operation is performed in the lower level of cache, such that write-through operations of varying sizes to a lower level of cache which requires write operations of all valid data of at least a predetermined size are performed with data merged from a higher level of cache.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The novel features believed characteristic of the invention are set forth in the appended claims. The invention itself, however, as well as a preferred mode of use, further objects and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein:</p><p>FIG. 1 depicts a high level block diagram of a multiprocessor data processing system in accordance with the present invention;</p><p>FIG. 2 illustrates a schematic diagram of a path for reading from and writing to L2 cache according to the method and system of the present invention;</p><p>FIG. 3 depicts a high level block diagram of the address pipeline for an L3 cache;</p><p>FIG. 4 illustrates a block diagram representation of the L2 to L3 store queue depicted in FIG. 3;</p><p>FIG. 5 depicts a timing diagram representation of two write-through store transactions in L3 cache;</p><p>FIG. 6 illustrates a high level logic flowchart of a process for passing a write-through store transaction from a higher level of cache to a lower level; and</p><p>FIG. 7 depicts a high level logic flowchart of a process for issuing a write-through store instruction to L3 cache.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENT</h4><p>With reference now to the figures, and in particular with reference to FIG. 1, there is illustrated a high level block diagram of a multiprocessor data processing system in accordance with the present invention. As depicted, data processing system <b>8</b> includes a number of processors <b>10</b><i>a</i>-<b>10</b><i>n</i>, which each preferably comprise one of the PowerPC line of processors available from International Business Machines Corporation. In addition to the conventional registers, instruction flow logic and execution units utilized to execute program instructions, each of processors <b>10</b><i>a</i>-<b>10</b><i>n </i>also includes an associated one of on-board level one (L1) caches <b>12</b><i>a</i>-<b>12</b><i>n</i>, which temporarily stores instructions and data that are likely to be accessed by the associated processor. Although L1 caches <b>12</b><i>a</i>-<b>12</b><i>n </i>are illustrated in FIG. 1, as unified caches that store both instruction and data(both referred to hereinafter simply as data), those skilled in the art will appreciate that each of L1 caches <b>12</b><i>a</i>-<b>12</b><i>n </i>could alternatively be implemented as bifurcated instruction and data caches.</p><p>In order to minimize latency, data processing system <b>8</b> may also include one or more additional levels of cache memory, such as level two (L2) caches <b>14</b><i>a</i>-<b>14</b><i>n</i>, which are utilized to stage data to L1 caches <b>12</b><i>a</i>-<b>12</b><i>n</i>. L2 caches <b>14</b><i>a</i>-<b>14</b><i>n </i>may be on-chip as depicted with L1 caches <b>12</b><i>a</i>-<b>12</b><i>n </i>or may be off-chip. L2 caches <b>14</b><i>a</i>-<b>14</b><i>n </i>function as intermediate storage between level three (L3) caches <b>15</b><i>a</i>-<b>15</b><i>n </i>and L1 caches <b>12</b><i>a</i>-<b>12</b><i>n</i>, and can typically store a much larger amount of data than L1 caches <b>12</b><i>a</i>-<b>12</b><i>n</i>, but at a longer access latency. For example, L2 caches <b>14</b><i>a</i>-<b>14</b><i>n </i>may have a storage capacity of 512 kilobytes, which L1 caches <b>12</b><i>a</i>-<b>12</b><i>n </i>may have a storage capacity of 128 kilobytes.</p><p>Data processing system <b>8</b> is further supported by lookaside L3 caches <b>15</b><i>a</i>-<b>15</b><i>n </i>which are positioned between interconnect <b>16</b> and processors <b>10</b><i>a</i>-<b>10</b><i>n</i>. Each of L3 caches <b>15</b><i>a</i>-<b>15</b><i>n </i>is preferably a 4 M byte static random access memory (SRAM). In particular, L3 caches <b>15</b><i>a</i>-<b>15</b><i>n </i>utilize high performance 8 megabit double data rate (DDR) SRAM. Such SRAMs, do not provide a capability to write data less than the width of the SRAM data bus which is typically 4 bytes for the SRAM described. However, these L3 caches <b>15</b><i>a</i>-<b>15</b><i>n </i>support write-through store transactions which can vary from one to sixteen bytes by the method and system of the present invention. While a particular SRAM has been described, additional fixed length SRAM caches may also be utilized as will be understood by one skilled in the art. In addition, although FIG. 1 depicts only three levels of cache, the memory hierarchy of data processing system <b>8</b> could be expanded to include additional levels (L4, L5, etc.) of serially-connected or lookaside caches.</p><p>As illustrated, data processing system <b>8</b> further includes input/output (I/O) devices <b>20</b>, system memory <b>18</b>, and non-volatile storage <b>22</b>, which are each coupled to interconnect <b>16</b>. I/O devices <b>20</b> comprise conventional peripheral devices, such as a display device, keyboard, and graphical pointer, which are interfaced to interconnect <b>16</b> via conventional adapters. Nonvolatile storage <b>22</b> stores an operating system and other software, which are loaded into volatile system memory <b>18</b> in response to data processing system <b>8</b> being powered on. Of course, those skilled in the art will appreciate that data processing system <b>8</b> can include many additional components which are not shown in FIG. 1, such as serial and parallel ports for connection to network or attached devices, a memory controller that regulates access to system memory <b>18</b>, etc.</p><p>Interconnect <b>16</b>, which can comprise one or more buses or a cross-point switch, serves as a conduit for communication transactions between processors <b>10</b><i>a</i>-<b>10</b><i>n</i>, system memory <b>18</b>, I/O devices <b>20</b>, and nonvolatile storage <b>22</b>. A typical communication transaction on interconnect <b>16</b> includes a source tag indicating the source of the transaction, a destination tag specifying the intended recipient of the transaction, an address and/or data. Each device coupled to interconnect <b>16</b> preferably snoops all communication transactions on interconnect <b>16</b>.</p><p>Referring now to FIG. 2, there is depicted a schematic diagram of a path for reading and writing the L2 cache according to the method and system of the present invention. An L2 data subarray <b>30</b> is provided where data is written to or read from L2 data subarray <b>30</b>. In particular, the write-through is performed in a high level of cache, such as L2 data subarray <b>30</b> which preferably contains a byte of a 32 byte sector of the 512 kilobytes available in an L2 cache. For write-through stores, when performing a write-through of a subset of the 32 byte sector, in the case of a cache hit, the bytes that are not being updated are read out of L2 data subarray <b>30</b> through sense amp <b>32</b>. The bytes being written are forwarded to a respective position of L2 data subarray <b>30</b> by write driver <b>34</b> and forwarded to L3 cache. The read and forwarded data is merged in a store queue (not shown). Write driver <b>34</b> is enabled by a write enable signal which also enables a multiplexer <b>36</b>. The write enable signal specifies byte by byte whether to output data read from L2 data subarray <b>30</b> through multiplexer <b>36</b>, or to forward the data being written to L2 data subarray <b>30</b> through multiplexer <b>36</b>.</p><p>For all write-through operations, bytes being written are preferably quickly forwarded to an L3 cache as illustrated by path <b>40</b>. In the case of a write-through operation of 32 bytes that hits in the L2 cache, all data is forwarded to the L3 cache as illustrated by path <b>40</b> without merging with data from L2 data subarray <b>30</b>. While the figure is depicted for a higher level L2 cache passing read/forward data to a lower level L3 cache, other higher level and lower level caches may also be utilized. In addition, while this figures and others following utilize a sector of 32 bytes of data, other sizes of data sectors may also be utilized.</p><p>With reference now to FIG. 3, there is illustrated a high level block diagram of the address pipeline for an L3 cache. An L2 to L3 store queue <b>80</b> holds write-through transaction data forwarded from the L2 cache. In particular, for each write-through operation, a 32-byte bit is set if the transaction includes 32 bytes of valid data either from the original write-through store or from a read of the L2 cache. If the 32-byte bit is not set, when the write-through transaction is issued from store queue <b>80</b>, the write-through transaction is synthesized to a flushing operation and a copy of the queued write-through store transaction is placed in held write-through register <b>82</b>. A multiplexer <b>84</b> decides between transactions from held write-through register <b>82</b>, store queue <b>80</b> and other L3 cache requests <b>96</b>. When a transaction passes through multiplexer <b>84</b>, the transaction is placed in L3 pipeline <b>88</b>. In the present embodiment, L3 pipeline <b>88</b> provides three pipeline stages by T1 register <b>90</b>, T2 register <b>92</b> and T3 register <b>94</b> where data passes through each pipeline register. In addition, an L3 tag array <b>86</b> is provided to maintain a record of data resident in the L3 cache.</p><p>Referring now to FIG. 4, there is illustrated a block diagram representation of the L2 to L3 store queue depicted in FIG. <b>3</b>. The queue passing operation preferably passes an address <b>42</b>, data <b>44</b>, original size attribute <b>46</b>, other instruction information <b>48</b>, and a 32-byte bit <b>50</b> with each write-through store transaction. 32-byte bit <b>50</b> provides an indicator that the write-through transaction has all 32 bytes valid and up-to-date with respect to cache coherency and was originally 32 bytes or hit in the L2 cache (i.e. the data is the merged result of the L2 cache with the write-through data). However, if the transaction included less than 32 valid bytes, then 32-byte bit <b>50</b> is not set. Original size attribute <b>46</b> tracks the size of the write-through store without merged data such that other memory devices which can write-through data of varying sizes may utilize the original data size in doing so.</p><p>With reference now to FIG. 5, there is depicted a timing diagram representation of two write-through store transactions in L3 cache. In a first example, the 32-byte bit is set as illustrated at reference numeral <b>66</b>. Since the 32-byte bit is set for the write-through transaction WTA, in the next clock cycle, WTA is passed to the L3 pipeline and possibly written to the L3 cache if that sector is resident and valid as depicted at reference numeral <b>72</b>. Thereafter, for the next two clock cycles, WTA is passed through the T2 and T3 stages of the L3 pipeline as illustrated at reference numerals <b>74</b> and <b>76</b>. In addition, WTA is passed on to main storage which ignores the 32-byte bit.</p><p>In a second example, the 32-byte bit is not set when the WTB store transaction is made as depicted at reference numeral <b>64</b>. Since the 32-byte bit is not set, the L3 cannot perform the transaction as is since the transaction may overwrite potentially valid data in L3 cache. In particular, when WTB enters the L3 pipeline, the transaction is interpreted as a flushing operation since the 32-byte bit is not set as depicted at reference numeral <b>66</b>. By setting the transaction as a WTB flush, a modified sector will be cast out of the L3 cache and the status of that sector updated to invalid. Further, for valid non-modified sectors, no cast out is generated, but the status of the sector is updated to invalid.</p><p>During the cycle that WTB flush is accepted into the L3 pipeline, the entire write-through transaction is registered in the held write-through register as WTB held as depicted at reference numerals <b>68</b> and <b>70</b>. Thereafter, during the cycle after WTB flush is accepted into the L3 pipeline, the WTB held copy of the write-through store requests access to the L3 cache and is placed on the L3 pipeline. When the WTB held is accepted from the pipeline, WTB held sees an invalid sector since it follows the synthesized flush operation. The original write-through store, WTB held, can now be passed directly to the main storage with no need to update the L3 cache since a cache hit will not occur for the write-through operation.</p><p>Referring now to FIG. 6 there is illustrated a high level logic flowchart of a process for passing a write-through store transaction from a higher level of cache to a lower level. As depicted, the process starts at block <b>100</b> and proceeds to block <b>101</b>. Block <b>101</b> depicts a determination of whether or not there is a cache hit for the write-through store within a segment of the higher level cache, in particular the L2 cache. If there is not a cache hit, the process passes to block <b>102</b>. Block <b>102</b> illustrates a determination of whether the original write-through store has 32 bytes of valid data. If the original size of the write-through operation is 32 bytes of valid data, the process passes to block <b>108</b>. Block <b>108</b> depicts setting the 32-byte bit for the write-through transaction whereafter the process passes to block <b>109</b>. If the original size of the write-through operation is not 32 bytes of valid data, the process passes to block <b>109</b>. Block <b>109</b> illustrates passing the write-through operation to the store queue of the L3 cache whereafter the process returns.</p><p>Returning to block <b>101</b>, if there is a cache hit, the process passes to block <b>104</b>. Block <b>104</b> depicts a determination of whether or not all 32-bytes have been transacted. If all <b>32</b> bytes have been transacted, the process passes to block <b>103</b>. Block <b>103</b> illustrates setting the 32-byte bit for the write-through store operation in the store queue.</p><p>Returning to block <b>104</b>, if all 32 bytes of data are not yet transacted, the process passes to block <b>105</b>. Block <b>105</b> illustrates a determination of whether or not the byte of data is a write-through byte of data. If the data is a write-through byte of data, the process passes to block <b>106</b>. Block <b>106</b> depicts setting the write enable signal for each write-through data byte whereafter the process passes to block <b>104</b>. If the data is not a write-through byte of data, the process passes to block <b>107</b>. Block <b>107</b> illustrates setting the write enable signal off, such that data is read out of the L2 cache whereafter the process passes to block <b>104</b>.</p><p>With reference now to FIG. 7 there is depicted a high level logic flowchart of a process for issuing a write-through store operation to L3 cache. As illustrated, the process starts at block <b>110</b> and proceeds to block <b>112</b>. Block <b>112</b> depicts a determination of whether or not a 32-byte bit is set for the write-through operation being processed next from the store queue. If the 32-byte bit is not set for the write-through store operation, the process passes to block <b>114</b>. Block <b>114</b> illustrates holding a copy of the write-through operation in a held write-through register. Thereafter, block <b>116</b> depicts sending a flush command for the write-through operation to the cache pipeline. Next, block <b>118</b> illustrates a determination of whether or not the data in the cache to be flushed is marked as modified. If the data in the cache is marked as modified, the process passes to block <b>120</b>. Block <b>120</b> depicts the generation of a cast out transaction for the data in the cache such that the data is written to another memory level, preferably system memory <b>18</b> of FIG. <b>1</b>. Thereafter, the process passes to block <b>122</b>. If the cache is not marked modified, the process passes to block <b>122</b>. Block <b>122</b> illustrates marking the data in the cache to be flushed as invalid. Thereafter, block <b>124</b> depicts sending the write-through operation in the held write-through register to the L3 cache pipeline. Next, block <b>126</b> illustrates passing the write-through operation onto the bus whereby other caches and memory locations may be updated. Since the matching data in the cache is flushed, the write-through operation will not encounter a cache hit in the L3 cache, and therefore will pass the write-through operation on to the next memory level without writing to the L3 cache.</p><p>Returning again to block <b>112</b>, if the 32-byte bit for the write-through operation being processed next is set, the process passes to block <b>128</b>. Block <b>128</b> depicts sending the write-through operation to the pipeline. Thereafter, block <b>130</b> illustrates a determination of whether or not there is a cache hit in the L3 cache for the write-through operation. If there is a cache hit in L3 cache, the process passes to block <b>132</b>. Block <b>132</b> depicts writing the write-through operation data to the L3 cache whereafter the process passes to block <b>134</b>. If there is not a cache hit in L3 cache, the process passes to block <b>134</b>. Block <b>134</b> illustrates passing the write-through operation to the next memory level whereafter the process returns.</p><p>While the invention has been particularly shown and described with reference to a preferred embodiment, it will be understood by those skilled in the art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "James Nolan", "last_name": "Hardage", "name": ""}, {"first_name": "Alexander Edward", "last_name": "Okpisz", "name": ""}, {"first_name": "Thomas Albert", "last_name": "Petersen", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTERNATIONAL BUSINESS MACHINES CORPORATION"}, {"first_name": "", "last_name": "", "name": "MOTOROLA, INC."}, {"first_name": "", "last_name": "FREESCALE SEMICONDUCTOR, INC.", "name": ""}, {"first_name": "", "last_name": "FREESCALE SEMICONDUCTOR, INC.", "name": ""}, {"first_name": "", "last_name": "FREESCALE SEMICONDUCTOR, INC.", "name": ""}, {"first_name": "", "last_name": "CITIBANK, N.A., AS COLLATERAL AGENT", "name": ""}, {"first_name": "", "last_name": "CITIBANK, N.A. AS COLLATERAL AGENT", "name": ""}, {"first_name": "", "last_name": "FREESCALE SEMICONDUCTOR, INC.", "name": ""}, {"first_name": "", "last_name": "INTERNATIONAL BUSINESS MACHINES CORPORATION", "name": ""}, {"first_name": "", "last_name": "MOTOROLA, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711142"}, {"primary": false, "label": "711E12024"}, {"primary": false, "label": "711139"}, {"primary": false, "label": "711109"}, {"primary": false, "label": "711122"}, {"primary": false, "label": "711212"}, {"primary": false, "label": "711117"}], "ecla_classes": [{"label": "G06F  12/08B4L"}, {"label": "S06F12:08B16V"}, {"label": "S06F12:08B4P4"}], "cpc_classes": [{"label": "G06F  12/0811"}, {"label": "G06F  12/0831"}, {"label": "G06F  12/0886"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1999-04-29", "application_date": "1999-04-29", "family_members": [{"ucid": "US-6415362-B1", "titles": [{"lang": "EN", "text": "Method and system for write-through stores of varying sizes"}]}]}