{"patent_number": "US-6609190-B1", "publication_id": 73498321, "family_id": 23899401, "publication_date": "2003-08-19", "titles": [{"lang": "EN", "text": "Microprocessor with primary and secondary issue queue"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50537637\"><p>A processor and data processing system suitable for dispatching an instruction to an issue unit. The issue unit includes a primary issue queue and a secondary issue queue. The instruction is stored in the primary issue queue if the instruction is currently eligible to issue for execution. The instruction is stored in the secondary issue queue if the instruction is currently ineligible to issue for execution. An instruction may be moved from the primary issue queue to the secondary issue queue if instruction is dependent upon results from another instruction. In one embodiment, the instruction may be moved from the primary issue queue to the secondary issue queue after issuing the instruction for execution. In this embodiment, the instruction may be maintained in the secondary issue queue for a specified duration. Thereafter, the secondary issue queue entry containing the instruction is deallocated if the instruction has not been rejected.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00001\" num=\"1\"><claim-text>1. A method of executing microprocessor instructions comprising:</claim-text><claim-text>dispatching a first instruction to an issue unit comprising a primary issue queue and a secondary issue queue; </claim-text><claim-text>storing the first instruction in the primary issue queue; </claim-text><claim-text>upon subsequently determining that the first instruction is currently ineligible for execution, moving the first instruction to the secondary issue queue; </claim-text><claim-text>dispatching a second instruction to the primary issue queue; </claim-text><claim-text>upon issuing the second instruction to an execution unit, moving the second instruction to the secondary issue queue wherein the instructions remaining in the primary issue queue are currently eligible for issue to an execution unit; and </claim-text><claim-text>determining a next instruction to issue for execution from the instructions in the primary issue queue. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00002\" num=\"2\"><claim-text>2. The method of <claim-ref idref=\"US-6609190-B1-CLM-00001\">claim 1</claim-ref>, wherein determining that the first instruction is ineligible comprises determining that the first instruction contains a dependency on a previously issued instruction.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00003\" num=\"3\"><claim-text>3. The method of <claim-ref idref=\"US-6609190-B1-CLM-00001\">claim 1</claim-ref>, further comprising maintaining the second instruction in the secondary issue queue for a specified duration and, thereafter, deallocating the secondary issue queue entry containing the second instruction if it has not been rejected.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00004\" num=\"4\"><claim-text>4. The method of <claim-ref idref=\"US-6609190-B1-CLM-00001\">claim 1</claim-ref>, wherein the dispatching of the first instruction to the issue unit comprises retrieving an instruction group containing the first instruction from a basic block cache.</claim-text></claim>"}, {"num": 5, "parent": 4, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00005\" num=\"5\"><claim-text>5. The method of <claim-ref idref=\"US-6609190-B1-CLM-00004\">claim 4</claim-ref>, further comprising, when subsequently dispatching the first instruction, detecting instruction history information in the basic block cache and issuing the first instruction initially to the secondary issue queue if the corresponding instruction history information indicates that the first instruction has a dependency on another instruction.</claim-text></claim>"}, {"num": 6, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00006\" num=\"6\"><claim-text>6. A microprocessor comprising:</claim-text><claim-text>an instruction cache; </claim-text><claim-text>a dispatch unit configured to receive instructions from the instruction cache; and </claim-text><claim-text>an issue unit configured to receive instructions from the dispatch unit, wherein the issue unit is adapted to: </claim-text><claim-text>allocate a dispatched first instruction to a primary issue queue and, upon subsequently determining that the first instruction is ineligible for execution, to move the first instruction to a secondary issue queue; and </claim-text><claim-text>allocate a dispatched second instruction to the primary issue queue and upon issuing the second instruction for execution, moving the second instruction to the secondary issue queue wherein the instructions remaining in the primary issue queue are eligible for execution. </claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00007\" num=\"7\"><claim-text>7. The processor of <claim-ref idref=\"US-6609190-B1-CLM-00006\">claim 6</claim-ref>, further comprising cracking logic intermediate between the instruction cache and the dispatch unit, wherein the cracking logic is adapted to organize a set of instructions received from the instruction cache as an instruction group sharing a common instruction group tag.</claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00008\" num=\"8\"><claim-text>8. The processor of <claim-ref idref=\"US-6609190-B1-CLM-00007\">claim 7</claim-ref>, further comprising a basic block cache intermediate between the cracking logic and the dispatch unit, wherein the basic block cache is suitable for caching</claim-text><claim-text>an instruction cache; </claim-text><claim-text>a dispatch unit configured to receive instructions from the instruction cache; and </claim-text><claim-text>an issue unit configured to receive instructions from the dispatch unit, wherein the issue unit is adapted to: </claim-text><claim-text>allocate a dispatched first instruction to a primary issue queue and, upon subsequently determining that the first instruction is ineligible for execution, to move the first instruction to a secondary issue queue; and </claim-text><claim-text>allocate a dispatched second instruction to the primary issue queue and upon issuing the second instruction for execution, moving the second instruction to the secondary issue queue wherein the instructions remaining in the primary issue queue are eligible for execution instruction groups formed by the cracking logic. </claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00009\" num=\"9\"><claim-text>9. The processor of <claim-ref idref=\"US-6609190-B1-CLM-00008\">claim 8</claim-ref>, wherein the basic block cache includes instruction history information and further wherein the issue unit is adapted to store an instruction initially in the secondary issue queue if the corresponding instruction history information indicates that the instruction has a dependency on another instruction.</claim-text></claim>"}, {"num": 10, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00010\" num=\"10\"><claim-text>10. The processor of <claim-ref idref=\"US-6609190-B1-CLM-00006\">claim 6</claim-ref>, wherein the issue unit is further characterized as adapted to determine that the first instruction is ineligible for execution if the first instruction has a dependency on a previously issued instruction.</claim-text></claim>"}, {"num": 11, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00011\" num=\"11\"><claim-text>11. The processor of <claim-ref idref=\"US-6609190-B1-CLM-00006\">claim 6</claim-ref>, wherein the issue unit is adapted to deallocate the second instruction from the secondary issue queue responsive to determining that execution of the instruction did not result in an instruction reject.</claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00012\" num=\"12\"><claim-text>12. The processor of <claim-ref idref=\"US-6609190-B1-CLM-00011\">claim 11</claim-ref>, wherein the issue unit determines whether the execution of the second instruction resulted in an instruction reject by determining the number of clock cycles that have elapsed since the second instruction was issued for execution, wherein the second instruction is determined not to have resulted in an instruction reject if the elapsed number of clock cycles exceeds a predetermined value.</claim-text></claim>"}, {"num": 13, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00013\" num=\"13\"><claim-text>13. A data processing system including at least one processor, memory, input means, and a display, wherein the microprocessor comprises:</claim-text><claim-text>an instruction cache; </claim-text><claim-text>a dispatch unit configured to receive instructions from the instruction cache; and </claim-text><claim-text>an issue unit configured to receive instructions from the dispatch unit, wherein the issue unit is adapted to: </claim-text><claim-text>allocate a dispatched first instruction to a primary issue queue and, upon subsequently determining that the first instruction is ineligible for execution, to move the first instruction to a secondary issue queue; and </claim-text><claim-text>allocate a dispatched second instruction to the primary issue queue and upon issuing the second instruction for execution, moving the second instruction to the secondary issue queue wherein the instructions remaining in the primary issue queue are eligible for execution. </claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00014\" num=\"14\"><claim-text>14. The data processing system of <claim-ref idref=\"US-6609190-B1-CLM-00013\">claim 13</claim-ref>, further comprising cracking logic intermediate between the instruction cache and the dispatch unit, wherein the cracking logic is adapted to organize a set of instructions received from the instruction cache as an instruction group sharing a common instruction group tag.</claim-text></claim>"}, {"num": 15, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00015\" num=\"15\"><claim-text>15. The data processing system of <claim-ref idref=\"US-6609190-B1-CLM-00014\">claim 14</claim-ref>, further comprising a basic block cache intermediate between the cracking logic and the dispatch unit, wherein the basic block cache is suitable for caching instruction groups formed by the cracking logic.</claim-text></claim>"}, {"num": 16, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00016\" num=\"16\"><claim-text>16. The data processing system of <claim-ref idref=\"US-6609190-B1-CLM-00015\">claim 15</claim-ref>, wherein the basic block cache includes instruction history information and further wherein the issue unit is adapted to store an instruction initially in the secondary issue queue if the corresponding instruction history information indicates that the instruction has a dependency on another instruction.</claim-text></claim>"}, {"num": 17, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00017\" num=\"17\"><claim-text>17. The data processing system of <claim-ref idref=\"US-6609190-B1-CLM-00013\">claim 13</claim-ref>, wherein the issue unit is further characterized as adapted to determine that the first instruction is currently ineligible for execution if the instruction has a dependency on a previously issued instruction.</claim-text></claim>"}, {"num": 18, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00018\" num=\"18\"><claim-text>18. The data processing system of <claim-ref idref=\"US-6609190-B1-CLM-00013\">claim 13</claim-ref>, wherein the issue unit is adapted to deallocate the second instruction from the secondary issue queue responsive to determining that execution of the second instruction did not result in an instruction reject.</claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"US-6609190-B1-CLM-00019\" num=\"19\"><claim-text>19. The data processing system of <claim-ref idref=\"US-6609190-B1-CLM-00018\">claim 18</claim-ref>, wherein the issue unit determines whether the execution of the second instruction resulted in an instruction reject by determining the number of clock cycles that have elapsed since the second instruction was issued for execution, wherein the second instruction is determined not to have resulted in an instruction reject if the elapsed number of clock cycles exceeds a predetermined value.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53972895\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>RELATED APPLICATION</h4><p>The subject matter disclosed herein is related to the subject matter disclosed in the U.S. patent application entitled Instruction Group Organization and Exception Handling in a Microprocessor, Ser. No. 09/428,399, filed Oct. 28, 1999, which shares a common assignee with the present application-and is incorporated by reference herein.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND</h4><p>1. Field of the Present Invention</p><p>The present invention generally relates to the field of microprocessor architectures and more particularly to a microprocessor utilizing an instruction group architecture, a corresponding cache facility, and useful extensions thereof.</p><p>2. History of Related Art</p><p>As microprocessor technology has enabled gigahertz performance, a major challenge for microprocessor designers is to take advantage of state-of-the-art technologies while maintaining compatibility with the enormous base of installed software designed for operation with a particular instruction set architecture (ISA). To address this problem, designers have implemented \u201clayered architecture\u201d microprocessors that are adapted to receive instructions formatted according to an existing ISA and to convert the instruction format of the received instructions to an internal ISA that is more suitable for operation in gigahertz execution pipelines. Turning to FIG. 4, selected portions of a layered architecture microprocessor <b>401</b> are presented. In this design, an instruction cache <b>410</b> of microprocessor <b>401</b> receives and stores instructions fetched from main memory by a fetch unit <b>402</b>. The instructions stored in instruction cache unit <b>410</b> are formatted according to a first ISA (i.e., the ISA in which the programs being executed by processor <b>401</b> are written). Instructions are then retrieved from instruction cache <b>410</b> and converted to a second ISA by an ISA conversion unit <b>412</b>. Because the conversion of instructions from the first ISA to the second ISA requires multiple cycles, the conversion process is typically pipelined and, accordingly, there may be multiple instructions being converted from the first ISA to the second ISA at any given time. The converted instructions are then forwarded for execution in the execution pipelines <b>422</b> of processor <b>401</b>. The fetch unit <b>402</b> includes branch prediction logic <b>406</b> that attempts to determine the address of the instruction that will be executed following a branch instruction by predicting the outcome of the branch decision. Instructions are then speculatively issued and executed based on the branch predictions. When a branch is mispredicted, however, the instructions that are pending between instruction cache <b>410</b> and finish stage <b>432</b> of processor <b>401</b> must be flushed. The performance penalty that is incurred when a mispredicted branch results in a system flush, is a function of the length of the pipeline. The greater the number of pipeline stages that must be flushed, the greater the branch mispredict performance penalty. Because the layered architecture adds to the processor pipeline and increases that number of instructions that are potentially \u201cin flight,\u201d at a given time, the branch mispredict penalty associated with a layered architecture can become a limiting factor in the processor's performance. It would therefore be highly desirable to implement a layered architecture microprocessor that addressed the branch mispredict performance penalty. In addition, it would be further desirable if the implemented solution addressed, at least in part, repetitive occurrences of exception conditions resulting from repeated execution of a piece of code. It would be further desirable if implemented solution enabled an effectively larger issue queue without sacrificing the ability to search the issue queue for the next instruction to execute.</p><h4>SUMMARY OF THE INVENTION</h4><p>The problems identified above are in large part addressed by a microprocessor that utilizes instruction groups and a cache facility that is matched to the instruction group format. One embodiment of the invention contemplates a microprocessor and an associated method and data processing system. The microprocessor includes an instruction cracking configured to receive a first set of microprocessor instructions. The cracking unit organizes the set of instructions as an instruction group where each of the instructions in the group shares a common instruction group tag. The processor further includes a basic block cache facility that is organized with the instruction group format and is configured to cache the instruction groups generated by the cracking unit. An execution unit of the processor is suitable for executing the instructions in an instruction group. In one embodiment, when an exception is generated during execution of an instruction in the instruction group that causes a flush, the flush flushes only those instructions that have been dispatched from the basic block cache. By flushing only those instructions that have arrived at the basic block cache, the processor spares the instructions pending in the cracking unit pipeline from being flushed. Because fewer instructions are flushed, the exception performance penalty is reduced. In one embodiment, the received instructions are formatted according to a first instruction format and the second set of instructions are formatted according to a second instruction format wherein the second instruction format is wider than the first instruction format. The basic block cache is suitably configured to store each instruction group in a corresponding entry of the basic block cache. In one embodiment, each entry in the basic block cache includes an entry field indicative of the corresponding basic block cache entry and a pointer predictive of the next of the instruction group to be executed. The processor is preferably configured to update a pointer of a cache entry responsive to a mispredicted branch.</p><p>The invention further contemplates a processor, data processing system and method utilizing instruction history information in conjunction with the basic block cache to improve performance. The processor is suitable for receiving a set of instructions and organizing the set of instructions into an instruction group. The instruction group is then dispatched for execution. Upon executing the instruction group, instruction history information indicative of an exception event associated with the instruction group is recorded. Thereafter, the execution of the instruction is modified responsive to the instruction history information to prevent the exception event from occurring during a subsequent execution of the instruction group. The processor includes a storage facility such as an instruction cache, an L<b>2</b> cache or a system memory, a cracking unit, and a basic block cache. The cracking unit is configured to receive a set of instructions from the storage facility. The cracking unit is adapted to organize the set of instructions into an instruction group. The cracking unit may modify the format of the set of instructions from a first instruction format to a second instruction format. The architecture of the basic block cache is suitable for storing the instruction groups. The basic block cache includes an instruction history field corresponding to each basic block cache entry. The instruction history information is indicative of an exception event associated with the instruction group. In the preferred embodiment, each entry in the basic block cache corresponds to a single instruction group generated by the cracking unit. The processor may further include completion table control logic configured to store information in the instruction history field when the instruction group completes. The instruction history information may be indicative of whether an instruction in the instruction group has a dependency on another instruction or may be indicative of whether the execution of the instruction group previously resulted in a store forwarding exception. In this embodiment, the processor is configured to execute in an in-order-mode responsive to detecting that the execution of the instruction group previously resulted in the store forwarding exception.</p><p>The invention still further contemplates a processor, data processing system and an associated method utilizing primary and secondary issue queues. The processor is suitable for dispatching an instruction to an issue unit. The issue unit includes a primary issue queue and a secondary issue queue. The instruction is stored in the primary issue queue if the instruction is currently eligible to issue for execution. The instruction is stored in the secondary issue queue if the instruction is currently ineligible to issue for execution. The processor determines the next instruction to issue from the instructions in the primary issue queue. An instruction may be moved from the primary issue queue to the secondary issue queue if instruction is dependent upon results from another instruction. In one embodiment, the instruction may be moved from the primary issue queue to the secondary issue queue after issuing the instruction for execution. In this embodiment, the instruction may be maintained in the secondary issue queue for a specified duration. Thereafter, the secondary issue queue entry containing the instruction is deallocated if the instruction has not been rejected. The microprocessor includes an instruction cache, a dispatch unit configured to received instructions from the instruction cache, and an issue unit configured to receive instructions from the dispatch unit. The issue unit is adapted to allocate dispatched instructions that are currently eligible for execution to a primary issue queue and to allocate dispatched instructions that are not currently eligible for execution to a secondary issue queue.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Other objects and advantages of the invention will become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p><p>FIG. 1 is a block diagram of selected components of a data processing system including a microprocessor according to one embodiment of the present invention;</p><p>FIG. 2 is a block diagram of selected components of a microprocessor according to one embodiment of the present invention;</p><p>FIG. 3 illustrates examples of the instruction cracking function performed by one embodiment of the processor of FIG. 2;</p><p>FIG. 4 is a block diagram of selected components of a microprocessor;</p><p>FIG. 5 is block diagram of a basic cache block of the microprocessor of FIG. 2;</p><p>FIG. 6 is an illustration of various branching scenarios that the processor of FIG. 2 may encounter;</p><p>FIG. 7 is a block diagram of a completion table suitable for use with the present invention;</p><p>FIG. 8 is a block diagram of a basic block cache that includes instruction history information; and</p><p>FIG. 9 is a block diagram of an issue queue including a primary and secondary issue queue according to one embodiment of the invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><p>While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description presented herein are not intended to limit the invention to the particular embodiment disclosed, but on the contrary, the intention is to cover all modifications, equivalents, and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><h4>DETAILED DESCRIPTION OF A PREFERRED EMBODIMENT OF THE PRESENT INVENTION</h4><p>Referring now to FIG. 1, an embodiment of a data processing system <b>100</b> according to the present invention is depicted. System <b>100</b> includes one or more central processing units (processors) <b>101</b><i>a</i>, <b>101</b><i>b</i>, <b>101</b><i>c</i>, etc. (collectively or generically referred to as processor(s) <b>101</b>. In one embodiment, each processor <b>101</b> may comprise a reduced instruction set computer (RISC) microprocessor. Additional information concerning RISC processors in general is available in C. May et al. Ed., <i>PowerPC Architecture: A Specification for a New Family of RISC Processors, (Morgan Kaufmann</i>, 1994 2d edition). Processors <b>101</b> are coupled to system memory <b>250</b> and various other components via system bus <b>113</b>. Read only memory (ROM) <b>102</b> is coupled to the system bus <b>113</b> and may include a basic input/output system (BIOS), which controls certain basic functions of system <b>100</b>. FIG. 1 further depicts an I/O adapter <b>107</b> and a network adapter <b>106</b> coupled to the system bus <b>113</b>. I/O adapter <b>107</b> links system bus <b>113</b> with mass storage devices <b>104</b> such as a hard disk <b>103</b> and/or a tape storage drive <b>105</b>. Network adapter <b>106</b> interconnects bus <b>113</b> with an external network enabling data processing system <b>100</b> to communicate with other such systems. Display monitor <b>136</b> is connected to system bus <b>113</b> by display adapter <b>112</b>, which may include a graphics adapter to improve the performance of graphics intensive applications and a video controller. In one embodiment, adapters <b>107</b>, <b>106</b>, and <b>112</b> may be connected to one or more I/O busses that are connected to system bus <b>113</b> via an intermediate bus bridge (not shown). Suitable I/O busses for connecting peripheral devices such as hard disk controllers, network adapters, and graphics adapters include the Peripheral Components Interface (PCI) bus as specified according to PCI Local Bus Specification Rev. 2.2 available from the PCI Special Interest Group, Hillsboro, OR, and incorporated by reference herein. Additional input/output devices are shown as connected to system bus <b>113</b> via user interface adapter <b>108</b>. A keyboard <b>109</b>, mouse <b>110</b>, and speaker <b>111</b> are all linked to bus <b>113</b> via user interface adapter <b>108</b>, which may include, for example, a SuperI/O chip integrating multiple device adapters into a single integrated circuit. For additional information concerning one such chip, the reader is referred to the PC87338/PC97338 ACPI 1.0 and PC98/99 Compliant SuperI/O data sheet from National Semiconductor. Corporation (November 1998) at www.national.com. Thus, as configured in FIG. 1, system <b>100</b> includes processing means in the form of processors <b>101</b>, storage means including system memory <b>250</b> and mass storage <b>104</b>, input means such as keyboard <b>109</b> and mouse <b>110</b>, and output means including speaker <b>111</b> and display <b>136</b>. In one embodiment a portion of system memory <b>250</b> and mass storage <b>104</b> collectively store an operating system such as the, AIX\u00ae operating system from IBM Corporation or other suitable operating system to coordinate the fictions of the various components shown in FIG. <b>1</b>. Additional detail concerning the AIX operating system is available in AIX Version 4.3 Technical Reference: Base Operating System and Extensions, Volumes 1 and 2 (order numbers SC23-4159 and SC23-4160); AIX Version 4.3 System User's Guide: Communications and Networks (order number SC23-4122); and AIX Version 4.3 System User's Guide: Operating System and Devices (order number SC23-4121) from IBM Corporation at www.ibm.com and incorporated by reference herein.</p><p>Turning now to FIG. 2, a simplified block diagram of a processor <b>101</b> according to one embodiment of the present invention is illustrated. Processor <b>101</b> as depicted in FIG. 2 includes an instruction fetch unit <b>202</b> suitable for generating an address of the next instruction to be fetched. The instruction address generated by fetch unit <b>202</b> provided to an instruction cache <b>210</b>. Fetch unit <b>202</b> may include branch prediction logic that, as its name suggests, is adapted to make an informed prediction of the outcome of a decision that effects the program execution flow. The ability to correctly predict branch decisions is a significant factor in the overall ability of processor <b>101</b> to achieve improved performance, by executing instructions speculatively and out-of-order. The instruction address generated by fetch unit <b>202</b> is provided to an instruction cache <b>210</b>, which contains a subset of the contents of system memory in a high speed storage facility. The instructions stored in instruction cache <b>210</b> are preferably formatted according to a first ISA, which is typically a legacy ISA such as, for example, the PowerPC or an \u00d786 compatible instruction set. Detailed information regarding the PowerPC\u00ae instruction set is available in the PowerPC 620 RISC Microprocessor User's Manual available from Motorola, Inc. (Order No. MPC620UM/AD), which is incorporated by reference herein. If the address instruction generated by fetch unit <b>202</b> corresponds to a system memory location that is currently replicated in instruction cache <b>210</b>, instruction cache <b>210</b> forwards the corresponding instruction to cracking unit <b>212</b>. If the instruction corresponding to the instruction address generated by fetch unit <b>202</b> does not currently reside in instruction cache <b>210</b> (i.e., the instruction address provided by fetch unit <b>202</b> misses in instruction cache <b>210</b>), the instructions must be fetched from an L<b>2</b> cache (not shown), or system memory before the instruction can be forwarded to cracking unit <b>212</b>.</p><p>Cracking unit <b>212</b> is adapted to modify an incoming instruction stream to produce a set of instructions optimized for executing in an underlying execution pipeline at high operating frequencies (i.e., operating frequencies exceeding 1 GHz). In one embodiment, for example, cracking unit <b>212</b> receives instructions in a 32-bit wide ISA such as the instruction set supported by the PowerPC\u00ae microprocessor and converts the instructions to a second, preferably wider, ISA that facilitates execution in a high speed execution unit operating in the gigahertz frequency range and beyond. The wider format of the instructions generated by cracking unit <b>212</b> may include, as an example, explicit fields that contain information (such as operand values) that is merely implied or referenced in the instructions received by cracking unit <b>212</b>, which are formatted according to a first format. In one embodiment; for example, the ISA of instructions generated by cracking unit <b>212</b> is 64 or more bits wide.</p><p>In one embodiment, cracking unit <b>212</b> as contemplated herein, in addition to converting instructions from a first format to a second, and preferably wider, format, is designed to organize a set of fetched instructions into instruction \u201cgroups\u201d <b>302</b>, examples of which are depicted in FIG. <b>3</b>. Each instruction group <b>302</b> includes a set of instruction slots <b>304</b><i>a</i>, <b>304</b><i>b</i>, etc. (collectively or generically referred to as instruction slots <b>304</b>). The organization of a set of instructions into instruction groups facilitates high speed execution by, among other things, simplifying the logic needed to maintain rename register mapping and completion tables for a large number of in-flight instructions. In FIG. 3, three examples of instruction grouping that may be performed d by cracking unit <b>212</b> are depicted.</p><p>In Example 1, a set of instructions indicated by reference numeral <b>301</b> is transformed into a single instruction group <b>302</b> by cracking unit <b>212</b>. In the depicted embodiment of the invention, each instruction group <b>302</b> includes five slots indicated by reference numerals <b>304</b><i>a</i>, <b>304</b><i>b</i>, <b>304</b><i>c</i>, <b>304</b><i>d</i>, and <b>304</b><i>e</i>. Each slot <b>304</b> may contain a single instruction. In this embodiment, each instruction group may include a maximum of five instructions. In one embodiment, the instructions in the set of instructions <b>301</b> received by cracking unit <b>212</b> are formatted according to a first ISA, as discussed previously, and the instructions stored in the groups <b>302</b> are formatted according to a second wider format. The use of instruction groups simplifies renaming recovery and completion table logic by reducing the number of instructions that must be individually tagged and tracked. The use of instruction groups thus contemplates sacrificing some information about each instruction in an effort to simplify the process of tracking pending instructions in an out-of-order processor.</p><p>Example 2 of FIG. 3 illustrates a second example of the instruction grouping performed by cracking unit <b>212</b> according to one embodiment of the invention. This example demonstrates the capability of cracking unit <b>212</b> to break down complex instructions into a group of simple instructions for higher speed execution. In the depicted example, a sequence of two load-with-update (LDU) instructions are broken down into an instruction group including a pair of load instructions in slots <b>304</b><i>a </i>and <b>304</b><i>c </i>respectively and a pair of ADD instructions in slots <b>304</b><i>b </i>and <b>304</b><i>d </i>respectively. In this example, because group <b>302</b> does not contain a branch instruction, the last slot <b>304</b><i>e </i>of instruction group <b>302</b> contains no instruction. The PowerPC\u00ae load-with-update instruction, like analogous instructions in other instruction sets, is a complex instruction in that the instruction affects the contents of multiple general purpose registers (GPRs). Specifically, the load-with-update instruction can be broken down into a load instruction that affects the contents of a first GPR and an ADD instruction that affects the contents of a second GPR. Thus, in instruction group <b>302</b> of Example 2 in FIG. 3, instructions in two or more instruction slots <b>304</b> correspond to a single instruction received by cracking unit <b>212</b>.</p><p>In Example 3, a single instruction entering cracking unit <b>212</b> is broken down into a set of instructions occupying multiple groups <b>302</b>. More specifically, Example 3 illustrates a load multiple (LM) instruction. The load multiple instruction (according to the PowerPC\u00ae instruction set) loads the contents of consecutive locations in memory into consecutively numbered GPRs. In the depicted example, a load multiple of six consecutive memory locations breaks down into six load instructions. Because each group <b>302</b> according to the depicted embodiment of processor <b>101</b> includes, at most, five instructions, and because the fifth slot <b>304</b><i>e </i>is reserved for branch instructions, a load multiple of six registers breaks down into two groups <b>302</b><i>a </i>and <b>302</b><i>b </i>respectively. Four of the load instructions are stored in the first group <b>302</b><i>a </i>while the remaining two load instructions are stored in the second group <b>302</b><i>b</i>. Thus, in Example 3, a single instruction is broken down into a set of instructions that span multiple instruction groups <b>302</b>.</p><p>Returning now to FIG. 2, the instruction groups <b>302</b> generated by the preferred embodiment of cracking unit <b>212</b> are forwarded to a basic block cache <b>213</b> where they are stored pending execution. Referring to FIG. 5, an embodiment of basic block cache <b>213</b> is depicted. In the depicted embodiment, basic block cache <b>213</b> includes a set of entries <b>502</b><i>a </i>through <b>502</b><i>n </i>(generically or collectively referred to as basic block cache entries <b>502</b>). In one embodiment, each entry <b>502</b> in basic block cache <b>213</b> contains a single instruction group <b>302</b>. In addition, each entry <b>502</b> may include an entry identifier <b>504</b>, a pointer <b>506</b>, and an instruction address (IA) field <b>507</b>. The instruction address field <b>507</b> for each entry <b>502</b> is analogous to the IA field <b>704</b> of completion table <b>218</b>. In one embodiment, each entry <b>502</b> in basic block cache <b>504</b> corresponds to an entry in completion table <b>218</b> and the instruction address field <b>507</b> indicates the instruction address of the first instruction in the corresponding instruction group <b>302</b>. In one embodiment, the pointer <b>506</b> indicates the entry identifier of the next instruction group <b>302</b> to be executed based upon a branch prediction algorithm, branch history table, or other suitable branch prediction mechanism. As indicated previously, the preferred implementation of forming instruction groups <b>302</b> with cracking unit <b>212</b> allocates branch instructions to the last slot <b>304</b><i>e </i>in each group <b>302</b>. In addition, the preferred embodiment of cracking unit <b>212</b> produces instruction groups <b>302</b> in which the number of branch instructions in a group <b>302</b> to one (or less). In this arrangement, each instruction group <b>302</b> can be thought of as representing a \u201cleg\u201d of a branch tree <b>600</b> as indicated in FIG. 6, in which instruction groups <b>302</b> are represented by their corresponding instruction group entry <b>504</b> values. First instruction group <b>302</b><i>a</i>, for example, is indicated by its entry number (<b>1</b>), and so forth. Suppose, as an example, that the branch prediction mechanism of processor <b>101</b> predicts that leg <b>2</b> (corresponding to second group <b>302</b><i>b</i>) will be executed following leg <b>1</b> and that leg <b>3</b> will be executed following leg <b>2</b>. The basic block cache <b>213</b>, according to one embodiment of the invention, reflects these branch predictions by setting the pointer <b>506</b> to indicate the next group <b>302</b> to be executed. The pointer <b>506</b> of each t entry <b>502</b> in basic block cache <b>213</b> may be utilized to determine the next instruction group <b>302</b> to be dispatched.</p><p>Basic block cache <b>213</b> works in conjunction with a block fetch unit <b>215</b> analogous to the manner in which fetch unit <b>202</b> works with instruction cache <b>210</b>. More specifically, block fetch unit <b>215</b> is responsible for generating an instruction address that is provided to basic block cache <b>213</b>. The instruction address provided by block fetch unit <b>215</b> is compared against addresses in the instruction address fields <b>507</b> in basic block cache <b>213</b>. If the instruction address provided by block fetch unit <b>215</b> hits in basic block cache <b>213</b>, the appropriate instruction group is forwarded to issue queues <b>220</b>. If the address provided by block fetch unit <b>215</b> misses in basic block cache <b>213</b>, the instruction address is fed back to fetch unit <b>202</b> to retrieve the appropriate instructions from instruction cache <b>210</b>. In one embodiment suitable for its conservation of area (die size), basic block cache <b>213</b> enables the elimination of instruction cache <b>210</b>. In this embodiment, instructions are retrieved from a suitable storage facility such as an L<b>2</b> cache or system memory and provided directly to cracking unit <b>212</b>. If an instruction address generated by block fetch unit <b>213</b> misses in basic block cache <b>213</b>, the appropriate instructions are retrieved from an L<b>2</b> cache or system memory rather than from instruction cache <b>210</b>.</p><p>The depicted embodiment of processor <b>101</b> further indicates a dispatch unit <b>214</b>. Dispatch unit <b>214</b> is responsible for ensuring that all necessary resources are available prior to forwarding the instructions in each instruction group to their appropriate issue queues <b>220</b>. In addition, dispatch unit <b>214</b> communicates with dispatch and completion control logic <b>216</b> to keep track of the order in which instructions were issued and the completion status of these instructions to facilitate out-of-order execution. In the embodiment of processor <b>101</b> in which cracking unit <b>212</b> organizes incoming instructions into instruction groups as discussed above, each instruction group <b>302</b> is assigned a group tag (GTAG) by completion and control logic <b>216</b> that conveys the ordering of the issued instruction groups. As an example, dispatch unit <b>214</b> may assign monotonically increasing values to consecutive instruction groups. With this arrangement, instruction groups with lower GTAG values are known to have issued prior to (i.e., are older than) instruction groups with larger GTAG values. Although the depicted embodiment of processor <b>101</b> indicates dispatch unit <b>214</b> as a distinct functional block, the group instruction organization of basic block cache <b>213</b> lends itself to incorporating the functionality of dispatch unit <b>214</b>. Thus, in one embodiment, dispatch unit <b>214</b> is incorporated within basic block cache <b>213</b>, which is connected directly to issue queues <b>220</b>.</p><p>In association with dispatch and completion control logic <b>216</b>, a completion table <b>218</b> is utilized in one embodiment of the present invention to track the status of issued instruction groups. Turning to FIG. 7, a block diagram of one embodiment of completion table <b>218</b> is presented. In the depicted embodiment, completion table <b>218</b> includes a set of entries <b>702</b><i>a </i>through <b>702</b><i>n </i>(collectively or generically referred to herein as completion table entries <b>702</b>); In this embodiment, each entry <b>702</b> in completion table <b>218</b> includes an instruction address (IA) field <b>704</b> and a status bit field <b>706</b>. In this embodiment, the GTAG value of each instruction group <b>302</b> identifies the entry <b>702</b> in completion table <b>218</b> in which completion information corresponding to the instruction group <b>302</b> is stored. Thus, the instruction group <b>302</b> stored in entry <b>1</b> of completion table <b>118</b> will have a GTAG value of <b>1</b>, and so forth. In this embodiment, completion table <b>118</b> may further include a \u201cwrap around\u201d bit to indicate that an instruction group with a lower GTAG value is actually younger than an instruction group with a higher GTAG value. In one embodiment, the instruction address field <b>704</b> includes the address of the instruction in first slot <b>304</b>a of the corresponding instruction group <b>302</b>. Status field <b>706</b> may. contain one or more status bits indicative of whether, for example, the corresponding entry <b>702</b> in completion table <b>218</b> is available or if the entry has been allocated to a pending instruction group. In the embodiment of processor <b>101</b> depicted in FIG. 2, instructions are issued from dispatch unit <b>214</b> to issue queues <b>220</b> where they await execution in corresponding execution pipes <b>222</b>. Processor <b>101</b> may include a variety of types of executions pipes, each designed to execute a subset of the processor's instruction set. In one embodiment, execution pipes <b>222</b> may include a branch unit pipeline <b>224</b>, a load store pipeline <b>226</b>, a fixed point arithmetic unit <b>228</b>, and a floating point unit <b>230</b>. Each execution pipe <b>222</b> may comprise two or more pipeline stages. Instructions stored in issue queues <b>220</b> may be issued to execution pipes <b>222</b> using any of a variety of issue priority algorithms. In one embodiment, for example, the oldest pending instruction in an issue queue <b>220</b> is the next instruction issued to execution pipes <b>222</b>. In this embodiment, the GTAG values assigned by dispatch unit <b>214</b> are utilized to determine the relative age of instructions pending in the issue queues <b>220</b>. Prior to issue, the destination register operand of the instruction is assigned to an available rename GPR. When an instruction is ultimately forwarded from issue queues <b>220</b> to the appropriate execution pipe, the execution pipe performs the appropriate operation as indicated by the instruction's opcode and writes the instruction's result to the instruction's rename GPR by the time the instruction reaches a finish stage (indicated by reference numeral <b>232</b>) of the pipeline. A mapping is maintained between the rename GPRs and their corresponding architected registers. When all instructions in an instruction group (and all instructions in younger instruction groups) finish without generating an exception, a completion pointer in the completion table <b>218</b> is incremented to the next instruction group. When the completion pointer is incremented to a new instruction group, the rename registers associated with the instructions in the old instruction group are released thereby committing the results of the instructions in the old instruction group. If one or more instructions older than a finished (but not yet committed) instruction generates an exception, the instruction generating the exception and all younger instructions are flushed and a rename recovery routine is invoked to return the GPR mapping to the last known valid state.</p><p>If a predicted branch is not taken (branch misprediction), the instructions pending in executions pipes <b>222</b> and issue queues <b>220</b> are flushed. In addition, the pointer <b>506</b> of the basic block cache entry <b>502</b> associated with the mispredicted branch is updated to reflect the most recent branch taken. An example of this updating process is illustrated in FIG. 5 for the case in which program execution results in a branch from leg <b>1</b> (instruction group <b>302</b><i>a</i>) to leg <b>4</b> (instruction group <b>302</b><i>d</i>). Because the pointer <b>506</b> of entry <b>502</b><i>a </i>had previously predicted a branch to the instruction group residing in the number <b>2</b> entry of basic block cache <b>213</b> (i.e., group <b>302</b><i>b</i>), the actual branch from instruction group <b>302</b><i>a </i>to group <b>302</b><i>d</i>was mispredicted. The mispredicted branch is detected and fed back to block fetch unit <b>215</b>, the instructions pending between basic block cache <b>213</b> and the finish stage <b>232</b> of each of the pipelines <b>222</b> are flushed, and execution is re-started with instruction group <b>302</b><i>d </i>in entry <b>4</b> of basic block cache <b>213</b>. In addition, the pointer <b>506</b> of basic block cache entry <b>502</b><i>a </i>is altered from its previous value of 2 to its new value of 4 reflecting the most recent branch information. By incorporating basic block cache <b>213</b> and block fetch unit <b>215</b> in close proximity to the execution pipelines <b>222</b>, the present invention contemplates a reduced performance penalty for a mispredicted branch. More specifically, by implementing basic block cache <b>213</b> on the \u201cdownstream\u201d side of instruction cracking unit <b>212</b>, the present invention eliminates instructions that are pending in cracking unit <b>212</b> from the branch misprediction flush path thereby reducing the number of pipeline stages that must be purged following a branch mispredict and an reducing the performance penalty. In addition, the basic block cache <b>213</b> contemplates a caching mechanism with a structure that matches the organization of dispatch and completion control unit <b>216</b> and completion table <b>218</b> thereby simplifying the organization of the intervening logic and facilitating the implementation of useful extensions to the basic block cache <b>213</b> as described below.</p><p>In one embodiment, basic block cache <b>213</b> further includes instruction history information that beneficially enables improved processor performance by recording information that might be used during subsequent execution of the same instruction group to avoid scenarios that are likely to result in exceptions, flushes, interrupts, or other performance limiting events (collectively referred herein as exception events). In the embodiment of basic block cache <b>213</b> depicted in FIG. 8, the instruction history information is stored in an instruction history field <b>508</b> of each entry <b>502</b>. As an example of the type of information that might be stored in instruction history field <b>508</b>, consider an instruction group that contains a particular load instruction that resulted in a store forwarding exception the last time the load instruction was executed. A store forward exception, as that term is used herein, occurs when a load instruction that follows (in program order) a store instruction sharing a common memory reference executes before the store instruction in an out-of-order machine. Because the load instruction retrieves an invalid value from the register if it executes prior to the store instruction, an exception is generated that results in an instruction flush. The parallelism between the structure of basic block cache <b>213</b> and the completion and control logic <b>216</b> greatly facilitates the task of forwarding information learned by dispatch and completion control logic <b>216</b> about the manner in which instructions executed and completed to a corresponding entry in basic block cache <b>213</b>. In the absence of this parallelism, the completion information from dispatch and completion control logic <b>216</b> would typically be required to pass through some manner of intervening hashing table or other suitable mechanism to correlate group instruction information to its component instructions. In the store forwarding example, upon detecting a store forwarding exception, dispatch and completion control unit <b>216</b> would write one or more bits in instruction history field <b>508</b> of the appropriate entry in basic block cache <b>213</b> that would signify the store forwarding exception. If the instruction group was subsequently executed, the instruction history information indicating the previous occurrence of a store forwarding exception could be used, for example, to place processor <b>101</b> into an in-order mode in which loads are prevented from executing prior to stores are completed. Thus, this embodiment of the invention contemplates recording instruction history information that is indicative of an exception event associated with an instruction group and thereafter modifying the execution of the instruction group to prevent the exception event from occurring during a subsequent execution of the instruction group. Although illustrated with the store forwarding example, the instruction history information field <b>508</b> as contemplated herein is suitable for recording information relevant to a variety of instruction history events that might enable the processor to avoid a recurring exception condition such as information relevant to the accuracy of any prediction mechanisms, operand value prediction, cache miss/hit information, and so forth.</p><p>One example of information that might be recorded in execution history field <b>508</b> of basic block cache <b>213</b> is emphasized by the embodiment depicted in FIG. 9 in which one or more of the issue queues <b>220</b> is sub-divided into a primary issue queue <b>902</b> and second issue queue <b>904</b>. The optimum size or depth of the issue queues <b>220</b> represents a balance between competing considerations. On the one hand, it is desirable to implement very large and deep issue queues to take maximum advantage of the ability of processor <b>101</b> to execute instructions out-of-order. The ability to issue instructions out-of-order is limited by the number of instructions that are pending in issue queues <b>220</b>. A larger number of issue queues corresponds to a larger number of instructions eligible for out-of-order processing. On the other hand, as the depth of issue queue <b>220</b> grows, the ability of the processor to determine the next instruction to be issued within the cycle time constraints of the processor decreases. In other words, the greater the number of instructions pending in issue queue <b>220</b>, the longer the time required to determine the next instruction to issue. For this reason, issue queues such as issue queue <b>220</b> are frequently limited to a depth of approximately 20 or less. One embodiment of the invention contemplates achieving the benefits of deep issue queues without requiring a significant increase in the logic required to search the issue queues for the next eligible instruction to issue. The invention takes advantage of the fact that frequently, an instruction pending in issue queue <b>220</b> is not eligible for immediate issue either because it has already been issued and is pending in the execution pipelines <b>222</b> of processor <b>101</b> or it is awaiting the completion of another instruction on which it depends for an operand value.</p><p>Referring to FIG. 9, an issue queue <b>220</b> according to one embodiment of the present invention comprises a primary issue queue <b>902</b> and a secondary issue queue <b>904</b>. Primary issue queue <b>902</b> contains instructions that are eligible for immediate issue. In one embodiment, instructions dispatched from dispatch unit <b>214</b> are initially stored in an available entry of primary issue queue <b>902</b>. If it is subsequently determined that an instruction has a dependency on another instruction, the dependent instruction is moved to secondary issue queue <b>904</b> until the instruction on which the dependent instruction depends has retrieved the necessary information. If, for example, an add instruction following a load instruction requires the result of the load instruction, both of the instructions may be initially dispatched to primary issue queue <b>902</b>. Upon subsequently determining that the add instruction has a dependency on the load instruction however, the add instruction is transferred from primary issue <b>902</b> to secondary issue queue <b>904</b>. In an embodiment utilizing the instruction history field <b>508</b> as discussed previously with respect to FIG. 8, the add instruction's dependency may be recorded such that, during a subsequent execution of the instructions, the add instruction may be stored directly into secondary issue queue <b>220</b>. The secondary issue queue <b>904</b> may also be used to store instructions that have been recently issued and are still pending in the execution pipelines of the processor. In this embodiment, an instruction is issued from the primary issue queue <b>902</b> and then transferred to the secondary issue queue <b>904</b>. In one embodiment, the instruction may reside in the secondary issue queue <b>904</b> until it is determined that the instruction will not be rejected. One method of determining that an instruction has not been rejected is to implement a timer/counter (not shown) associated with each entry in secondary issue queue <b>904</b>. When an instruction is initially transferred from primary issue queue <b>902</b> to secondary issue queue <b>904</b>, the counter/timer is initiated. In one embodiment, the counter/timer counts the number of clock cycles that have expired since the counter/timer was initiated. If the counter/timer continues to count for a predetermined number of cycles, without detecting that the instruction has been rejected, instruction is presumed to have completed successfully and the entry in secondary issue queue <b>904</b> is de-allocated. By utilizing an issue queue comprising a primary issue queue that is dedicated for instructions that are currently eligible to be issued for execution in conjunction with a secondary issue queue in which instructions that have been dispatched, but are not currently eligible for execution, either because of an instruction dependency or because the instruction was recently issued from the primary issue queue, the effective size or depth of the issue queue is increased without significantly increasing the time (i.e., number of logic levels) required to determine a next instruction to issue.</p><p>It will be apparent to those skilled in the art having the benefit of this disclosure that the present invention contemplates a various embodiment of a microprocessor including a cache facility suitable for storing grouped instructions (i.e., instructions that have been converted from a first format to a second format) to reduce the latency associated with a mispredicted branch. It is understood that the form of the invention shown and described in the detailed description and the drawings are to be taken merely as presently preferred examples. It is intended that the following claims be interpreted broadly to embrace all the variations of the preferred embodiments disclosed.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "James Allan", "last_name": "Kahle", "name": ""}, {"first_name": "Charles Roberts", "last_name": "Moore", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTERNATIONAL BUSINESS MACHINES CORPORATION"}, {"first_name": "", "last_name": "INTEL CORPORATION", "name": ""}, {"first_name": "", "last_name": "INTERNATIONAL BUSINESS MACHINES CORPORAITON", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F   9/30"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/38        20060101A I20051008RMEP"}, {"label": "G06F   9/30        20060101A I20051008RMEP"}, {"label": "G06F   9/318       20060101A I20051008RMEP"}, {"label": "G06F   9/06        20060101AFI20060101BMKR"}], "national_classes": [{"primary": true, "label": "712214"}, {"primary": false, "label": "712244"}, {"primary": false, "label": "712E09029"}, {"primary": false, "label": "712E09054"}, {"primary": false, "label": "712E0906"}, {"primary": false, "label": "712E09049"}, {"primary": false, "label": "712216"}, {"primary": false, "label": "712E09037"}], "ecla_classes": [{"label": "G06F   9/38H3"}, {"label": "G06F   9/30U"}, {"label": "G06F   9/30U2"}, {"label": "G06F   9/38E"}, {"label": "G06F   9/38E6"}, {"label": "G06F   9/30T2"}], "cpc_classes": [{"label": "G06F   9/30174"}, {"label": "G06F   9/3853"}, {"label": "G06F   9/3838"}, {"label": "G06F   9/384"}, {"label": "G06F   9/3836"}, {"label": "G06F   9/30149"}, {"label": "G06F   9/3857"}, {"label": "G06F   9/3865"}, {"label": "G06F   9/3017"}, {"label": "G06F   9/30149"}, {"label": "G06F   9/3836"}, {"label": "G06F   9/30174"}, {"label": "G06F   9/3865"}, {"label": "G06F   9/3857"}, {"label": "G06F   9/3017"}, {"label": "G06F   9/384"}, {"label": "G06F   9/3838"}, {"label": "G06F   9/3853"}, {"label": "G06F   9/06"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "2000-01-06", "application_date": "2000-01-06", "family_members": [{"ucid": "JP-3540743-B2", "titles": [{"lang": "JA", "text": "\uff11\u6b21\u767a\u884c\u30ad\u30e5\u30fc\u3068\uff12\u6b21\u767a\u884c\u30ad\u30e5\u30fc\u3092\u6301\u3064\u30de\u30a4\u30af\u30ed\u30d7\u30ed\u30bb\u30c3\u30b5"}, {"lang": "EN", "text": "Microprocessor with primary issue queue and secondary issue queue"}]}, {"ucid": "CN-1163822-C", "titles": [{"lang": "ZH", "text": "\u5fae\u5904\u7406\u5668\u6307\u4ee4\u6267\u884c\u65b9\u6cd5\u53ca\u76f8\u5173\u5fae\u5904\u7406\u5668\u548c\u6570\u636e\u5904\u7406\u7cfb\u7edf"}, {"lang": "EN", "text": "Microprocessor-possessing first and second emitting groups"}]}, {"ucid": "KR-100407013-B1", "titles": [{"lang": "EN", "text": "MICROPROCESSOR WITH PRIMARY AND SECONDARY ISSUE QUEUE"}, {"lang": "KO", "text": "1\ucc28 \ubc0f 2\ucc28 \uc1a1\ucd9c\ud050\ub97c \uac16\ub294 \ub9c8\uc774\ud06c\ub85c\ud504\ub85c\uc138\uc11c"}]}, {"ucid": "US-6609190-B1", "titles": [{"lang": "EN", "text": "Microprocessor with primary and secondary issue queue"}]}, {"ucid": "CN-1303045-A", "titles": [{"lang": "EN", "text": "Microprocessor-possessing first and second emitting groups"}, {"lang": "ZH", "text": "\u5177\u6709\u7b2c\u4e00\u548c\u7b2c\u4e8c\u53d1\u51fa\u961f\u7684\u5fae\u5904\u7406\u5668"}]}, {"ucid": "JP-2001297000-A", "titles": [{"lang": "JA", "text": "\uff11\u6b21\u767a\u884c\u30ad\u30e5\u30fc\u3068\uff12\u6b21\u767a\u884c\u30ad\u30e5\u30fc\u3092\u6301\u3064\u30de\u30a4\u30af\u30ed\u30d7\u30ed\u30bb\u30c3\u30b5"}, {"lang": "EN", "text": "MICROPROCESSOR HAVING FIRST-ORDER ISSUING QUEUE AND SECOND-ORDER ISSUING QUEUE"}]}, {"ucid": "KR-20010070435-A", "titles": [{"lang": "KO", "text": "1\ucc28 \ubc0f 2\ucc28 \uc1a1\ucd9c\ud050\ub97c \uac16\ub294 \ub9c8\uc774\ud06c\ub85c\ud504\ub85c\uc138\uc11c"}, {"lang": "EN", "text": "MICROPROCESSOR WITH PRIMARY AND SECONDARY ISSUE QUEUE"}]}, {"ucid": "HK-1037248-A1", "titles": [{"lang": "EN", "text": "Method of executing microprocessor instructions, associated microprocessor and data processing system."}]}]}