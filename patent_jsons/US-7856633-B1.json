{"patent_number": "US-7856633-B1", "publication_id": 112290267, "family_id": 24129052, "publication_date": "2010-12-21", "titles": [{"lang": "EN", "text": "LRU cache replacement for a partitioned set associative cache"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA81897627\" source=\"national office\"><p>A method of partitioning a memory resource, associated with a multi-threaded processor, includes defining the memory resource to include first and second portions that are dedicated to the first and second threads respectively. A third portion of the memory resource is then designated as being shared between the first and second threads. Upon receipt of an information item, (e.g., a microinstruction associated with the first thread and to be stored in the memory resource), a history of Least Recently Used (LRU) portions is examined to identify a location in either the first or the third portion, but not the second portion, as being a least recently used portion. The second portion is excluded from this examination on account of being dedicated to the second thread. The information item is then stored within a location, within either the first or the third portion, identified as having been least recently used.</p></abstract>"}, {"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA81861290\"><p id=\"p-0001\" num=\"0000\">A method of partitioning a memory resource, associated with a multi-threaded processor, includes defining the memory resource to include first and second portions that are dedicated to the first and second threads respectively. A third portion of the memory resource is then designated as being shared between the first and second threads. Upon receipt of an information item, (e.g., a microinstruction associated with the first thread and to be stored in the memory resource), a history of Least Recently Used (LRU) portions is examined to identify a location in either the first or the third portion, but not the second portion, as being a least recently used portion. The second portion is excluded from this examination on account of being dedicated to the second thread. The information item is then stored within a location, within either the first or the third portion, identified as having been least recently used.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"CLM-00001\" num=\"00001\"><claim-text>1. A method including:\n<claim-text>dedicating a first portion comprising one or more ways of an N way set associative cache exclusively to a first thread;</claim-text>\n<claim-text>dedicating a second portion comprising one or more ways of the cache exclusively to a second thread;</claim-text>\n<claim-text>dynamically sharing a third portion comprising one or more ways of the cache between the first and second threads; and</claim-text>\n<claim-text>performing victim selection in the cache by,\n<claim-text>examining a Least Recently Used (LRU) history for a selected set to identify a least recently used way within the cache as a candidate way to store an information item associated with the first thread;</claim-text>\n<claim-text>determining whether the candidate way is within the first or the third portion of the cache;</claim-text>\n<claim-text>if the candidate way is within the first or the third portion of the cache, then storing the information associated with the first thread in the candidate way; and</claim-text>\n<claim-text>if the candidate way is within the second portion of the cache, then identifying a further way within the cache as being the candidate way, wherein the further way is not the candidate way previously identified.</claim-text>\n</claim-text>\n</claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00002\" num=\"00002\"><claim-text>2. The method of <claim-ref idref=\"CLM-00001\">claim 1</claim-ref> wherein the dynamic sharing of the third portion of the cache is performed according to resource demands of the respective first and second threads.</claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00003\" num=\"00003\"><claim-text>3. The method of <claim-ref idref=\"CLM-00001\">claim 1</claim-ref> wherein the examining includes examining the LRU history to identify M least recently used ways within the cache, wherein M&gt;1, and wherein the determining is performed for the M candidate ways, and wherein the further way is another of the M candidate ways.</claim-text></claim>"}, {"num": 4, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00004\" num=\"00004\"><claim-text>4. The method of <claim-ref idref=\"CLM-00001\">claim 1</claim-ref> wherein the identifying of the further way as being the candidate way comprises identifying a second-least recently used way within the cache.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00005\" num=\"00005\"><claim-text>5. The method of <claim-ref idref=\"CLM-00001\">claim 1</claim-ref> wherein the examining includes examining entries within the LRU history for the selected set, each entry corresponding to a respective one of the ways, wherein the entries are ordered in a sequence determined by least recent usage of the respective ways.</claim-text></claim>"}, {"num": 6, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00006\" num=\"00006\"><claim-text>6. The method of <claim-ref idref=\"CLM-00001\">claim 1</claim-ref> wherein the cache is a trace cache memory, and wherein the information item associated with the first thread comprises a microinstruction of the first thread.</claim-text></claim>"}, {"num": 7, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"CLM-00007\" num=\"00007\"><claim-text>7. An N way set associative cache comprising:\n<claim-text>a first portion comprising one or more ways dedicated to utilization by a first thread executing within a multi-threaded processor;</claim-text>\n<claim-text>a second portion comprising one or more ways dedicated to utilization by a second thread executing within the multi-threaded processor;</claim-text>\n<claim-text>a third portion comprising one or more ways shared by the first and second threads; and</claim-text>\n<claim-text>selection logic to examine a Least Recently Used (LRU) history for a selected set to identify a least recently used way within the cache as a candidate way within which to store an information item associated with the first thread, to store the information associated the first thread in the candidate way if the candidate way is within the first or third portions of the cache, but if the candidate way is within the second portion of the cache, then to identify a further way within the cache as being the candidate way, wherein the further way is not the candidate way previously identified.</claim-text>\n</claim-text></claim>"}, {"num": 8, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00008\" num=\"00008\"><claim-text>8. The cache of <claim-ref idref=\"CLM-00007\">claim 7</claim-ref> wherein the third portion of the cache is shared according to resource demands of the first and second threads.</claim-text></claim>"}, {"num": 9, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00009\" num=\"00009\"><claim-text>9. The cache of <claim-ref idref=\"CLM-00007\">claim 7</claim-ref> wherein the selection logic is to examine for the selected set the LRU history to identify M least recently used ways within the cache, and wherein M&gt;1.</claim-text></claim>"}, {"num": 10, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00010\" num=\"00010\"><claim-text>10. The cache of <claim-ref idref=\"CLM-00007\">claim 7</claim-ref> wherein the selection logic identifies as the further way a second-least recently used way within the cache.</claim-text></claim>"}, {"num": 11, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00011\" num=\"00011\"><claim-text>11. The cache of <claim-ref idref=\"CLM-00007\">claim 7</claim-ref> wherein the selection logic examines entries within the LRU history for the selected set to identify the least recently used way, wherein the entries are ordered in a sequence determined by least recent usage of the ways to which they correspond.</claim-text></claim>"}, {"num": 12, "parent": 7, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00012\" num=\"00012\"><claim-text>12. The cache of <claim-ref idref=\"CLM-00007\">claim 7</claim-ref> wherein the cache is a trace cache memory, and wherein the information item associated with the first thread comprises a microinstruction of the first thread.</claim-text></claim>"}, {"num": 13, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"CLM-00013\" num=\"00013\"><claim-text>13. A method including:\n<claim-text>configuring an N way set associative cache, associated with a multi-threaded processor, to include first and second portions comprising one or more ways dedicated to the first and second threads respectively and a third portion comprising one or more ways shared between the first and second threads;</claim-text>\n<claim-text>for an information item associated with the first thread, examining for a selected set a history of least recently used ways until one is found that is within one of the first and third portions; and</claim-text>\n<claim-text>storing the information item within the found way.</claim-text>\n</claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00014\" num=\"00014\"><claim-text>14. The method of <claim-ref idref=\"CLM-00013\">claim 13</claim-ref> wherein the sharing of the third portion of the cache is performed according to resource demands of the respective first and second threads.</claim-text></claim>"}, {"num": 15, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00015\" num=\"00015\"><claim-text>15. The method of <claim-ref idref=\"CLM-00013\">claim 13</claim-ref>, wherein the examining is performed on M ways of the history of least recently used ways, and where M&gt;1.</claim-text></claim>"}, {"num": 16, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00016\" num=\"00016\"><claim-text>16. The method of <claim-ref idref=\"CLM-00013\">claim 13</claim-ref> wherein the examination of the history of least recently used ways includes examining entries within the history, each entry corresponding to a respective one of the ways, wherein the entries are ordered in a sequence determined by least recent usage of the respective ways.</claim-text></claim>"}, {"num": 17, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00017\" num=\"00017\"><claim-text>17. The method of <claim-ref idref=\"CLM-00013\">claim 13</claim-ref> wherein the cache is a trace cache memory, and wherein the information item associated with the first thread comprises a microinstruction of the first thread.</claim-text></claim>"}, {"num": 18, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"CLM-00018\" num=\"00018\"><claim-text>18. A computer storage storing a sequence of instructions that, when executed within a processor, causes the processor to perform the steps of:\n<claim-text>dedicating a first portion comprising one or more ways of an N way set associative cache exclusively to a first thread;</claim-text>\n<claim-text>dedicating a second portion comprising one or more ways of the cache exclusively to a second thread;</claim-text>\n<claim-text>dynamically sharing a third portion comprising one or more ways of the cache between the first and second threads; and</claim-text>\n<claim-text>performing victim selection in the cache by,\n<claim-text>examining a Least Recently Used (LRU) history for a selected set to identify a least recently used way within the cache as a candidate way to store an information item associated with the first thread;</claim-text>\n<claim-text>determining whether the candidate way is within the first or the third portion of the cache;</claim-text>\n<claim-text>if the candidate way is within the first or the third portion of the cache, then storing the information associated with the first thread in the candidate way; and</claim-text>\n<claim-text>if the candidate way is within the second portion of the cache, then identifying a further way within the cache as being the candidate way, wherein the further way is not the candidate way previously identified.</claim-text>\n</claim-text>\n</claim-text></claim>"}, {"num": 19, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00019\" num=\"00019\"><claim-text>19. The computer storage of <claim-ref idref=\"CLM-00018\">claim 18</claim-ref> wherein the dynamic sharing of the third portion of the cache is performed according to resource demands of the respective first and second threads.</claim-text></claim>"}, {"num": 20, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00020\" num=\"00020\"><claim-text>20. The computer storage of <claim-ref idref=\"CLM-00018\">claim 18</claim-ref> wherein the examining includes examining the LRU history to identify M least recently used ways within the cache, wherein M&gt;1, and wherein the determining is performed for the M candidate ways, and wherein the further way is another of the M candidate ways.</claim-text></claim>"}, {"num": 21, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00021\" num=\"00021\"><claim-text>21. The computer storage of <claim-ref idref=\"CLM-00018\">claim 18</claim-ref> wherein the identifying of the further way as being the candidate way comprises identifying a second-least recently used way within the cache.</claim-text></claim>"}, {"num": 22, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00022\" num=\"00022\"><claim-text>22. The computer storage of <claim-ref idref=\"CLM-00018\">claim 18</claim-ref> wherein the examining includes examining entries within the LRU history for the selected set, each entry corresponding to a respective one of the ways, wherein the entries are ordered in a sequence determined by least recent usage of the respective ways.</claim-text></claim>"}, {"num": 23, "parent": 18, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00023\" num=\"00023\"><claim-text>23. The computer storage of <claim-ref idref=\"CLM-00018\">claim 18</claim-ref> wherein the cache is a trace cache memory, and wherein the information item associated with the first thread comprises a microinstruction of the first thread.</claim-text></claim>"}, {"num": 24, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"CLM-00024\" num=\"00024\"><claim-text>24. A method comprising:\n<claim-text>detecting misses in an N way set associative cache of a processor;</claim-text>\n<claim-text>performing victim selection responsive to each of the misses in a manner that partitions the capacity of the cache to make a first and second portion each comprising one or more ways available for replacement respectively only by each of a first and second instruction threads while at the same time that defines a shared portion comprising one or more ways available to both the first and second instruction threads, wherein the performing victim selection responsive to each of the misses includes examining entries of a least recently used history until an available one of said ways within a selected set is found.</claim-text>\n</claim-text></claim>"}, {"num": 25, "parent": 24, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00025\" num=\"00025\"><claim-text>25. The method of <claim-ref idref=\"CLM-00024\">claim 24</claim-ref>, further comprising:\n<claim-text>performing fine multithreading of the first and second threads in the processor.</claim-text>\n</claim-text></claim>"}, {"num": 26, "parent": 24, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00026\" num=\"00026\"><claim-text>26. The method of <claim-ref idref=\"CLM-00024\">claim 24</claim-ref>, wherein the examining includes examining M of the entries corresponding to the M least recently used of the ways, and wherein M&gt;1.</claim-text></claim>"}, {"num": 27, "parent": 24, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00027\" num=\"00027\"><claim-text>27. The method of <claim-ref idref=\"CLM-00024\">claim 24</claim-ref> wherein the shared portion of the cache is shared according to resource demands of the respective first and second threads.</claim-text></claim>"}, {"num": 28, "parent": 24, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00028\" num=\"00028\"><claim-text>28. The method of <claim-ref idref=\"CLM-00024\">claim 24</claim-ref> wherein each of the entries corresponds to a respective one of the ways, and wherein the entries are ordered in a sequence determined by least recent usage of the respective ways.</claim-text></claim>"}, {"num": 29, "parent": 24, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00029\" num=\"00029\"><claim-text>29. The method of <claim-ref idref=\"CLM-00024\">claim 24</claim-ref> wherein the cache is a trace cache memory to store microinstructions of the first and second instruction threads.</claim-text></claim>"}, {"num": 30, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"CLM-00030\" num=\"00030\"><claim-text>30. An apparatus comprising:\n<claim-text>a processor including,\n<claim-text>an N way set associative cache having a storage capacity, and</claim-text>\n<claim-text>victim selection logic including logic to partition the storage capacity of the cache having a dedicated portion comprising one or more ways for each of a first and second instruction threads while at the same time having a shared portion comprising one or more ways accessible by both the first and second threads, wherein the victim selection logic responsive to each of the misses in the cache to examine entries of a least recently used history for a selected set of the cache until one of said ways is found that is available to the instruction thread causing that cache miss.</claim-text>\n</claim-text>\n</claim-text></claim>"}, {"num": 31, "parent": 30, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00031\" num=\"00031\"><claim-text>31. The apparatus of <claim-ref idref=\"CLM-00030\">claim 30</claim-ref>, wherein the cache is a trace cache and the processor is capable of fine simultaneous multithreading.</claim-text></claim>"}, {"num": 32, "parent": 30, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00032\" num=\"00032\"><claim-text>32. The apparatus of <claim-ref idref=\"CLM-00030\">claim 30</claim-ref>, wherein the cache is a trace cache, wherein the trace cache is to store microinstructions of the first and second threads.</claim-text></claim>"}, {"num": 33, "parent": 30, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00033\" num=\"00033\"><claim-text>33. The apparatus of <claim-ref idref=\"CLM-00030\">claim 30</claim-ref> wherein the entries are ordered in a sequence determined by least recent usage of the ways to which they correspond.</claim-text></claim>"}, {"num": 34, "parent": 30, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00034\" num=\"00034\"><claim-text>34. The apparatus of <claim-ref idref=\"CLM-00030\">claim 30</claim-ref> wherein the victim selection logic responsive to each of the misses in the cache to examine M entries corresponding to M least recently used ways within the cache, and wherein M&gt;1.</claim-text></claim>"}, {"num": 35, "parent": 30, "type": "dependent", "paragraph_markup": "<claim id=\"CLM-00035\" num=\"00035\"><claim-text>35. The apparatus of <claim-ref idref=\"CLM-00030\">claim 30</claim-ref> wherein the shared portion of the cache is shared according to resource demands of the respective first and second threads.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES39884025\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4 id=\"h-0001\">FIELD OF THE INVENTION</h4><p id=\"p-0002\" num=\"0001\">The present invention relates generally to the field of multi-threaded processors and, more specifically, to a method and apparatus for partitioning a processor resource within a multi-threaded processor.</p><h4 id=\"h-0002\">BACKGROUND OF THE INVENTION</h4><p id=\"p-0003\" num=\"0002\">Multi-threaded processor design has recently been considered as an increasingly attractive option for increasing the performance of processors. Multithreading within a processor, inter alia, provides the potential for more effective utilization of various processor resources, and particularly for more effective utilization of the execution logic within a processor. Specifically, by feeding multiple threads to the execution logic of a processor, clock cycles that would otherwise have been idle due to a stall or other delay in the processing of a particular thread may be utilized to service another thread. A stall in the processing of a particular thread may result from a number of occurrences within a processor pipeline. For example, a cache miss or a branch missprediction (i.e., a long-latency operation) for an instruction included within a thread typically results in the processing of the relevant thread stalling. The negative effect of long-latency operations on execution logic efficiencies is exacerbated by the recent increases in execution logic throughput that have outstripped advances in memory access and retrieval rates.</p><p id=\"p-0004\" num=\"0003\">Multi-threaded computer applications are also becoming increasingly common in view of the support provided to such multi-threaded applications by a number of popular operating systems, such as the Windows NT\u00ae and Unix operating systems. Multi-threaded computer applications are particularly efficient in the multi-media arena.</p><p id=\"p-0005\" num=\"0004\">Multi-threaded processors may broadly be classified into two categories (i.e., fine or coarse designs) according to the thread interleaving or switching scheme employed within the relevant processor. Fine multi-threaded designs support multiple active threads within a processor and typically interleave two different threads on a cycle-by-cycle basis. Coarse multi-threaded designs typically interleave the instructions of different threads on the occurrence of some long-latency event, such as a cache miss. A coarse multi-threaded design is discussed in Eickemayer, R.; Johnson, R.; et al., \u201cEvaluation of Multithreaded Uniprocessors for Commercial Application Environments\u201d, <i>The </i>23<i>rd Annual International Symposium on Computer Architecture</i>, pp. 203-212, May 1996. The distinctions between fine and coarse designs are further discussed in Laudon, J; Gupta, A, \u201cArchitectural and Implementation Tradeoffs in the Design of Multiple-Context Processors\u201d, <i>Multithreaded Computer Architectures: A Summary of the State of the Art</i>, edited by R. A. Iannuci et al., pp. 167-200, Kluwer Academic Publishers, Norwell, Mass., 1994. Laudon further proposes an interleaving scheme that combines the cycle-by-cycle switching of a fine design with the full pipeline interlocks of a coarse design (or blocked scheme). To this end, Laudon proposes a \u201cback off\u201d instruction that makes a specific thread (or context) unavailable for a specific number of cycles. Such a \u201cback off\u201d instruction may be issued upon the occurrence of predetermined events, such as a cache miss. In this way, Laudon avoids having to perform an actual thread switch by simply making one of the threads unavailable.</p><p id=\"p-0006\" num=\"0005\">Where resource sharing is implemented within a multi-threaded processor (i.e., there is limited or no duplication of function units for each thread supported by the processor) it is desirable to effectively share resources between the threads.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><description-of-drawings><h4 id=\"h-0003\">BRIEF DESCRIPTION OF THE DRAWINGS</h4><p id=\"p-0007\" num=\"0006\">The present invention is illustrated by way of example and not limited in the figures of the accompanying drawings, in which like references indicate similar elements and in which:</p><p id=\"p-0008\" num=\"0007\"><figref idrefs=\"DRAWINGS\">FIG. 1</figref> is a block diagram illustrating an exemplary pipeline of a processor within which the present invention may be implemented.</p><p id=\"p-0009\" num=\"0008\"><figref idrefs=\"DRAWINGS\">FIG. 2</figref> is a block diagram illustrating an exemplary embodiment of a processor, in the form of a general-purpose multi-threaded microprocessor, within which the present invention may be implemented.</p><p id=\"p-0010\" num=\"0009\"><figref idrefs=\"DRAWINGS\">FIG. 3</figref> is a block diagram illustrating selected components of an exemplary multi-threaded microprocessor, and specifically depicts various functional units that provide a buffering (or storage) capability as being logically partitioned to accommodate multiple thread.</p><p id=\"p-0011\" num=\"0010\"><figref idrefs=\"DRAWINGS\">FIG. 4</figref> is a block diagram showing further details regarding various components of an exemplary trace delivery engine (TDE).</p><p id=\"p-0012\" num=\"0011\"><figref idrefs=\"DRAWINGS\">FIG. 5</figref> is a block diagram illustrating further architectural details of an exemplary trace cache fill buffer.</p><p id=\"p-0013\" num=\"0012\"><figref idrefs=\"DRAWINGS\">FIG. 6</figref> is a block diagram illustrating further architectural details of an exemplary trace cache (TC).</p><p id=\"p-0014\" num=\"0013\"><figref idrefs=\"DRAWINGS\">FIG. 7</figref> is a block diagram illustrating further structural details of an exemplary trace cache (TC)</p><p id=\"p-0015\" num=\"0014\"><figref idrefs=\"DRAWINGS\">FIG. 8</figref> is a block diagram illustrating various inputs and outputs of exemplary thread selection logic.</p><p id=\"p-0016\" num=\"0015\"><figref idrefs=\"DRAWINGS\">FIG. 9</figref> is a block diagram illustrating three exemplary components of exemplary thread selection logic in the form of a thread selection state machine, and a counter and comparator for a second thread.</p><p id=\"p-0017\" num=\"0016\"><figref idrefs=\"DRAWINGS\">FIG. 10</figref> is a state diagram illustrating exemplary operation of an exemplary thread selection state machine.</p><p id=\"p-0018\" num=\"0017\"><figref idrefs=\"DRAWINGS\">FIG. 11</figref> is a block diagram illustrating architectural details of an exemplary embodiment of victim selection logic.</p><p id=\"p-0019\" num=\"0018\"><figref idrefs=\"DRAWINGS\">FIG. 12</figref> is a flow chart illustrating an exemplary method of partitioning a memory resource, such as for example a trace cache, within a multi-threaded processor.</p><p id=\"p-0020\" num=\"0019\"><figref idrefs=\"DRAWINGS\">FIG. 13</figref> is a flow chart illustrating an exemplary method of partitioning a resource, in the exemplary form of a memory resource, utilizing a Least Recently Used (LRU) history associated with the relevant memory resource.</p><p id=\"p-0021\" num=\"0020\"><figref idrefs=\"DRAWINGS\">FIG. 14</figref> is a block diagram illustrating an exemplary LRU history data structure.</p><p id=\"p-0022\" num=\"0021\"><figref idrefs=\"DRAWINGS\">FIG. 15</figref> is a block diagram illustrating further details pertaining to inputs to, and outputs from, exemplary victim selection logic.</p></description-of-drawings><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4 id=\"h-0004\">DETAILED DESCRIPTION</h4><p id=\"p-0023\" num=\"0022\">A method and apparatus for partitioning a processor resource within a multi-threaded processor are described. In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the present invention. It will be evident, however, to one skilled in the art that the present invention may be practiced without these specific details.</p><p id=\"p-0024\" num=\"0023\">For the purposes of the present specification, the term \u201cevent\u201d shall be taken to include any event, internal or external to a processor, that causes a change or interruption to the servicing of an instruction stream (macro- or micro-instruction) within a processor. Accordingly, the term \u201cevent\u201d shall be taken to include, but not limited to, branch instructions, exceptions and interrupts that may be generated within or outside the processor.</p><p id=\"p-0025\" num=\"0024\">For the purposes of the present specification, the term \u201cprocessor\u201d shall be taken to refer to any machine that is capable of executing a sequence of instructions (e.g., macro- or micro-instructions), and shall be taken to include, but not be limited to, general purpose microprocessors, special purpose microprocessors, graphics controllers, audio controllers, multi-media controllers and microcontrollers. Further, the term \u201cprocessor\u201d shall be taken to refer to, inter alia, Complex Instruction Set Computers (CISC), Reduced Instruction Set Computers (RISC), or Very Long Instruction Word (VLIW) processors.</p><p id=\"p-0026\" num=\"0025\">For the purposes of the present specification, the term \u201cresource\u201d shall be taken to include any unit, component or module of a processor, and shall be taken to include, but not be limited to, a memory resource, a processing resource, a buffering resource, a communications resource or bus, a sequencing resource or a translating resource.</p><h4 id=\"h-0005\">Processor Pipeline</h4><p id=\"p-0027\" num=\"0026\"><figref idrefs=\"DRAWINGS\">FIG. 1</figref> is a high-level block diagram illustrating an exemplary embodiment of processor pipeline <b>10</b> within which the present invention may be implemented. The pipeline <b>10</b> includes a number of pipe stages, commencing with a fetch pipe stage <b>12</b> at which instructions (e.g., macroinstructions) are retrieved and fed into the pipeline <b>10</b>. For example, a macroinstruction may be retrieved from a cache memory that is integral with the processor, or closely associated therewith, or may be retrieved from an external main memory via a processor bus. From the fetch pipe stage <b>12</b>, the macroinstructions are propagated to a decode pipe stage <b>14</b>, where macroinstructions are translated into microinstructions (also termed \u201cmicrocode\u201d) suitable for execution within the processor. The microinstructions are then propagated downstream to an allocate pipe stage <b>16</b>, where processor resources are allocated to the various microinstructions according to availability and need. The microinstructions are then executed at an execute stage <b>18</b> before being retired, or \u201cwritten-back\u201d (e.g., committed to an architectural state) at a retire pipe stage <b>20</b>.</p><h4 id=\"h-0006\">Microprocessor Architecture</h4><p id=\"p-0028\" num=\"0027\"><figref idrefs=\"DRAWINGS\">FIG. 2</figref> is a block diagram illustrating an exemplary embodiment of a processor <b>30</b>, in the form of a general-purpose microprocessor, within which the present invention may be implemented. The processor <b>30</b> is described below as being a multi-threaded (MT) processor, and is accordingly able simultaneously to process multiple instruction threads (or contexts). However, a number of the teachings provided below in the specification are not specific to a multi-threaded processor, and may find application in a single threaded processor. In an exemplary embodiment, the processor <b>30</b> may comprise an Intel Architecture (IA) microprocessor that is capable of executing the Intel Architecture instruction set. An example of such an Intel Architecture microprocessor is the Pentium Pro\u00ae microprocessor or the Pentium III\u00ae microprocessor manufactured by Intel Corporation of Santa Clara, Calif.</p><p id=\"p-0029\" num=\"0028\">The processor <b>30</b> comprises an in-order front end and an out-of-order back end. The in-order front end includes a bus interface unit <b>32</b>, which functions as the conduit between the processor <b>30</b> and other components (e.g., main memory) of a computer system within which the processor <b>30</b> may be employed. To this end, the bus interface unit <b>32</b> couples the processor <b>30</b> to a processor bus (not shown) via which data and control information may be received at and propagated from the processor <b>30</b>. The bus interface unit <b>32</b> includes Front Side Bus (FSB) logic <b>34</b> that controls communications over the processor bus. The bus interface unit <b>32</b> further includes a bus queue <b>36</b> that provides a buffering function with respect to communications over the processor bus. The bus interface unit <b>32</b> is shown to receive bus requests <b>38</b> from, and to send snoops or bus returns <b>40</b> to, a memory execution unit <b>42</b> that provides a local memory capability within the processor <b>30</b>. The memory execution unit <b>42</b> includes a unified data and instruction cache <b>44</b>, a data Translation Lookaside Buffer (TLB) <b>46</b>, and memory ordering buffer <b>48</b>. The memory execution unit <b>42</b> receives instruction fetch requests from, and delivers raw instructions (i.e., coded macroinstructions) to, a microinstruction translation engine <b>54</b> that translates the received macroinstructions into a corresponding set of microinstructions.</p><p id=\"p-0030\" num=\"0029\">The microinstruction translation engine <b>54</b> effectively operates as a trace cache \u201cmiss handler\u201d in that it operates to deliver microinstructions to a trace cache <b>62</b> in the event of a trace cache miss. To this end, the microinstruction translation engine <b>54</b> functions to provide the fetch and decode pipe stages <b>12</b> and <b>14</b> in the event of a trace cache miss. The microinstruction translation engine <b>54</b> is shown to include a next instruction pointer (NIP) <b>100</b>, an instruction Translation Lookaside Buffer (TLB) <b>102</b>, a branch predictor <b>104</b>, an instruction streaming buffer <b>106</b>, an instruction pre-decoder <b>108</b>, instruction steering logic <b>110</b>, an instruction decoder <b>112</b>, and a branch address calculator <b>114</b>. The next instruction pointer <b>100</b>, TLB <b>102</b>, branch predictor <b>104</b> and instruction streaming buffer <b>106</b> together constitute a branch prediction unit (BPU). The instruction decoder <b>112</b> and branch address calculator <b>114</b> together comprise an instruction translate (IX) unit.</p><p id=\"p-0031\" num=\"0030\">The next instruction pointer <b>100</b> issues next instruction requests to the unified cache <b>44</b>. In the exemplary embodiment where the processor <b>30</b> comprises a multi-threaded microprocessor capable of processing two threads, the next instruction pointer <b>100</b> may include a multiplexer (MUX) (not shown) that selects between instruction pointers associated with either the first or second thread for inclusion within the next instruction request issued therefrom. In one embodiment, the next instruction pointer <b>100</b> will interleave next instruction requests for the first and second threads on a cycle-by-cycle (\u201cping pong\u201d) basis, assuming instructions for both threads have been requested, and instruction streaming buffer <b>106</b> resources for both of the threads have not been exhausted. The next instruction pointer requests may be for either 16, 32 or 64-bytes depending on whether the initial request address is in the upper half of a 32-byte or 64-byte aligned line. The next instruction pointer <b>100</b> may be redirected by the branch predictor <b>104</b>, the branch address calculator <b>114</b> or by the trace cache <b>62</b>, with a trace cache miss request being the highest priority redirection request.</p><p id=\"p-0032\" num=\"0031\">When the next instruction pointer <b>100</b> makes an instruction request to the unified cache <b>44</b>, it generates a two-bit \u201crequest identifier\u201d that is associated with the instruction request and functions as a \u201ctag\u201d for the relevant instruction request. When returning data responsive to an instruction request, the unified cache <b>44</b> returns the following tags or identifiers together with the data:\n</p><ul><li id=\"ul0001-0001\" num=\"0000\"><ul><li id=\"ul0002-0001\" num=\"0032\">1. The \u201crequest identifier\u201d supplied by the next instruction pointer <b>100</b>;</li><li id=\"ul0002-0002\" num=\"0033\">2. A three-bit \u201cchunk identifier\u201d that identifies the chunk returned; and</li><li id=\"ul0002-0003\" num=\"0034\">3. A \u201cthread identifier\u201d that identifies the thread to which the returned data belongs.</li></ul></li></ul>\n<p id=\"p-0033\" num=\"0035\">Next instruction requests are propagated from the next instruction pointer <b>100</b> to the instruction TLB <b>102</b>, which performs an address lookup operation, and delivers a physical address to the unified cache <b>44</b>. The unified cache <b>44</b> delivers a corresponding macroinstruction to the instruction streaming buffer <b>106</b>. Each next instruction request is also propagated directly from the next instruction pointer <b>100</b> to the instruction streaming buffer <b>106</b> so as to allow the instruction streaming buffer <b>106</b> to identify the thread to which a macroinstruction received from the unified cache <b>44</b> belongs. The macroinstructions from both first and second threads are then issued from the instruction streaming buffer <b>106</b> to the instruction pre-decoder <b>108</b>, which performs a number of length calculation and byte marking operations with respect to a received instruction stream (of macroinstructions). Specifically, the instruction pre-decoder <b>108</b> generates a series of byte marking vectors that serve, inter alia, to demarcate macroinstructions within the instruction stream propagated to the instruction steering logic <b>110</b>.</p><p id=\"p-0034\" num=\"0036\">The instruction steering logic <b>110</b> then utilizes the byte marking vectors to steer discrete macroinstructions to the instruction decoder <b>112</b> for the purposes of decoding. Macroinstructions are also propagated from the instruction steering logic <b>110</b> to the branch address calculator <b>114</b> for the purposes of branch address calculation. Microinstructions are then delivered from the instruction decoder <b>112</b> to the trace delivery engine <b>60</b>.</p><p id=\"p-0035\" num=\"0037\">During decoding, flow markers are associated with each microinstruction. A flow marker indicates a characteristic of the associated microinstruction and may, for example, indicate the associated microinstruction as being the first or last microinstruction in a microcode sequence representing a macroinstruction. The flow markers include a \u201cbeginning of macroinstruction\u201d (BOM) and an \u201cend of macroinstruction\u201d (EOM) flow markers. According to the present invention, the decoder <b>112</b> may further decode the microinstructions to have shared resource (multiprocessor) (SHRMP) flow markers and synchronization (SYNC) flow markers associated therewith. Specifically, a shared resource flow marker identifies a microinstruction as a location within a particular thread at which the thread may be interrupted (e.g., re-started or paused) with less negative consequences than elsewhere in the thread. The decoder <b>112</b>, in an exemplary embodiment of the present invention, is constructed to mark microinstructions that comprise the end or the beginning of a parent macroinstruction with a shared resource flow marker. A synchronization flow market identifies a microinstruction as a location within a particular thread at which the thread may be synchronized with another thread responsive to, for example, a synchronization instruction within the other thread.</p><p id=\"p-0036\" num=\"0038\">From the microinstruction translation engine <b>54</b>, decoded instructions (i.e., microinstructions) are sent to a trace delivery engine <b>60</b>. The trace delivery engine <b>60</b> includes the trace cache <b>62</b>, a trace branch predictor (BTB) <b>64</b>, a microcode sequencer <b>66</b> and a microcode (uop) queue <b>68</b>. The trace delivery engine <b>60</b> functions as a microinstruction cache, and is the primary source of microinstructions for a downstream execution unit <b>70</b>. By providing a microinstruction caching function within the processor pipeline, the trace delivery engine <b>60</b>, and specifically the trace cache <b>62</b>, allows translation work done by the microinstruction translation engine <b>54</b> to be leveraged to provide an increased microinstruction bandwidth. In one exemplary embodiment, the trace cache <b>62</b> may comprise a 256 set, 8 way set associate memory. The term \u201ctrace\u201d, in the present exemplary embodiment, may refer to a sequence of microinstructions stored within entries of the trace cache <b>62</b>, each entry including pointers to preceding and proceeding microinstructions comprising the trace. In this way, the trace cache <b>62</b> facilitates high-performance sequencing in that the address of the next entry to be accessed for the purposes of obtaining a subsequent microinstruction is known before a current access is complete. Traces may be viewed as \u201cblocks\u201d of instructions that are distinguished from one another by trace heads, and are terminated upon encountering an indirect branch or by reaching one of many present threshold conditions, such as the number of conditioned branches that may be accommodated in a single trace or the maximum number of total microinstructions that may comprise a trace. The trace cache branch prediction unit <b>64</b> provides local branch predictions pertaining to traces within the trace cache <b>62</b>. The trace cache <b>62</b> and the microcode sequencer <b>66</b> provide microinstructions to the microcode queue <b>68</b>, from where the microinstructions are then fed to an out-of-order execution cluster. The microcode sequencer <b>66</b> furthermore includes a number of event handlers embodied in microcode, that implement a number of operations within the processor <b>30</b> in response to the occurrence of an event such as an exception or an interrupt. The event handlers <b>67</b> are invoked by an event detector (not shown) included within a register renamer <b>74</b> in the back end of the processor <b>30</b>.</p><p id=\"p-0037\" num=\"0039\">The processor <b>30</b> may be viewed as having an in-order front-end, comprising the bus interface unit <b>32</b>, the memory execution unit <b>42</b>, the microinstruction translation engine <b>54</b> and the trace delivery engine <b>60</b>, and an out-of-order back-end that will be described in detail below.</p><p id=\"p-0038\" num=\"0040\">Microinstructions dispatched from the microcode queue <b>68</b> are received into an out-of-order cluster <b>71</b> comprising a scheduler <b>72</b>, the register renamer <b>74</b>, an allocator <b>76</b>, a reorder buffer <b>78</b> and a replay queue <b>80</b>. The scheduler <b>72</b> includes a set of reservation stations, and operates to schedule and dispatch microinstructions for execution by the execution unit <b>70</b>. The register renamer <b>74</b> performs a register renaming function with respect to hidden integer and floating point registers (that may be utilized in place of any of the eight general purpose registers or any of the eight floating-point registers, where a processor <b>30</b> executes the Intel Architecture instruction set). The allocator <b>76</b> operates to allocate resources of the execution unit <b>70</b> and the cluster <b>71</b> to microinstructions according to availability and need. In the event that insufficient resources are available to process a microinstruction, the allocator <b>76</b> is responsible for asserting a stall signal <b>82</b>, that is propagated through the trace delivery engine <b>60</b> to the microinstruction translation engine <b>54</b>, as shown at <b>58</b>. Microinstructions, which have had their source fields adjusted by the register renamer <b>74</b>, are placed in a reorder buffer <b>78</b> in strict program order. When microinstructions within the reorder buffer <b>78</b> have completed execution and are ready for retirement, they are then removed from the reorder buffer <b>162</b>. The replay queue <b>80</b> propagates microinstructions that are to be replayed to the execution unit <b>70</b>.</p><p id=\"p-0039\" num=\"0041\">The execution unit <b>70</b> is shown to include a floating-point execution engine <b>84</b>, an integer execution engine <b>86</b>, and a level 0 data cache <b>88</b>. In one exemplary embodiment in which is the processor <b>30</b> executes the Intel Architecture instruction set, the floating point execution engine <b>84</b> may further execute MMX\u00ae instructions.</p><h4 id=\"h-0007\">Multithreading Implementation</h4><p id=\"p-0040\" num=\"0042\">In the exemplary embodiment of the processor <b>30</b> illustrated in <figref idrefs=\"DRAWINGS\">FIG. 2</figref>, there may be limited duplication or replication of resources to support a multithreading capability, and it is accordingly necessary to implement some degree of resource sharing between threads. The resource sharing scheme employed, it will be appreciated, is dependent upon the number of threads that the processor is able simultaneously to process. As functional units within a processor typically provide some buffering (or storage) functionality and propagation functionality, the issue of resource sharing may be viewed as comprising (1) storage and (2) processing/propagating bandwidth sharing components. For example, in a processor that supports the simultaneous processing of two threads, buffer resources within various functional units may be statically or logically partitioned between two threads. Similarly, the bandwidth provided by a path for the propagation of information between two functional units must be divided and allocated between the two threads. As these resource sharing issues may arise at a number of locations within a processor pipeline, different resource sharing schemes may be employed at these various locations in accordance with the dictates and characteristics of the specific location. It will be appreciated that different resource sharing schemes may be suited to different locations in view of varying functionalities and operating characteristics.</p><p id=\"p-0041\" num=\"0043\"><figref idrefs=\"DRAWINGS\">FIG. 3</figref> is a block diagram illustrating selected components of the processor <b>30</b> illustrated in <figref idrefs=\"DRAWINGS\">FIG. 2</figref>, and depicts various functional units that provide a buffering capability as being logically partitioned to accommodate two threads (i.e., thread <b>0</b> and thread <b>1</b>). The logical partitioning for two threads of the buffering (or storage) and processing facilities of a functional unit may be achieved by allocating a first predetermined set of entries within a buffering resource to a first thread and allocating a second predetermined set of entries within the buffering resource to a second thread. Specifically, this may be achieved by providing two pairs of read and write pointers, a first pair of read and write pointers being associated with a first thread and a second pair of read and write pointers being associated with a second thread. The first set of read and write pointers may be limited to a first predetermined number of entries within a buffering resource, while the second set of read and write pointers may be limited to a second predetermined number of entries within the same buffering resource. In the exemplary embodiment, the instruction streaming buffer <b>106</b>, the trace cache <b>62</b>, and an instruction queue <b>103</b> are shown to each provide a storage capacity that is logically partitioned between the first and second threads. Each of these units is also shown to include a \u201cshared\u201d capacity that may, according to respective embodiments, be dynamically allocated to either the first or the second thread according to certain criteria.</p><h4 id=\"h-0008\">Trace Delivery Engine</h4><p id=\"p-0042\" num=\"0044\">One embodiment of the present invention is described below as being implemented within a trace delivery engine <b>60</b>, and specifically with respect to a trace cache <b>62</b>. However, it will be appreciated that the present invention may be applied to a partition any resources within or associated with a processor, and the trace delivery engine <b>60</b> is merely provided as an exemplary embodiment.</p><p id=\"p-0043\" num=\"0045\">As alluded to above, the trace delivery engine <b>60</b> may function as a primary source of microinstructions during periods of high performance by providing relatively low latency and high bandwidth. Specifically, for a CISC instruction set, such as the Intel Architecture x86 instruction set, decoding of macroinstructions to deliver microinstructions may introduce a performance bottleneck as the variable length of such instructions complicates parallel decoding operations. The trace delivery engine <b>60</b> attempts to address this problem to a certain extent by providing for the caching of microinstructions, thus obviating the need for microinstructions executed by the execution unit to be continually decoded.</p><p id=\"p-0044\" num=\"0046\">To provide high-performance sequencing of cached microinstructions, the trace delivery engine <b>60</b> creates sequences of entries (or microinstructions) that may conveniently be labeled \u201ctraces\u201d. A trace may, in one embodiment, facilitate sequencing in that the address of a subsequent entry can be known during a current access operation, and before a current access operation is complete. In one embodiment, a trace of microinstructions may only be entered through a so-called \u201chead\u201d entry, that includes a linear address that determines a set of subsequent entries of the trace event stored in successive sets, with every entry (except a tail entry) containing a way pointer to a next entry. Similarly, every entry (except a head entry) contains a way pointer to a previous entry.</p><p id=\"p-0045\" num=\"0047\">In one embodiment, the trace delivery engine <b>60</b> may implement two modes to either provide input thereto or output therefrom. The trace delivery engine <b>60</b> may implement a \u201cbuild mode\u201d when a miss occurs with respect to a trace cache <b>62</b>, such a miss being passed on to the microinstruction translation engine <b>54</b>. In the \u201cbuild mode\u201d, the microinstruction translation engine <b>54</b> will then perform a translation operation on a macroinstruction received either from the unified cache <b>44</b>, or by performing a memory access operation via the processor bus. The microinstruction translation engine <b>54</b> then provides the microinstructions, derived from the macroinstruction(s), to the trace delivery engine <b>60</b> which populates the trace cache <b>62</b> with these microinstructions.</p><p id=\"p-0046\" num=\"0048\">When a trace cache hit occurs, the trace delivery engine <b>60</b> operates in a \u201cstream mode\u201d where a trace, or traces, of microinstructions are fed from the trace delivery engine <b>60</b>, and specifically the trace cache <b>62</b>, to the processor back end via the microinstruction queue <b>68</b>.</p><p id=\"p-0047\" num=\"0049\"><figref idrefs=\"DRAWINGS\">FIG. 4</figref> is a block diagram showing further details regarding the various components of the trace delivery engine (TDE) <b>60</b> shown in <figref idrefs=\"DRAWINGS\">FIG. 2</figref>. The next instruction pointer <b>100</b>, which forms part of the microinstruction translation engine <b>54</b>, is shown to receive a prediction output <b>65</b> from the trace branch prediction unit <b>64</b>. The next instruction pointer <b>100</b> provides an instruction pointer output <b>67</b>, which may correspond to the prediction output <b>65</b>, to the trace cache <b>62</b>.</p><p id=\"p-0048\" num=\"0050\">A trace branch address calculator (TBAC) <b>120</b> monitors the output of the microsequencer microinstruction queue <b>68</b>, and performs a number of functions to provide output to a trace branch information table <b>122</b>. Specifically, the trace branch address calculator <b>120</b> is responsible for bogus branch detection, the validation of branch target and branch prediction operations, for computing a Next Linear Instruction Pointer (NILIP) for each instruction, and for detecting limit violations for each instruction.</p><p id=\"p-0049\" num=\"0051\">The trace branch information table (TBIT) <b>122</b> stores information required to update the trace branch prediction unit <b>64</b>. The table <b>122</b> also holds information for events and, in one embodiment, is hard partitioned to support multithreading. Of course, in an alternative embodiment, the table <b>122</b> may be dynamically partitioned.</p><p id=\"p-0050\" num=\"0052\">The trace branch information table <b>122</b> provides input to a trace branch target buffer (trace BTB) <b>124</b> that operates to predict \u201cleave trace\u201d conditions and \u201cend-of-trace\u201d branches. To this end, the buffer <b>124</b> may operate to invalidate microinstructions.</p><p id=\"p-0051\" num=\"0053\">When operating in the above-mentioned \u201cbuild mode\u201d, microinstructions are received into the trace cache <b>62</b> via a trace cache fill buffer (TCFB) <b>125</b>, which is shown in <figref idrefs=\"DRAWINGS\">FIG. 4</figref> to provide input into the trace cache <b>62</b>.</p><p id=\"p-0052\" num=\"0054\"><figref idrefs=\"DRAWINGS\">FIG. 5</figref> is a block diagram illustrating further architectural details of the trace cache fill buffer <b>125</b>. In one embodiment of the buffer <b>125</b> includes first and second buffers <b>134</b> and <b>136</b>, each of which is dedicated to a specific thread (e.g., thread <b>0</b> and thread <b>1</b>). Each of the buffers <b>134</b> and <b>136</b> provides four (4) entries for an associated thread, and outputs microinstructions to a staging buffer <b>138</b>, from where the microinstructions are communicated to the trace cache <b>62</b>. The trace cache fill buffer <b>125</b> implements a build algorithm in hardware that realizes the \u201cbuild mode\u201d, and provides microinstruction positioning, and the detection of \u201cend-of-line\u201d and \u201cend-of-trace\u201d conditions.</p><p id=\"p-0053\" num=\"0055\">The trace cache <b>62</b> is shown in <figref idrefs=\"DRAWINGS\">FIG. 4</figref> to include a data array <b>128</b> and an associated tag array <b>126</b>. The data array <b>128</b> provides a storage for, in one embodiment, 12 KB of microinstructions.</p><p id=\"p-0054\" num=\"0056\"><figref idrefs=\"DRAWINGS\">FIG. 6</figref> is a block diagram illustrating further architectural details pertinent to the trace cache <b>62</b>. The thread selection logic <b>140</b> implements a thread selection state machine that, in one embodiment, decides on a cycle-by-cycle basis which of multiple threads (e.g., thread <b>0</b> or thread <b>1</b>) is propagated to subsequent pipe stages of a processor <b>30</b>.</p><p id=\"p-0055\" num=\"0057\"><figref idrefs=\"DRAWINGS\">FIG. 6</figref> also illustrates the partitioning of the trace cache <b>62</b> into three portions (or sections), namely a first portion <b>148</b> dedicated to a first thread, a second portion <b>152</b> dedicated to a second thread, and a third portion <b>150</b> that is dynamically shared between the first and second threads. In the exemplary embodiment, each of the first and second portions <b>148</b> and <b>152</b> comprises two (2) ways of the data array <b>128</b> (and the associated tag array <b>126</b>) of the trace cache <b>62</b>. The third, shared portion <b>150</b> constitutes four (4) of the data array <b>128</b>, and the associated tag array <b>126</b>. The illustrated partitioning of the trace cache <b>62</b> is implemented by victim selection logic <b>154</b>, which will be described in further detail below.</p><p id=\"p-0056\" num=\"0058\"><figref idrefs=\"DRAWINGS\">FIG. 7</figref> is a block diagram illustrating an exemplary structure of the trace cache <b>62</b>, according to one embodiment. Each of the tag array <b>126</b> and the data array <b>128</b> are each shown to comprise an eight-way, set associative arrangement, including 256 sets thus providing a total of 2048 entries within each of the tag and data arrays <b>126</b> and <b>128</b>. Each entry <b>148</b> within the tag array <b>126</b> is show to store, inter alia, tag field information <b>151</b>, a thread bit <b>153</b>, a valid bit <b>155</b> and a Least Recently Used (LRU) bit <b>240</b> for each corresponding entry <b>156</b> within the data <b>128</b>. The thread bit <b>153</b> marks the data within the associated entry <b>156</b> as belonging, for example, to either a first or a second thread. The valid bit <b>155</b> marks the data within the corresponding entry <b>156</b> of the data array <b>128</b> as being valid or invalid.</p><p id=\"p-0057\" num=\"0059\">One embodiment of the trace cache <b>62</b> may also include a further minitag array <b>127</b>, as illustrated in <figref idrefs=\"DRAWINGS\">FIG. 7</figref>, that is a subset of the full tag array <b>126</b> and that is utilized to perform high-speed tag match operations and for reducing power consumption related to performing a lookup with respect to the trace cache <b>62</b>. A hit on the minitag array <b>127</b> may be regarded as \u201cmutually exclusive\u201d, as will be described in further detail below.</p><h4 id=\"h-0009\">Thread Selection Logic</h4><p id=\"p-0058\" num=\"0060\">Dealing first with the thread selection logic <b>140</b>, which determines, inter alia, the output of the trace cache <b>62</b>, <figref idrefs=\"DRAWINGS\">FIG. 8</figref> is a block diagram illustrating the various inputs and outputs of the thread selection logic <b>140</b>. The thread selection logic <b>140</b> is shown to take inputs from (1) a trace cache build engine <b>139</b>, located in the microinstruction translation engine interface, (2) the microinstruction queue <b>68</b> and (3) trace cache/microsequencer control logic <b>137</b>. Utilizing these inputs, the thread selection logic <b>140</b> attempts to generate an advantageous thread selection (e.g., thread <b>0</b> or thread <b>1</b>) for a particular cycle. Thread selection, in one embodiment, is performed on a cycle-by-cycle basis and attempts to optimize performance while not starving either thread of processor resources.</p><p id=\"p-0059\" num=\"0061\">The output of the thread selection logic <b>140</b> is shown to be communicated to the microcode sequencer <b>66</b>, the trace branch prediction unit <b>60</b> and the trace cache <b>62</b> to affect thread selection within each of these units.</p><p id=\"p-0060\" num=\"0062\"><figref idrefs=\"DRAWINGS\">FIG. 9</figref> is a block diagram illustrating three components of the thread selection logic <b>140</b>, namely a thread selection state machine <b>160</b>, a counter and comparator <b>162</b> for a first thread (e.g., thread <b>0</b>) and a further counter and comparator <b>164</b> for a second thread (e.g., thread <b>1</b>).</p><p id=\"p-0061\" num=\"0063\">The thread selection state machine <b>160</b> is shown to receive build and mode signals <b>161</b>, indicating whether the processor is operating in a multithreaded (MT) or a single threaded (ST) mode and if operating in a multithreaded mode, indicating whether or not each thread is in a build mode. The thread selection state machine <b>160</b> is also shown to receive respective full inputs <b>172</b> and <b>174</b> from the counter and comparator units <b>162</b> and <b>164</b>. The full signals <b>172</b> and <b>174</b> indicate whether a threshold number of microinstructions for a particular thread are within the trace delivery engine <b>160</b>. In one embodiment, each of the units <b>162</b> and <b>164</b> allow a total 4\u00d76 microinstruction lines within the trace delivery engine <b>60</b>. The full signals <b>172</b> and <b>174</b> are routed to all the units within the trace delivery engine <b>160</b>, responsive to which such units are responsible for recycling their states. Each of the counter comparator units <b>162</b> and <b>164</b> is shown to receive a queue deallocation signal <b>166</b> from the microcode sequencer <b>66</b>, a collection of clear, nuke, reset and store signals <b>168</b> and valid bits <b>170</b> from the trace cache tag array <b>126</b>.</p><p id=\"p-0062\" num=\"0064\"><figref idrefs=\"DRAWINGS\">FIG. 10</figref> is a state diagram illustrating operation of the thread selection state machine <b>160</b>, illustrated in <figref idrefs=\"DRAWINGS\">FIG. 9</figref>. When in multithreading mode, the state machine attempts to time-multiplex multiple threads on a cycle-by-cycle basis. When a thread encounters a relatively long stall, the state machine <b>160</b> attempts to provide full bandwidth to the thread that has not stalled. When multiple threads (e.g., thread <b>0</b> and thread <b>1</b>) experience long latency stalls, the state machine <b>160</b> may, in certain circumstances, require a one-cycle bubble (e.g., if both threads are stalled and the state machine <b>160</b> is in \u201cthread <b>0</b>\u201d state and a \u201cthread <b>1</b>\u201d stall is removed).</p><p id=\"p-0063\" num=\"0065\">Referring back to <figref idrefs=\"DRAWINGS\">FIG. 6</figref>, it will be noted that the selection signal <b>141</b>, outputted from the thread selection logic <b>140</b>, is not itself regarded as a \u201cvalid bit\u201d, but is rather used as a 2-1 MUX selection control to the MUX <b>142</b>. The MUX <b>142</b> operates to select between control signals outputted from a first thread control <b>144</b> and a second thread control <b>146</b>. The outputs of the controls <b>144</b> and <b>146</b> are dependent upon valid bits being set for the relevant threads. For example, the selection signal <b>141</b> may indicate a thread entry for a particular thread (e.g., thread <b>0</b>) to be outputted from the trace cache <b>62</b>. However, the valid bit for the relevant entry may be set to 0, indicating an invalid entry.</p><h4 id=\"h-0010\">Victim Selection Logic</h4><p id=\"p-0064\" num=\"0066\">The partitioning of the trace cache <b>62</b>, as illustrated in <figref idrefs=\"DRAWINGS\">FIG. 6</figref>, may, in one embodiment, be implemented by the victim selection logic <b>154</b>. The victim selection logic <b>154</b> is responsible for identifying the way (in both the tag array <b>126</b> and the data array <b>128</b>) to which a microinstruction is written.</p><p id=\"p-0065\" num=\"0067\"><figref idrefs=\"DRAWINGS\">FIG. 11</figref> is a block diagram illustrating architectural details of one embodiment of the victim selection logic <b>154</b>. The victim selection logic <b>154</b> is shown to include minitag victim selection logic <b>180</b>, valid victim selection logic <b>182</b> and Least Recently Used (LRU) victim selection logic <b>184</b>. A priority multiplexing operation is performed on the outputs of the selection logics <b>180</b>, <b>182</b> and <b>184</b> by a priority MUX <b>186</b>. The priority ordering implemented by the priority MUX <b>186</b> is as follows:</p><p id=\"p-0066\" num=\"0068\">1. Minitag victim;</p><p id=\"p-0067\" num=\"0069\">2. Valid victim; and</p><p id=\"p-0068\" num=\"0070\">3. LRU victim.</p><p id=\"p-0069\" num=\"0071\">A multi-threaded latch structure <b>190</b> is used to pass the results of the priority MUX to the trace cache <b>62</b>.</p><p id=\"p-0070\" num=\"0072\"><figref idrefs=\"DRAWINGS\">FIG. 12</figref> is a flow chart illustrating an exemplary method <b>200</b>, according to one embodiment, of partitioning a memory resource, such as for example, the trace cache, within a multi-threaded processor. The operation of the various units of the victim selection logic <b>154</b> illustrated in <figref idrefs=\"DRAWINGS\">FIG. 11</figref> will be described with reference to the flow chart shown in <figref idrefs=\"DRAWINGS\">FIG. 12</figref>.</p><p id=\"p-0071\" num=\"0073\">The method <b>200</b> commences at block <b>202</b> where the minitag victim selection logic <b>180</b> performs a minitag victim determination with respect to the minitag array <b>127</b>. Specifically, the logic <b>180</b> attempts to identify a conflict between an existing valid minitag array entry and a current instruction pointer (e.g., the current Linear Instruction Pointer (CLIP)).</p><p id=\"p-0072\" num=\"0074\">At decision box <b>204</b>, a determination is made as to whether a minitag victim was located at block <b>202</b>. If so, the method <b>200</b> advances to block <b>212</b>, where relevant trace cache data (e.g., a microinstruction) is written to the identified victim entry within the trace cache <b>62</b>. As a minitag hit is regarded as being \u201cmutually exclusive\u201d, an identified minitag victim is given the highest priority by the victim selection logic <b>154</b>.</p><p id=\"p-0073\" num=\"0075\">Following a negative determination at decision box <b>204</b>, at block <b>206</b>, a valid victim determination operation is performed by the valid victim selection logic <b>182</b>. This operation involves simply identifying an invalid entry within the trace cache <b>62</b> by examining valid bits <b>155</b> stored within the tag array <b>126</b> of the trace cache <b>62</b>. Following a positive determination at decision box <b>208</b>, the method <b>200</b> advances to block <b>212</b>. On the other hand, following a negative determination (i.e., no invalid entries are identified) at decision box <b>208</b>, the method <b>200</b> proceeds to box <b>210</b>, where a LRU victim determination operation is performed. Following completion of the operation at block <b>210</b>, the method <b>200</b> again advances to block <b>212</b>. The method <b>200</b> then terminates at step <b>214</b>.</p><p id=\"p-0074\" num=\"0076\"><figref idrefs=\"DRAWINGS\">FIG. 13</figref> is a flow chart illustrating an exemplary method <b>210</b>, according to one embodiment, of partitioning a resource, in the exemplary form of a memory resource, utilizing a LRU history associated with the relevant memory resource.</p><p id=\"p-0075\" num=\"0077\"><figref idrefs=\"DRAWINGS\">FIG. 14</figref> is a block diagram illustrating an exemplary LRU history <b>240</b> that may be utilized in the performance of the method <b>210</b>, the execution of which will be described with reference to <figref idrefs=\"DRAWINGS\">FIG. 10</figref>.</p><p id=\"p-0076\" num=\"0078\">The method <b>210</b> commences at block <b>222</b> with the receipt of a microinstruction, and associated tag information, at the victim selection logic <b>154</b>.</p><p id=\"p-0077\" num=\"0079\">At block <b>224</b>, a set into which the microinstruction may potentially be written is identified (e.g., by a write pointer).</p><p id=\"p-0078\" num=\"0080\">At block <b>226</b>, having identified a victim set, the LRU victim selection logic <b>184</b> examines the LRU history for the relevant set. <figref idrefs=\"DRAWINGS\">FIG. 14</figref> illustrates the LRU history <b>240</b>, as maintained within the tag array <b>126</b> of the trace cache <b>184</b>, the LRU history <b>240</b> containing a LRU history for each set within the data array <b>128</b>.</p><p id=\"p-0079\" num=\"0081\">At decision box <b>228</b>, the LRU victim selection logic <b>184</b> determines whether the tail entry, indicating a specific way within the set, is available to a relevant thread (e.g., thread <b>0</b> or thread <b>1</b>). As mentioned above, in an exemplary embodiment, ways <b>0</b> and <b>1</b> may be available exclusively to a first thread (e.g., thread <b>0</b>), ways <b>6</b> and <b>7</b> may be available exclusively to a second thread (e.g., thread <b>1</b>) and ways <b>2</b>-<b>5</b> may be dynamically shared multiple threads. Referencing the exemplary LRU history for a set N, way <b>6</b> is indicated by the tail entry as being the least recently used way in the relevant set N. Assume, for example, that the microinstruction to be cache belongs to a first thread (e.g., thread <b>0</b>) in which way <b>6</b> would not be available to receive the microinstruction on account of way <b>6</b> having been dedicated exclusively to the storage of microinstructions for a second thread (e.g., thread <b>1</b>).</p><p id=\"p-0080\" num=\"0082\">Returning to <figref idrefs=\"DRAWINGS\">FIG. 13</figref>, following a negative determination at decisions box <b>228</b>, the LRU victim selection logic <b>184</b> proceeds to examine entries within the LRU history <b>252</b> for the relevant set behind the tail entry to identify a way that may receive the microinstruction for the relevant thread. As indicated at block <b>230</b>, the LRU victim selection logic <b>184</b> examines a predetermined set M of tail entries (e.g., the three entries closest to the tail of the LRU history <b>252</b> for the set) to locate a way, closest to the tail of the LRU history, that is available to the relevant thread.</p><p id=\"p-0081\" num=\"0083\">In the example provided in <figref idrefs=\"DRAWINGS\">FIG. 14</figref>, the next-to-last entry within the LRU history for the relevant set identifies way <b>3</b> which, under the scheme described above, would be available to receive a microinstruction for a first thread (e.g., thread <b>0</b>) as way <b>3</b> is located in the \u201cshared\u201d portion of the trace cache <b>62</b>.</p><p id=\"p-0082\" num=\"0084\"><figref idrefs=\"DRAWINGS\">FIG. 14</figref> illustrates how the entry for way <b>3</b>, within the LRU history <b>252</b> for the relevant set, is moved to the head of the LRU history <b>252</b> on account of this way being designated for storage of the relevant microinstruction.</p><p id=\"p-0083\" num=\"0085\">Returning to the flow chart in <figref idrefs=\"DRAWINGS\">FIG. 13</figref>, at block <b>232</b>, the victim entry (i.e., the victim way) within the relevant set that is available to the relevant thread is identified, and the microinstruction written to that way within the set. The method <b>220</b> then ends at step <b>234</b>.</p><p id=\"p-0084\" num=\"0086\"><figref idrefs=\"DRAWINGS\">FIG. 15</figref> is a block diagram illustrating further details regarding the inputs to, and output from, the victim selection logic <b>184</b>. The victim selection logic <b>184</b>, in one embodiment, comprises discrete logic components that implement the methodology described above. In an alternative embodiment, the victim selection logic <b>184</b> may execute code to implement the described methodology. Specifically, the logic is shown to receive a 7-bit pending multi-thread (PENDING_MT) signal <b>250</b>, a 28-bit least recently used (LRU) signal <b>252</b>, a second thread status (NT1) signal <b>254</b> and a first thread status (MT0) signal <b>256</b> as inputs. The signal <b>250</b> indicates the way selected to receive a micro-instruction of a current thread or further thread (other than a thread currently being considered) by the selection logic <b>184</b> as indicated by the selection logic <b>184</b> during a previous victim selection operation, or as determined by further victim selection logic <b>154</b> associated with the further thread. The signal <b>250</b> is utilized by the LRU victim selection logic <b>184</b> to insure that the selection logic <b>184</b> does not \u201cdoubly select\u201d the same way between two threads, or that multiple LRU victim selection logics <b>184</b> do not select the same way between two threads. To this end, the victim selection logic <b>184</b> implements discrete logic that prevents it from selecting the same way as indicated by the signal <b>250</b>.</p><p id=\"p-0085\" num=\"0087\">The signal <b>250</b> accordingly, in one embodiment, indicates the way that was previously selected as a victim, while the LRU signal <b>252</b> provides the LRU history <b>252</b> for the relevant set to the logic <b>184</b>. The status signals <b>254</b> and <b>256</b> indicate to the logic <b>184</b> which of the threads are \u201calive\u201d or executing within a processor <b>30</b>. The logic <b>184</b> then outputs a 7-bit selection signal <b>260</b> for a relevant set, indicating the way within a relevant set to which the microinstruction should be written for caching purposes within the trace cache <b>62</b>.</p><p id=\"p-0086\" num=\"0088\">By implementing a pseudo-dynamic partitioning of a resource, such as the trace cache <b>62</b>, the present invention ensures that a certain predetermined minimum threshold of the capacity of a resource is always reserved and available for a particular thread within a multithreaded processor. Nonetheless, by defining a \u201cshared\u201d portion that is accessible to both threads, the present invention facilitates dynamic redistribution of a resource's capacity between multiple threads according to the requirements of such threads.</p><p id=\"p-0087\" num=\"0089\">Further, the LRU victim selection methodology discussed above enables hits to occur on ways allocated to a further thread, but simply disallows the validation of such a hit, and forces the LRU victim selection algorithm to select a further way, according to an LRU history, that is available to a particular thread.</p><p id=\"p-0088\" num=\"0090\">As mentioned above, the logic for implementing any one of the methodologies discussed above may be implemented as discrete logic within a functional unit, or may comprise a sequence of instructions (e.g., code) that is executed within the processor to implement the method. The sequence of instructions, it will be appreciated, may be stored on any medium from which it is retrievable for execution. Examples of these mediums may be a removable storage medium (e.g., a diskette, CD-ROM) or a memory resource associated with, or included within, a processor (e.g., Random Access Memory (RAM), cache memories or the like). Accordingly, any such medium should be regarded as comprising a \u201ccomputer-readable\u201d medium and may be included in a processor, or accessible by a processor employed within a computer system.</p><p id=\"p-0089\" num=\"0091\">Thus, a method and apparatus for partitioning a processor resource within a multi-threaded processor have been described. Although the present invention has been described with reference to specific exemplary embodiments, it will be evident that various modifications and changes may be made to these embodiments without departing from the broader spirit scope of the invention. Accordingly, the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Chan W.", "last_name": "Lee", "name": ""}, {"first_name": "Glenn", "last_name": "Hinton", "name": ""}, {"first_name": "Robert", "last_name": "Krick", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTEL CORPORATION"}, {"first_name": "", "last_name": "INTEL CORPORATION", "name": ""}], "ipc_classes": [], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/50        20060101A I20051008RMEP"}, {"label": "G06F   9/38        20060101A I20051008RMEP"}, {"label": "G06F   9/318       20060101A I20051008RMEP"}, {"label": "G06F   9/46        20060101AFI20101221BHUS"}, {"label": "G06F  13/00        20060101ALI20101221BHUS"}, {"label": "G06F  12/12        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "718105"}, {"primary": false, "label": "711136"}, {"primary": false, "label": "711129"}], "ecla_classes": [{"label": "G06F   9/38E"}, {"label": "G06F  12/12B"}, {"label": "G06F   9/38E4"}, {"label": "G06F   9/38B"}, {"label": "G06F   9/38B4"}, {"label": "G06F   9/30U"}, {"label": "G06F   9/50"}, {"label": "G06F   9/30U2"}], "cpc_classes": [{"label": "G06F  12/0842"}, {"label": "G06F   9/3855"}, {"label": "G06F  12/121"}, {"label": "G06F   9/50"}, {"label": "G06F   9/384"}, {"label": "G06F   9/3808"}, {"label": "G06F   9/382"}, {"label": "G06F   9/3017"}, {"label": "G06F   9/3802"}, {"label": "G06F   9/3851"}, {"label": "G06F   9/3836"}, {"label": "G06F   9/3836"}, {"label": "G06F   9/3017"}, {"label": "G06F   9/384"}, {"label": "G06F   9/50"}, {"label": "G06F   9/3855"}, {"label": "G06F   9/3808"}, {"label": "G06F  12/0842"}, {"label": "G06F   9/382"}, {"label": "G06F   9/3802"}, {"label": "G06F   9/3851"}, {"label": "G06F  12/121"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "2000-03-24", "application_date": "2000-03-24", "family_members": [{"ucid": "WO-2001077820-A2", "titles": [{"lang": "EN", "text": "METHOD AND APPARATUS FOR PARTITIONING A RESOURCE BETWEEN MULTIPLE THREADS WITHIN A MULTI-THREADED PROCESSOR"}, {"lang": "FR", "text": "PROCEDE ET DISPOSITIF SERVANT A DIVISER UNE RESSOURCE ENTRE LES UNITES D'EXECUTION MULTIPLES D'UN PROCESSEUR"}]}, {"ucid": "DE-10195962-T1", "titles": [{"lang": "EN", "text": "Method and device for dividing a resource between several threads in a multifilament processor"}, {"lang": "DE", "text": "Verfahren und Einrichtung zum Aufteilen einer Ressource zwischen mehreren Threads in einem mehrf\u00e4digen Prozessor"}]}, {"ucid": "US-7856633-B1", "titles": [{"lang": "EN", "text": "LRU cache replacement for a partitioned set associative cache"}]}, {"ucid": "TW-I233051-B", "titles": [{"lang": "EN", "text": "Method and apparatus for partitioning a resource between multiple threads within a multi-threaded processor"}]}, {"ucid": "DE-10195962-T0", "titles": []}, {"ucid": "AU-2001229567-A1", "titles": [{"lang": "EN", "text": "Method and apparatus for partitioning a resource between multiple threads withina multi-threaded processor"}]}, {"ucid": "HK-1049529-B", "titles": [{"lang": "ZH", "text": "\u5728\u4e00\u500b\u591a\u7dda\u7a0b\u8655\u7406\u5668\u5167\u4e0d\u540c\u7dda\u7a0b\u4e4b\u9593\u5283\u5206\u4e00\u500b\u8cc7\u6e90\u7684\u65b9\u6cd5\u53ca\u88dd\u7f6e"}, {"lang": "EN", "text": "METHOD AND APPARATUS FOR PARTITIONING A RESOURCE BETWEEN MULTIPLE THREADS WITHIN A MULTI-THREADED PROCESSOR"}]}, {"ucid": "WO-2001077820-A3", "titles": [{"lang": "EN", "text": "METHOD AND APPARATUS FOR PARTITIONING A RESOURCE BETWEEN MULTIPLE THREADS WITHIN A MULTI-THREADED PROCESSOR"}, {"lang": "FR", "text": "PROCEDE ET DISPOSITIF SERVANT A DIVISER UNE RESSOURCE ENTRE LES UNITES D'EXECUTION MULTIPLES D'UN PROCESSEUR"}]}, {"ucid": "AU-2956701-A", "titles": []}, {"ucid": "CN-1429361-B", "titles": [{"lang": "ZH", "text": "\u7528\u4e8e\u5728\u4e00\u4e2a\u591a\u7ebf\u7a0b\u5904\u7406\u5668\u5185\u5728\u591a\u4e2a\u7ebf\u7a0b\u4e4b\u95f4\u5212\u5206\u8d44\u6e90\u7684\u65b9\u6cd5\u548c\u88c5\u7f6e"}, {"lang": "EN", "text": "Method and device for partitioning resource between multiple threads within multi-threaded processor"}]}, {"ucid": "GB-0224460-D0", "titles": [{"lang": "EN", "text": "Method and apparatus for partitioning a resource between multiple threads within a multi-threaded processor"}]}, {"ucid": "HK-1049529-A1", "titles": [{"lang": "EN", "text": "Method and apparatus for partitioning a resource between multiple threads within a multi-threaded processor."}]}, {"ucid": "GB-2377529-A", "titles": [{"lang": "EN", "text": "Method and apparatus for partitioning a resource between multiple threads within a multi-threaded processor"}]}, {"ucid": "GB-2377529-B", "titles": [{"lang": "EN", "text": "Method and apparatus for partitioning a resource between multiple threads within a multi-threaded processor"}]}, {"ucid": "CN-1429361-A", "titles": [{"lang": "EN", "text": "Method and device for partitioning resource between multiple threads within multi-threaded processor"}, {"lang": "ZH", "text": "\u7528\u4e8e\u5728\u4e00\u4e2a\u591a\u7ebf\u7a0b\u5904\u7406\u5668\u5185\u5728\u591a\u4e2a\u7ebf\u7a0b\u4e4b\u95f4\u5212\u5206\u8d44\u6e90\u7684\u65b9\u6cd5\u548c\u88c5\u7f6e"}]}]}