{"patent_number": "US-6397300-B1", "publication_id": 73041916, "family_id": 23331777, "publication_date": "2002-05-28", "titles": [{"lang": "EN", "text": "High performance store instruction management via imprecise local cache update mechanism"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50326619\"><p>A method of improving memory access for a computer system, by sending load requests to a lower level storage subsystem along with associated information pertaining to intended use of the requested information by the requesting processor, without using a high level load queue. Returning the requested information to the processor along with the associated use information allows the information to be placed immediately without using reload buffers. A register load bus separate from the cache load bus (and having a smaller granularity) is used to return the information. An upper level (L<b>1</b>) cache may then be imprecisely reloaded (the upper level cache can also be imprecisely reloaded with store instructions). The lower level (L<b>2</b>) cache can monitor L<b>1 </b>and L<b>2 </b>cache activity, which can be used to select a victim cache block in the L<b>1 </b>cache (based on the additional L<b>2 </b>information), or to select a victim cache block in the L<b>2 </b>cache (based on the additional L<b>1 </b>information). L<b>2 </b>control of the L<b>1 </b>directory also allows certain snoop requests to be resolved without waiting for L<b>1 </b>acknowledgement. The invention can be applied to, e.g., instruction, operand data and translation caches.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00001\" num=\"1\"><claim-text>1. A method of storing data in a memory hierarchy of a computer system, said memory hierarchy including a lower latency upper level cache and a higher latency lower level cache, said method comprising:</claim-text><claim-text>in response to receipt at the memory hierarchy of a store operation from a processor of the computer system, said store operation including data and a target address, passing the store operation to the lower level cache and storing said data in the lower level cache; </claim-text><claim-text>determining at said memory hierarchy whether or not said upper level cache is busy; </claim-text><claim-text>in response to a determination that said upper level cache is not busy, allocating a cache line in the upper level cache to receive the data from the store operation if the store operation misses the upper level cache and writing the data into the upper level cache, such that said data is cached in both said upper and lower level caches; and </claim-text><claim-text>in response to a determination that said upper level cache is busy, refraining from writing said data into said upper level cache and, in response to said store operation hitting in said upper level cache, invalidating a cache line in said upper level cache associated with said target address, such that said data is cached in said lower level cache but not said upper level cache. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00002\" num=\"2\"><claim-text>2. The method of <claim-ref idref=\"US-6397300-B1-CLM-00001\">claim 1</claim-ref> wherein:</claim-text><claim-text>the store operation is issued by a load/store unit of the processor; and </claim-text><claim-text>said passing step includes the step of writing the data to the lower level cache from a register of the processor. </claim-text></claim>"}, {"num": 3, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00003\" num=\"3\"><claim-text>3. The method of <claim-ref idref=\"US-6397300-B1-CLM-00001\">claim 1</claim-ref>, wherein storing said data in the lower level cache includes the step of loading the store operation into a store operations cache of the lower level cache.</claim-text></claim>"}, {"num": 4, "parent": 3, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00004\" num=\"4\"><claim-text>4. The method of <claim-ref idref=\"US-6397300-B1-CLM-00003\">claim 3</claim-ref> further comprising the step of gathering a plurality of store operations in the store operations cache of the lower level cache into an operation directed to a single cache line.</claim-text></claim>"}, {"num": 5, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00005\" num=\"5\"><claim-text>5. The method of <claim-ref idref=\"US-6397300-B1-CLM-00001\">claim 1</claim-ref> wherein storing said data in the lower level cache includes storing the data in a store queue of the lower level cache.</claim-text></claim>"}, {"num": 6, "parent": 5, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00006\" num=\"6\"><claim-text>6. The method of <claim-ref idref=\"US-6397300-B1-CLM-00005\">claim 5</claim-ref> further comprising the step of snooping a requested address against each entry in the store queue.</claim-text></claim>"}, {"num": 7, "parent": 6, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00007\" num=\"7\"><claim-text>7. The method of <claim-ref idref=\"US-6397300-B1-CLM-00006\">claim 6</claim-ref> further comprising the step of sourcing the data to a requesting device from the store queue.</claim-text></claim>"}, {"num": 8, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00008\" num=\"8\"><claim-text>8. The method of <claim-ref idref=\"US-6397300-B1-CLM-00001\">claim 1</claim-ref> wherein said determining step includes determining whether or not a store queue of the upper level cache has at least one empty entry.</claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00009\" num=\"9\"><claim-text>9. The method of <claim-ref idref=\"US-6397300-B1-CLM-00008\">claim 8</claim-ref>, wherein said writing step includes the step of storing the data in the store queue, and said method further comprises snooping a requested address against each entry in the store queue.</claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00010\" num=\"10\"><claim-text>10. The method of <claim-ref idref=\"US-6397300-B1-CLM-00009\">claim 9</claim-ref> further comprising the step of sourcing the data to a requesting device from the store queue.</claim-text></claim>"}, {"num": 11, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00011\" num=\"11\"><claim-text>11. A computer system comprising:</claim-text><claim-text>a processor including at least one execution unit for processing program instructions; </claim-text><claim-text>a memory hierarchy coupled to the processor, said memory hierarchy including: </claim-text><claim-text>at least a lower latency upper level cache and a higher latency lower level cache; </claim-text><claim-text>means for passing a store operation including data and a target address from said processor to said lower level cache, and for storing said data in the lower level cache; </claim-text><claim-text>means for determining whether or not said upper level cache is busy; </claim-text><claim-text>means, responsive to a determination that said upper level cache is not busy, for allocating a cache line in the upper level cache to receive the data if the store misses the upper level cache and for writing said data into said upper level cache, such that said data is cached in both said upper and lower level caches; and </claim-text><claim-text>means, responsive to a determination that said upper level cache is busy, for refraining from writing said data into said upper level cache and, in response to said store operation hitting in said upper level cache, invalidating a cache line in said upper level cache associated with said target address, such that said data is cached in said lower level cache but not said upper level cache. </claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00012\" num=\"12\"><claim-text>12. The computer system of <claim-ref idref=\"US-6397300-B1-CLM-00011\">claim 11</claim-ref> wherein:</claim-text><claim-text>said upper level cache is an L<b>1</b> cache; and </claim-text><claim-text>said lower level cache is an L<b>2</b> cache. </claim-text></claim>"}, {"num": 13, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00013\" num=\"13\"><claim-text>13. The computer system of <claim-ref idref=\"US-6397300-B1-CLM-00011\">claim 11</claim-ref>, wherein said processor includes a register and a load/store unit that issues the store operation, and the data is written to said lower level cache from the register specified by said load/store unit.</claim-text></claim>"}, {"num": 14, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00014\" num=\"14\"><claim-text>14. The computer system of <claim-ref idref=\"US-6397300-B1-CLM-00011\">claim 11</claim-ref> wherein said means for storing said data in the lower level cache loads the store operation into a store operations cache of the lower level cache.</claim-text></claim>"}, {"num": 15, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00015\" num=\"15\"><claim-text>15. The computer system of <claim-ref idref=\"US-6397300-B1-CLM-00014\">claim 14</claim-ref> further comprising means for gathering a plurality of store operations in said store operations cache into an operation directed to a single cache line.</claim-text></claim>"}, {"num": 16, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00016\" num=\"16\"><claim-text>16. The computer system of <claim-ref idref=\"US-6397300-B1-CLM-00011\">claim 11</claim-ref> wherein said passing means stores the data in a store queue of the lower level cache.</claim-text></claim>"}, {"num": 17, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00017\" num=\"17\"><claim-text>17. The computer system of <claim-ref idref=\"US-6397300-B1-CLM-00016\">claim 16</claim-ref> further comprising means for snooping a requested address against each entry in said store queue.</claim-text></claim>"}, {"num": 18, "parent": 17, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00018\" num=\"18\"><claim-text>18. The computer system of <claim-ref idref=\"US-6397300-B1-CLM-00017\">claim 17</claim-ref> further comprising means for sourcing the data to a requesting device from said store queue.</claim-text></claim>"}, {"num": 19, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00019\" num=\"19\"><claim-text>19. The computer system of <claim-ref idref=\"US-6397300-B1-CLM-00011\">claim 11</claim-ref> said means for determining comprising means for determining whether or not a store queue of said upper level cache has at least one empty entry.</claim-text></claim>"}, {"num": 20, "parent": 19, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00020\" num=\"20\"><claim-text>20. The computer system of <claim-ref idref=\"US-6397300-B1-CLM-00019\">claim 19</claim-ref> further comprising means for snooping a requested address against each entry in said store queue.</claim-text></claim>"}, {"num": 21, "parent": 20, "type": "dependent", "paragraph_markup": "<claim id=\"US-6397300-B1-CLM-00021\" num=\"21\"><claim-text>21. The computer system of <claim-ref idref=\"US-6397300-B1-CLM-00020\">claim 20</claim-ref> further comprising means for sourcing the data to a requesting device from said store queue.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES53574077\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>CROSS-REFERENCES TO RELATED APPLICATIONS</h4><p>The present invention is related to the following applications filed concurrently with this application: U.S. patent application Ser. No. 09/340,077 entitled \u201cQUEUE-LESS AND STATE-LESS LAYERED LOCAL DATA CACHE MECHANISM\u201d; U.S. patent application Ser. No. 09/340,076 entitled \u201cLAYERED LOCAL CACHE MECHANISM WITH SPLIT REGISTER LOAD BUS AND CACHE LOAD BUS\u201d; U.S. patent application Ser. No. 09/340,075 entitled \u201cLAYERED LOCAL CACHE WITH IMPRECISE RELOAD MECHANISM\u201d; U.S. patent application Ser. No. 09/340,074 entitled \u201cLAYERED LOCAL CACHE WITH LOWER LEVEL CACHE OPTIMIZING ALLOCATION MECHANISM\u201d; U.S. patent application Ser. No. 09/340,073 entitled \u201cMETHOD FOR UPPER LEVEL CACHE VICTIM SELECTION MANAGEMENT BY A LOWER LEVEL CACHE\u201d; U.S. patent application Ser. No. 09/340,082 entitled \u201cLAYERED LOCAL CACHE WITH LOWER LEVEL CACHE UPDATING UPPER AND LOWER LEVEL CACHE DIRECTORIES\u201d; U.S. patent application Ser. No. 09/340,079 entitled \u201cHIGH PERFORMANCE LOAD INSTRUCTION MANAGEMENT VIA SYSTEM BUS WITH EPLICIT REGISTER LOAD AND/OR CACHE RELOAD PROTOCOLS\u201d; U.S. patent application Ser. No. 09/340,080 entitled \u201cMETHOD FOR LAYERING LOCAL INSTRUCTION CACHE MANAGEMENT\u201d; and U.S. patent application Ser. No. 09/340,081 entitled \u201cMETHOD FOR LAYERING LOCAL TRANSLATION CACHE MANAGEMENT\u201d.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>The present invention generally relates to computer systems, and more specifically to an improved method of accessing memory values (operand data or instructions) used by a processor of a computer system. In particular, the present invention makes more efficient use of a multi-level cache hierarchy, and ports values directly to, e.g., a rename register, instruction buffer, or translation table of the processor without the need for load queues or reload buffers in high level caches.</p><p>2. Description of Related Art</p><p>The basic structure of a conventional computer system includes one or more processing units connected to various input/output devices for the user interface (such as a display monitor, keyboard and graphical pointing device), a permanent memory device (such as a hard disk, or a floppy diskette) for storing the computer's operating system and user programs, and a temporary memory device (such as random access memory or RAM) that is used by the processor(s) in carrying out program instructions. The evolution of computer processor architectures has transitioned from the now widely-accepted reduced instruction set computing (RISC) configurations, to so-called superscalar computer architectures, wherein multiple and concurrently operable execution units within the processor are integrated through a plurality of registers and control mechanisms.</p><p>The objective of superscalar architecture is to employ parallelism to maximize or substantially increase the number of program instructions (or \u201cmicro-operations\u201d) simultaneously processed by the multiple execution units during each interval of time (processor cycle), while ensuring that the order of instruction execution as defined by the programmer is reflected in the output. For example, the control mechanism must manage dependencies among the data being concurrently processed by the multiple execution units, and the control mechanism must ensure that integrity of sequentiality is maintained in the presence of precise interrupts and restarts. The control mechanism preferably provides instruction deletion capability such as is needed with instruction-defined branching operations, yet retains the overall order of the program execution. It is desirable to satisfy these objectives consistent with the further commercial objectives of minimizing electronic device count and complexity.</p><p>An illustrative embodiment of a conventional processing unit for processing information is shown in FIG. 1, which depicts the architecture for a PowerPC\u2122 microprocessor <b>12</b> manufactured by International Business Machines Corp. (IBM\u2014assignee of the present invention). Processor <b>12</b> operates according to reduced instruction set computing (RISC) techniques, and is a single integrated circuit superscalar microprocessor. As discussed further below, processor <b>12</b> includes various execution units, registers, buffers, memories, and other functional units, which are all formed by integrated circuitry.</p><p>Processor <b>12</b> is coupled to a system bus <b>20</b> via a bus interface unit BIU <b>30</b> within processor <b>12</b>. BIU <b>30</b> controls the transfer of information between processor <b>12</b> and other devices coupled to system bus <b>20</b> such as a main memory <b>18</b>. Processor <b>12</b>, system bus <b>20</b>, and the other devices coupled to system bus <b>20</b> together form a host data processing system. Bus <b>20</b>, as well as various other connections described, include more than one line or wire, e.g., the bus could be a 32-bit bus. BIU <b>30</b> is connected to a high speed instruction cache <b>32</b> and a high speed data cache <b>34</b>. A lower level (L<b>2</b>) cache (not shown) may be provided as an intermediary between processor <b>12</b> and system bus <b>20</b>. An L<b>2</b> cache can store a much larger amount of information (instructions and operand data) than the on-board caches can, but at a longer access penalty. For example, the L<b>2</b> cache may be a chip having a storage capacity of 512 kilobytes, while the processor may be an IBM PowerPC\u2122 604-series processor having on-board caches with 64 kilobytes of total storage. A given cache line usually has several memory words, e.g., a 64-byte line contains eight 8-byte words.</p><p>The output of instruction cache <b>32</b> is connected to a sequencer unit <b>36</b> (instruction dispatch unit). In response to the particular instructions received from instruction cache <b>32</b>, sequencer unit <b>36</b> outputs instructions to other execution circuitry of processor <b>12</b>, including six execution units, namely, a branch unit <b>38</b>, a fixed-point unit A (FXUA) <b>40</b>, a fixed-point unit B (FXUB) <b>42</b>, a complex fixed-point unit (CFXU) <b>44</b>, a load/store unit (LSU) <b>46</b>, and a floating-point unit (FPU) <b>48</b>.</p><p>The inputs of FXUA <b>40</b>, FXUB <b>42</b>, CFXU <b>44</b> and LSU <b>46</b> also receive source operand information from general-purpose registers (GPRs) <b>50</b> and fixed-point rename buffers <b>52</b>. The outputs of FXUA <b>40</b>, FXUB <b>42</b>, CFXU <b>44</b> and LSU <b>46</b> send destination operand information for storage at selected entries in fixed-point rename buffers <b>52</b>. CFXU <b>44</b> further has an input and an output connected to special-purpose registers (SPRs) <b>54</b> for receiving and sending source operand information and destination operand information, respectively. An input <b>30</b> of FPU <b>48</b> receives source operand information from floating-point registers (FPRs) <b>56</b> and floating-point rename buffers <b>58</b>. The output of FPU <b>48</b> sends destination operand information to selected entries in floating-point rename buffers <b>58</b>.</p><p>As is well known by those skilled in the art, each of execution units <b>38</b>-<b>48</b> executes one or more instructions within a particular class of sequential instructions during each processor cycle. For example, FXUA <b>42</b> performs fixed-point mathematical operations such as addition, subtraction, ANDing, ORing, and XORing utilizing source operands received from specified GPRs <b>50</b>. Conversely, FPU <b>48</b> performs floating-point operations, such as floating-point multiplication and division, on source operands received from FPRs <b>56</b>. As its name implies, LSU <b>46</b> executes floating-point and fixed-point instructions which either load operand data from memory (i.e., from data cache <b>34</b>) into selected GPRs <b>50</b> or FPRs <b>56</b>, or which store data from selected GPRs <b>50</b> or FPRs <b>56</b> to memory <b>18</b>.</p><p>Processor <b>12</b> may include other registers, such as configuration registers, memory management registers, exception handling registers, and miscellaneous registers, which are not shown. Processor <b>12</b> carries out program instructions from a user application or the operating system, by routing the instructions and operand data to the appropriate execution units, buffers and registers, and by sending the resulting output to the system memory device (RAM), or to some output device such as a display console.</p><p>Register sets such as those described above limit superscalar processing, simply due to the number of registers that are available to a particular execution unit at the beginning of instruction execution (i.e., the registers must be shared among the different execution units). Moreover, superscalar operations are typically \u201cpipelined,\u201d that is, a plurality of processing stages are provided for a given execution unit, with each stage able to operate on one instruction at the same time that a different stage is operating on another instruction, so the registers must be further shared. The problem is exacerbated when a long sequence of instructions requires access to the same register set. Furthermore, programmers often use the same registers as temporary storage registers rather than moving data to and from system memory (since the latter process takes a large amount of time relative to processor speed), so a small register set can cause a \u201cbottleneck\u201d in the performance stream. Techniques have been devised for expanding the effective number of available registers, such as by providing register renaming (using rename buffers <b>52</b> and <b>58</b>). Register renaming provides a larger set of registers by assigning a new physical register every time a register (architected) is written. A physical register is released for re-use when an instruction that overwrites the architected state maintained in that register completes.</p><p>One problem with conventional processing is that operations are often delayed as they must be issued or completed using queues or buffers. For example, when the processor executes a load instruction (via load/store unit <b>46</b>), the data (L<b>1</b>) cache <b>34</b> is first examined to see if the requested memory block is already in the cache. If not (a \u201ccache miss\u201d), the load operation will be entered into a load queue (not shown) of the cache. The load queue severely limits the number of outstanding loads that can be pending in the system. Typically, there are only two or three entries in the load queue, as most systems rely on the assumption that the majority of accesses will be for operand data that is already in the L<b>1</b> cache (cache \u201chits\u201d). If the load queue is already full and another cache miss occurs, the processor core stalls until an entry in the queue becomes available.</p><p>Several other processing delays are associated with the operation of, or interaction with, the caches, particularly the L<b>1</b> cache. For example, on a cache miss with a set associative cache, it is necessary to select a cache line in a particular set of the cache for use with the newly requested data (a process referred to as eviction or victimization). The request cannot be passed down to the lower storage subsystem until a victim is chosen. If the chosen victim has been previously modified (the object of a store operation), then the modified value must be aged out (cast out). The logic unit used to select the victim, such as a least-recently (or less recently) used (LRU) algorithm, must also be updated in the L<b>1</b> cache. These steps are located in the critical path of processor core execution.</p><p>Similarly, a reload buffer (not shown) is used to temporarily hold values before they are written to the L<b>1</b> cache to handle cache read/write collisions. When the lower level memory hierarchy supplies the value requested by a load operation, the response (operand data and address) first enters the reload buffer.</p><p>Delays may likewise occur for store (write) operations which use a store queue. These types of delays can also arise with operations whose targets are other than register renames, such as instruction fetch units, or translation tables requesting addresses. Translation tables commonly used in processors include translation lookaside buffers which convert physical addresses to virtual addresses (for either instructions or operand data, i.e., ITLBs and DTLBs), or effective-to-real address tables (ERATs).</p><p>An additional delay is presented by the requirement that the entire cache line be received by the L<b>1</b> cache prior to passing the critical value on to the appropriate element within the processor (e.g., to a register rename buffer, translation lookaside buffer, or instruction dispatch unit). In fact, the entire cache line of, say, 64 bytes must be loaded into the L<b>1</b> cache even though the processor only requested an 8-byte word (the L<b>1</b> cache controller provides the smaller granularity on the processor output side).</p><p>As noted above, a cache line victim representing modified data must be written to the lower levels of the memory hierarchy; this is true for a \u201cwrite-back\u201d cache, where data values are not immediately passed on to the remainder of the memory hierarchy after a store operation. Caches can also be \u201cwrite-through,\u201d but this leads to increased demands on bus bandwidth. Write-back caches use state information bits to maintain consistency within the overall memory hierarchy (coherency), combined with the monitoring (snooping) of memory operations. One example of the state information is that supplied by the \u201cMESI\u201d cache coherency protocol, wherein a cache line can be in one of four coherency states: Modified, Exclusive, Shared or Invalid. Cache coherency protocols introduce further complexities and requirements into the interaction of the caches.</p><p>In light of the foregoing, it would be desirable to provide a method of speeding up core processing by improving the operation of the caches, particularly the L<b>1</b> cache. It would be particularly advantageous if the method could provide values (instructions or operand data) more directly to processor components, i.e., without requiring the use of so many intervening queues and buffers, and allow more flexibility in the interaction between a cache and a processor or between vertically adjacent caches (e.g., L<b>1</b> and L<b>2</b>) in a multi-cache hierarchy.</p><h4>SUMMARY OF THE INVENTION</h4><p>It is therefore one object of the present invention to provide an improved data processing system having one or more local caches in the memory hierarchy.</p><p>It is another object of the present invention to provide such an improved data processing system having a multi-level cache structure, and at least one layered cache wherein one or more cache functions are handled by a lower level cache.</p><p>It is yet another object of the present invention to provide a memory structure for a computer system which speeds up memory accesses by removing or distancing cache functions from the critical path of core execution.</p><p>The foregoing objects are achieved in a method of storing data in a memory hierarchy of a computer system, comprising the steps of issuing a store operation from a processor of the computer system, passing the store operation to a lower level cache associated with the processor (whether the store operation hits or misses an upper level cache associated with the processor), determining if it would be efficient to currently write data from the store operation into the upper level cache and, in response to this determination, writing the data into the upper level cache. The writing of the data may occur whether the store operation hit or missed the upper level cache (if the operation missed, then a cache line is allocated). If the store operation hit the upper level cache, and it is determined that it is not efficient to write the data, then the cache line in the upper level cache is simply invalidated. This determination could be based on a read/write collision in the upper level cache. If the upper level cache controller is attempting to write the store data to the upper level cache, and this write collides with a higher priority cache read, then the upper level cache controller can choose to simply invalidate the cache line rather than consume queue space while waiting to write to the store data at a later time. The determination could also be made by the lower level cache controller. When the lower level cache controller receives the cache line (either from the lower level cache or from the memory subsystem) to which the requested store data is targeted, it merges the store data into the cache line, and writes the cache line back to the lower level cache. Then the lower level cache controller may or may not choose to allocate and write the upper level cache with this cache lines. Using the lower level cache controller to perform the upper level cache allocation removes complex cache allocation logic from the critical path of core execution and from upper level cache controller execution. This impression is made possible by the fact that the upper level cache is store-through and all stores are forwarded down to the lower level cache.</p><p>The above as well as additional objectives, features, and advantages of the present invention will become apparent in the following detailed written description.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The novel features believed characteristic of the invention are set forth in the appended claims. The invention itself, however, as well as a preferred mode of use, further objectives, and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein:</p><p>FIG. 1 is a block diagram of a conventional superscalar computer processor, depicting execution units, buffers, registers, and the on-board (L<b>1</b>) data and instruction caches;</p><p>FIG. 2 is an illustration of one embodiment of a data processing system in which the present invention can be practiced;</p><p>FIG. 3 is a block diagram illustrating selected components that can be included in the data processing system of FIG. 2 according to the teachings of the present invention;</p><p>FIG. 4 is a block diagram of a processing unit constructed in accordance with one embodiment of the present invention, depicting operation of a cache structure which includes an L<b>1</b> operand data cache;</p><p>FIG. 5 is a block diagram of a processing unit constructed in accordance with another embodiment of the present invention, depicting operation of a cache structure which includes an L<b>1</b> instruction cache; and</p><p>FIG. 6 is a block diagram of a memory management unit constructed in accordance with another embodiment of the present invention, depicting operation of a translation lookaside buffer for storing page table entries.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF AN ILLUSTRATIVE EMBODIMENT</h4><p>With reference now to the figures, and in particular with reference to FIG. 2, a data processing system <b>120</b> is shown in which the present invention can be practiced. The data processing system <b>120</b> includes processor <b>122</b>, keyboard <b>182</b>, and display <b>196</b>. Keyboard <b>182</b> is coupled to processor <b>122</b> by a cable <b>128</b>. Display <b>196</b> includes display screen <b>130</b>, which may be implemented using a cathode ray tube (CRT), a liquid crystal display (LCD), an electrode luminescent panel or the like. The data processing system <b>120</b> also includes pointing device <b>184</b>, which may be implemented using a track ball, a joy stick, touch sensitive tablet or screen, track path, or as illustrated a mouse. The pointing device <b>184</b> may be used to move a pointer or cursor on display screen <b>130</b>. Processor <b>122</b> may also be coupled to one or more peripheral devices such a modem <b>192</b>, CD-ROM <b>178</b>, network adapter <b>190</b>, and floppy disk drive <b>140</b>, each of which may be internal or external to the enclosure or processor <b>122</b>. An output device such as a printer <b>100</b> may also be coupled with processor <b>122</b>.</p><p>It should be noted and recognized by those persons of ordinary skill in the art that display <b>196</b>, keyboard <b>182</b>, and pointing device <b>184</b> may each be implemented using any one of several known off-the-shelf components.</p><p>Reference now being made to FIG. 3, a high level block diagram is shown illustrating selected components that can be included in the data processing system <b>120</b> of FIG. 2 according to the teachings of the present invention. The data processing system <b>120</b> is controlled primarily by computer readable instructions, which can be in the form of software, wherever, or by whatever means such software is stored or accessed. Such software may be executed within the Central Processing Unit (CPU) <b>150</b> to cause data processing system <b>120</b> to do work.</p><p>Memory devices coupled to system bus <b>105</b> include Random Access Memory (RAM) <b>156</b>, Read Only Memory (ROM) <b>158</b>, and nonvolatile memory <b>160</b>. Such memories include circuitry that allows information to be stored and retrieved. ROMs contain stored data that cannot be modified. Data stored in RAM can be changed by CPU <b>150</b> or other hardware devices. Nonvolatile memory is memory that does not lose data when power is removed from it. Nonvolatile memories include ROM, EPROM, flash memory, or battery-pack CMOS RAM. As shown in FIG. 3, such battery-pack CMOS RAM may be used to store configuration information.</p><p>An expansion card or board is a circuit board that includes chips and other electronic components connected that adds functions or resources to the computer. Typically, expansion cards add memory, disk-drive controllers <b>166</b>, video support, parallel and serial ports, and internal modems. For lap top, palm top, and other portable computers, expansion cards usually take the form of PC cards, which are credit card-sized devices designed to plug into a slot in the side or back of a computer. An example of such a slot is PCMCIA slot (Personal Computer Memory Card International Association) which defines type I, II and III card slots. Thus, empty slots <b>168</b> may be used to receive various types of expansion cards or PCMCIA cards.</p><p>Disk controller <b>166</b> and diskette controller <b>170</b> both include special purpose integrated circuits and associated circuitry that direct and control reading from and writing to hard disk drive <b>172</b>, and a floppy disk or diskette <b>74</b>, respectively. Such disk controllers handle tasks such as positioning read/write head, mediating between the drive and the CPU <b>150</b>, and controlling the transfer of information to and from memory. A single disk controller may be able to control more than one disk drive.</p><p>CD-ROM controller <b>176</b> may be included in data processing <b>120</b> for reading data from CD-ROM <b>178</b> (compact disk read only memory). Such CD-ROMs use laser optics rather than magnetic means for reading data.</p><p>Keyboard mouse controller <b>180</b> is provided in data processing system <b>120</b> for interfacing with keyboard <b>182</b> and pointing device <b>184</b>. Such pointing devices are typically used to control an on-screen element, such as a graphical pointer or cursor, which may take the form of an arrow having a hot spot that specifies the location of the pointer when the user presses a mouse button. Other pointing devices include a graphics tablet, stylus, light pin, joystick, puck, track ball, track pad, and the pointing device sold under the trademark \u201cTrack Point\u201d by International Business Machines Corp. (IBM).</p><p>Communication between processing system <b>120</b> and other data processing systems may be facilitated by serial controller <b>188</b> and network adapter <b>190</b>, both of which are coupled to system bus <b>105</b>. Serial controller <b>188</b> is used to transmit information between computers, or between a computer and peripheral devices, one bit at a time over a single line. Serial communications can be synchronous (controlled by some standard such as a clock) or asynchronous (managed by the exchange of control signals that govern the flow of information). Examples of serial communication standards include RS-232 interface and the RS-422 interface. As illustrated, such a serial interface may be used to communicate with modem <b>192</b>. A modem is a communication device that enables a computer to transmit information over standard telephone lines. Modems convert digital computer signals to interlock signals suitable for communications over telephone lines. Modem <b>192</b> can be utilized to connect data processing system <b>120</b> to an on-line information service or an Internet service provider. Such service providers may offer software that can be down loaded into data processing system <b>120</b> via modem <b>192</b>. Modem <b>192</b> may provide a connection to other sources of software, such as a server, an electronic bulletin board (BBS), or the Internet (including the World Wide Web).</p><p>Network adapter <b>190</b> may be used to connect data processing system <b>120</b> to a local area network <b>194</b>. Network <b>194</b> may provide computer users with means of communicating and transferring software and information electronically.</p><p>Additionally, network <b>194</b> may provide distributed processing, which involves several computers in the sharing of workloads or cooperative efforts in performing a task. Network <b>194</b> can also provide a connection to other systems like those mentioned above (a BBS, the Internet, etc.).</p><p>Display <b>196</b>, which is controlled by display controller <b>198</b>, is used to display visual output generated by data processing system <b>120</b>. Such visual output may include text, graphics, animated graphics, and video. Display <b>196</b> may be implemented with CRT-based video display, an LCD-based flat panel display, or a gas plasma-based flat-panel display. Display controller <b>198</b> includes electronic components required to generate a video signal that is sent to display <b>196</b>.</p><p>Printer <b>100</b> may be coupled to data processing system <b>120</b> via parallel controller <b>102</b>. Printer <b>100</b> is used to put text or a computer-generated image (or combinations thereof) on paper or on another medium, such as a transparency sheet. Other types of printers may include an image setter, a plotter, or a film recorder.</p><p>Parallel controller <b>102</b> is used to send multiple data and control bits simultaneously over wires connected between system bus <b>105</b> and another parallel communication device, such as a printer <b>100</b>.</p><p>CPU <b>150</b> fetches, decodes, and executes instructions, and transfers information to and from other resources via the computers main data-transfer path, system bus <b>105</b>. Such a bus connects the components in a data processing system <b>120</b> and defines the medium for data exchange. System bus <b>105</b> connects together and allows for the exchange of data between memory units <b>156</b>, <b>158</b>, and <b>160</b>, CPU <b>150</b>, and other devices as shown in FIG. <b>3</b>. Those skilled in the art will appreciate that a data processing system constructed in accordance with the present invention may have multiple components selected from the foregoing, including even multiple processors.</p><p>Referring now to FIG. 4, one embodiment of the present invention allows data processing system <b>120</b> to more efficiently process information by speeding up the memory accesses performed by CPU <b>150</b>. In the illustrative embodiment, CPU <b>150</b> includes a multi-level cache hierarchy comprised of an upper, or L<b>1</b> cache <b>200</b>, and a lower, or L<b>2</b> cache <b>202</b>. Also depicted are a load/store unit <b>204</b>, and a plurality of register renames <b>206</b>. CPU <b>150</b> includes other (conventional) components not shown, such as fixed-point units, floating-point units, branch units, general purpose registers, special purpose registers, etc., some of which are interconnected with load/store unit <b>204</b> and register renames <b>206</b>. L<b>1</b> cache <b>200</b> includes both operand data and instruction caches, although only the operand data components are shown. Those components include the L<b>1</b> data directory <b>208</b> and the L<b>1</b> data entry array <b>210</b>.</p><p>Noticeably absent in the L<b>1</b> cache is any load queues (for requests from load/store unit <b>204</b>), and any reload buffers (for data provided to L<b>1</b> data entry array from L<b>2</b> cache <b>202</b> or system bus <b>105</b>). Any request for a load operation is sent along request bus <b>212</b> to L<b>1</b> data directory <b>208</b> and L<b>1</b> data entry array <b>210</b>. Directory <b>208</b> searches to see if the requested address matches one already present (an L<b>1</b> hit). If the operation results in a cache hit, then the mechanism proceeds as in the prior art, with the read data being sourced by entry array <b>210</b> to one of the register renames <b>206</b> via a controller or multiplexer <b>220</b>.</p><p>If the load operation results in a miss, however, the load address that is coming out of request bus <b>212</b> is also being piped out to the lower level storage subsystem, specifically, to an L<b>2</b> controller <b>214</b>, L<b>2</b> directory <b>216</b>, and L<b>2</b> entry array <b>218</b> (as explained further below, the requested address is delivered to the L<b>2</b> components even if the load operation resulted in an L<b>1</b> hit). This interconnection between request bus <b>212</b> and the L<b>2</b> components lacks any load queues, which allows load/store unit <b>204</b> to issue several sequential load operations without generating a stall condition for the core, as would happen in the prior art. Instead of load queues, the present invention handles L<b>1</b> misses by passing down other information to L<b>2</b> cache <b>200</b>, which allows for the later placement of the requested data in the appropriate register rename <b>206</b>.</p><p>Each load operation has a \u201chome\u201d in one of the register renames, i.e., the target register rename, and this specific register rename is identified in the information passed down to L<b>2</b> cache <b>202</b> with the load request. This information is then mirrored back to L<b>1</b> cache <b>200</b> when the data becomes available. For example, consider a load operation designating that data being loaded into logical register <b>12</b> (R<b>12</b>). R<b>12</b> is a logical register, but is also a physical register, so other information is passed to completely identify the dispatch. L<b>2</b> cache <b>202</b> does not use the information, but serves only to mirror the information back to L<b>1</b> cache <b>200</b>. When the data is received by L<b>1</b> cache <b>200</b>, the associated dispatch information mirrored back is used to place the data in the proper register rename. This mirrored bus protocol allows for more compact CPU cores which lends itself to higher frequencies implementation and more space for other features. L<b>2</b> controller <b>214</b> resolves any L<b>1</b> collision's.</p><p>In the specific implementation of FIG. 4, two separate ports are provided from L<b>2</b> cache <b>202</b> to L<b>1</b> cache <b>200</b>. A first port and associated reload buses <b>222</b> and <b>222</b><i>a </i>are provided for the entire cache line (e.g., 64 bytes of data), while a second port and associated register bus <b>224</b> are provided for the smaller (e.g., 8-byte) word that was specifically requested by the load operation. A single port could be used at L<b>2</b> entry array <b>218</b> instead, with the target data being tapped off to form the second bus. Reload bus <b>222</b><i>a </i>is connected to L<b>1</b> cache <b>200</b> via another multiplexer <b>226</b>, while register bus <b>224</b> is connected to register renames <b>206</b> via multiplexer <b>220</b>. Thus, upon an L<b>1</b> cache miss, if the requested data is present in L<b>2</b> cache <b>202</b>, it can be directly ported to register rename <b>206</b> via register bus <b>224</b> without first having to wait for the entire cache line to be received by the L<b>1</b> entry array <b>210</b>. Each register rename is provided with a flag that gets validated once the target data is loaded.</p><p>As mentioned above, the load requests are always delivered to L<b>2</b> cache <b>202</b>, whether or not the L<b>1</b> cache missed. In order to allow the L<b>2</b> cache to determine whether it needs to honor the request, a flag is provided to L<b>2</b> controller <b>214</b> from L<b>1</b> data directory <b>208</b> to indicate the hit/miss status.</p><p>In this manner, even if the core is executing an excessive number of sequential or nearly sequential load operations, there is no stalling as the load request is simply piped downstream, and then the data is piped back up to the register renames. Once the register rename flag is validated, any ensuing dependencies on that register rename may proceed. This approach effectively provides what looks like an infinite load queue from the core's perspective. Those skilled in the art will appreciate that this approach is further facilitated by providing an increased number of register renames, e.g., 128 physical registers (logically only 32 are provided in the instruction set).</p><p>Providing two separate ports also allows for the \u201cimprecise\u201d operation of L<b>1</b> cache <b>200</b> meaning that, while the specific piece of 8-byte data must be loaded into the register rename, it is not necessary to load the entire 64-byte line. While it makes some sense to reload that data in L<b>1</b> cache <b>200</b>, because that cache is closest to the processor core, the mechanism of reloading from L<b>2</b> to L<b>1</b> is a separate side behavior which does not hold up the core. In the illustrative embodiment, the imprecise nature of the L<b>1</b> cache operation is twofold. First, L<b>2</b> cache <b>202</b> may choose to not reload L<b>1</b> cache <b>200</b>. Second, L<b>1</b> cache <b>200</b> may refuse to accept a reload that has been proffered from L<b>2</b> cache <b>202</b>. Generally, the reload will occur whenever a determination is made that it would be efficient to currently load the cache line into the upper level.</p><p>There are several situations where it would be beneficial to not reload the entire cache line. There may be an insufficient amount of queues in the L<b>2</b> controller to allow the reload (reload queues require relatively large buffers, and many of them). Also, oftentimes there may be many reload requests which hit in the L<b>2</b> cache. When these requests are contending for the reload bus with data being returned from a lower level of the memory hierarchy for a previous L<b>2</b> miss, it is often more efficient to not reload one or the other of the requests. This approach simplifies the reload bus pipeline control logic, which leads to higher frequency implementations. L<b>1</b> cache misses are monitored using the flag that is provided by L<b>1</b> directory <b>208</b> to L<b>2</b> controller <b>214</b> to indicate the hit/miss status. The L<b>2</b> cache may thus maintain a history of L<b>1</b> cache misses. The L<b>1</b> cache hit/miss information is used to update the L<b>2</b> victim selection state.</p><p>Even if the L<b>2</b> cache directs a reload of data into the L<b>1</b> cache, the L<b>1</b> cache may still refuse the reload. For example, load/store unit <b>204</b> may be attempting a read operation at the same time that the L<b>2</b> cache is trying to reload the data into the L<b>1</b> cache. In such a case, L<b>1</b> cache <b>200</b> may elect to ignore the reload operation and proceed with the read, so as not to stall the core or subsequent reload requests. Thus, the reload mechanism is not only imprecise, it is also out of the critical path of processor core execution.</p><p>Further layering of the L<b>1</b> cache may be achieved by allowing the L<b>2</b> to control the L<b>1</b> victimization process. For example, L<b>1</b> cache <b>200</b> may be 8-way set associative, and L<b>2</b> cache <b>202</b> explicitly picks the victim and set for the reload, using an L<b>1</b> least recently used (LRU) unit <b>228</b> and victim select logic <b>230</b> controlled by L<b>2</b> controller <b>214</b>. This approach has the added benefit of more easily maintaining inclusivity. It also moves the victimization process further away from the critical path. Moreover, since L<b>2</b> controller <b>214</b> sees all of the L<b>1</b> load addresses, it can maintain a hybrid L<b>2</b> LRU <b>232</b> which includes information based on not only L<b>1</b> misses, but further on L<b>1</b> hits; such information would not be available to the L<b>2</b> LRU unit in the prior art. L<b>2</b> victim select logic <b>234</b> uses the information from hybrid L<b>2</b> LRU <b>232</b>.</p><p>Another advantage in having the L<b>2</b> cache control the updating of the L<b>1</b> cache, is that certain snoop requests can resolve faster, as they do not require an acknowledgement from the L<b>1</b> cache. For example, the PowerPC instruction set provides several commands that allow a device to gain ownership of a memory block. These commands often result when a device issues a read-with-intent-to-modify (RWITM) instruction. The PowerPC flush instructions (e.g., data cache block flush\u2014\u201cDCBF\u201d) cause a cache block to be made available by invalidating the cache block if it contains an unmodified (\u201cshared\u201d or \u201cexclusive\u201d) copy of a memory block or, if the cache block contains a modified copy of a memory block, then by first writing the modified value downward in the memory hierarchy (a \u201cpush\u201d), and thereafter invalidating the block. The kill instructions (data cache block invalidate\u2014\u201cDCBI,\u201d instruction cache block invalidate\u2014\u201cICBI,\u201d or data cache block set to zero\u2014\u201cDCBZ\u201d) are similar to the flush instructions except that a kill instruction immediately forces a cache block to an invalidate state, so any modified block is killed without pushing it out of the cache. For these instructions, the prior art requires that the L<b>1</b> cache acknowledge to the L<b>2</b> cache when the operation was completed by the L<b>1</b> cache. In the present invention, however, the invalidate request is guaranteed to occur, since L<b>1</b> directory writes (invalidates) are controlled explicitly by the L<b>2</b> cache, so no handshaking between the caches is necessary, and the L<b>2</b> may immediately send a \u201cclean\u201d (or \u201cnull\u201d) response to the requesting device. The result is deeply-pipelined, no-acknowledge control flow.</p><p>Still another advantage of the foregoing construction is that the L<b>1</b> cache may be a store-through (write-through) cache, simplifying state information. Only one state bit is provided, a valid/invalid bit, unlike in traditional art such as the MESI protocol mentioned in the Background which uses two bits with a write-back cache. Using a store-through L<b>1</b> cache, there are no cast outs or retry pushes at that level. All store operations issued by the core are forwarded to the L<b>2</b> cache regardless of whether they hit or miss. A store-through cache additionally simplifies the handling of a parity error\u2014it can be treated as if the cache line is just invalid (for a parity error arising from either L<b>1</b> directory <b>208</b> or entry array <b>210</b>).</p><p>If a load request misses both the L<b>1</b> cache and the L<b>2</b> cache, then load queues <b>236</b> may be used at the L<b>2</b> level as in the prior art. The use of load queues at the L<b>2</b> level is not as critical as at the L<b>1</b> level, since the L<b>2</b> cache is much larger and so there are generally fewer misses. However, the present invention further contemplates extending the above-noted concept of providing a separate path for the critical (8-byte) data, to the system bus <b>105</b>. One reload bus <b>222</b><i>b </i>is provided for the entire cache line, and is connected to multiplexer <b>226</b>, while a separate register bus <b>238</b> having a smaller granularity is provided for the 8-bytes of data (and mirrored information). Reload buffers <b>239</b> may be used to write the cache line back to the L<b>2</b> cache.</p><p>When a load request is issued, a pair of flags may be sent along with the request which specify which granularities are requested from the memory subsystem. The first granularity may be, e.g., 64 bytes, and the second granularity (which is a specific subset of the first granularity) may be, e.g., 8 bytes. If both granularities of data are to be returned to the requesting device, then the two granularities are returned in two separate bus transactions along system bus <b>105</b>. The invention may support heterogenous devices on the system bus, i.e., wherein one device is not capable of receiving only the second granularity transaction. The requesting device could be an I/O device which may only be able to use the first granularity, in which case it sets the outbound flags to request only the first granularity. More particularly, the device may be a processing unit which includes at least one cache with cache lines having the first granularity, and a requested value having the second granularity is register data. When the cache issues a system bus address transaction due to a processor load request which missed in the cache, the cache may set the outbound flags to request only the second granularity, or the first granularity, or both granularities.</p><p>When the memory subsystem returns the requested data, the granularity of the data bus transaction may be determined by a pair of inbound flags. The first flag identifies the data as being of the first granularity of the second granularity. If both granularities were requested, the second (smaller) granularity is always returned with the first of two separate bus transactions. When the second granularity is returned (in the first bus transaction), the second flag indicates whether the first granularity (the second bus transaction) will occur or not. This approach allows the memory subsystem to imprecisely return the first granularity even though both granularities were requested.</p><p>If two (or more) L<b>1</b> load misses are directed to the same cache line, then L<b>2</b> controller <b>214</b> can collapse those multiple requests into a single load operation that is passed on to system bus <b>105</b>. Two different sets of data are still mirrored back for the register renames (this situation presents another opportunity for imprecise operation of the L<b>1</b> cache, i.e., not honoring one of the writes to the L<b>1</b> cache, as discussed above).</p><p>Another novel aspect of the invention relates to store operations. If load/store unit <b>204</b> issues a store operation, it is passed down from register renames <b>206</b> to a store cache <b>240</b> in L<b>2</b> cache <b>202</b>. It also enters a queue <b>242</b> to the L<b>1</b> cache, and a queue <b>244</b> to the L<b>2</b> cache. Load requests always snoop against store queues <b>242</b> and <b>244</b>, and these queues may be used to source the data when requested by another device, via a multiplexer having the queue entries as inputs. In the preferred embodiment, the store port is 8 bytes wide. If a store operation misses the L<b>1</b> cache, and queue <b>242</b> is full, L<b>1</b> cache <b>200</b> can simply ignore the store operation (as long as it always forwards the store operations to the L<b>2</b> cache), without holding up the core (imprecise L<b>1</b> allocation on L<b>1</b> store misses). If a store operation hits the L<b>1</b> cache and queue <b>242</b> is full, the L<b>1</b> cache line can simply be invalidated and the store ignored without holding up the core (imprecise L<b>1</b> update on L<b>1</b> store hits). Again, the data will generally be written to the upper level cache whenever a determination is made that it would be efficient to do so.</p><p>This construction is distinct from the prior art which provided only a store queue. Store cache <b>240</b> is different from a queue in that, among other things, it includes an address directory as well as an entry array. Store cache <b>240</b> thus allows for \u201cstore gathering.\u201d In the prior art, if a processor does a one-byte store and misses the cache, the core is stalled until completion of a RWITM instruction. There may be other store operations following that instruction for the same line. The use of a store cache allows such related store operations to be gathered into a single operation for the entire line. In testing using DPC traces on a standard <b>172</b> million instruction reference set, this feature improved performance by resulting in only 6,000 stores stalling the core, as opposed to 24 million stores stalling the core on existing processors.</p><p>While the foregoing description has been made with reference to the operand data cache <b>200</b>, those skilled in the art will appreciate that the present invention may likewise be applied to an instruction cache. As shown in FIG. 5, an instruction fetch unit (IFU) <b>250</b> is allowed to issue fetch instructions to the L<b>2</b> cache <b>202</b> without the use of intervening I-fetch reload queues. The feature is especially useful for issuing speculative instruction fetches. The fetch instruction, when passed down to the L<b>2</b> cache, may include appropriate bits to indicate whether it was due to a real demand, or due to a speculative fetch (i.e., a predicted branch). The L<b>2</b> cache is required to honor only the demand-based requests. In particular, the L<b>2</b> cache might ignore non-demand requests that result in L<b>2</b> misses. A sixteen-deep instruction buffer may be used for demanded instructions (8-byte) sent up by the L<b>2</b> cache. Alternatively, the requested information may be directly ported to the appropriate execution unit <b>252</b> which was indicated as the target by the sequencer. Various feature described with reference to data cache <b>200</b> also apply to the instruction cache <b>254</b>, such as a separate port for the critical requested 8-bytes, imprecise operation of the L<b>1</b> instruction cache, control of L<b>1</b> victimization using the L<b>2</b> controller, etc.</p><p>A further extension of the present invention similarly applies to translation caches, such as caches for a translation lookaside buffer (TLB) or an effective-to-real address translation table (ERAT). For such implementations, the target data may be destined for components other than the register renames, i.e., execution units (adders) within the memory management unit using the TLB As shown in FIG. 6, the load/store unit <b>204</b> may further be connected to a memory management unit <b>260</b> utilized to translate effective addresses (EAs), specified within operand data access requests received from LSU <b>204</b>, into physical addresses assigned to locations within system memory, and to translate EAs specified in memory mapped I/O requests into addresses of devices within the data processing system. In order to permit simultaneous address translation of operand data and instruction addresses, a corresponding address translation mechanism for instructions may be provided (not shown), i.e., for translating EAs contained within instruction requests, received from instruction fetch unit <b>250</b>, into physical addresses with the system memory.</p><p>In the illustrative embodiment, MMU <b>260</b> includes segment registers <b>262</b> which are utilized to store segment identifiers for different subdivided regions of an effective address space of the processor. MMU <b>260</b> also includes a (data) translation lookaside buffer (TLB) <b>264</b> which, in the preferred embodiment, is a two-way set associative cache for storing copies of recently accessed page table entries. MMU <b>260</b> further includes a block address table <b>266</b> which is used to translate EAs falling within predefined data blocks.</p><p>When an operand data access request is received from LSU <b>204</b>, TLB <b>264</b> is examined to see if the effective address of the operand data is present. If so, the corresponding page table entry (PTE) found within TLB <b>264</b> is assigned to the request and used by MMU <b>260</b> to determine the physical address of the requested block (e.g., using a special adder that performs concatenation). However, if the requested EA is not present in TLB <b>264</b>, then the PTE must be retrieved elsewhere, such as from the memory controller for the system memory device, via the system bus <b>105</b>. As with the prior implementations, this request may be passed on to lower levels of the memory hierarchy along with use information but without requiring a higher level queue, and the requested PTE may be returned with the use information directly to MMU <b>260</b> (in parallel transmission to TLB <b>264</b>) without the need for the higher level buffer, thereby enhancing performance of MMU <b>260</b> and hence improving the overall processing speed. Again, the other features described above may apply to the implementation for a translation cache.</p><p>Although the invention has been described with reference to specific embodiments, this description is not meant to be construed in a limiting sense. Various modifications of the disclosed embodiments, as well as alternative embodiments of the invention, will become apparent to persons skilled in the art upon reference to the description of the invention. For example, while the illustrative embodiment provides only vertical L<b>1</b> and L<b>2</b> cache levels for a single associated processor, the invention can be extended to additional cache levels as well, or to multi-processor systems, or to cache hierarchies having vertical caches that support a processor core cluster. It is therefore contemplated that such modifications can be made without departing from the spirit or scope of the present invention as defined in the appended claims.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Ravi Kumar", "last_name": "Arimilli", "name": ""}, {"first_name": "Leo James", "last_name": "Clark", "name": ""}, {"first_name": "John Steven", "last_name": "Dodson", "name": ""}, {"first_name": "Guy Lynn", "last_name": "Guthrie", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "INTERNATIONAL BUSINESS MACHINES CORPORATION"}, {"first_name": "", "last_name": "INTERNATIONAL BUSINESS MACHINES CORPORATION", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711138"}, {"primary": false, "label": "711E1202"}, {"primary": false, "label": "711122"}, {"primary": false, "label": "711E12043"}, {"primary": false, "label": "711117"}], "ecla_classes": [{"label": "G06F  12/08B22L"}, {"label": "G06F  12/08B14"}], "cpc_classes": [{"label": "G06F  12/0897"}, {"label": "G06F  12/0875"}, {"label": "G06F  12/0897"}, {"label": "G06F  12/0875"}], "f_term_classes": [], "legal_status": "Expired - Fee Related", "priority_date": "1999-06-25", "application_date": "1999-06-25", "family_members": [{"ucid": "US-6397300-B1", "titles": [{"lang": "EN", "text": "High performance store instruction management via imprecise local cache update mechanism"}]}]}