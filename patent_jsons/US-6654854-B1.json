{"patent_number": "US-6654854-B1", "publication_id": 73631656, "family_id": 29584254, "publication_date": "2003-11-25", "titles": [{"lang": "EN", "text": "Caching method using cache tag and cache data stored in dynamic RAM embedded in logic chip"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA50584116\"><p>A caching method for using cache tag and cache data stored in dynamic RAM embedded in a logic chip. In general, there are at least two cache applications where this method can be employed. First, there are caches integral to a processor and interfaced to a processor pipeline. Second, there are caches external to a processor and interfaced with a shared bus.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6654854-B1-CLM-00001\" num=\"1\"><claim-text>1. A computer system comprising:</claim-text><claim-text>a processor having embedded logic; </claim-text><claim-text>a cache memory comprising a DRAM embedded in the processor wherein at least a portion of the cache data is stored in the embedded DRAM and wherein at least a portion of the cache tags are stored in the embedded DRAM; </claim-text><claim-text>wherein the processor includes an address buffer coupled to the embedded DRAM, a data buffer coupled to the embedded DRAM, a register file coupled to the data buffer, and a pipeline coupled to the address buffer, the data buffer, and the register file. </claim-text></claim>"}, {"num": 2, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6654854-B1-CLM-00002\" num=\"2\"><claim-text>2. A shared bus computer system, comprising:</claim-text><claim-text>at least one shared bus; </claim-text><claim-text>at least one processor coupled to the bus; </claim-text><claim-text>a bus interface having embedded logic coupled to the at least one shared bus; </claim-text><claim-text>a cache memory comprising a DRAM embedded in the bus interface wherein at least a portion of the cache data is stored in the embedded DRAM and wherein at least a portion of the cache tags are stored in the embedded DRAM; </claim-text><claim-text>a second shared bus coupled to the bus interface; </claim-text><claim-text>a second bus interface coupled to the second shared bus; </claim-text><claim-text>a memory coupled to the second bus interface; </claim-text><claim-text>a third bus interface having embedded logic coupled to the second shared bus; </claim-text><claim-text>a second cache memory comprising a second DRAM embedded in the third bus interface wherein at least a portion of the second cache data is stored in the embedded DRAM and wherein at least a portion of the second cache tags are stored in the embedded DRAM; </claim-text><claim-text>a third shared bus coupled to the third bus interface; and </claim-text><claim-text>a second processor coupled to the third shared bus. </claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6654854-B1-CLM-00003\" num=\"3\"><claim-text>3. The shared bus computer system according to <claim-ref idref=\"US-6654854-B1-CLM-00002\">claim 2</claim-ref>, further comprising a second processor connected to the at least one shared bus.</claim-text></claim>"}, {"num": 4, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6654854-B1-CLM-00004\" num=\"4\"><claim-text>4. The shared bus computer system according to <claim-ref idref=\"US-6654854-B1-CLM-00002\">claim 2</claim-ref>, further comprising a third processor coupled to the at least one shared bus.</claim-text></claim>"}, {"num": 5, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6654854-B1-CLM-00005\" num=\"5\"><claim-text>5. The shared bus computer system according to <claim-ref idref=\"US-6654854-B1-CLM-00002\">claim 2</claim-ref>, further comprising a third processor coupled to the third shared bus.</claim-text></claim>"}, {"num": 6, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6654854-B1-CLM-00006\" num=\"6\"><claim-text>6. The shared bus computer system according to <claim-ref idref=\"US-6654854-B1-CLM-00002\">claim 2</claim-ref>, further comprising:</claim-text><claim-text>a third processor coupled to the at least one shared bus; and </claim-text><claim-text>a fourth processor coupled to the third shared bus. </claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54023504\"><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of Invention</p><p>The present invention relates generally to the field of computer system memory and pertains more particularly to a caching method using cache tag and cache data stored in dynamic RAM embedded in a logic chip.</p><p>2. Discussion of the Prior Art</p><p>Modern computer systems are often comprised of multiple forms and locations of memory. The memory subsystem is typically organized hierarchically. For example, from cache memory of various levels at the top to main memory and finally to hard disc memory. A processor in search of data or instructions looks first in the cache memory, which is closest to the processor. If the information is not found there, then the request is passed next to the main memory and finally to the hard disc. The relative sizes and performance of the memory units are conditioned primarily by economic considerations. Generally, the higher the memory unit is in the hierarchy the higher its performance and the higher its cost. For reference purposes, the memory subsystem will be divided into \u201ccaches\u201d and \u201cmemory.\u201d The term memory will cover every form of memory other than caches. Information that is frequently accessed is stored in caches and information that is less frequently accessed is stored in memory. Caches allow higher system performance because the information can typically be accessed from the cache faster than from the memory. Relatively speaking, this is especially true when the memory is in the form of a hard disk.</p><p>A cache consists of a cache data portion and a cache tag portion. The cache data portion contains the information that is currently stored in the cache. The cache tag portion contains the addresses of the locations where the information is stored. Generally, the cache data will be larger than the cache tags. The cache data and the cache tags will not necessarily be stored together depending on the design. When a specific piece of information is requested, one or more of the cache tags are searched for the address of the requested information. Which cache tags are searched will depend on the cache design. If the address of the requested information is present in the cache tags, then the information will be available from that address in the cache data. If the address is not present, then the information may be available from memory.</p><p>In general, there are two cache applications that will be considered. First, there are caches integral to a processor and interfaced to a processor pipeline. Second, there are caches external to a processor and interfaced with a shared bus. Caches must be designed in such a way that their latency meets the timing requirements of the requesting components such as the processor pipeline or the shared bus. For example, consider the design of the shared bus. A cache or other agent on the bus that requires a specific piece of information will issue the address of the information on the bus. This is known as the address phase. Subsequently, all caches or other agents attached to the bus must indicate whether the information at the issued address is located there. This is known as the snoop phase. Typically, the bus design specifies that the cache must supply its snoop response within a fixed time interval after the address has been issued on the bus. If the cache is not designed to satisfy this timing requirement, it will lead to sub-optimal usage of the bus thus lowering system performance.</p><p>Examples of prior art systems will now be discussed in greater detail. Turning first to FIGS. 1-3, block diagrams of a processor <b>10</b> having an integral cache <b>12</b> that is interfaced to a processor pipeline <b>14</b> are shown. The processor <b>10</b> further consists of a register file <b>16</b>, an address buffer <b>18</b>, and a data buffer <b>20</b>. The various elements are connected together by unidirectional and bi-directional conductors as shown. When the cache <b>12</b> of FIG. 1 is integral to the processor <b>10</b>, conventionally both the cache tags and the cache data are stored in fast static random access memory (SRAM) technology. In general, such an implementation is shown as cache <b>12</b> in FIG. <b>2</b>. Sometimes, insufficient cache is provided integral to the processor, so a supplemental cache is provided external to the processor. Such an implementation is shown as caches <b>12</b><i>a </i>and <b>12</b><i>b </i>in FIG. <b>3</b>. Among the drawbacks to implementations of caches exclusively in SRAM are that, relatively speaking, SRAM is expensive, is less dense, and uses more power than dynamic random access memory (DRAM) technology.</p><p>With reference to FIGS. 4-6, block diagrams of a cache <b>12</b> external to a processor <b>10</b> and interfaced with a shared bus <b>22</b> are shown. Also interfaced with the shared bus <b>22</b> is a memory <b>24</b>. The cache <b>12</b> and the memory <b>24</b> are interfaced with the shared bus <b>22</b> through a bus interface <b>26</b> as shown. When the cache <b>12</b> of FIG. 4 is external to the processor <b>10</b>, conventionally the cache tags are stored in a SRAM cache and the cache data is stored in a DRAM cache. In one implementation, both the SRAM cache <b>12</b><i>a </i>containing cache tags and the DRAM cache <b>12</b><i>b </i>containing cache data are external to the bus interface <b>26</b> as shown in FIG. <b>5</b>. In another implementation, only the DRAM cache <b>12</b><i>b </i>containing cache data is external to the bus interface <b>26</b> while the SRAM cache <b>12</b><i>a </i>containing cache tags is integral to the bus interface as shown in FIG. <b>6</b>. Among the drawbacks to these implementations are that the latency of accessing the cache data is long since it is stored in slower DRAM external to the logic chip. This may force a delay in transferring data to the shared bus thus degrading the system performance. Further, when the cache tags are implemented in SRAM embedded on the logic chip, the size of the cache is limited by the higher cost, the lower density, and the greater power consumption of SRAM.</p><p>A definite need exists for a system having an ability to meet the latency timing requirements of the requesting components of the system. In particular, a need exists for a system which is capable of accessing cache memory in a timely manner. Ideally, such a system would have a lower cost and a higher capacity than conventional systems. With a system of this type, system performance can be enhanced. A primary purpose of the present invention is to solve this need and provide further, related advantages.</p><h4>SUMMARY OF THE INVENTION</h4><p>A caching method is disclosed for using cache tag and cache data stored in dynamic RAM embedded in a logic chip. In general, there are at least two cache applications where this method can be employed. First, there are caches integral to a processor and interfaced to a processor pipeline. Second, there are caches external to a processor and interfaced with a shared bus.</p><h4>BRIEF DESCRIPTION OF THE DRAWING</h4><p>The above and other objects and advantages of the present invention will be more readily appreciated from the following detailed description when read in conjunction with the accompanying drawing, wherein:</p><p>FIG. 1 is a block diagram of a processor having an integral cache that is interfaced to a processor pipeline according to the prior art;</p><p>FIG. 2 is a prior art block diagram of a processor having an integral SRAM cache that is interfaced to a processor pipeline;</p><p>FIG. 3 is a prior art block diagram of a processor having an integral SRAM cache and an external supplemental SRAM cache both of which are interfaced to a processor pipeline;</p><p>FIG. 4 is a prior art block diagram of a cache external to a processor and interfaced with a shared bus;</p><p>FIG. 5 is a prior art block diagram of a SRAM cache containing cache tags and a DRAM cache containing cache data both of which are external to a processor and interfaced with a shared bus;</p><p>FIG. 6 is a prior art block diagram of a DRAM cache containing cache data and a SRAM cache containing cache tags which is integral to a bus interface both of which are external to a processor and interfaced with a shared bus;</p><p>FIG. 7 is a block diagram of a logic chip having embedded logic and embedded DRAM cache containing cache tag and cache data according to one embodiment of the present invention;</p><p>FIG. 8 is a block diagram of a processor having an embedded DRAM cache containing cache tag and cache data that is interfaced to a processor pipeline according to another embodiment of the present invention;</p><p>FIG. 9 is a block diagram of a processor having an integral SRAM cache containing cache tag and cache data and an external embedded DRAM cache containing cache tag and cache data both of which are interfaced to a processor pipeline according to a further embodiment of the present invention;</p><p>FIG. 10 is a block diagram of an embedded DRAM cache containing cache tag and cache data which is integral to a bus interface which is external to a processor and interfaced with a shared bus according to yet another embodiment of the present invention; and</p><p>FIG. 11 is a block diagram of a pair of embedded DRAM caches containing cache tag and cache data each of which is integral to one of a pair of bus interfaces each of which is external to a processor and interfaced with a shared sub-bus according to a further embodiment of the present invention.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>Turning now to FIG. 7, a block diagram of a logic chip <b>30</b> having embedded logic <b>32</b> and embedded DRAM cache <b>34</b> containing cache tag and cache data according to one embodiment of the present invention is shown. The embedded logic <b>32</b> can be any of a wide variety of logic that is well known to one of ordinary skill in the art. For example, the embedded logic <b>32</b> may be a floating point unit or a bus interface. In general, there are at least two cache applications where this method can be employed. First, there are caches integral to a processor and interfaced to a processor pipeline. Second, there are caches external to a processor and interfaced with a shared bus. For example, in a shared bus design, the embedded DRAM cache <b>34</b> can be accessed within the minimum time delay specified between the address and snoop phases of the shared bus. The latency of accessing the embedded DRAM cache <b>34</b> is substantially lower than accessing the external DRAM cache <b>12</b><i>b </i>as in FIGS. 5 and 6 above. Among the advantages of the method of the present invention are that the embedded DRAM cache results in a cache with a larger capacity than a cache implemented with an integral SRAM as DRAM is cheaper, is more dense, and consumes less power. Further, by storing both the cache tags and the cache data in embedded DRAM, the method of the present invention allows for simpler cache interface logic than designs employing both SRAM and DRAM technologies.</p><p>With reference to FIG. 8, a block diagram of a processor <b>10</b> having an embedded DRAM cache <b>34</b> containing cache tag and cache data that is interfaced to a processor pipeline <b>14</b> according to one embodiment of the present invention is shown. As above with respect to FIGS. 1-3, the processor <b>10</b> further consists of a register file <b>16</b>, an address buffer <b>18</b>, and a data buffer <b>20</b>. Such an implementation is able to meet the stringent time requirements of the processor.</p><p>FIG. 9 is a block diagram of a processor <b>10</b> having an integral SRAM cache <b>12</b> containing cache tag and cache data and an external embedded DRAM cache <b>34</b> containing cache tag and cache data, both of which are interfaced to a processor pipeline <b>14</b> according to another embodiment of the present invention. Note that the processor <b>10</b> is essentially the same as that shown in FIGS. 2 and 3 above. In this configuration, the integral SRAM cache <b>12</b> would serve as level-1 cache in the hierarchical memory structure while the external embedded DRAM cache <b>34</b> would serve as level-2 cache. The external embedded DRAM cache <b>34</b> may be embedded in any of a number of other logic chips (not shown) that are connected to the processor <b>10</b>.</p><p>Turning now to FIGS. 10 and 11, block diagrams of caches external to a processor and interfaced with a shared bus are shown. The implementation shown in FIG. 10 is for a single shared bus while the implementation shown in FIG. 11 is for a hierarchical shared bus. FIG. 10 shows an embedded DRAM cache <b>34</b> containing cache tag and cache data which is integral to a bus interface <b>26</b> which is external to a processor <b>10</b> and interfaced with a shared bus <b>22</b> according to a further embodiment of the present invention. FIG. 11 is a block diagram of a system having a pair of embedded DRAM caches <b>34</b> containing cache tag and cache data, each of which is integral to one of a pair of bus interfaces <b>26</b>, each of which is external to a processor <b>10</b> and interfaced with a shared sub-bus <b>36</b>, according to still another embodiment of the present invention. As above with respect to FIGS. 4-6, also interfaced with the shared bus <b>22</b> is a memory <b>24</b>. Both such implementations support faster access to cache data than conventional approaches while continuing to meet the requirements of the shared bus.</p><p>While the invention has been illustrated and described by means of specific embodiments, it is to be understood that numerous changes and modifications may be made therein without departing from the spirit and scope of the invention as defined in the appended claims and equivalents thereof.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "Gopalakrishnan", "last_name": "Janakiraman", "name": ""}, {"first_name": "Fong", "last_name": "Pong", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "HEWLETT-PACKARD DEVELOPMENT COMPANY, L.P."}, {"first_name": "", "last_name": "HEWLETT PACKARD ENTERPRISE DEVELOPMENT LP", "name": ""}, {"first_name": "", "last_name": "HEWLETT-PACKARD DEVELOPMENT COMPANY L.P.", "name": ""}, {"first_name": "", "last_name": "HEWLETT-PACKARD COMPANY", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  12/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F  12/08        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "711118"}, {"primary": false, "label": "711E12041"}, {"primary": false, "label": "711170"}], "ecla_classes": [{"label": "G06F  12/08B22"}], "cpc_classes": [{"label": "G06F  12/0893"}, {"label": "G06F  12/0893"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1999-06-25", "application_date": "1999-06-25", "family_members": [{"ucid": "US-6654854-B1", "titles": [{"lang": "EN", "text": "Caching method using cache tag and cache data stored in dynamic RAM embedded in logic chip"}]}]}