{"patent_number": "US-6192465-B1", "publication_id": 72587094, "family_id": 22564562, "publication_date": "2001-02-20", "titles": [{"lang": "EN", "text": "Using multiple decoders and a reorder queue to decode instructions out of order"}], "abstracts": [{"lang": "EN", "paragraph_markup": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA72516037\"><p>A microprocessor capable of out-of-order instruction decoding and in-order dependency checking is disclosed. The microprocessor may include an instruction cache, two decode units, a reorder queue, and dependency checking logic. The instruction cache is configured to output cache line portions to the decode units. The decode units operate independently and in parallel. One of the decode units may be a split decoder that receives all instruction bytes from instructions that extend across cache line portion boundaries. The split decode unit may be configured to reassemble the instruction bytes into instructions. These instructions are then decoded by the split decode unit. A reorder queue may be used to store the decoded instructions according to their relative cache line positions. The decoded instructions are read out of the reorder queue in program order, thereby enabling the dependency checking logic to perform dependency checking in program order.</p></abstract>"}], "claims": [{"lang": "EN", "claims": [{"num": 1, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00001\" num=\"1\"><claim-text>1. A microprocessor comprising:</claim-text><claim-text>an instruction cache configured to receive and store instruction bytes from a main memory, wherein said instruction cache is configured to output cache lines of sequential instruction bytes in response to receiving corresponding fetch addresses; </claim-text><claim-text>a first decoder coupled to said instruction cache, wherein said first decoder is configured to receive and independently decode a first portion of a first cache line; </claim-text><claim-text>a second decoder coupled to said instruction cache, wherein said second decoder is configured to receive and independently decode a second portion of said first cache line, wherein said second decoder is capable of decoding said second portion of said first cache line and beginning decoding of a portion of a second cache line before said first decoder completes decoding said first portion of said first cache line; and </claim-text><claim-text>a decode reorder queue coupled to said first and second decoders, wherein said decode reorder queue comprises a plurality of storage locations, wherein each storage location is configured to store one decoded instruction, wherein said decode reorder queue is configured to receive instructions decoded from said cache lines by said first and second decoders, wherein said decode reorder queue is configured to store said decoded instructions in storage locations according to program order. </claim-text></claim>"}, {"num": 2, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00002\" num=\"2\"><claim-text>2. The microprocessor as recited in claim <b>1</b>, further comprising dependency checking logic coupled to said decode reorder queue, wherein said dependency checking logic is configured to perform in-order dependency checking on decoded instructions output from said decode reorder queue.</claim-text></claim>"}, {"num": 3, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00003\" num=\"3\"><claim-text>3. The microprocessor as recited in claim <b>2</b>, further comprising a reorder buffer coupled to said dependency checking logic, wherein said reorder buffer is configured to store said decoded instructions until said decoded instructions are ready for execution.</claim-text></claim>"}, {"num": 4, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00004\" num=\"4\"><claim-text>4. The microprocessor as recited in claim <b>2</b>, wherein each storage location comprises a status bit, wherein said status bit is indicative of whether the storage location is storing a valid decoded instruction.</claim-text></claim>"}, {"num": 5, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00005\" num=\"5\"><claim-text>5. The microprocessor as recited in claim <b>2</b>, wherein said storage locations are grouped into a plurality of storage lines, and wherein each storage line further comprises an address tag field configured to store address tags indicative of the program order of the storage lines.</claim-text></claim>"}, {"num": 6, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00006\" num=\"6\"><claim-text>6. The microprocessor as recited in claim <b>2</b>, wherein said storage locations are grouped into a plurality of storage lines, and wherein the number of instruction storage locations within each storage line is equal to the maximum possible number of instructions in each cache line portion.</claim-text></claim>"}, {"num": 7, "parent": 2, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00007\" num=\"7\"><claim-text>7. The microprocessor as recited in claim <b>2</b>, wherein said reorder queue is capable of receiving instructions out of program order and is configured to output instructions in program order.</claim-text></claim>"}, {"num": 8, "parent": 1, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00008\" num=\"8\"><claim-text>8. The microprocessor as recited in claim <b>1</b>, wherein said first and second decoder each comprise a fixed number of instruction outputs, wherein said fixed number equals the maximum possible number of instructions within each cache line portion, and wherein each instruction output corresponds to a particular instruction position within said cache line portion.</claim-text></claim>"}, {"num": 9, "parent": 8, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00009\" num=\"9\"><claim-text>9. The microprocessor as recited in claim <b>8</b>, further comprising predecode logic coupled to said instruction cache, wherein said predecode logic is configured to generate start and end bits indicative of the first and last bytes of instructions, wherein said predecode bits are stored in said instruction cache and are conveyed with said instruction bytes to said decoders.</claim-text></claim>"}, {"num": 10, "parent": 9, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00010\" num=\"10\"><claim-text>10. The microprocessor as recited in claim <b>9</b>, wherein said decode reorder queue is configured to store each instruction received from said first and second decoders into storage locations that correspond to the particular instruction output upon which they are conveyed.</claim-text></claim>"}, {"num": 11, "parent": 10, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00011\" num=\"11\"><claim-text>11. The microprocessor as recited in claim <b>10</b>, further comprising routing logic and a third decoder coupled, wherein said routing logic is configured to receive the cache lines from said instruction cache and route whole instructions to one of said first or second decoders, and wherein said routing logic is configured to route partial instructions that extend across cache line portion boundaries to said third decoder, wherein said third decoder is configured to reassemble said partial instructions into whole instructions, and wherein said third decoder is configured to decode said whole instructions.</claim-text></claim>"}, {"num": 12, "parent": 11, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00012\" num=\"12\"><claim-text>12. The microprocessor as recited in claim <b>11</b>, wherein said storage locations are grouped into a plurality of storage lines, and wherein said third decoder is further configured to convey said decoded whole instructions to said decode reorder queue, and wherein said decode reorder queue is configured to store said decoded whole instructions in the final storage location of the corresponding storage line.</claim-text></claim>"}, {"num": 13, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00013\" num=\"13\"><claim-text>13. A method for operating a microprocessor comprising:</claim-text><claim-text>fetching a plurality of instruction bytes; </claim-text><claim-text>decoding the instructions contained within the plurality of instruction bytes out of program order, wherein the decoding is performed by: </claim-text><claim-text>using a first decoder to decode a first instruction contained within the plurality of instructions bytes, </claim-text><claim-text>using a second decoder to decode a second instruction contained within the plurality of instructions bytes, wherein the second instruction occurs after the first instruction in program order, and </claim-text><claim-text>using the second decoder to decode a third instruction contained within the plurality of instructions bytes, wherein the third instruction occurs after the first and second instructions in program order, wherein the second decoder is configured to complete decoding the second instruction and begin decoding the third instruction after the first decoder begins decoding the first instruction and before the first decoder completes decoding the first instruction; </claim-text><claim-text>reordering the decoded instructions to program order; </claim-text><claim-text>performing dependency checking on the decoded and reordered instructions; </claim-text><claim-text>issuing the instructions to reservation stations for eventual execution out of program order; and </claim-text><claim-text>executing the instructions out of program order. </claim-text></claim>"}, {"num": 14, "parent": 13, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00014\" num=\"14\"><claim-text>14. The method as recited in claim <b>13</b>, further comprising:</claim-text><claim-text>allocating storage locations within a decode reorder buffer for potential instructions within a first portion of the plurality of instruction bytes; </claim-text><claim-text>allocating storage locations within the decode reorder buffer for potential instructions within a second portion of the plurality of instruction bytes; </claim-text><claim-text>allocating storage locations within the decode reorder buffer for potential instructions within a third portion of the plurality of instruction bytes; </claim-text><claim-text>independently decoding the first portion of the plurality of instruction bytes using the first decoder; and </claim-text><claim-text>independently decoding the second portion of the plurality of instruction bytes using the second decoder </claim-text><claim-text>independently decoding the third portion of the plurality of instruction bytes using the second decoder, wherein the second and third portions occur after the first portion in program order, and wherein the second decoder is configured to complete decoding the second portion and begin decoding the third portion after the first decoder begins decoding the first portion and before the first decoder completes decoding the first portion. </claim-text></claim>"}, {"num": 15, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00015\" num=\"15\"><claim-text>15. The method as recited in claim <b>14</b>, wherein said reordering further comprises: storing each decoded instruction within the corresponding allocated storage location within the decode reorder buffer.</claim-text></claim>"}, {"num": 16, "parent": 15, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00016\" num=\"16\"><claim-text>16. The method as recited in claim <b>15</b>, wherein said dependency checking is performed on the decoded instructions stored in program order in the decode reorder buffer.</claim-text></claim>"}, {"num": 17, "parent": 16, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00017\" num=\"17\"><claim-text>17. The method as recited in claim <b>16</b>, further comprising:</claim-text><claim-text>storing the results of the executed instructions in a future file/retire queue; and </claim-text><claim-text>retiring the instructions in order by committing the results to the architectural state of the microprocessor. </claim-text></claim>"}, {"num": 18, "parent": 14, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00018\" num=\"18\"><claim-text>18. The method as recited in claim <b>14</b>, further comprising:</claim-text><claim-text>grouping the plurality of instructions into cache line portions; and </claim-text><claim-text>routing selected instruction bytes to a third decoder, wherein the selected instruction bytes belong to instructions that extend beyond cache line portion boundaries, wherein the third decoder is configured reassemble and decode the selected instruction bytes. </claim-text></claim>"}, {"num": 19, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00019\" num=\"19\"><claim-text>19. A computer system comprising:</claim-text><claim-text>a first microprocessor; </claim-text><claim-text>a CPU bus coupled to said first microprocessor; and </claim-text><claim-text>a modem coupled to said CPU bus, </claim-text><claim-text>wherein said first microprocessor comprises: </claim-text><claim-text>an instruction cache configured to receive and store instruction bytes from a main memory, wherein said instruction cache is configured to output a cache line of sequential instruction bytes in response to receiving a corresponding fetch address; </claim-text><claim-text>a first decoder coupled to said instruction cache, wherein said first decoder is configured to receive and independently decode a first portion of a first cache line; </claim-text><claim-text>a second decoder coupled to said instruction cache, wherein said second decoder is configured to receive and independently decode a second portion of said first cache line, wherein said second decoder is capable of decoding said second portion of said first cache line and beginning decoding of a portion of a second cache line before said first decoder completes decoding said first portion of said first cache line; </claim-text><claim-text>a decode reorder buffer coupled to said first decoder and said second decoder, wherein said decode reorder buffer comprises a plurality of storage locations, wherein each storage location is configured to store one decoded instruction, wherein said decode reorder buffer is configured to receive instructions decoded from said cache line by said first decoder and said second decoder, wherein said decode reorder buffer is configured to store said decoded instructions in storage locations according to program order; and </claim-text><claim-text>dependency checking logic coupled to said decode reorder buffer, wherein said dependency checking logic is configured to perform dependency checking on decoded instructions stored in program order in said decode reorder buffer, wherein said dependency checking logic is configured to issue said decoded instructions to a plurality of functional units for out-of-order execution. </claim-text></claim>"}, {"num": 20, "parent": 19, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00020\" num=\"20\"><claim-text>20. The computer system as recited in claim <b>19</b>, further comprising:</claim-text><claim-text>a second microprocessor coupled to said CPU bus, wherein said second processor also comprises: </claim-text><claim-text>an instruction cache configured to receive and store instruction bytes from a main memory, wherein said second instruction cache is configured to output a cache line of sequential instruction bytes in response to receiving a corresponding fetch address; </claim-text><claim-text>a first decoder coupled to said instruction cache, wherein said first decoder is configured to receive and independently decode a first portion of said cache line; </claim-text><claim-text>a second decoder coupled to said instruction cache, wherein said second decoder is configured to receive and independently decode a second portion of said cache line; </claim-text><claim-text>a decode reorder buffer coupled to said first decoder and said second decoder, wherein said decode reorder buffer comprises a plurality of storage locations, wherein each storage location is configured to store one decoded instruction, wherein said decode reorder buffer is configured to receive instructions decoded from said cache line by said first decoder and said second decoder, wherein said decode reorder buffer is configured to store said decoded instructions in storage locations according to program order; and </claim-text><claim-text>dependency checking logic coupled to said decode reorder buffer, wherein said dependency checking logic is configured to perform dependency checking on decoded instructions stored in program order in said decode reorder buffer, wherein said dependency checking logic is configured to issue said decoded instructions to a plurality of functional units for out-of-order execution. </claim-text></claim>"}, {"num": 21, "parent": -1, "type": "independent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00021\" num=\"21\"><claim-text>21. A microprocessor comprising:</claim-text><claim-text>an instruction cache configured to receive and store instruction bytes from a main memory, wherein said instruction cache is configured to output cache lines of sequential instruction bytes in response to receiving corresponding fetch addresses; </claim-text><claim-text>a first decoder coupled to said instruction cache, wherein said first decoder is configured to receive and independently decode a first portion of a first cache line; </claim-text><claim-text>a second decoder coupled to said instruction cache, wherein said second decoder is configured to receive and independently decode a second portion of said first cache line, wherein said second decoder is capable of decoding said second portion of said first cache line and beginning decoding of a portion of a second cache line before said first decoder completes decoding said first portion of said cache line; </claim-text><claim-text>a third decoder; </claim-text><claim-text>routing logic configured to receive the cache lines from the instruction cache and route whole instructions to one of said first or second decoders and route partial instructions that extend across cache line portion boundaries to the third decoder, wherein the third decoder is configured to reassemble the partial instructions into whole instructions, and wherein the third decoder is configured to decode the reassembled whole instructions; and </claim-text><claim-text>a decode reorder queue coupled to said first, second, and third decoders, wherein said decode reorder queue comprises a plurality of storage locations, wherein each storage location is configured to store one decoded instruction, wherein said decode reorder queue is configured to receive instructions decoded from said cache lines by said first and second decoders, wherein said decode reorder queue is configured to store said decoded instructions in storage locations according to program order. </claim-text></claim>"}, {"num": 22, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00022\" num=\"22\"><claim-text>22. The microprocessor as recited in claim <b>21</b>, wherein said first and second decoder each comprise a fixed number of instruction outputs, wherein said fixed number equals the maximum possible number of instructions within each cache line portion, and wherein each instruction output corresponds to a particular instruction position within said cache line portion.</claim-text></claim>"}, {"num": 23, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00023\" num=\"23\"><claim-text>23. The microprocessor as recited in claim <b>21</b>, further comprising predecode logic coupled to said instruction cache, wherein said predecode logic is configured to generate one or more predecode bits for each instruction byte, wherein the predecode bits are indicative of whether the corresponding instruction bytes start new instructions, wherein the predecode bits are conveyed with said instruction bytes to said decoders.</claim-text></claim>"}, {"num": 24, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00024\" num=\"24\"><claim-text>24. The microprocessor as recited in claim <b>21</b>, wherein said decode reorder queue is configured to store each instruction received from said first and second decoders into storage locations that correspond to the particular instruction output upon which they are conveyed.</claim-text></claim>"}, {"num": 25, "parent": 21, "type": "dependent", "paragraph_markup": "<claim id=\"US-6192465-B1-CLM-00025\" num=\"25\"><claim-text>25. The microprocessor as recited in claim <b>21</b>, wherein said storage locations are grouped into a plurality of storage lines, and wherein said third decoder is further configured to convey said decoded whole instructions to said decode reorder queue, and wherein said decode reorder queue is configured to store said decoded whole instructions in the final storage location of the corresponding storage line.</claim-text></claim>"}]}], "descriptions": [{"lang": "EN", "paragraph_markup": "<description lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PDES54500305\"><?RELAPP description=\"Other Patent Relations\" end=\"lead\"?><h4>CROSS REFERENCE TO RELATED APPLICATIONS</h4><p>The following applications are related to this application: \u201cCompressing Variable-Length Instruction Prefix Bytes\u201d, U.S. patent application Ser. No 09/158,440, filed on Sep. 21, 1998; \u201cMethod for Calculating Indirect Branch Targets\u201d, U.S. patent application Ser. No 09/157,721, filed on Sep. 21, 1998; \u201cUsing Three-Dimensional Storage to Make Variable-Length Instructions Appear Uniform in Two Dimensions\u201d; U.S. patent application Ser. No 09/150,310; filed on Sep. 9, 1998; and \u201cExpanding Instructions with Variable-Length Operands to a Fixed Length\u201d, U.S. patent application Ser. No. 09/165,968, filed on Oct. 2, 1998.</p><?RELAPP description=\"Other Patent Relations\" end=\"tail\"?><?BRFSUM description=\"Brief Summary\" end=\"lead\"?><h4>BACKGROUND OF THE INVENTION</h4><p>1. Field of the Invention</p><p>This invention relates to decoding instructions out of program order within a microprocessor.</p><p>2. Description of the Relevant Art</p><p>Superscalar microprocessors achieve high performance through the use of pipelining, parallel execution, and high clock rates. Pipelining is an implementation technique whereby multiple instructions are overlapped during the execution process. Parallel execution refers to the simultaneously executing multiple instructions in a single clock cycle. As used herein, the term \u201cclock cycle\u201d refers to an interval of time during which the pipeline stages of a microprocessor perform their intended functions. At the end of a clock cycle, the resulting values are moved to the next pipeline stage.</p><p>Pipelining has several hazards associated with it. One particular hazard is stalling the pipeline due to branch instructions. When a branch instruction propagates through the pipeline, it is difficult to determine which instructions after the branch should be processed until the results of the branch instruction are know. For example, if the branch instruction is \u201ctaken\u201d, then the next instruction to be executed after the branch may be located at a particular address that is offset from the branch instruction's address. In contrast, if the branch instruction is \u201cnot taken\u201d, then the next instruction to be executed may be located at the address immediately following the branch instruction. As a result, the initial stages of the pipeline may be unable to determine which instructions should begin execution in the pipeline following the branch instruction. Thus, the pipeline may stall awaiting the results of the branch instruction.</p><p>In order to prevent the instruction pipeline from stalling, microprocessor designers may implement branch prediction schemes to provide the initial pipeline stages with a predicted result for each branch instruction. The initial stages of the pipeline speculatively execute instructions along the predicted path until the branch instruction executes and one of the following occurs: (1) the prediction is found to correct, in which case the instructions continue to execute and are no longer speculative, or (2) the prediction is found to be incorrect, in which case all pipeline stages executing instructions after the branch are flushed and the pipeline starts anew using the correct path.</p><p>While parallel execution and branch prediction improve overall instruction throughput for a microprocessor at a given clock cycle, process improvements have led to dramatically increased operating frequencies that have further increased the number of instructions that a microprocessor may execute in a fixed period of time. These advancements have placed increasing importance upon a microprocessor's ability to decode instructions. Instruction decoding typically refers to identifying the different fields within the instruction (e.g., the opcode field and any prefixes or operands) and then expanding the instruction into an internal format so that the microprocessor's functional units may easily execute the instruction.</p><p>While RISC (Reduced Instruction Set Computer) microprocessors have been implemented to simplify instruction decoding, microprocessors capable of executing older variable-length instruction sets such as the x86 instruction set have remained commercially important due to the vast amount of software available for the older instruction sets. Furthermore, operating frequencies have climbed so quickly that even RISC microprocessors may eventually need faster methods for decoding instructions.</p><p>One proposed method for quickly decoding large numbers of instructions involves using a number of parallel decoders. However, current implementations using parallel decoders have been limited in their throughput because of the \u201cin-order\u201d (i.e., in program order) nature of decoding. Most programs rely upon their instructions being executed in a particular order. This order is referred to as \u201cprogram order\u201d. As previously noted, most modern microprocessors support out-of-order execution. However, these microprocessors must ensure that the instructions that are executed out-of-order do not aversely affect the intended operation of the program. This is accomplished through \u201cdependency checking\u201d. Dependency checking refers to determining which instructions rely upon other instructions' prior execution to finction properly. Thus, dependency checking ensures that the only instructions that are executed out of order are those that will not adversely affect the desired operation of the program. For typical dependency checking hardware to operate correctly, it relies upon receiving decoded instructions that are in-order. Thus, typical instruction decoders receive and decode instructions in program order so that the program order will be preserved for the dependency checking hardware (typically the next stage in the instruction processing pipeline).</p><p>This in-order configuration affects decoder throughput by causing some decoders to stall in certain instances. For example, when a new set of instruction bytes is received by the decoders, each decoder must wait to output its results (i.e., its decoded instructions) until all decoders before it have output their results. If not, the following pipeline stages may receive the decoded instructions out-of-order.</p><p>For these reasons, a method and apparatus for quickly decoding a large number of instructions is desirable. In particular, a method capable of quickly decoding large numbers of instructions out of order is desirable.</p><h4>SUMMARY OF THE INVENTION</h4><p>The problems outlined above are in large part solved by a microprocessor capable of decoding instructions out-of-order while still performing dependency checking in program order. Broadly speaking, in one embodiment the microprocessor comprises an instruction cache, two decode units, a reorder queue, and dependency checking logic. The instruction cache may be configured to output sequential groups of instruction bytes called cache lines. The cache line is divided into portions, which are routed to respective decode units, which decode the individual instructions contained therein. The decode units operate independently of each other and may decode the cache line portions out of program order. The decode units output the decoded instructions according to their relative position within the cache line portions. The decoded instructions are received by the reorder queue, which comprises a plurality of storage lines. Each storage line in turn comprises a fixed number of instruction storage locations. The number of storage locations may equal the maximum possible number of instructions within each cache line portion. The reorder queue allocates one storage line for each decoded cache line, and the decoded instructions are stored according to their relative cache line portion positions. The decoded instructions may be read out of the reorder queue in program order, thereby enabling the dependency checking logic to perform dependency checking in program order.</p><p>In another embodiment, the microprocessor may further comprise a third decoder and routing logic. The routing logic may be configured to receive cache lines as they are output from the instruction cache and then route portions of them to one of the three decoders. The third decoder may be configured to operate as a split instruction decoder, and the routing logic may be configured to route instructions that extend across cache line portion boundaries to the third decoder.</p><p>A method for decoding instructions out-of-order and then reordering them for dependency checking is also disclosed. In one embodiment, the method may comprise fetching a plurality of instruction bytes and then decoding the instructions contained within the plurality of instruction bytes out of program order. The decoded instructions are then reordered to match program order and dependency checking is performed. The instructions may then be issued to reservation stations for eventual out of order execution.</p><p>In another embodiment, the method may further comprise allocating a first line of a reorder queue for potential instructions within the first half of the plurality of instruction bytes. A second line is allocated for storage locations within the for potential instructions within the second half of the plurality of instruction bytes. The first and second halves of the plurality of instructions are decoded independently, and the resulting decoded instructions are stored in a reorder queue. The instructions are read out of the reorder queue in program order, thereby allowing dependency checking to be performed in order.</p><p>A computer system capable of out-of-order instruction decoding is also contemplated. In one embodiment, the computer system may comprise one or two microprocessors are described above. In the case of two microprocessors they may be coupled to each other via a CPU bus. External devices, e.g., a modem, may also be coupled to the CPU bus via a bus bridge.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Other objects and advantages of the invention will become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p><p>FIG. 1 is a block diagram of one embodiment of a microprocessor that is configured to decode instructions out of order.</p><p>FIG. 2A is a block diagram showing more details of one embodiment of the decode units and reorder queue from the microprocessor of FIG. <b>1</b>.</p><p>FIG. 2B is a block diagram showing one possible method for operating the decode units and reorder queue from FIG. <b>2</b>A.</p><p>FIG. 3 is a block diagram showing more detail of one embodiment of the reorder queue from FIG. <b>2</b>A.</p><p>FIG. 4 is a block diagram showing another embodiment of the microprocessor from FIG. <b>1</b>.</p><p>FIG. 5 is a block diagram illustrating one embodiment of a padding scheme for instructions.</p><p>FIG. 6 is a block diagram of one embodiment of a computer system configured to use the microprocessor from FIG. <b>1</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><p>While the present invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><h4>DETAILED DESCRIPTION OF SEVERAL EMBODIMENTS</h4><p>Turning now to FIG. 1, a block diagram of one embodiment of a microprocessor <b>10</b> that is configured to decode instructions out of order is shown. In this embodiment, microprocessor <b>10</b> includes a prefetch/predecode unit <b>12</b> and a branch prediction unit <b>14</b> coupled to an instruction cache <b>16</b>. Decode units <b>20</b>A-B are coupled between instruction cache <b>16</b> and a reorder queue <b>22</b>. A microcode read-only memory (MROM) unit <b>18</b> is also coupled to each decode unit <b>20</b>A-B. Reorder queue <b>22</b> is coupled to a reorder buffer <b>26</b> by a multiplexer <b>24</b>. Reorder buffer <b>26</b> is coupled to a register/future file <b>28</b>, a number of reservations stations <b>30</b>A-C, and a load/store unit <b>34</b>. Reservations stations <b>30</b>A-C are coupled to a corresponding number of functional units <b>32</b>A-C, and load/store unit <b>34</b> is coupled to a data cache <b>36</b>. Finally, a result bus <b>38</b> couples functional units <b>32</b>A-C and data cache <b>36</b> to reorder buffer <b>26</b>, register/future file <b>28</b>, load/store unit <b>34</b>, and reservations stations <b>30</b>A-C.</p><p>Generally speaking, instruction cache <b>16</b> is a high speed cache memory provided to temporarily store instructions prior to their dispatch to decode units <b>20</b>A-B. In one embodiment, instruction cache <b>16</b> is configured to cache up to 32 kilobytes of instruction code organized in cache lines of 16 bytes each (where each byte consists of 8 bits). During operation, instruction bytes are provided to instruction cache <b>16</b> by prefetching bytes from a main memory (not shown) through prefetch/predecode unit <b>12</b>. It is noted that instruction cache <b>16</b> could be implemented in a set-associative, fully-associative, or direct-mapped configuration.</p><p>Prefetch/predecode unit <b>12</b> prefetches instruction code from the main memory for storage within instruction cache <b>16</b>. In one embodiment, prefetch/predecode unit <b>12</b> is configured to burst 64-bit wide code from the main memory into instruction cache <b>16</b>. It is understood that a variety of specific code prefetching techniques and algorithms may be employed by prefetch/predecode unit <b>12</b>.</p><p>In one embodiment, as prefetch/predecode unit <b>12</b> fetches instructions from the main memory, it generates the following three predecode bits for each instruction byte: a start bit, an end bit, and a functional bit. Asserted start bits mark the first byte of each instruction. Asserted end bits mark the last byte of each instruction. Asserted functional bits mark the opcode bytes of each instruction. The predecode bits form tags indicative of the boundaries of each instruction. The predecode tags may also convey additional information such as whether a given instruction may be decoded directly by decode units <b>20</b>A-B or whether the instruction is to be executed by invoking a microcode procedure stored within MROM unit <b>18</b>, as described in further detail below. The predecode tags may be stored along with the instruction bytes in instruction cache <b>16</b>.</p><p>In one embodiment, when instruction cache <b>16</b> receives a fetch address, it outputs a 16-byte cache line to decode units <b>20</b>A-B. Decode unit <b>20</b>A receives the first eight instruction bytes and decodes them into instructions. Since the x86 instruction set has instructions varying in length from one byte to sixteen bytes, an eight byte sequence of instruction bytes may have up to eight instructions encoded within it. Thus, as shown in the figure, decode unit <b>20</b>A may output up to eight decoded instructions. Similarly, decode unit <b>20</b>B, which is configured to receive the second eight instruction bytes and decode them, may also output up to eight decoded instructions. Decode units <b>20</b>A-B operate independently of each other. For example, if decode unit <b>20</b>B completes decoding a set of eight instruction bytes before decode unit <b>20</b>A completes decoding its set of eight instruction bytes, then decode unit <b>20</b>B may accept a new set of eight instruction bytes from instruction cache <b>16</b>. Decode unit <b>20</b>B may begin decoding these instructions while decode unit <b>20</b>A is still busy decoding its initial set of eight instruction bytes.</p><p>To improve the flow of data from instruction cache <b>16</b> to decode units <b>20</b>A-B, each decode unit may have a FIFO (first-in first-out) memory buffer at its input to receive and store the eight byte sequences until the respective decode unit is ready to begin decoding them. For example, each decode unit may have a 24-byte FIFO configured to store three 8-byte sequences. Note that decode units <b>20</b>A-B are drawn as single boxes for exemplary purposes only. Each decode unit <b>20</b>A-B may in fact comprises a number of individual decoders each configured to decode a single instruction. Furthermore, in some embodiments decode units <b>20</b>A-B may be configured to use a variable number of clock cycles to decode each 8-byte block of instructions, depending upon the complexity of the instructions contained therein. Note that 16-byte cache lines and 8-byte sequences are used for exemplary purposes only and that other configurations are possible and contemplated (e.g., 32-byte cache lines, with four independent decoders each receiving 8-byte sequences).</p><p>Advantageously, by configuring decode units <b>20</b>A-B to decode independently and out-of-order, the chance of either decode unit <b>20</b>A-B stalling while waiting for the other to complete its decoding may be reduced. Furthermore, an out-of-order decoding structure may allow multiple decoders (e.g., two or more) to be more effectively utilized.</p><p>However, while out-of-order decoding may improve the performance of decode units <b>20</b>A-B, decoding instructions out of order may cause difficulties further down the instruction processing pipeline when dependency checking is performed. Because most programs rely upon instructions executing in a particular order (i.e., \u201cprogram order\u201d), dependency checking is typically performed to determine which instructions may execute in an out-or-order fashion. Out-of-order execution may be used in conjunction with speculative execution of instructions to increase overall instruction throughout and performance.</p><p>In the embodiment illustrated in the figure, microprocessor <b>10</b> is configured to decode instructions out-of-order and then reorder them to allow in-order dependency checking. Reorder queue <b>22</b> is configured to perform this reordering. In one embodiment, reorder queue <b>22</b> comprises a plurality of storage lines, wherein each storage line comprises a fixed number of individual storage locations. Each storage location is configured to store a single decoded instruction. Accordingly, the size of each storage location will vary according to the particular implementation. For example, decoders <b>20</b>A-B may decode instructions to a fixed 16-byte width. The number of storage locations within each line equals the maximum possible number of instructions in each instruction byte sequence decoded by one of decode units <b>20</b>A-B. In the embodiment illustrated in FIG. 1, each line has eight storage locations. In addition to the eight storage locations, each line may further comprise additional storage locations for storing address information associated with the stored instructions.</p><p>Each clock cycle, multiplexer <b>24</b> is configured to select the oldest three instructions within reorder queue <b>22</b> for dispatch to reorder buffer <b>26</b>. The instructions are dispatched in program order, thereby allowing reorder buffer to perform dependency checking in-order. Multiplexer <b>24</b> is configured to ignore empty storage locations within each line of instructions. Empty storage locations may occur when the number of instructions within a decoded instruction sequence is less than the maximum possible number of instructions. Furthermore, multiplexer <b>24</b> may be configured to select instructions from more than one line in a particular clock cycle. For example, assuming a particular line is only storing two instructions, multiplexer may be configured to read an additional instruction from the following line in order to provide reorder buffer <b>26</b> with three instruction during that clock cycle. As previously noted, in one embodiment instructions are read and conveyed in program order to reorder buffer <b>26</b>.</p><p>Once an instruction is read from a storage location within reorder queue <b>22</b>, the storage location may be cleared or marked as empty, e.g., by setting or clearing a corresponding status bit. In one embodiment, once all storage locations within a particular line are empty, reorder queue <b>22</b> may be configured to shift the contents of each following line to fill in the empty line. In other embodiments reorder queue <b>22</b> may be configured as a line-oriented FIFO or a line-oriented circular buffer.</p><p>In the embodiment shown, reorder buffer <b>26</b> receives three decoded and reordered instructions from multiplexer <b>24</b> each clock cycle. The instructions are stored in the order that they are received. The lines each have room for three instructions. Each pending instruction is allocated a \u201creorder buffer tag\u201d that identifies it as it proceeds throughout the execution pipeline. In one embodiment, the tag identifies which line within reorder buffer <b>26</b> the instruction is stored in. The tag may further include offset information to identify whether the instruction is the first, second, or third instruction within the line. Note, in other embodiments reorder buffer <b>26</b> may be configured with a different number of instructions per line, e.g., four instructions.</p><p>Reorder buffer <b>26</b> operates to keep track of the original program sequence for register read and write operations, implements register renaming, allows for speculative instruction execution and branch misprediction recovery, and facilitates precise exceptions. Reorder buffer <b>26</b> performs dependency checking to determine when an instruction may be \u201cissued\u201d. Reorder buffer <b>26</b> issues instructions by conveying them to reservation stations <b>30</b>A-C or load/store unit <b>34</b>. Each reservation station acts as a buffer for the corresponding functional unit, storing instructions until they are executed. In one embodiment, reservations stations <b>30</b>A-C monitor result bus <b>38</b> for results that are referenced as operands by stored instructions. If such a result is detected, the reservation station may forward the result to the corresponding pending instruction. Similarly, data from load instructions executed by load/store unit <b>34</b> may also be monitored and forwarded. The issued instructions then wait in reservation stations <b>30</b>A-C or load/store unit <b>34</b> until the following criteria are met: (1) they have received all necessary operands and data, and (2) the corresponding functional unit's first execution pipeline stage is available to accept a new instruction. At that point, the instructions may enter functional units <b>32</b>A-C for execution. As long as their are no dependencies, reorder buffer <b>26</b> allows instructions to issue and execute out-of-order. Advantageously, out-of-order execution in combination with speculative execution tends to increase performance by preventing functional units <b>32</b>A-C from stalling. In the embodiment illustrated, instructions may be speculatively executed based upon branch prediction information stored in branch prediction unit <b>14</b>.</p><p>In one embodiment, each functional unit <b>30</b>A-C is configured to perform integer arithmetic operations of addition and subtraction, as well as shifts, rotates, logical operations, and branch operations. It is noted that a floating point unit (not shown) may also be employed to accommodate floating point operations.</p><p>Results produced by functional units <b>30</b>A-C are sent to reorder buffer <b>26</b> if a register value is being updated, and to the load/store unit <b>34</b> if the contents of a memory location is changed. As stated previously, results are also broadcast to reservation station units <b>26</b> where pending instructions may be waiting to receive their operand values from the results of previous instruction executions.</p><p>Register/future file <b>28</b> comprises two sets of registers. One set comprises the x86 architectural registers, including eight 32-bit real registers (i.e., typically referred to as EAX, EBX, ECX, EDX, EBP, ESI, EDI and ESP). The second set comprises registers for storing the most recent speculative set of values for each architectural register. This \u201cfuture file\u201d of registers provides a convenient place from which to forward speculative register values to pending instructions. If following decode of an instruction it is determined that reorder buffer <b>26</b> has a previous location or locations assigned to a register used as an operand in the instruction, then reorder buffer <b>26</b> forwards to the corresponding reservation station either: 1) the value in the most recently assigned location, or 2) a tag for the most recently assigned location if the value has not yet been produced by the functional unit that will eventually execute the previous instruction. If the reorder buffer has a location reserved for a given register, the operand value (or tag) is provided from reorder buffer <b>26</b> rather than from register file <b>28</b>. If there is no location reserved for a required register in reorder buffer <b>26</b>, the value is taken directly from register file <b>28</b>. If the operand corresponds to a memory location, the operand value is provided to the reservation station unit through load/store unit <b>22</b>.</p><p>The results of each executed instruction are stored in reorder buffer <b>26</b> until the instruction is \u201cretired\u201d. Retiring an instruction refers to copying the instruction's results to architectural register file <b>28</b> and thereby updating the microprocessor's non-speculative architectural state. As previously noted, reorder buffer tags follow each instruction through reservation stations <b>30</b>A-C and functional units <b>32</b>A-C. Thus, the results may be identified and attributed to the appropriate instruction within reorder buffer <b>26</b>. Once the results are received, reorder buffer <b>26</b> retires instruction in-order in a line-by-line fashion, waiting to retire a line of instructions until the following conditions are met: (1) the line is the oldest line of instructions stored within reorder buffer <b>26</b>, and (2) each instruction in the line has completed execution without an exception or branch misprediction. Note that other variations of reorder buffer <b>26</b> are also possible. For example, in another embodiment reorder buffer <b>26</b> may individually retire instructions as opposed to retiring them in a line-by-line manner. Reorder buffer <b>26</b> may be implemented in a first-in-first-out configuration wherein speculative results move to the \u201cbottom\u201d of the buffer as they are validated and written to register file <b>28</b>, thus making room for new entries at the \u201ctop\u201d of the buffer.</p><p>More details regarding suitable reorder buffer implementations may be found within the publication \u201cSuperscalar Microprocessor Design\u201d by Mike Johnson, Prentice-Hall, Englewood Cliffs, N.J., 1991, which is incorporated herein by reference in its entirety.</p><p>In the event of a branch misprediction, reorder buffer <b>26</b>, reservation stations <b>30</b>A-C, and load/store unit <b>34</b> may be configured to flush all pending instructions occurring after the misprediction branch instruction in program order. Furthermore, the contents of the architectural register file within register/future file <b>28</b> are copied to the future file to replace any erroneous values created by the execution of instructions along the mispredicted branch path. Branch mispredictions may be detected by functional units <b>32</b>A-B, which forward the results of branch instructions to branch prediction unit <b>14</b>.</p><p>Generally speaking, load/store unit <b>34</b> provides an interface between functional units <b>32</b>A-C and data cache <b>36</b>. In one embodiment, load/store unit <b>34</b> is configured with a load/store buffer that has eight storage locations for storing data and address information from pending loads or stores. Load/store unit <b>34</b> also performs dependency checking for load instructions against pending store instructions to ensure that data coherency is maintained. Data cache <b>36</b> is a high speed cache memory provided to temporarily store data being transferred between load/store unit <b>34</b> and the main memory subsystem. In one embodiment, data cache <b>36</b> has a capacity of storing up to 32 kilobytes of data. It is understood that data cache <b>36</b> may be implemented in a variety of sizes and specific memory configurations, including set associative, fully associative, and direct mapped configurations.</p><p>Out of Order Decoding</p><p>Turning now to FIG. 2A, more details regarding one embodiment of decode units <b>20</b>A-B and reorder queue <b>22</b> are shown. In this embodiment, branch predication array <b>56</b> is coupled to instruction cache <b>16</b>, which is in turn coupled to decode units <b>20</b>A-B, and a split decode unit <b>50</b>A by routing logic <b>88</b>. Decode units <b>20</b>A-B and split decode unit <b>50</b>A are each coupled to reorder queue <b>22</b>. Decode units <b>20</b>A-B each comprise a FIFO memory <b>84</b>A-B, respectively, while split decode unit <b>50</b>A comprises a buffer <b>86</b>A.</p><p>Branch prediction array <b>56</b> stores branch prediction information. For example, branch prediction array <b>56</b> may store predicted branch target addresses and predictions as to whether a particular branch instruction will be taken or not taken. In one embodiment, branch prediction array <b>56</b> may be configured to mirror instruction cache <b>16</b>. For example, both instruction cache <b>16</b> and branch prediction array <b>56</b> may be configured to be 4-way set associative. In other embodiments, branch prediction array <b>56</b> may be direct mapped or fully associative.</p><p>Fetch addresses are conveyed to branch prediction array <b>56</b> and instruction cache <b>16</b> via a fetch address bus <b>70</b>. Upon receiving a fetch address, branch prediction array <b>56</b> performs address or tag comparisons to determine whether or not a branch prediction is stored that corresponds to the cache line being fetched from instruction cache <b>16</b>. If a prediction is stored, branch prediction array <b>56</b> routes the address of the predicted next cache line to instruction cache <b>16</b>.</p><p>In one embodiment, instruction cache <b>16</b> has two read ports <b>82</b>A and <b>82</b>B. This allows instruction cache <b>16</b> to output cache lines corresponding to the original fetch address and the predicted next cache line address concurrently. In another embodiment, instruction cache <b>16</b> may have three read ports and branch prediction array may output the addresses of the next two predicted cache lines simultaneously.</p><p>When instruction cache <b>16</b> outputs a cache line <b>68</b>A, it is accompanied by predecode information and a decode reorder queue tag <b>66</b>A. Decode reorder queue tag <b>66</b>A indicates which storage location in reorder queue <b>22</b> the cache line will be stored in after it is decoded. For each cache line (or portion thereof, depending upon the implementation), reorder queue <b>22</b> is configured to allocate a storage line.</p><p>Routing logic <b>88</b> is configured to receive the cache lines and route them to decode units <b>20</b>A-B and split decode unit <b>50</b>A according to the accompanying predecode information. As previously described, the predecode information associated with each instruction cache line indicates the first and last bytes of each instruction within the cache line. In one embodiment, routing logic <b>88</b> is configured to route all complete instructions to decode unit <b>20</b>A. All instruction bytes that are part of incomplete instructions (e.g., instructions extending across cache line boundaries) are routed to split decode unit <b>50</b>A.</p><p>One way to accomplish this sorting is to utilize the predecode bits accompanying each cache line. For example, routing logic <b>88</b> may route all instruction bytes before the first start bit (i.e., before the first start byte) and after the final end bit (i.e., after the last byte of the last full instruction within the cache line) to split decode unit <b>50</b>A. Thus, routing logic <b>88</b> is configured to route complete instructions to decode unit <b>20</b>A and partial instructions to split decode unit <b>50</b>A.</p><p>Routing logic <b>88</b> may be configured to perform the same routing on cache lines received from the second read port <b>82</b>B of instruction cache <b>16</b>. Thus, complete instructions are routed to decode unit <b>20</b>B, while partial or incomplete instruction bytes are routed to split decode unit <b>50</b>A. In addition, routing logic <b>88</b> may be configured to route copies of the decode reorder queue tags <b>66</b>A and <b>66</b>B with each instruction or partial instruction to decode units <b>20</b>A-B and split decode unit <b>50</b>A.</p><p>Decoder units <b>20</b>A-B and split decode unit <b>50</b>A are configured to decode the instruction they receive independently and without regard to what order the instructions occur (relative to other instructions being decoded by the other decode units). Thus, decode units <b>20</b>A-B and split decode unit <b>50</b>A are each able to decode instructions out of order.</p><p>Within split decode unit <b>50</b>A, buffer <b>86</b>A is configured to receive and store partial instructions with their corresponding decode reorder queue tags. Split decoder <b>50</b>A is also configured to receive the predicted cache line addresses from branch prediction array <b>56</b>. Using this information, split decode unit <b>50</b>A is configured to reassemble the partial instructions stored within buffer <b>86</b>A.</p><p>As previously noted, each cache line output by instruction cache <b>16</b> is allocated a storage line (e.g., storage line <b>80</b>A or <b>80</b>B) within reorder queue <b>22</b>. Each storage line comprises storage for sixteen decoded instructions. Since the maximum number of instructions within a single cache line is sixteen (assuming a minimum instruction length of one byte), all instructions decoded within a cache line may be stored within a single storage line. If the cache line has less than sixteen instructions, the remaining storage locations within the storage line are designated empty or are padded with NOP (no operation) instructions. In some embodiments, storage lines within reorder queue <b>22</b> may each have an additional storage location for storing the corresponding cache line's address information. The address information may be used by reorder buffer <b>26</b> and functional units <b>32</b>A-C to track the EIP (instruction pointer) and resolve branch predictions.</p><p>In one embodiment, each decode unit <b>20</b>A-B has outputs equaling the maximum possible number of instructions that may be decoded from a particular cache line (e.g., sixteen outputs). Reorder queue <b>22</b> stores each instruction in the storage line corresponding to the instruction's decode reorder queue tag and the storage location corresponding to the output upon which the instruction is conveyed. For example, the fifth instruction decoded from cache line <b>68</b>A will be conveyed to reorder queue on line I<sub>A4 </sub>and will be stored in location I<sub>4 </sub>within storage line <b>80</b>A. Assuming cache line <b>68</b>A comprises five complete instructions and a sixth partial instruction beginning after the last end bit, locations I<sub>0</sub>-I<sub>4 </sub>in storage line <b>80</b>A will be filled with instructions from decode unit <b>20</b>A. Locations I<sub>5</sub>-I<sub>14 </sub>are left empty or filled with no-ops. Location I<sub>15</sub>, however, is filled with the decoded partial instruction by split decode unit <b>50</b>A once it has received the corresponding portion of the partial instruction from cache line <b>66</b>B.</p><p>Exemplary Operation of One Embodiment</p><p>Turning now to FIG. 2B, details of one possible method for operating the microprocessor from FIG. 2A are shown. As the figure illustrates, a fetch address A is conveyed along fetch address bus <b>70</b> to branch prediction array <b>56</b>, instruction cache <b>16</b>, and split decode unit <b>50</b>A. In response, instruction cache <b>16</b> outputs the cache line having an address tag matching A. This cache line is represented within routing logic <b>88</b> as block <b>68</b>A. Reorder queue <b>22</b> allocates the next available storage line <b>80</b>A to the cache line and provides a corresponding decode reorder queue tag \u03b1 (see block <b>66</b>A) that points to the allocated storage line.</p><p>Routing logic <b>88</b> routes all whole instructions within the cache line (i.e., instructions represented by \u201cbbb\u201d and \u201cccc\u201d) to decode unit <b>20</b>A along with decode reorder queue tag \u03b1. FIFO <b>84</b>A within decode unit <b>20</b>A receives and stores the whole instructions until decoder <b>20</b>A is ready to decode them. As the figure illustrates, decode unit <b>20</b>A decodes instruction \u201cbbb\u201d as the first instruction in the cache line and outputs it though output I<sub>A0 </sub>to reorder queue <b>22</b>. Similarly, decode unit <b>20</b>A decodes instruction \u201cccc\u201d as the second instruction in the cache line and outputs it though output I<sub>A1 </sub>to reorder queue <b>22</b>. All other instruction outputs from decode unit <b>20</b>A (i.e., outputs I<sub>A2</sub>-I<sub>A15</sub>) are zeroed out (represented by \u201c\u2212\u201d).</p><p>Reorder queue <b>22</b> receives the decoded instructions (represented by \u201cb\u201d and \u201cc\u201d in the figure) and stores them within the first two storage locations within storage line <b>80</b>A, as designated by the decode reorder queue tag \u03b1 accompanying the instructions from decode unit <b>20</b>A.</p><p>In parallel with the operations described above, branch prediction array <b>56</b> outputs the address tag for the next predicted cache line. In the example shown, the next predicted cache line has an address tag B. This tag is conveyed to the second read port of instruction cache <b>16</b> via bus <b>72</b>. In one embodiment, reorder queue <b>22</b> monitors all address tags conveyed to both ports of instruction cache <b>16</b> and allocates storage lines (e.g., storage line <b>80</b>B) and decode reorder queue tags accordingly. Split decode unit <b>50</b>A may monitor the decode reorder queue tags allocated by reorder queue <b>22</b>. This may enable split decode unit <b>50</b>A to track the order in which the corresponding cache lines appear.</p><p>Instruction cache <b>16</b> responds to the address tag by outputting the corresponding cache line having an address tag B to routing logic <b>88</b>. The cache line (represented as block <b>68</b>B) is accompanied by is corresponding decode reorder queue tag \u03b2 (represented by block <b>66</b>B) from reorder queue <b>22</b>.</p><p>Routing logic <b>88</b> performs a similar function upon cache line <b>68</b>B as with cache line <b>68</b>A. The complete or whole instructions (represented as \u201ce\u201d, \u201cffff\u201d, \u201cg\u201d, and \u201chh\u201d) are routed to decode unit <b>20</b>B, along with decode reorder queue tag \u03b2. Partial instructions (in this example, \u201cdd\u201d and \u201ci\u201d) are routed to split decode unit <b>50</b>A, also with decode reorder queue tag \u03b2.</p><p>Decode unit <b>20</b>B decodes the whole instructions and outputs them one per instruction output. As with decode unit <b>20</b>A, any outputs not used may be zeroed out or may convey no-op instructions. When these decoded instructions are received by reorder queue <b>22</b>, they are stored in storage locations (according to their output order) within storage line <b>80</b>B, which corresponds to decode reorder queue tag \u03b2.</p><p>While decode units <b>20</b>A and <b>20</b>B are independently decoding their instructions, split decode unit <b>50</b>A reassembles the instruction fragments it receives according to the decode reorder queue tag order it has established from the tags it has received from reorder queue <b>22</b> and branch prediction array <b>72</b>. Once reassembled, the instructions are decoded and output to reorder queue <b>22</b> according to their decode reorder queue tags. Split decode unit <b>50</b>A may be configured to output only the decode reorder queue tag corresponding to the first part of the instruction, thereby assuring the instruction will be stored in the final storage location within the storage line allocated to cache line from which the first instruction portion came. For example, instruction \u201cdddd\u201d is split between cache line <b>68</b>A and <b>68</b>B. When split decode unit <b>50</b>A reassembles the instruction, it will be output with decode reorder queue tag \u03b1, thereby assuring that it will be stored in storage line <b>80</b>A. The output or outputs of split decode unit may automatically be assigned to the last instruction storage location in each storage line.</p><p>Advantageously, decode units <b>20</b>A-B and split decode unit <b>50</b>A are able to decode instructions out of order. For example, decode unit <b>20</b>B may complete decoding cache line <b>68</b>B and begin decoding a second cache line before decoder <b>20</b>A completes decoding cache line <b>68</b>A. Furthermore, reorder queue <b>22</b> may output instructions in program order by selecting storage lines according to their address tags and by simply discarding or skipping over storage locations that are empty or have no-op instructions within them. Thus, out-of-order decoding may be possible for greater performance while still providing for in-order dependency checking (using the instructions as they are output from reorder queue <b>22</b>).</p><p>Details of Reorder Queue</p><p>Turning now to FIG. 3, more detail of one embodiment of reorder queue <b>22</b> is shown. In this embodiment, all storage lines are similarly configured. For example, storage line <b>80</b>A comprises an address tag field <b>90</b>A, a line status field <b>92</b>A, and sixteen instruction storage locations (e.g., storage location <b>94</b>A) each having a full/empty bit <b>96</b>A. Address tag fields <b>90</b>A-B store address information for instruction within the storage line. Line status fields <b>92</b>A-B indicate whether the storage line is in use or available to be allocated to a new cache line. As previously noted, each instruction storage location <b>94</b>A-B is configured to store a decoded instruction. The exact format of the decoded instruction may vary across different implementations. In one embodiment, the op-code and each operand (register or memory) have defined fixed-length fields. Full/empty bits <b>96</b>A-B indicate whether the particular instruction storage location within the storage line is taken.</p><p>Once all the instructions within storage line <b>90</b>A have been output, reorder queue <b>22</b> may be configured to clear the line's status bit <b>92</b>A and all full/empty bits <b>96</b>A in the line. Note, the fields described above for reorder queue <b>22</b> are merely exemplary and may be modified. For example, line status field <b>92</b>A may be incorporated into address tag field <b>90</b>A. To indicate that a particular line is available, reorder queue <b>22</b> would then set address tag field <b>90</b>A to a predetermined value (e.g., all zeros), wherein the predetermined value corresponds to an address tag that reorder queue <b>22</b> will not normally receive from instruction cache <b>16</b> or branch prediction array <b>56</b>.</p><p>Turning now to FIG. 4, another embodiment of microprocessor <b>10</b> is shown. In this embodiment, predecode unit <b>12</b> is configured to pad instructions as they are stored in instruction cache <b>16</b> so that no instructions are split across cache line boundaries. In this case, routing logic <b>88</b> may be greatly simplified and split decode unit <b>50</b>A may be eliminated. Alternatively, this embodiment of microprocessor <b>10</b> may be configured to execute fixed-length instructions (e.g., RISC instructions). By selecting cache line lengths that correspond to natural instruction boundaries, split decode unit <b>50</b>A may be omitted. Decode units <b>20</b>A-B and reorder queue <b>22</b> may basically operate in the same manner as previously discussed. Note however, that the number of instruction storage locations within each storage line in reorder queue <b>22</b> may vary across different implementations according to the maximum possible number of instruction within a single cache line.</p><p>In either embodiment, i.e., the embodiments described in either FIG. 3 or FIG. 4, the number of decode units <b>20</b>A-<b>20</b>B may be increased. For example, in one embodiment microprocessor <b>10</b> may comprise three \u201cwhole instruction\u201d decode units, each configured to independently and in parallel decode instructions from cache lines and route the corresponding decoded instructions to reorder queue <b>22</b>.</p><p>Turning now to FIG. 5, an example of one padding scheme for instructions stored in instruction cache <b>16</b> is shown. The example assumes a sixteen byte cache line with variable length instructions. Instruction C <b>108</b> may be split between the cache lines, but instead NULL instructions <b>104</b> and <b>106</b> are padded to the cache line. NULL instructions are one byte instructions similar to NOP instructions except that they do not cause the program counter (PC) to advance. Note, in some embodiments NULL instructions may be used in place of NOP instructions within reorder queue <b>22</b> and decode units <b>20</b>A-B. Further note, this padding scheme is optimized for variable-length instructions. If microprocessor <b>10</b> is configured to execute fixed-length RISC instructions, padding may be omitted if the cache line length is adjusted so that instructions do not extend across cache line boundaries.</p><p>Exemplary Computer System</p><p>Turning now to FIG. 6, a block diagram of one embodiment of a computer system <b>200</b> configured to use microprocessor <b>10</b> is disclosed. Computer system <b>200</b> is coupled to a variety of system components through a bus bridge <b>202</b> as shown. Other embodiments are possible and contemplated. In the depicted system, a main memory <b>204</b> is coupled to bus bridge <b>202</b> through a memory bus <b>206</b>, and a graphics controller <b>208</b> is coupled to bus bridge <b>202</b> through an AGP bus <b>210</b>. Finally, a plurality of PCI devices <b>212</b>A-<b>212</b>B are coupled to bus bridge <b>202</b> through a PCI bus <b>214</b>. A secondary bus bridge <b>216</b> may further be provided to accommodate an electrical interface to one or more EISA or ISA devices <b>218</b> through an EISA/ISA bus <b>220</b>. Microprocessor <b>10</b> is coupled to bus bridge <b>202</b> through a CPU bus <b>224</b>.</p><p>Bus bridge <b>202</b> provides an interface between microprocessor <b>10</b>, main memory <b>204</b>, graphics controller <b>208</b>, and devices attached to PCI bus <b>214</b>. When an operation is received from one of the devices connected to bus bridge <b>202</b>, bus bridge <b>202</b> identifies the target of the operation (e.g. a particular device or, in the case of PCI bus <b>214</b>, that the target is on PCI bus <b>214</b>). Bus bridge <b>202</b> routes the operation to the targeted device. Bus bridge <b>202</b> generally translates an operation from the protocol used by the source device or bus to the protocol used by the target device or bus.</p><p>In addition to providing an interface to an ISA/EISA bus for PCI bus <b>214</b>, secondary bus bridge <b>216</b> may further incorporate additional functionality, as desired. For example, in one embodiment, secondary bus bridge <b>216</b> includes a master PCI arbiter (not shown) for arbitrating ownership of PCI bus <b>214</b>. An input/output controller (not shown), either external from or integrated with secondary bus bridge <b>216</b>, may also be included within computer system <b>200</b> to provide operational support for a keyboard and mouse <b>222</b> and for various serial and parallel ports (e.g., a modem port for connecting a modem), as desired. An external cache unit (not shown) may further be coupled to CPU bus <b>224</b> between microprocessor <b>10</b> and bus bridge <b>202</b> in other embodiments. Alternatively, the external cache may be coupled to bus bridge <b>202</b> and cache control logic for the external cache may be integrated into bus bridge <b>202</b>.</p><p>Main memory <b>204</b> is a memory in which application programs are stored and from which microprocessor <b>10</b> primarily executes. A suitable main memory <b>204</b> comprises DRAM (Dynamic Random Access Memory), and preferably a plurality of banks of SDRAM (Synchronous DRAM).</p><p>PCI devices <b>212</b>A-<b>212</b>B are illustrative of a variety of peripheral devices such as, for example, network interface cards, video accelerators, audio cards, hard or floppy disk drives or drive controllers, SCSI (Small Computer Systems Interface) adapters and telephony cards. Similarly, ISA device <b>218</b> is illustrative of various types of peripheral devices, such as a modem, a sound card, and a variety of data acquisition cards such as GPIB or field bus interface cards.</p><p>Graphics controller <b>208</b> is provided to control the rendering of text and images on a display <b>226</b>. Graphics controller <b>208</b> may embody a typical graphics accelerator generally known in the art to render three-dimensional data structures which can be effectively shifted into and from main memory <b>204</b>. Graphics controller <b>208</b> may therefore be a master of AGP bus <b>210</b> in that it can request and receive access to a target interface within bus bridge <b>202</b> to thereby obtain access to main memory <b>204</b>. A dedicated graphics bus accommodates rapid retrieval of data from main memory <b>204</b>. For certain operations, graphics controller <b>208</b> may further be configured to generate PCI protocol transactions on AGP bus <b>210</b>. The AGP interface of bus bridge <b>202</b> may thus include functionality to support both AGP protocol transactions as well as PCI protocol target and initiator transactions. Display <b>226</b> is any electronic display upon which an image or text can be presented. A suitable display <b>226</b> includes a cathode ray tube (\u201cCRT\u201d), a liquid crystal display (\u201cLCD\u201d), etc.</p><p>It is noted that, while the AGP, PCI, and ISA or EISA buses have been used as examples in the above description, any bus architectures may be substituted as desired. It is further noted that computer system <b>200</b> may be a multiprocessing computer system including additional microprocessors (e.g. microprocessor <b>10</b><i>a </i>shown as an optional component of computer system <b>200</b>). Microprocessor <b>10</b><i>a </i>may be similar to microprocessor <b>10</b>. More particularly, microprocessor <b>10</b><i>a </i>may be an identical copy of microprocessor <b>10</b>. Microprocessor <b>10</b><i>a </i>may share CPU bus <b>224</b> with microprocessor <b>10</b> (as shown in FIG. 5) or may be connected to bus bridge <b>202</b> via an independent bus.</p><?DETDESC description=\"Detailed Description\" end=\"tail\"?></description>"}], "inventors": [{"first_name": "James S.", "last_name": "Roberts", "name": ""}], "assignees": [{"first_name": "", "last_name": "", "name": "ADVANCED MICRO DEVICES, INC."}, {"first_name": "", "last_name": "GLOBALFOUNDRIES U.S. INC.", "name": ""}, {"first_name": "", "last_name": "GLOBALFOUNDRIES INC.", "name": ""}, {"first_name": "", "last_name": "ADVANCED MICRO DEVICES, INC.", "name": ""}], "ipc_classes": [{"primary": true, "label": "G06F  15/00"}], "locarno_classes": [], "ipcr_classes": [{"label": "G06F   9/30        20060101A I20051008RMEP"}, {"label": "G06F   9/38        20060101A I20051008RMEP"}], "national_classes": [{"primary": true, "label": "712212"}, {"primary": false, "label": "712E09072"}, {"primary": false, "label": "712E09055"}, {"primary": false, "label": "712216"}, {"primary": false, "label": "712E09029"}], "ecla_classes": [{"label": "G06F   9/38C2"}, {"label": "G06F   9/38C4"}, {"label": "G06F   9/38B"}, {"label": "G06F   9/30T2A"}], "cpc_classes": [{"label": "G06F   9/382"}, {"label": "G06F   9/3802"}, {"label": "G06F   9/3802"}, {"label": "G06F   9/3822"}, {"label": "G06F   9/30152"}, {"label": "G06F   9/382"}, {"label": "G06F   9/3822"}, {"label": "G06F   9/30152"}], "f_term_classes": [], "legal_status": "Expired - Lifetime", "priority_date": "1998-09-21", "application_date": "1998-09-21", "family_members": [{"ucid": "US-6192465-B1", "titles": [{"lang": "EN", "text": "Using multiple decoders and a reorder queue to decode instructions out of order"}]}]}