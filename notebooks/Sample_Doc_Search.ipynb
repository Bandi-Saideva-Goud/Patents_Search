{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "a7J-69Q9UGTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "6VoFxXIAUITI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ZlNhlwULoqHB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGQ1A3Gymi3Z"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import zipfile\n",
        "from pprint import pprint\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzipping the zipped file"
      ],
      "metadata": {
        "id": "qWOXuglYo7mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('/content/patent_jsons_ML Assignment.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('patents')"
      ],
      "metadata": {
        "id": "PR02gbKho7Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Sample Data"
      ],
      "metadata": {
        "id": "Rabm_T4houHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Opening JSON file\n",
        "f = open('/content/patents/patent_jsons/JP-H09311786-A.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "\n",
        "(data.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5-owE9WotGG",
        "outputId": "e1c72faf-4d9c-49ba-c5dc-a79c483af729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['patent_number', 'publication_id', 'family_id', 'publication_date', 'titles', 'abstracts', 'claims', 'descriptions', 'inventors', 'assignees', 'ipc_classes', 'locarno_classes', 'ipcr_classes', 'national_classes', 'ecla_classes', 'cpc_classes', 'f_term_classes', 'legal_status', 'priority_date', 'application_date', 'family_members'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['abstracts']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS6omNaLb7nD",
        "outputId": "b943ca7b-325c-4f03-fbce-f7c1b335dde0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'lang': 'JA',\n",
              "  'paragraph_markup': '<abstract lang=\"JA\" load-source=\"patent-office\" mxw-id=\"PA58764623\"><p>(57)【要約】\\n【課題】  スーパスカラ方式におけるデータ・ハザード<br/>のよるパイプライン・ストールを削減し、処理速度の向<br/>上を実現することにある。\\n【解決手段】  隣接する２つの２オペランド命令が、１<br/>つの３オペランド命令と同等であることを検出する回路<br/>と、そうであれば２つの命令を１つの３オペランド命令<br/>に統合して後続の実行ステージに送出する回路を命令デ<br/>コーダに設ける。また隣接する２つの命令がデータフロ<br/>ーの関係にあるが１つの３オペランド命令には統合でき<br/>ないことを検出すると、先行命令のソースデータを後続<br/>命令のための演算器に送る回路を設ける。\\n【効果】  隣接命令間のデータフローにより従来であれ<br/>ば２クロックの時間を要していた２つの命令処理を１ク<br/>ロックで実行できる。したがって、全体としての実行ク<br/>ロック数を削減できる。\\n</p></abstract>'},\n",
              " {'lang': 'EN',\n",
              "  'paragraph_markup': '<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA114921630\" source=\"PAJ\"><p>PROBLEM TO BE SOLVED: To reduce a pipeline stall due to a data hazard of a superscalar system and to improve the processing speed by changing an instruction in 1st instruction format stored in an instruction memory into an instruction in 2nd instruction format. SOLUTION: The instruction is taken in a 1st stage from the instruction memory and the instruction taken in the 1st stage 101 is decoded in a 2nd stage 103. The decoded instruction is executed in a 3rd stage and when the execution result is written in a register in a 4th stage 107, the instruction in the 1st instruction format stored in the instruction memory is changed into the instruction in the 2nd instruction format and executed. Consequently, the pipeline stall due to the data hazard of the superscalar system can be reduced and the processing speed is improved.</p></abstract>'},\n",
              " {'lang': 'EN',\n",
              "  'paragraph_markup': '<abstract lang=\"EN\" load-source=\"google\" mxw-id=\"PA385658575\" source=\"translation\"><p>(57) [Abstract] [PROBLEMS] To reduce the pipeline stall due to data hazard in the superscalar method and to improve the processing speed. Two adjacent two-operand instructions are merged into one. <br/> The instruction decoder is provided with a circuit for detecting equality with three three-operand instructions and, if so, a circuit for integrating two instructions into one three-operand instruction and sending it to the subsequent execution stage. Further, when it is detected that two adjacent instructions have a data flow relationship but cannot be integrated into one 3-operand instruction, a circuit for sending the source data of the preceding instruction to the arithmetic unit for the succeeding instruction is provided. [Effect] With the data flow between adjacent instructions, it is possible to execute two instruction processing in one clock, which conventionally takes two clocks. Therefore, the number of execution clocks as a whole can be reduced. </p></abstract>'}]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Going through each key for better understanding\n",
        "abstract_data = data['abstracts'][1]['paragraph_markup'] # abstract where language is english"
      ],
      "metadata": {
        "id": "-npZcznLpX12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "gEocTAuBfiCk",
        "outputId": "1222f343-6992-40e8-9d73-6f1a9c1bf5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA114921630\" source=\"PAJ\"><p>PROBLEM TO BE SOLVED: To reduce a pipeline stall due to a data hazard of a superscalar system and to improve the processing speed by changing an instruction in 1st instruction format stored in an instruction memory into an instruction in 2nd instruction format. SOLUTION: The instruction is taken in a 1st stage from the instruction memory and the instruction taken in the 1st stage 101 is decoded in a 2nd stage 103. The decoded instruction is executed in a 3rd stage and when the execution result is written in a register in a 4th stage 107, the instruction in the 1st instruction format stored in the instruction memory is changed into the instruction in the 2nd instruction format and executed. Consequently, the pipeline stall due to the data hazard of the superscalar system can be reduced and the processing speed is improved.</p></abstract>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking whether each patent has english language abstract or not"
      ],
      "metadata": {
        "id": "BfzWMt1Cbzxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_patent_jsons(directory):\n",
        "    patents = []\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    # Iterate through all files in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".json\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "\n",
        "            # Read the content of each JSON file\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                data = json.load(file)\n",
        "                patents.append(data)\n",
        "                count += 1\n",
        "    print(f\"total patent json files: {count}\")\n",
        "\n",
        "    return patents\n",
        "\n",
        "patents = read_patent_jsons(\"/content/patents/patent_jsons\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3xFvGRZbzF_",
        "outputId": "eca5a00a-b1b9-4c42-e355-3a414316fd16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total patent json files: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# counting the no.of patents with english abstract\n",
        "\n",
        "english_abstract_patent_counts = 0\n",
        "for patent in patents:\n",
        "  if \"abstracts\" in list(patent.keys()):\n",
        "    for abstract in patent[\"abstracts\"]:\n",
        "      if abstract['lang'] == 'EN':\n",
        "        english_abstract_patent_counts += 1\n",
        "        break\n",
        "\n",
        "english_abstract_patent_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhRhHV6PcCa_",
        "outputId": "33cee5fa-8852-4b7c-e168-33b9ef301f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This proves we have english patent for every json patent"
      ],
      "metadata": {
        "id": "IuvpeDN3o5BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample searching using transformers"
      ],
      "metadata": {
        "id": "lzZRxWKLUYN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is a simple application for sentence embeddings: semantic search\n",
        "\n",
        "We have a corpus with various sentences. Then, for a given query sentence,\n",
        "we want to find the most similar sentence in this corpus.\n",
        "\n",
        "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
        "\"\"\"\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Corpus with example sentences\n",
        "corpus = [\n",
        "    \"\"\"<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA114921630\" source=\"PAJ\"><p>PROBLEM TO BE SOLVED: To reduce a pipeline stall due \"\"\",\n",
        "    \"A man is eating a piece of bread.\",\n",
        "    \"The girl is carrying a baby.\",\n",
        "    \"A man is riding a horse.\",\n",
        "]\n",
        "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "\n",
        "# Query sentences:\n",
        "queries = [\n",
        "    \"\"\"<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA114921630\" source=\"PAJ\"><p>PROBLEM TO BE SOLVED: To reduce a pipeline stall due to a data hazard of a superscalar system and to improve the processing speed by changing an instruction in 1st instruction format stored in an instruction memory into an instruction in 2nd instruction format. SOLUTION: The instruction is taken in a 1st stage from the instruction memory and the instruction taken in the 1st stage 101 is decoded in a 2nd stage 103. The decoded instruction is executed in a 3rd stage and when the execution result is written in a register in a 4th stage 107, the instruction in the 1st instruction format stored in the instruction memory is changed into the instruction in the 2nd instruction format and executed. Consequently, the pipeline stall due to the data hazard of the superscalar system can be reduced and the processing speed is improved.</p></abstract>\",\n",
        "    \"Someone in a gorilla costume is playing a set of drums.\",\n",
        "    \"A cheetah chases prey on across a field.\"\"\",\n",
        "]\n",
        "\n",
        "\n",
        "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
        "top_k = min(5, len(corpus))\n",
        "for query in queries:\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        print(corpus[idx][:200], \"(Score: {:.4f})\".format(score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCFlBallpsD-",
        "outputId": "406445bc-54dd-4961-eada-6c46c6931d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: <abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA114921630\" source=\"PAJ\"><p>PROBLEM TO BE SOLVED: To reduce a pipeline stall due to a data hazard of a superscalar system and to improve the processing speed by changing an instruction in 1st instruction format stored in an instruction memory into an instruction in 2nd instruction format. SOLUTION: The instruction is taken in a 1st stage from the instruction memory and the instruction taken in the 1st stage 101 is decoded in a 2nd stage 103. The decoded instruction is executed in a 3rd stage and when the execution result is written in a register in a 4th stage 107, the instruction in the 1st instruction format stored in the instruction memory is changed into the instruction in the 2nd instruction format and executed. Consequently, the pipeline stall due to the data hazard of the superscalar system can be reduced and the processing speed is improved.</p></abstract>\",\n",
            "    \"Someone in a gorilla costume is playing a set of drums.\",\n",
            "    \"A cheetah chases prey on across a field.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA114921630\" source=\"PAJ\"><p>PROBLEM TO BE SOLVED: To reduce a pipeline stall due  (Score: 0.4680)\n",
            "A man is riding a horse. (Score: 0.0274)\n",
            "A man is eating a piece of bread. (Score: 0.0117)\n",
            "The girl is carrying a baby. (Score: 0.0116)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5bkIaLiR1DG",
        "outputId": "5cbe6718-436f-49eb-93fb-24247d88cd46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: sentence-transformers\n",
            "Version: 2.5.1\n",
            "Summary: Multilingual text embeddings\n",
            "Home-page: https://www.SBERT.net\n",
            "Author: Nils Reimers\n",
            "Author-email: info@nils-reimers.de\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface-hub, numpy, Pillow, scikit-learn, scipy, torch, tqdm, transformers\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKuSQrtv52Oe",
        "outputId": "a7020ad7-2466-4528-83fc-7b6259635124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: torch\n",
            "Version: 2.1.0+cu121\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, sympy, triton, typing-extensions\n",
            "Required-by: fastai, sentence-transformers, torchaudio, torchdata, torchtext, torchvision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A0lo3Pu6dhi",
        "outputId": "1326397d-c9bf-4a66-f79f-1c3ab57dd96c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CdKbyGcb77di"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}